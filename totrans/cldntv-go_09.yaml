- en: Chapter 6\. It’s All About Dependability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most important property of a program is whether it accomplishes the intention
    of its user.^([1](ch06.xhtml#idm45983630648552))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: C.A.R. Hoare, Communications of the ACM (October 1969)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Professor Sir Charles Antony Richard (Tony) Hoare is a brilliant guy. He invented
    quicksort, authored Hoare Logic for reasoning about the correctness of computer
    programs, and created the formal language “communicating sequential processes”
    (CSP) that inspired Go’s beloved concurrency model. Oh, and he developed the structured
    programming paradigm^([2](ch06.xhtml#idm45983630641848)) that forms the foundation
    of all modern programming languages in common use today. He also invented the
    null reference. Please don’t hold that against him, though. He publicly apologized^([3](ch06.xhtml#idm45983630640824))
    for it in 2009, calling it his “billion-dollar mistake.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Tony Hoare literally invented programming as we know it. So when he says that
    the single most important property of a program is whether it accomplishes the
    intention of its user, you can take that on some authority. Think about this for
    a second: Hoare specifically (and quite rightly) points out that it’s the intention
    of a program’s *users*—not its *creators*—that dictates whether a program is performing
    correctly. How inconvenient that the intentions of a program’s users aren’t always
    the same as those of its creator!'
  prefs: []
  type: TYPE_NORMAL
- en: Given this assertion, it stands to reason that a user’s first expectation about
    a program is that *the program works*. But when is a program “working”? This is
    actually a pretty big question, one that lies at the heart of cloud native design.
    The first goal of this chapter is to explore that very idea, and in the process,
    introduce concepts like “dependability” and “reliability” that we can use to better
    describe (and meet) user expectations. Finally, we’ll briefly review a number
    of practices commonly used in cloud native development to ensure that services
    meet the expectations of its users. We’ll discuss each of these in-depth throughout
    the remainder of this book.
  prefs: []
  type: TYPE_NORMAL
- en: What’s the Point of Cloud Native?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.xhtml#chapter_1) we spent a few pages defining “cloud native,”
    starting with the Cloud Native Computing Foundation’s definition and working forward
    to the properties of an ideal cloud native service. We spent a few more pages
    talking about the pressures that have driven cloud native to be a thing in the
    first place.
  prefs: []
  type: TYPE_NORMAL
- en: What we didn’t spend so much time on, however, was the *why* of cloud native.
    Why does the concept of cloud native even exist? Why would we even want our systems
    to be cloud native? What’s its purpose? What makes it so special? Why should I
    care?
  prefs: []
  type: TYPE_NORMAL
- en: 'So, why *does* cloud native exist? The answer is actually pretty straightforward:
    it’s all about dependability. In the first part of this chapter, we’ll dig into
    the concept of dependability, what it is, why it’s important, and how it underlies
    all the patterns and techniques that we call cloud native.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s All About Dependability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Holly Cummins, the worldwide development community practice lead for the IBM
    Garage, famously said that “if cloud native has to be a synonym for anything,
    it would be idempotent.”^([4](ch06.xhtml#idm45983630609624)) Cummins is absolutely
    brilliant, and has said a lot of absolutely brilliant things,^([5](ch06.xhtml#idm45983630608616))
    but I think she only has half of the picture on this one. I think that idempotence
    is very important—perhaps even necessary for cloud native—but not sufficient.
    I’ll elaborate.
  prefs: []
  type: TYPE_NORMAL
- en: The history of software, particularly the network-based kind, has been one of
    struggling to meet the expectations of increasingly sophisticated users. Long
    gone are the days when a service could go down at night “for maintenance.” Users
    today rely heavily on the services they use, and they expect those services to
    be available and to respond promptly to their requests. Remember the last time
    you tried to start a Netflix movie and it took the longest five seconds of your
    life? Yeah, that.
  prefs: []
  type: TYPE_NORMAL
- en: Users don’t care that your services have to be maintained. They won’t wait patiently
    while you hunt down that mysterious source of latency. They just want to finish
    binge-watching the second season of Breaking Bad.^([6](ch06.xhtml#idm45983630606248))
  prefs: []
  type: TYPE_NORMAL
- en: All of the patterns and techniques that we associate with cloud native—*every
    single one*—exist to allow services to be deployed, operated, and maintained at
    scale in unreliable environments, driven by the need to produce dependable services
    that keep users happy.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, I think that if “cloud native” has to be a synonym for anything,
    it would be “dependability.”
  prefs: []
  type: TYPE_NORMAL
- en: What Is Dependability and Why Is It So Important?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I didn’t choose the word “dependability” arbitrarily. It’s actually a core
    concept in the field of *systems engineering*, which is full of some very smart
    people who say some very smart things about the design and management of complex
    systems. The concept of dependability in a computing context was first rigorously
    defined by Jean-Claude Laprie about 35 years ago,^([7](ch06.xhtml#idm45983630599160))
    who defined a system’s dependability according to the expectations of its users.
    Laprie’s original definition has been tweaked and extended over the years by various
    authors, but here’s my favorite:'
  prefs: []
  type: TYPE_NORMAL
- en: The dependability of a computer system is its ability to avoid failures that
    are more frequent or more severe, and outage durations that are longer, than is
    acceptable to the user(s).^([8](ch06.xhtml#idm45983630595768))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fundamental Concepts of Computer System Dependability (2001)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In other words, a dependable system consistently does what its users expect
    and can be quickly fixed when it doesn’t.
  prefs: []
  type: TYPE_NORMAL
- en: 'By this definition, a system is dependable only when it can *justifiably* be
    trusted. Obviously, a system can’t be considered dependable if it falls over any
    time one of its components glitch, or if it requires hours to recover from a failure.
    Even if it’s been running for months without interruption, an undependable system
    may still be one bad day away from catastrophe: lucky isn’t dependable.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, it’s hard to objectively gauge “user expectations.” For this
    reason, as illustrated in [Figure 6-1](#img_ch06_dependability_tree), dependability
    is an umbrella concept encompassing several more specific and quantifiable attributes—availability,
    reliability, and maintainability—all of which are subject to similar threats that
    may be overcome by similar means.
  prefs: []
  type: TYPE_NORMAL
- en: '![cngo 0601](Images/cngo_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. The system attributes and means that contribute to dependability
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So while the concept of “dependability” alone might be a little squishy and
    subjective, the attributes that contribute to it are quantitative and measurable
    enough to be useful:'
  prefs: []
  type: TYPE_NORMAL
- en: Availability
  prefs: []
  type: TYPE_NORMAL
- en: The ability of a system to perform its intended function at a random moment
    in time. This is usually expressed as the probability that a request made of the
    system will be successful, defined as uptime divided by total time.
  prefs: []
  type: TYPE_NORMAL
- en: Reliability
  prefs: []
  type: TYPE_NORMAL
- en: 'The ability of a system to perform its intended function for a given time interval.
    This is often expressed as either the mean time between failures (MTBF: total
    time divided by the number of failures) or failure rate (number of failures divided
    by total time).'
  prefs: []
  type: TYPE_NORMAL
- en: Maintainability
  prefs: []
  type: TYPE_NORMAL
- en: The ability of a system to undergo modifications and repairs. There are a variety
    of indirect measures for maintainability, ranging from calculations of cyclomatic
    complexity to tracking the amount of time required to change a system’s behavior
    to meet new requirements or to restore it to a functional state.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Later authors extended Laprie’s definition of dependability to include several
    security-related properties, including safety, confidentiality, and integrity.
    I’ve reluctantly omitted these, not because security isn’t important (it’s *SO*
    important!), but for brevity. A worthy discussion of security would require an
    entire book of its own.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dependability: It’s Not Just for Ops Anymore'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the introduction of networked services, it’s been the job of developers
    to build services, and of systems administrators (“operations”) to deploy those
    services onto servers and keep them running. This worked well enough for a time,
    but it had the unfortunate side-effect of incentivizing developers to prioritize
    feature development at the expense of stability and operations.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, over the past decade or so—coinciding with the DevOps movement—a
    new wave of technologies has become available with the potential to completely
    change the way technologists of all kinds do their jobs.
  prefs: []
  type: TYPE_NORMAL
- en: On the operations side, with the availability of infrastructure and platforms
    as a service (IaaS/PaaS) and tools like Terraform and Ansible, working with infrastructure
    has never been more like writing software.
  prefs: []
  type: TYPE_NORMAL
- en: On the development side, the popularization of technologies like containers
    and serverless functions has given developers an entire new set of “operations-like”
    capabilities, particularly around virtualization and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the once-stark line between software and infrastructure is getting
    increasingly blurry. One could even argue that with the growing advancement and
    adoption of infrastructure abstractions like virtualization, container orchestration
    frameworks like Kubernetes, and software-defined behavior like service meshes,
    we may even be at the point where they could be said to have merged. Everything
    is software now.
  prefs: []
  type: TYPE_NORMAL
- en: The ever-increasing demand for service dependability has driven the creation
    of a whole new generation of cloud native technologies. The effects of these new
    technologies and the capabilities they provide has been considerable, and the
    traditional developer and operations roles are changing to suit them. At long
    last, the silos are crumbling, and, increasingly, the rapid production of dependable,
    high-quality services is a fully collaborative effort of all of its designers,
    implementors, and maintainers.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving Dependability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is where the rubber meets the road. If you’ve made it this far, congratulations.
  prefs: []
  type: TYPE_NORMAL
- en: So far we’ve discussed Laprie’s definition of “dependability,” which can be
    (very) loosely paraphrased as “happy users,” and we’ve discussed the attributes—availability,
    reliability, and maintainability—that contribute to it. This is all well and good,
    but without actionable advice for how to achieve dependability the entire discussion
    is purely academic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Laprie thought so too, and defined four broad categories of techniques that
    can be used together to improve a system’s dependability (or which, by their absence,
    can reduce it):'
  prefs: []
  type: TYPE_NORMAL
- en: Fault prevention
  prefs: []
  type: TYPE_NORMAL
- en: Fault prevention techniques are used during system construction to prevent the
    occurrence or introduction of faults.
  prefs: []
  type: TYPE_NORMAL
- en: Fault tolerance
  prefs: []
  type: TYPE_NORMAL
- en: Fault tolerance techniques are used during system design and implementation
    to prevent service failures in the presence of faults.
  prefs: []
  type: TYPE_NORMAL
- en: Fault removal
  prefs: []
  type: TYPE_NORMAL
- en: Fault removal techniques are used to reduce the number and severity of faults.
  prefs: []
  type: TYPE_NORMAL
- en: Fault forecasting
  prefs: []
  type: TYPE_NORMAL
- en: Fault forecasting techniques are used to identify the presence, the creation,
    and the consequences of faults.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, as illustrated in [Figure 6-2](#img_ch06_means_pyramid), these
    four categories correspond surprisingly well to the five cloud native attributes
    that we introduced all the way back in [Chapter 1](ch01.xhtml#chapter_1).
  prefs: []
  type: TYPE_NORMAL
- en: '![cngo 0602](Images/cngo_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. The four means of achieving dependability, and their corresponding
    cloud native attributes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Fault prevention and fault tolerance make up the bottom two layers of the pyramid,
    corresponding with scalability, loose coupling, and resilience. Designing a system
    for scalability prevents a variety of faults common among cloud native applications,
    and resiliency techniques allow a system to tolerate faults when they do inevitably
    arise. Techniques for loose coupling can be said to fall into both categories,
    preventing and enhancing a service’s fault tolerance. Together these can be said
    to contribute to what Laprie terms *dependability procurement*: the means by which
    a system is provided with the ability to perform its designated function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Techniques and designs that contribute to manageability are intended to produce
    a system that can be easily modified, simplifying the process of removing faults
    when they’re identified. Similarly, observability naturally contributes to the
    ability to forecast faults in a system. Together fault removal and forecasting
    techniques contribute to what Laprie termed *dependability validation*: the means
    by which confidence is gained in a system’s ability to perform its designated
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the implications of this relationship: what was a purely academic
    exercise 35 years ago has essentially been rediscovered—apparently independently—as
    a natural consequence of years of accumulated experience building reliable production
    systems. Dependability has come full-circle.'
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent sections we’ll explore these relationships more fully and
    preview later chapters, in which we discuss exactly how these two apparently disparate
    systems actually correspond quite closely.
  prefs: []
  type: TYPE_NORMAL
- en: Fault Prevention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the base of our “Means of Dependability” pyramid are techniques that focus
    on preventing the occurrence or introduction of faults. As veteran programmers
    can attest, many—if not most—classes of errors and faults can be predicted and
    prevented during the earliest phases of development. As such, many fault prevention
    techniques come into play during the design and implementation of a service.
  prefs: []
  type: TYPE_NORMAL
- en: Good programming practices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fault prevention is one of the primary goals of software engineering in general,
    and is the explicit goal of any development methodology, from pair programming
    to test-driven development and code review practices. Many such techniques can
    really be grouped into what might be considered to be “good programming practice,”
    about which innumerable excellent books and articles have already been written,
    so we won’t explicitly cover it here.
  prefs: []
  type: TYPE_NORMAL
- en: Language features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Your choice of language can also greatly affect your ability to prevent or fix
    faults. Many language features that some programmers have sometimes come to expect,
    such as dynamic typing, pointer arithmetic, manual memory management, and thrown
    exceptions (to name a few) can easily introduce unintended behaviors that are
    difficult to find and fix, and may even be maliciously exploitable.
  prefs: []
  type: TYPE_NORMAL
- en: These kinds of features strongly motivated many of the design decisions for
    Go, resulting in the strongly typed garbage-collected language we have today.
    For a refresher for why Go is particularly well suited for the development of
    cloud native services, take a look back at [Chapter 2](ch02.xhtml#chapter_2).
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We briefly introduced the concept of scalability way back in [Chapter 1](ch01.xhtml#chapter_1),
    where it was defined as the ability of a system to continue to provide correct
    service in the face of significant changes in demand.
  prefs: []
  type: TYPE_NORMAL
- en: In that section we introduced two different approaches to scaling—vertical scaling
    (scaling up) by resizing existing resources, and horizontal scaling (scaling out)
    by adding (or removing) service instances—and some of the pros and cons of each.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll go quite a bit deeper into each of these in [Chapter 7](ch07.xhtml#chapter_7),
    especially into the gotchas and downsides. We’ll also talk a lot about the problems
    posed by state.^([11](ch06.xhtml#idm45983630519656)) For now, though, it’ll suffice
    to say that having to scale your service adds quite a bit of overhead, including
    but not limited to cost, complexity, and debugging.
  prefs: []
  type: TYPE_NORMAL
- en: While scaling resources is eventually often inevitable, it’s often better (and
    cheaper!) to resist the temptation to throw hardware at the problem and postpone
    scaling events as long as possible by considering runtime efficiency and algorithmic
    scaling. As such, we’ll cover a number of Go features and tooling that allow us
    to identify and fix common problems like memory leaks and lock contention that
    tend to plague systems at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Loose coupling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Loose coupling, which we first defined in [“Loose Coupling”](ch01.xhtml#section_ch01_loose_coupling),
    is the system property and design strategy of ensuring that a system’s components
    have as little knowledge of other components as possible. The degree of coupling
    between services can have an enormous—and too often under-appreciated—impact on
    a system’s ability to scale and to isolate and tolerate failures.
  prefs: []
  type: TYPE_NORMAL
- en: Since the beginning of microservices there have been dissenters who point to
    the difficulty of deploying and maintaining microservice-based systems as evidence
    that such architectures are just too complex to be viable. I don’t agree, but
    I can see where they’re coming from, given how incredibly easy it is to build
    a *distributed monolith*. The hallmark of a distributed monolith is the tight
    coupling between its components, which results in an application saddled with
    all of the complexity of microservices plus the all of the tangled dependencies
    of the typical monolith. If you have to deploy most of your services together,
    or if a failed health check sends cascading failures through your entire system,
    you probably have a distributed monolith.
  prefs: []
  type: TYPE_NORMAL
- en: Building a loosely coupled system is easier said than done, but is possible
    with a little discipline and reasonable boundaries. In [Chapter 8](ch08.xhtml#chapter_8)
    we’ll cover how to use data exchange contracts to establish those boundaries,
    and different synchronous and asynchronous communication models and architectural
    patterns and packages used to implement them and avoid the dreaded distributed
    monolith.
  prefs: []
  type: TYPE_NORMAL
- en: Fault Tolerance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fault tolerance has a number of synonyms—self-repair, self-healing, resilience—that
    all describe a system’s ability to detect errors and prevent them from cascading
    into a full-blown failure. Typically, this consists of two parts: *error detection*,
    in which an error is discovered during normal service, and *recovery*, in which
    the system is returned to a state where it can be activated again.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps the most common strategy for providing resilience is redundancy: the
    duplication of critical components (having multiple service replicas) or functions
    (retrying service requests). This is a broad and very interesting field with a
    number of subtle gotchas that we’ll dig into in [Chapter 9](ch09.xhtml#chapter_9).'
  prefs: []
  type: TYPE_NORMAL
- en: Fault Removal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fault removal, the third of the four dependability means, is the process of
    reducing the number and severity of faults—latent software flaws that can cause
    errors—before they manifest as errors.
  prefs: []
  type: TYPE_NORMAL
- en: Even under ideal conditions, there are plenty of ways that a system can error
    or otherwise misbehave. It might fail to perform an expected action, or perform
    the wrong action entirely, perhaps maliciously. Just to make things even more
    complicated, conditions aren’t always—or often—ideal.
  prefs: []
  type: TYPE_NORMAL
- en: Many faults can be identified by testing, which allows you to verify that the
    system (or at least its components) behaves as expected under known test conditions.
  prefs: []
  type: TYPE_NORMAL
- en: But what about unknown conditions? Requirements change, and the real world doesn’t
    care about your test conditions. Fortunately, with effort, a system can be designed
    to be manageable enough that its behavior can often be adjusted to keep it secure,
    running smoothly, and compliant with changing requirements.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll briefly discuss these next.
  prefs: []
  type: TYPE_NORMAL
- en: Verification and testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are exactly four ways of finding latent software faults in your code:
    testing, testing, testing, and bad luck.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Yes, I joke, but that’s not so far from the truth: if you don’t find your software
    faults, your users will. If you’re lucky. If you’re not, then they’ll be found
    by bad actors seeking to take advantage of them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bad jokes aside, there are two common approaches to finding software faults
    in development:'
  prefs: []
  type: TYPE_NORMAL
- en: Static analysis
  prefs: []
  type: TYPE_NORMAL
- en: Automated, rule-based code analysis performed without actually executing programs.
    Static analysis is useful for providing early feedback, enforcing consistent practices,
    and finding common errors and security holes without depending on human knowledge
    or effort.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic analysis
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the correctness of a system or subsystem by executing it under controlled
    conditions and evaluating its behavior. More commonly referred to simply as “testing.”
  prefs: []
  type: TYPE_NORMAL
- en: Key to software testing is having software that’s *designed for testability*
    by minimizing the *degrees of freedom*—the range of possible states—of its components.
    Highly testable functions have a single purpose, with well-defined inputs and
    outputs and few or no *side effects*; that is, they don’t modify variables outside
    of their scope. If you’ll forgive the nerdiness, this approach minimizes the *search
    space*—the set of all possible solutions—of each function.
  prefs: []
  type: TYPE_NORMAL
- en: Testing is a critical step in software development that’s all too often neglected.
    The Go creators understood this and baked unit testing and benchmarking into the
    language itself in the form of the `go test` command and the [testing package](https://oreil.ly/PrhXq).
    Unfortunately, a deep dive into testing theory is well beyond the scope of this
    book, but we’ll do our best to scratch the surface in [Chapter 9](ch09.xhtml#chapter_9).
  prefs: []
  type: TYPE_NORMAL
- en: Manageability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Faults exist when your system doesn’t behave according to requirements. But
    what happens when those requirements change?
  prefs: []
  type: TYPE_NORMAL
- en: Designing for *manageability*, first introduced back in [“Manageability”](ch01.xhtml#section_ch01_manageability),
    allows a system’s behavior to be adjusted without code changes. A manageable system
    essentially has “knobs” that allow real-time control to keep your system secure,
    running smoothly, and compliant with changing requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Manageability can take a variety of forms, including (but not limited to!) adjusting
    and configuring resource consumption, applying on-the-fly security remediations,
    *feature flags* that can turn features on or off, or even loading plug-in-defined
    behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, manageability is a broad topic. We’ll review a few of the mechanisms
    Go provides for it in [Chapter 10](ch10.xhtml#chapter_10).
  prefs: []
  type: TYPE_NORMAL
- en: Fault Forecasting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the peak of our “Means of Dependability” pyramid ([Figure 6-2](#img_ch06_means_pyramid))
    is *fault forecasting*, which builds on the knowledge gained and solutions implemented
    in the levels below it to attempt to estimate the present number, the future incidence,
    and the likely consequence of faults.
  prefs: []
  type: TYPE_NORMAL
- en: Too often this consists of guesswork and gut feelings instead, generally resulting
    in unexpected failures when a starting assumption stops being true. More systematic
    approaches include [Failure Mode and Effects Analysis](https://oreil.ly/sNe6P)
    and stress testing, which are very useful for understanding a system’s possible
    failure modes.
  prefs: []
  type: TYPE_NORMAL
- en: In a system designed for *observability*, which we’ll discuss in depth in [Chapter 11](ch11.xhtml#chapter_11),
    failure mode indicators can be tracked so that they can be forecast and corrected
    before they manifest as errors. Furthermore, when unexpected failures occur—as
    they inevitably will—observable systems allow the underlying faults to be quickly
    identified, isolated, and corrected.
  prefs: []
  type: TYPE_NORMAL
- en: The Continuing Relevance of the Twelve-Factor App
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the early 2010s, developers at Heroku, a platform as a service (PaaS) company
    and early cloud pioneer, realized that they were seeing web applications being
    developed again and again with the same fundamental flaws.
  prefs: []
  type: TYPE_NORMAL
- en: 'Motivated by what they felt were systemic problems in modern application development,
    they drafted *The Twelve-Factor App*. This was a set of twelve rules and guidelines
    constituting a development methodology for building web applications, and by extension,
    cloud native applications (although “cloud native” wasn’t a commonly used term
    at the time). The methodology was for building web applications that: ^([12](ch06.xhtml#idm45983630456168))'
  prefs: []
  type: TYPE_NORMAL
- en: Use declarative formats for setup automation, to minimize time and cost for
    new developers joining the project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have a clean contract with the underlying operating system, offering maximum
    portability between execution environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are suitable for deployment on modern cloud platforms, obviating the need for
    servers and systems administration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimize divergence between development and production, enabling continuous
    deployment for maximum agility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can scale up without significant changes to tooling, architecture, or development
    practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While not fully appreciated when it was first published in 2011, as the complexities
    of cloud native development have become more widely understood (and felt), *The
    Twelve Factor App* and the properties it advocates have started to be cited as
    the bare minimum for any service to be cloud native.
  prefs: []
  type: TYPE_NORMAL
- en: I. Codebase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One codebase tracked in revision control, many deploys.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Twelve-Factor App
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For any given service, there should be exactly one codebase that’s used to produce
    any number of immutable releases for multiple deployments to multiple environments.
    These environments typically include a production site, and one or more staging
    and development sites.
  prefs: []
  type: TYPE_NORMAL
- en: Having multiple services sharing the same code tends to lead to a blurring of
    the lines between modules, trending in time to something like a monolith, making
    it harder to make changes in one part of the service without affecting another
    part (or another service!) in unexpected ways. Instead, shared code should be
    refactored into libraries that can be individually versioned and included through
    a dependency manager.
  prefs: []
  type: TYPE_NORMAL
- en: Having a single service spread across multiple repositories, however, makes
    it nearly impossible to automatically apply the build and deploy phases of your
    service’s life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: II. Dependencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Explicitly declare and isolate (code) dependencies.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Twelve-Factor App
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For any given version of the codebase, `go build`, `go test`, and `go run`
    should be deterministic: they should have the same result, however they’re run,
    and the product should always respond the same way to the same inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: But what if a dependency—an imported code package or installed system tool beyond
    the programmer’s control—changes in such a way that it breaks the build, introduces
    a bug, or becomes incompatible with the service?
  prefs: []
  type: TYPE_NORMAL
- en: Most programming languages offer a packaging system for distributing support
    libraries, and Go is no different.^([13](ch06.xhtml#idm45983630435608)) By using
    [Go modules](https://oreil.ly/68ds1) to declare all dependencies, completely and
    exactly, you can ensure that imported packages won’t change out from under you
    and break your build in unexpected ways.
  prefs: []
  type: TYPE_NORMAL
- en: To extend this somewhat, services should generally try to avoid using the `os/exec`
    package’s `Command` function to shell out to external tools like ImageMagick or
    `curl`.
  prefs: []
  type: TYPE_NORMAL
- en: Yes, your target tool might be available on all (or most) systems, but there’s
    no way to *guarantee* that they both exist and are fully compatible with the service
    everywhere that it might run in the present or future. Ideally, if your service
    requires an external tool, that tool should be *vendored* into the service by
    including it in the service’s repository.
  prefs: []
  type: TYPE_NORMAL
- en: III. Configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Store configuration in the environment.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Twelve-Factor App
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Configuration—anything that’s likely to vary between environments (staging,
    production, developer environments, etc)—should always be cleanly separated from
    the code. Under no circumstances should an application’s configuration be baked
    into the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Configuration items may include, but certainly aren’t limited to:'
  prefs: []
  type: TYPE_NORMAL
- en: URLs or other resource handles to a database or other upstream service dependencies—even
    if it’s not likely to change any time soon.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secrets of *any* kind, such as passwords or credentials for external services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Per-environment values, such as the canonical hostname for the deploy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A common means of extracting configuration from code is by *externalizing* them
    into some configuration file—often YAML^([14](ch06.xhtml#idm45983630418920))—which
    may or may not be checked into the repository alongside the code. This is certainly
    an improvement over configuration-in-code, but it’s also less than ideal.
  prefs: []
  type: TYPE_NORMAL
- en: First, if your configuration file lives outside of the repository, it’s all
    too easy to accidentally check it in. What’s more, such files tend to proliferate,
    with different versions for different environments living in different places,
    making it hard to see and manage configurations with any consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you *could* have different versions of your configurations for
    each environment in the repository, but this can be unwieldy and tends to lead
    to some awkward repository acrobatics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of configurations as code or even as external configurations, *The
    Twelve Factor App* recommends that configurations be stored as *environment variables*.
    Using environment variables in this way actually has a lot of advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: They are standard and largely OS and language agnostic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That are easy to change between deploys without changing any code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They’re very easy to inject into containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go has several tools for doing this.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first—and most basic—is the `os` package, which provides the `os.Getenv`
    function for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For more sophisticated configuration options, there are several excellent packages
    available. Of these, [`spf13/viper`](https://oreil.ly/8giE4) seems to be particularly
    popular. A snippet of Viper in action might look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, Viper provides a number of features that the standard packages
    do not, such as default values, typed variables, and reading from command-line
    flags, variously formatted configuration files, and even remote configuration
    systems like etcd and Consul.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll dive more deeply into Viper and other configuration topics in [Chapter 10](ch10.xhtml#chapter_10).
  prefs: []
  type: TYPE_NORMAL
- en: IV. Backing Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Treat backing services as attached resources.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Twelve-Factor App
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A backing service is any downstream dependency that a service consumes across
    the network as part of its normal operation (see [“Upstream and Downstream Dependencies”](ch01.xhtml#sidebar_ch01_dependencies)).
    A service should make no distinction between backing services of the same type.
    Whether it’s an internal service that’s managed within the same organization or
    a remote service managed by a third party should make no difference.
  prefs: []
  type: TYPE_NORMAL
- en: To the service, each distinct upstream service should be treated as just another
    resource, each addressable by a configurable URL or some other resource handle,
    as illustrated oi [Figure 6-3](#img_ch06_backing_services). All resources should
    be treated as equally subject to the *Fallacies of Distributed Computing* (see
    [Chapter 4](ch04.xhtml#chapter_4) for a refresher, if necessary).
  prefs: []
  type: TYPE_NORMAL
- en: '![cngo 0603](Images/cngo_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. Each upstream service should be treated as just another resource,
    each addressable by a configurable URL or some other resource handle, each equally
    subject to the Fallacies of Distributed Computing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In other words, a MySQL database run by your own team’s sysadmins should be
    treated no differently than an AWS-managed RDS instance. The same goes for *any*
    upstream service, whether it’s running in a data center in another hemisphere
    or in a Docker container on the same server.
  prefs: []
  type: TYPE_NORMAL
- en: A service that’s able to swap out any resource at will with another one of the
    same kind—internally managed or otherwise—just by changing a configuration value
    can be more easily deployed to different environments, can be more easily tested,
    and more easily maintained.
  prefs: []
  type: TYPE_NORMAL
- en: V. Build, Release, Run
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Strictly separate build and run stages.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Twelve-Factor App
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Each (nondevelopment) deployment—the union of a specific version of the built
    code and a configuration—should be immutable and uniquely labeled. It should be
    possible, if necessary, to precisely recreate a deployment if (heaven forbid)
    it is necessary to roll a deployment back to an earlier version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, this is accomplished in three distinct stages, illustrated in [Figure 6-4](#img_ch06_build_release_run)
    and described in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Build
  prefs: []
  type: TYPE_NORMAL
- en: In the build stage, an automated process retrieves a specific version of the
    code, fetches dependencies, and compiles an executable artifact we call a *build*.
    Every build should always have a unique identifier, typically a timestamp or an
    incrementing build number.
  prefs: []
  type: TYPE_NORMAL
- en: Release
  prefs: []
  type: TYPE_NORMAL
- en: 'In the release stage, a specific build is combined with a configuration specific
    to the target deployment. The resulting *release* is ready for immediate execution
    in the execution environment. Like builds, releases should also have a unique
    identifier. Importantly, producing releases with same version of a build shouldn’t
    involve a rebuild of the code: to ensure environment parity, each environment-specific
    configuration should use the same build artifact.'
  prefs: []
  type: TYPE_NORMAL
- en: Run
  prefs: []
  type: TYPE_NORMAL
- en: In the run stage, the release is delivered to the deployment environment and
    executed by launching the service’s processes.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, a new versioned build will be automatically produced whenever new code
    is deployed.
  prefs: []
  type: TYPE_NORMAL
- en: '![cngo 0604](Images/cngo_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. The process of deploying a codebase to a (nondevelopment) environment
    should be performed in distinct build, release, and run stages
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: VI. Processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Execute the app as one or more stateless processes.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Twelve-Factor App
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Service processes should be stateless and share nothing. Any data that has to
    be persisted should be stored in a stateful backing service, typically a database
    or external cache.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve already spent some time talking about statelessness—and we’ll spend more
    in the next chapter—so we won’t dive into this point any further.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you’re interested in reading ahead, feel free to take a look at
    [“State and Statelessness”](ch07.xhtml#section_ch07_state_and_statelessness).
  prefs: []
  type: TYPE_NORMAL
- en: VII. Data Isolation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each service manages its own data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Cloud Native, Data Isolation
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Each service should be entirely *self-contained*. That is, it should manage
    its own data, and make its data accessible only via an API designed for that purpose.
    If this sounds familiar to you, good! This is actually one of the core principles
    of microservices, which we’ll discuss more in [“The Microservices System Architecture”](ch07.xhtml#section_ch07_microservices).
  prefs: []
  type: TYPE_NORMAL
- en: Very often this will be implemented as a request-response service like a RESTful
    API or RPC protocol that’s exported by listening to requests coming in on a port,
    but this can also take the form of an asynchronous, event-based service using
    a publish-subscribe messaging pattern. Both of these patterns will be described
    in more detail in [Chapter 8](ch08.xhtml#chapter_8).
  prefs: []
  type: TYPE_NORMAL
- en: And finally, although this is something you don’t see in the Go world, some
    languages and frameworks allow the runtime injection of an application server
    into the execution environment to create a web-facing service. This practice limits
    testability and portability by breaking data isolation and environment agnosticism,
    and is *very strongly* discouraged.
  prefs: []
  type: TYPE_NORMAL
- en: VIII. Scalability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scale out via the process model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Twelve-Factor App
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Services should be able to scale horizontally by adding more instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'We talk about scalability quite a bit in this book. We even dedicated all of
    [Chapter 7](ch07.xhtml#chapter_7) to it. With good reason: the importance of scalability
    can’t be understated.'
  prefs: []
  type: TYPE_NORMAL
- en: Sure, it’s certainly convenient to just beef up the one server your service
    is running on—and that’s fine in the (very) short term—but vertical scaling is
    a losing strategy in the long run. If you’re lucky, you’ll eventually hit a point
    where you simply can’t scale up any more. It’s more likely that your single server
    will either suffer load spikes faster than you can scale up, or just die without
    warning and without a redundant failover.^([16](ch06.xhtml#idm45983630197768))
    Both scenarios end with a lot of unhappy users. We’ll discuss scalability quite
    a bit more in [Chapter 7](ch07.xhtml#chapter_7).
  prefs: []
  type: TYPE_NORMAL
- en: IX. Disposability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Maximize robustness with fast startup and graceful shutdown.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Twelve-Factor App
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Cloud environments are fickle: provisioned servers have a funny way of disappearing
    at odd times. Services should account for this by being *disposable*: service
    instances should be able to be started or stopped—intentionally or not—at any
    time.'
  prefs: []
  type: TYPE_NORMAL
- en: Services should strive to minimize the time it takes to start up to reduce the
    time it takes for the service to be deployed (or redeployed) to elastically scale.
    Go, having no virtual machine or other significant overhead, is especially good
    at this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Containers provide fast startup time and are also very useful for this, but
    care must be taken to keep image sizes small to minimize the data transfer overhead
    incurred with each initial deploy of a new image. This is another area in which
    Go excels: its self-sufficient binaries can generally be installed into `SCRATCH`
    images, without requiring an external language runtime or other external dependencies.
    We demonstrated this in the previous chapter, in [“Containerizing Your Key-Value
    Store”](ch05.xhtml#section_ch05_containerizing).'
  prefs: []
  type: TYPE_NORMAL
- en: Services should also be capable of shutting down when they receive a `SIGTERM`
    signal by saving all data that needs to be saved, closing open network connections,
    or finishing any in-progress work that’s left or by returning the current job
    to the work queue.
  prefs: []
  type: TYPE_NORMAL
- en: X. Development/Production Parity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keep development, staging, and production as similar as possible.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Twelve-Factor App
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Any possible differences between development and production should be kept
    as small as possible. This includes code differences, of course, but it extends
    well beyond that:'
  prefs: []
  type: TYPE_NORMAL
- en: Code divergence
  prefs: []
  type: TYPE_NORMAL
- en: Development branches should be small and short-lived, and should be tested and
    deployed into production as quickly as possible. This minimizes functional differences
    between environments and reduces the risk of both deploys and rollbacks.
  prefs: []
  type: TYPE_NORMAL
- en: Stack divergence
  prefs: []
  type: TYPE_NORMAL
- en: Rather than having different components for development and production (say,
    SQLite on OS X versus MySQL on Linux), environments should remain as similar as
    possible. Lightweight containers are an excellent tool for this. This minimizes
    the possibility that inconvenient differences between almost-but-not-quite-the-same
    implementations will emerge to ruin your day.
  prefs: []
  type: TYPE_NORMAL
- en: Personnel divergence
  prefs: []
  type: TYPE_NORMAL
- en: Once it was common to have programmers who wrote code and operators who deployed
    code, but that arrangement created conflicting incentives and counter-productive
    adversarial relationships. Keeping code authors involved in deploying their work
    and responsible for its behavior in production helps break down development/operations
    silos and aligns incentives around stability and velocity.
  prefs: []
  type: TYPE_NORMAL
- en: Taken together, these approaches help to keep the gap between development and
    production small, which in turn encourages rapid, automated, continuous deployment.
  prefs: []
  type: TYPE_NORMAL
- en: XI. Logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Treat logs as event streams.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Twelve-Factor App
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Logs—a service’s never-ending stream of consciousness—are incredibly useful
    things, particularly in a distributed environment. By providing visibility into
    the behavior of a running application, good logging can greatly simplify the task
    of locating and diagnosing misbehavior.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, services wrote log events to a file on the local disk. At cloud
    scale, however, this just makes valuable information awkward to find, inconvenient
    to access, and impossible to aggregate. In dynamic, ephemeral environments like
    Kubernetes your service instances (and their log files) may not even exist by
    the time you get around to viewing them.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, a cloud native service should treat log information as nothing more
    than a stream of events, writing each event, unbuffered, directly to `stdout`.
    It shouldn’t concern itself with implementation trivialities like routing or storage
    of its log events, and allow the executor to decide what happens to them.
  prefs: []
  type: TYPE_NORMAL
- en: Though seemingly simple (and perhaps somewhat counterintuitive), this small
    change provides a great deal of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: During local development, a programmer can watch the event stream in a terminal
    to observe the service’s behavior. In deployment, the output stream can be captured
    by the execution environment and forwarded to one or more destinations, such as
    a log indexing system like Elasticsearch, Logstash, and Kibana (ELK) or Splunk
    for review and analysis, or a data warehouse for long-term storage.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll discuss logs and logging, in the context of observability, in more detail
    in [Chapter 11](ch11.xhtml#chapter_11).
  prefs: []
  type: TYPE_NORMAL
- en: XII. Administrative Processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Run administrative/management tasks as one-off processes.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Twelve-Factor App
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Of all of the original Twelve Factors, this is the one that most shows its age.
    For one thing, it explicitly advocates shelling into an environment to manually
    execute tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be clear: *making manual changes to a server instance creates snowflakes.
    This is a bad thing.* See [“Special Snowflakes”](#sidebar_ch06_snowflakes).'
  prefs: []
  type: TYPE_NORMAL
- en: Assuming you even have an environment that you can shell into, you should assume
    that it can (and eventually will) be destroyed and re-created any moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ignoring all of that for a moment, let’s distill the point to its original
    intent: administrative and management tasks should be run as one-off processes.
    This could be interpreted in two ways, each requiring its own approach:'
  prefs: []
  type: TYPE_NORMAL
- en: If your task is an administrative process, like a data repair job or database
    migration, it should be run as a short-lived process. Containers and functions
    are excellent vehicles for such purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your change is an update to your service or execution environment, you should
    instead modify your service or environment construction/configuration scripts,
    respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we considered the question “what’s the point of cloud native?”
    The common answer is “a computer system that works in the cloud.” But “work” can
    mean anything. Surely we can do better.
  prefs: []
  type: TYPE_NORMAL
- en: 'So we went back to thinkers like Tony Hoare and J-C Laprie, who provided the
    first part of the answer: *dependability*. That is, to paraphrase, computer systems
    that behave in ways that users find acceptable, despite living in a fundamentally
    unreliable environment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, that’s more easily said than done, so we reviewed three schools
    of thought regarding how to achieve it:'
  prefs: []
  type: TYPE_NORMAL
- en: Laprie’s academic “means of dependability,” which include preventing, tolerating,
    removing, and forecasting faults
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adam Wiggins’ *Twelve Factor App*, which took a more prescriptive (and slightly
    dated, in spots) approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our own “cloud native attributes,” based on the Cloud Native Computing Foundation’s
    definition of “cloud native,” that we introduced in [Chapter 1](ch01.xhtml#chapter_1)
    and organized this entire book around
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although this chapter was essentially a short survey of theory, there’s a lot
    of important, foundational information here that describes the motivations and
    means used to achieve what we call “cloud native.”
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch06.xhtml#idm45983630648552-marker)) Hoare, C.A.R. “An Axiomatic Basis
    for Computer Programming.”. *Communications of the ACM*, vol. 12, no. 10, October
    1969, pp. 576–583\. [*https://oreil.ly/jOwO9*](https://oreil.ly/jOwO9).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch06.xhtml#idm45983630641848-marker)) When Edsger W. Dijkstra coined the
    expression “GOTO considered harmful,” he was referencing Hoare’s work in structured
    programming.
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch06.xhtml#idm45983630640824-marker)) Hoare, Tony. “Null References:
    The Billion Dollar Mistake.” *InfoQ.com*. 25 August 2009\. [*https://oreil.ly/4QWS8*](https://oreil.ly/4QWS8).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch06.xhtml#idm45983630609624-marker)) *Cloud Native Is About Culture,
    Not Containers*. Cummins, Holly. Cloud Native London 2018.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch06.xhtml#idm45983630608616-marker)) If you ever have a chance to see
    her speak, I strongly recommend you take it.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch06.xhtml#idm45983630606248-marker)) Remember what Walt did to Jane that
    time? That was so messed up.
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch06.xhtml#idm45983630599160-marker)) Laprie, J-C. “Dependable Computing
    and Fault Tolerance: Concepts and Terminology.” *FTCS-15 The 15th Int’l Symposium
    on Fault-Tolerant Computing*, June 1985, pp. 2–11\. [*https://oreil.ly/UZFFY*](https://oreil.ly/UZFFY).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch06.xhtml#idm45983630595768-marker)) A. Avižienis, J. Laprie, and B.
    Randell. “Fundamental Concepts of Computer System Dependability.” *Research Report
    No. 1145, LAAS-CNRS*, April 2001\. [*https://oreil.ly/4YXd1*](https://oreil.ly/4YXd1).
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch06.xhtml#idm45983630572776-marker)) If you haven’t, start with [*Site
    Reliability Engineering: How Google Runs Production Systems*](https://oreil.ly/OJn99).
    It really is very good.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch06.xhtml#idm45983630567896-marker)) Many organizations use service-level
    objectives (SLOs) for precisely this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch06.xhtml#idm45983630519656-marker)) Application state is hard, and
    when done wrong it’s poison to scalability.
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch06.xhtml#idm45983630456168-marker)) Wiggins, Adam. *The Twelve-Factor
    App.* 2011\. [*https://12factor.net*](https://12factor.net).
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch06.xhtml#idm45983630435608-marker)) Although it was for too long!
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch06.xhtml#idm45983630418920-marker)) The world’s worst configuration
    language (except for all the other ones).
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch06.xhtml#idm45983630247848-marker)) Wiggins, Adam. “Port Binding.”
    *The Twelve-Factor App.* 2011\. [*https://oreil.ly/bp8lC*](https://oreil.ly/bp8lC).
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch06.xhtml#idm45983630197768-marker)) Probably at three in the morning.
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch06.xhtml#idm45983630143304-marker)) “Baking” is a term sometimes used
    to refer to the process of creating a new container or server image.
  prefs: []
  type: TYPE_NORMAL
