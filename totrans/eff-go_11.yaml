- en: Chapter 11\. Optimization Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all we’ve learned from the past 10 chapters, it’s time to go through various
    patterns and common pitfalls I found when developing efficient code in Go. As
    I mentioned in [Chapter 10](ch10.html#ch-opt), the optimization suggestion doesn’t
    generalize well. However, given you should know at this point how to assess code
    changes effectively, there is no harm in stating some common patterns that improve
    efficiency in certain cases.
  prefs: []
  type: TYPE_NORMAL
- en: Be a Mindful Go Developer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember that most optimization ideas you will see here are highly deliberate.
    This means we have to have a good reason to add them as they take the developer’s
    time to get right and maintain in the future. Even if you learn about some common
    optimization, ensure it improves efficiency for your specific workload.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t use this chapter as a strict manual but as a list of potential options
    you did not think about. Nevertheless, always stick to the observability, benchmarking,
    and profiling tools we learned in previous chapters to ensure the optimizations
    you do are pragmatic, follow [YAGNI](https://oreil.ly/G9OLQ), and are needed.
  prefs: []
  type: TYPE_NORMAL
- en: We will start with [“Common Patterns”](#ch-opt-patterns), where I describe some
    high-level optimization patterns we could see from optimization examples in [Chapter 10](ch10.html#ch-opt).
    Then I will introduce you to the [“The Three Rs Optimization Method”](#ch-hw-rrr),
    an excellent memory optimization framework from the Go (and Prometheus) community.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in [“Don’t Leak Resources”](#ch-basic-leaks), [“Pre-Allocate If You
    Can”](#ch-basic-prealloc), [“Overusing Memory with Arrays”](#ch-basic-subslice),
    and [“Memory Reuse and Pooling”](#ch-basic-pool), we will go through a set of
    specific optimizations, tips, and gotchas I wish I’d known when I started my journey
    with making Go code more efficient. I have chosen the most common ones that are
    worth being aware of!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with common optimization patterns. Some of them I used in previous
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Common Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How can you find optimizations? After benchmarking, profiling, and studying
    the code, the process requires us to figure out a better algorithm, data structure,
    or code that will be more efficient. Of course, this is easier said than done.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some practice and experience help, but we can outline a few patterns that repeat
    in our optimization journeys. Let’s now walk through four generic patterns we
    see in the programming community and literature: doing less work, and trading
    functionality for efficiency, trading space for time, and trading time for space.'
  prefs: []
  type: TYPE_NORMAL
- en: Do Less Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing we should focus on is avoiding unnecessary work. Especially
    in [“Optimizing Latency”](ch10.html#ch-opt-latency-example), we improved the CPU
    time multiple times by removing a lot of unnecessary code. It might feel simplistic,
    but it’s a powerful pattern we often forget. If some portion of the code is critical
    and requires optimization, we can go through bottlenecks (e.g., lines of code
    with large contributions we see in Source view as we discussed in [“go tool pprof
    Reports”](ch09.html#ch-obs-profiling-res)) and check if we can:'
  prefs: []
  type: TYPE_NORMAL
- en: Skip unnecessary logic
  prefs: []
  type: TYPE_NORMAL
- en: Can we remove this line? For example, in [“Optimizing Latency”](ch10.html#ch-opt-latency-example),
    `strconv.ParseInt` had a lot of checks that weren’t needed in our implementation.
    We can use the assumptions and requirements we have to our advantage and trim
    down the functionality that isn’t strictly needed. This also includes potential
    resources we can clean early or any resource leaks (see [“Don’t Leak Resources”](#ch-basic-leaks)).
  prefs: []
  type: TYPE_NORMAL
- en: Generic Implementations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s very tempting to approach programming problems with a generic solution.
    We are trained to see patterns, and programming languages offer many abstractions
    and object-oriented paradigms to reuse more code.
  prefs: []
  type: TYPE_NORMAL
- en: As we could see in [“Optimizing Latency”](ch10.html#ch-opt-latency-example),
    while the `bytes.Split` and `strconv.ParseInt` functions are well designed, safe
    to use, and richer in features, they might not always be suitable for critical
    paths. Being “generic” has many drawbacks, and efficiency is usually the first
    victim.
  prefs: []
  type: TYPE_NORMAL
- en: Do things once
  prefs: []
  type: TYPE_NORMAL
- en: Was it done already? Perhaps we already loop over the same array somewhere else,
    so we could do more things “in place,” as we did in [Example 10-3](ch10.html#code-sum2).
  prefs: []
  type: TYPE_NORMAL
- en: There might be cases where we validate some invariant even though it was validated
    before. Or we sort again “just in case,” but when we double-check the code, it
    was sorted already. For example, in the Thanos project, we can do a [k-way merge](https://oreil.ly/LxjZq)
    instead of a naive merge and sort again when merging different metric streams
    because of the invariant that each stream gives metrics in lexicographic order.
  prefs: []
  type: TYPE_NORMAL
- en: Another common example is reusing memory. For instance, we can create a small
    buffer once and reuse it, as in [Example 10-8](ch10.html#code-sum6), instead of
    creating a new one every time we need it. We can also use caching or [“Memory
    Reuse and Pooling”](#ch-basic-pool).
  prefs: []
  type: TYPE_NORMAL
- en: Leverage math to do less
  prefs: []
  type: TYPE_NORMAL
- en: Using math is an amazing way to reduce the work we have to do. For example,
    to calculate the number of samples retrieved through the Prometheus API, we don’t
    decode chunks and iterate over all samples to count them. Instead, we estimate
    the number of samples by dividing the size of the chunk by the average sample
    size.
  prefs: []
  type: TYPE_NORMAL
- en: Use the knowledge or precomputed information
  prefs: []
  type: TYPE_NORMAL
- en: Many APIs and functions are designed to be smart and automate certain work,
    even if it means doing more work. One example is pre-allocation possibilities,
    discussed in [“Pre-Allocate If You Can”](#ch-basic-prealloc).
  prefs: []
  type: TYPE_NORMAL
- en: In another, more complex example, the [`minio-go`](https://oreil.ly/YqDZ6) object
    storage client we use in [objstore](https://oreil.ly/l8xHu) can upload an arbitrary
    `io.Reader` implementation. However, the implementation requires calculating the
    checksum before upload. Thus, if we don’t give the total expected size of the
    bytes available in a reader, `minio-go` will use additional CPU cycles and memory
    to buffer the whole, potentially gigabytes-large object. All this just to calculate
    a checksum that has to be sometimes sent up front. On the other hand, if we notice
    this and have the total size handy, providing this information through the API
    can dramatically improve upload efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: These elements seem like they focus on CPU time and latency, but we can use
    the same toward memory or any other resource usage. For example, consider a small
    example in [Example 11-1](#code-emptystruct) that shows what it means to do “less
    work” focused on lower memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-1\. The function finding if the slice has a duplicated element optimized
    with an empty struct. Uses [“Generics”](ch02.html#ch-go-generics).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Since we don’t use the `map` value, we can use the `struct{}` statement, which
    uses no memory. Thanks to this, the `HasDuplicates2` on my machine is 22% faster
    and allocates 5 times less memory for a `float64` slice with 1 million elements.
    The same pattern can be used in places where we don’t care about value. For example,
    for channels we use to synchronize goroutines, we can use `make(chan struct{})`
    to avoid unnecessary space we don’t need.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, there is always room to reduce some effort in our programs. We can
    use profiling to our advantage to check all expensive parts and their relevance
    to our problem. Often we can remove or transform those into cheaper forms, gaining
    efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Be Strategic!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, doing less work now means more work or resource usage later. We can
    be strategic about this and ensure that our local benchmark doesn’t miss the important
    trade-off elsewhere. This problem is highlighted in [“Memory Reuse and Pooling”](#ch-basic-pool),
    where the macrobenchmark results give opposite conclusions to the microbenchmark.
  prefs: []
  type: TYPE_NORMAL
- en: Trading Functionality for Efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some cases, we have to negotiate or remove certain functionality to improve
    efficiency. In [“Optimizing Latency”](ch10.html#ch-opt-latency-example), we can
    improve the CPU time by removing support for negative integers in the file. Without
    this requirement, we can remove the check for negative sign in the [Example 10-5](ch10.html#code-sum4)
    `ParseInt` function! Perhaps this feature is not well used, and it can be traded
    for cheaper execution!
  prefs: []
  type: TYPE_NORMAL
- en: This is also why accepting all the possible features in the project is often
    not very sustainable. In many cases, an extra API, extra parameter, or functionality
    might add a significant efficiency penalty for critical paths, which could be
    avoided if we just limit the functionality to a minimum.^([1](ch11.html#idm45606819377840))
  prefs: []
  type: TYPE_NORMAL
- en: Trading Space for Time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What else can we do if we limit our program’s work to a minimum by reducing
    unnecessary logic, features, and leaks? Generally, we can shift to systems, algorithms,
    or code that use less time but cost us more in terms of storage, like memory,
    disk, and so on. Let’s walk through some possible changes like this:^([2](ch11.html#idm45606819373392))
  prefs: []
  type: TYPE_NORMAL
- en: Precomputing result
  prefs: []
  type: TYPE_NORMAL
- en: Instead of computing the same expensive function, we could try to precompute
    it and store the result in some table lookup or variable.
  prefs: []
  type: TYPE_NORMAL
- en: These days, it’s very common to see a compiler adapting optimization like this.
    The compiler trades compiler latency and program code space for faster execution.
    For example, statements like `10*1024*1024` or `20 * time.Seconds` can be precomputed
    by a compiler, so they don’t have to be computed at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: But there might be cases of more complex function statements that the compiler
    can’t precompute for us. For example, we could use `regexp.Must​Com⁠pile("…​").MatchString(`
    in some condition, which is on a critical path. Perhaps it will be efficient to
    create a variable `pattern := regexp.Must​Com⁠pile("…​")` and operate on `pattern.MatchString(`
    in that heavily used code instead. On top of that, some cryptographic encryption
    offer [precompute methods](https://oreil.ly/2VBL4) that speed up execution.
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  prefs: []
  type: TYPE_NORMAL
- en: 'When the computed results heavily depend on the input, precomputing it for
    one input that is only used from time to time is not very helpful. Instead, we
    can introduce caching as we did in [Example 4-1](ch04.html#code-sum). Writing
    our caching solution is a nontrivial effort and should be done with care.^([3](ch11.html#idm45606819974288))
    There are many [caching policies](https://oreil.ly/UAhqT), with the Least Recently
    Used (LRU) being the most popular in my experience. In [“Bonus: Thinking Out of
    the Box”](ch10.html#ch-opt-bonus), I mentioned a few off-the-shelf solutions in
    open source that we can use.'
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting data structure
  prefs: []
  type: TYPE_NORMAL
- en: We can often change the data structure so certain information can be accessed
    more easily, or by adding more information to the structure. For example, we can
    store the size next to a file descriptor to know the file size instead of asking
    for it every time.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we can maintain a map of elements next to the slice we already
    have in our structure, so we deduplicate or find elements easier (similar to the
    deduplication map I did in [Example 11-1](#code-emptystruct)).
  prefs: []
  type: TYPE_NORMAL
- en: Decompressing
  prefs: []
  type: TYPE_NORMAL
- en: Compression algorithms are great for saving disk or memory space. However, any
    compression—e.g., string interning, gzip, [zstd](https://oreil.ly/OEx9B), etc.—have
    some CPU (thus, time) overhead, so when time is money, we might want to get rid
    of compression. Be careful, though, as enabled compression can improve program
    latency, e.g., when used for messages across slow networks. Therefore, spending
    more CPU time to reduce message size so that we can send more with a smaller number
    of network packets can potentially be faster.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, the decision is deliberate. For example, perhaps we know that based
    on the RAERs, our program can still use more memory, but we are not meeting the
    latency goal. In such a case, we could check if there is anything we can add,
    cache, or store that would allow you to spend less time in our program.
  prefs: []
  type: TYPE_NORMAL
- en: Trading Time for Space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we can spare some latency or extra CPU time but are low on memory during
    the execution, we can try the opposite rule to the previous one, trading space
    for time. The methods are usually exactly the opposite of those in [“Trading Space
    for Time”](#ch-basic-less-work3): compressing and encoding more, removing extra
    fields from the struct, recomputing results, removing caches, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Trading Space for Time or Time for Space Optimizations Is Not Always Intuitive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes to save memory resource usage, we have to allocate more first!
  prefs: []
  type: TYPE_NORMAL
- en: For example, in [“Overusing Memory with Arrays”](#ch-basic-subslice) and [“Memory
    Reuse and Pooling”](#ch-basic-pool), I mention situations where allocating more
    memory or explicitly copying memory is better, despite looking like more work.
    So it can save us more memory space in the long run.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, consider the four general rules as higher-level patterns of possible
    optimizations. Let me now introduce you to the “three Rs,” which helped me a lot
    to guide some of the optimizations in my efficiency development tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The Three Rs Optimization Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The three Rs technique is an excellent method to reduce waste. It is generally
    applicable for all computer resources, but it is often used for [ecology purposes](https://oreil.ly/p6elc)
    to reduce literal waste. Thanks to those three ingredients—reduce, reuse, and
    recycle—we can reduce the impact we have on the Earth’s environment and ensure
    sustainable living.
  prefs: []
  type: TYPE_NORMAL
- en: At [FOSDEM](https://fosdem.org) 2018, I saw [Bryan Boreham’s amazing talk](https://oreil.ly/BLIiT),
    where he described using this method to mitigate memory issues. Indeed, the three
    Rs method is especially effective against memory allocations, which is the most
    common source of memory efficiency and GC overhead problems. So, let’s explore
    each “R” component and how each can help.
  prefs: []
  type: TYPE_NORMAL
- en: Reduce Allocations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Attempting to directly affect the pace [e.g., using `GOGC` or `GOMEMLIMIT`]
    of [garbage] collection has nothing to do with being sympathetic with the collector.
    It’s really about getting more work done between each collection or during the
    collection. You affect that by reducing the amount or the number of allocations
    any piece of work adds to heap memory.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'William Kennedy, [“Garbage Collection in Go: Part I—Semantics”](https://oreil.ly/DVdNm)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There is almost always room to reduce allocations—look for the waste! Some ways
    to reduce the number of objects our code puts on the heap are obvious (reasonable
    optimizations like the pre-allocations of slices we saw in [Example 1-4](ch01.html#code-prea2)).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, other optimizations require certain trade-offs—typically more CPU
    time or less readable code, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[String interning](https://oreil.ly/qJu7u), where we avoid operating on the
    `string` type by providing a dictionary and using a much smaller, pointer-free
    dictionary of integers representing the ID of the string.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsafe conversion [from `[]byte` to `string` (and vice versa) without copying
    memory](https://oreil.ly/Y10YT), which potentially saves allocations, but if done
    wrongly can keep more memory in a heap (discussed in [Example 11-15](#code-overuse-mem)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring that a variable does not escape to the heap can also be considered
    an effort that reduces allocations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are unlimited different ways we could reduce allocations. We already mentioned
    some earlier. For example, when doing less work, we typically can allocate less!
    Another tip is to look for reducing allocations on all optimization design levels
    ([“Optimization Design Levels”](ch03.html#ch-conq-opt-levels)), not only code.
    In most cases, the algorithm must change first so we can have big improvements
    in the space complexity before we move to the code level.
  prefs: []
  type: TYPE_NORMAL
- en: Reuse Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reusing is also an effective technique. As we learned in [“Garbage Collection”](ch05.html#ch-hw-garbage),
    the Go runtime already reuses memory somehow. Still, there are ways to explicitly
    reuse objects like variables, slices, or maps for repeated operations instead
    of re-creating them in every loop. We will discuss some techniques in [“Memory
    Reuse and Pooling”](#ch-basic-pool).
  prefs: []
  type: TYPE_NORMAL
- en: Again, utilize all optimization design levels (see [“Optimization Design Levels”](ch03.html#ch-conq-opt-levels)).
    We can choose the designs of systems or algorithms that reuse memory; for example,
    see [“Moving to Streaming Algorithm”](ch10.html#ch-opt-mem-example-stream). Another
    example of a “reuse” optimization on the system level is the TCP protocol. It
    offers to keep connections alive for reuse, which also helps with the network
    latency required to establish a new connection.
  prefs: []
  type: TYPE_NORMAL
- en: Be Careful When Reusing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Treating this tip literally is tempting—many try to go as far as reusing every
    little thing, including variables. As we learned in [“Values, Pointers, and Memory
    Blocks”](ch05.html#ch-hw-allocations), variables are boxes that require some memory,
    but usually it’s on the stack, so we should not be afraid to create more of them
    if needed. On the contrary, overusing variables can lead to hard-to-find bugs
    when [we shadow variables](https://oreil.ly/9Dfvb).
  prefs: []
  type: TYPE_NORMAL
- en: Reusing complex structures can also be very dangerous for two reasons:^([4](ch11.html#idm45606819332528))
  prefs: []
  type: TYPE_NORMAL
- en: It is often not easy to reset the state of a complex structure before using
    it a second time (instead of allocating a new one, which creates a deterministic,
    empty structure).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We cannot concurrently use those structures, which can limit further optimizations
    or surprise us and cause data races.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recycle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recycling is a minimum of what we must have in our programs if we use any memory.
    Fortunately, we don’t need anything extra in our Go code, as it’s the built-in
    GC’s responsibility to recycle unused memory to the OS, unless we utilize advanced
    utilities like [“mmap Syscall”](ch05.html#ch-hw-memory-mmap) or other off-heap
    memory techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we can’t “reduce” or “reuse” more memory, we can sometimes optimize
    our code or GC configuration, so the recycling is more efficient for the garbage
    collection. Let’s go through some ways to improve recycling:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimize the structure of the allocated object
  prefs: []
  type: TYPE_NORMAL
- en: If we can’t reduce the number of allocations, maybe we can reduce the number
    of pointers in our objects! However, avoiding pointers is not always possible,
    given popular structures like [`time`](https://oreil.ly/3ZmWi), [`string`](https://oreil.ly/CIoPc),
    or [slices](https://oreil.ly/Ow484), which contain pointers. Especially `string`
    doesn’t look like it, but it is just a special `[]byte`, which means it has a
    pointer to a byte array. In extreme cases, in certain conditions, it might be
    worth changing [`[]string` into `offsets []int` and `bytes []byte`](https://oreil.ly/0zi89)
    to make it a pointer-free structure!
  prefs: []
  type: TYPE_NORMAL
- en: Another widespread example where it’s easy to get very pointer-rich structures
    is when implementing data structures that are supposed to be marshaled and unmarshaled
    to different byte formats like JSON, YAML, or [protobuf](https://oreil.ly/yZVuB).
    It is tempting to use pointers for nested structures to allow optionality of the
    field (the ability to differentiate if the field was set or not). Some code generation
    engines like [Go protobuf generator](https://oreil.ly/SeNub) put all fields as
    pointers by default. This is fine for smaller Go programs, but if we use a lot
    of objects (which is common, especially if we use them for messages over the network),
    we might consider trying to remove pointers from those data structures (many generators
    and marshalers offer that option).
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the number of pointers in our structures is better for GC and can make
    our data structure more L-cache friendly, decreasing the program latency. It also
    increases the chances that the compiler will put the data structure on the stack
    instead of the heap!
  prefs: []
  type: TYPE_NORMAL
- en: The main downside, however, is more overhead when you pass that struct by value
    (copy overhead mentioned in [“Values, Pointers, and Memory Blocks”](ch05.html#ch-hw-allocations)).
  prefs: []
  type: TYPE_NORMAL
- en: GC tuning
  prefs: []
  type: TYPE_NORMAL
- en: 'I mentioned in [“Garbage Collection”](ch05.html#ch-hw-garbage) about two tuning
    options for Go GC: `GOGC` and `GOMEMLIMIT`.'
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting the `GOGC` option from the default 100% value might sometimes positively
    affect your program efficiency. Moving the next GC collection to happen sooner
    or later (depending on need) might be beneficial. Unfortunately, it requires lots
    of benchmarking to find the right number. It also does not guarantee that this
    tuning will work well for all possible states of your applications. On top of
    that, this technique has poor sustainability if you change the critical path in
    your code a lot. Every change requires another tuning session. This is why some
    bigger companies like Google and [Uber](https://oreil.ly/8YMRi) invest in automated
    tools that adjust `GOGC` automatically in runtime!
  prefs: []
  type: TYPE_NORMAL
- en: The `GOMEMLIMIT` is another option you can adjust on top of the `GOGC`. It’s
    a relatively new option for GC to run more frequently when the heap is close to
    or above the desired soft memory limit.
  prefs: []
  type: TYPE_NORMAL
- en: See [a more detailed guide on GC tuning](https://oreil.ly/3nGzV) with the interactive
    visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: Triggering GC and freeing OS memory manually
  prefs: []
  type: TYPE_NORMAL
- en: In extreme cases, we might want to experiment with manually triggered GC collections
    using `runtime.GC()`. For example, we might want to trigger GC manually after
    an operation that allocated a lot of memory and no longer reference it. Note that
    a manual GC trigger is usually a strong anti-pattern, especially in libraries
    as it has global effects.^([5](ch11.html#idm45606819295376))
  prefs: []
  type: TYPE_NORMAL
- en: Allocating objects off-heap
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned trying to allocate objects on the stack first instead of the heap.
    But the stack and heap are not our only options. There are ways to allocate memory
    off-heap, so that it’s outside of the Go runtime’s responsibility to manage.
  prefs: []
  type: TYPE_NORMAL
- en: We can achieve that with [the explicit `mmap` syscall](https://oreil.ly/yko2o)
    we learned in [“mmap Syscall”](ch05.html#ch-hw-memory-mmap). Some have even tried
    [calling C functions like `jemalloc` through the CGO](https://oreil.ly/6se5i).
  prefs: []
  type: TYPE_NORMAL
- en: While possible, we need to acknowledge that doing this can be compared to reimplementing
    parts of the Go Allocator from scratch, not to mention dealing with the manual
    allocations and lack of memory safety. It is the last thing we might want to try
    for the ultimate high-performance Go implementation!
  prefs: []
  type: TYPE_NORMAL
- en: On the bright side, this space is continuously improving. At the time of writing
    this book, the Go team approved and implemented an exciting [proposal](https://oreil.ly/jXgHY)
    behind the `GOEXPERIMENT=arena` environment variable. It allows allocating a set
    of objects from the contiguous region of memory (`arena`) that lives outside of
    heap regions managed by GC. As a result, we will be able to isolate, track, and
    quickly release that memory explicitly when we need it (e.g., when an HTTP request
    is handled) without waiting or paying for garbage collection cycles. What’s special
    about `arenas` is that it’s meant to panic your program when you accidentally
    use the memory that was unused before assuring a certain level of memory safety.
    I can’t wait to start playing with it once it is released—it might mean safe and
    easier-to-use off-heap optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking and measuring all the effects of these optimizations is essential
    before trying any recycle improvements on our production code. Some of these can
    be considered tricky to maintain and unsafe if used without extensive tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, keep the three Rs method in mind, ideally in the same order: reduce,
    reuse, and recycle. Let’s now dive into some common Go optimizations I have seen
    in my experience. Some of them might surprise you!'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t Leak Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Resource leak is a common problem that reduces the efficiency of our Go programs.
    The leak occurs when we create some resource or background goroutine, and after
    using it, we want it to get released or stopped, but it is accidentally left behind.
    This might not be noticeable on a smaller scale, but sooner or later this can
    become a large and hard-to-debug issue. I suggest always clearing something you
    created, even if you expect to exit the program in the next cycle!^([6](ch11.html#idm45606819276864))
  prefs: []
  type: TYPE_NORMAL
- en: “This Program Has a Memory Leak!”
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not every higher memory utilization behavior can be considered a leak. For example,
    we could generally “waste” more memory for some operations, resulting in a spike
    in heap usage, but it gets cleared at some point.
  prefs: []
  type: TYPE_NORMAL
- en: Technically a leak is only when, for the same amount of load on the program
    (e.g., the same amount of HTTP traffic for a long-living service), we use an unbounded
    amount of resources (e.g., disk space, memory, rows in the database), which eventually
    run out.
  prefs: []
  type: TYPE_NORMAL
- en: There are cases of unexpected nondeterministic memory usage on the edge of the
    leak and waste. These are sometimes called pseudomemory leaks, and we will discuss
    some of them in [“Overusing Memory with Arrays”](#ch-basic-subslice).
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps we might think that memory should be an exception to this rule. The
    stack memory is automatically removed, and the garbage collection in Go dynamically
    removes the memory allocated on the heap.^([7](ch11.html#idm45606819271728)) There
    is no way to trigger the cleanup of a memory block other than stop referencing
    it and waiting (or triggering) a full GC cycle. However, don’t let that fool you.
    There are many cases when the Go developer writes code that leaks memory, despite
    eventual garbage collection!
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few reasons our program leaks memory:'
  prefs: []
  type: TYPE_NORMAL
- en: Our program constantly creates custom `mmap` syscalls and never closes them
    (or closes them slower than creating them). This will typically end with a process
    or machine OOM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our program calls too many nested functions, typically infinite or large recursion.
    Our process will then exit with a stack overflow error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are referencing a slice with a tiny length, but we forgot that its capacity
    is very large, as explained in [“Overusing Memory with Arrays”](#ch-basic-subslice).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our program constantly creates memory blocks on the heap, which are always referenced
    by some variables in the execution scope. This typically means we have leaked
    goroutines or infinitely growing slices or maps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s easy to fix memory leaks when we know where they are, but it’s not easy
    to spot them. We often learn about leaks after the fact, when our application
    has already crashed. Without advanced tools like those in [“Continuous Profiling”](ch09.html#ch-obs-cont-profiling),
    we have to hope to reproduce the problem with local tests, which is not always
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: Even with the past heap profile, during the leak, we only see memory in the
    code that allocated memory blocks, not the code that currently references it.^([8](ch11.html#idm45606819262800))
    Some of the memory leaks, especially those caused by leaked goroutines, can be
    narrowed down thanks to the goroutine, but not always.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, a few best practices can proactively prevent us from leaking any
    incompressible resource (e.g., disk space, memory, etc.) and avoid that painful
    leak analysis. Consider the suggestions in this section as something we always
    care for and use as reasonable optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: Control the Lifecycle of Your Goroutines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every time you use the go keyword in your program to launch a goroutine, you
    must know how, and when, that goroutine will exit. If you don’t know the answer,
    that’s a potential memory leak.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dave Cheney, [“Never Start a goroutine Without Knowing How It Will Stop”](https://oreil.ly/eZKzr)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Goroutines are an elegant and clean framework for concurrent programming but
    have some downsides. One is that each goroutine is fully isolated from other goroutines
    (unless we use an explicit synchronization paradigm). There is no central dispatch
    in the Go runtime that we could call and, for example, ask to close the goroutines
    created by the current goroutine (or even check which one it created). This is
    not a lack of maturity of the framework, but rather a design choice allowing goroutines
    to be very efficient. As a trade-off, we have to implement potential code that
    will stop them when the job is done—or, to be specific, the code inside the goroutine
    to stop itself (the only way!).
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution is never to create a goroutine and leave it on its own without
    strict control, even if we think the computation is fast. Instead, when scheduling
    goroutines, think about two aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: How to stop them
  prefs: []
  type: TYPE_NORMAL
- en: We should always ask ourselves when the goroutine will finish. Will it finish
    on its own, or do I have to trigger the finish using context, channels, and so
    on (as in the examples that follow)? Should I be able to abort the goroutine long
    execution if, e.g., the request was cancelled?
  prefs: []
  type: TYPE_NORMAL
- en: Should my function wait for the goroutine to finish?
  prefs: []
  type: TYPE_NORMAL
- en: Do I want my code to continue the execution without waiting for my goroutines
    to finish? Usually, the answer is no, and you should wait for the goroutine to
    stop, for example, using channels [`sync.WaitGroup`](https://oreil.ly/PQHom) (e.g.,
    in [Example 10-10](ch10.html#code-sum-concurrent1)), [`errgroup`](https://oreil.ly/G1Aqx),
    or the excellent [`run.Group`](https://oreil.ly/B1ABL) abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: There are many cases where it feels safe just to let the goroutines “eventually”
    stop, but in practice, not waiting for them has dangerous consequences. For example,
    consider the HTTP server handler that computes some number asynchronously in [Example 11-2](#code-leakhandle1).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-2\. Showcase of a common leak in a concurrent function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Small function simulating longer computation. Imagine it takes around two seconds
    to complete all.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a handler that schedules asynchronous computation.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimization_patterns_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Our code does not depend on someone closing the channel, but as a good practice,
    the sender closes it.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_optimization_patterns_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: If cancellation happens, we return immediately. Otherwise, we wait for the result.
    At first glance, the above code does not look too bad. It feels like we control
    the lifecycle of the scheduled goroutine.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_optimization_patterns_CO2-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the detail is hidden in more information. We control the lifecycle
    only in a good case (when no cancellation occurs). If our code hits this line,
    we are doing something bad here. We return without caring about the goroutine
    lifecycle. We don’t stop it. We don’t wait for it. Even worse, this is a permanent
    leak, i.e., the goroutine with `ComplexCalculation` will be starved—as no one
    reads from the `respCh` channel.
  prefs: []
  type: TYPE_NORMAL
- en: While the goroutine looks like it’s controlled, it isn’t in all cases. This
    leaky code is commonly seen in the Go codebase because it requires a lot of detailed
    focus to not forget about every little edge case. As a result of these mistakes,
    we tend to delay using goroutines in our Go, as it’s easy to create leaks like
    this.
  prefs: []
  type: TYPE_NORMAL
- en: The worst part about leaks is that our Go program might survive long before
    someone notices the adverse effects of such leaks. For example, running `Handle_VeryWrong`
    and cancelling it periodically will eventually OOM this Go program, but if we
    cancel only from time to time and restart our application periodically, without
    good observability we might never notice it!
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, an amazing tool allows us to discover those leaks at the unit test
    level. Therefore, I suggest using a leak test in every unit (or test file) that
    uses concurrent code. One of them is called [`goleak`](https://oreil.ly/4N4bb)
    from Uber, and its basic use is presented in [Example 11-3](#code-leakhandletest).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-3\. Testing for leaks in [Example 11-2](#code-leakhandle1) code
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create tests that verify cancel behavior. This is where the leak is suspected
    to be triggered.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: To verify goroutine leaks, just defer [`goleak.VerifyNone`](https://oreil.ly/bgcwF)
    at the top of our test. It runs at the end of our test and fails if any unexpected
    goroutine is still running. We can also verify whole package tests using the [`goloak.VerifyTestMain`
    method](https://oreil.ly/zyPjr).
  prefs: []
  type: TYPE_NORMAL
- en: Running such a test causes the test to fail with the output in [Example 11-4](#code-leakhandletest-out).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-4\. Output of two failed runs of [Example 11-3](#code-leakhandletest)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We see the goroutines still running at the end of the test and what they were
    executing.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: If we waited a few seconds after cancelling, we could see that the goroutine
    was still running. However, this time it was waiting on a read from `respCh`,
    which would never happen.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to such an edge case leak is to fix the [Example 11-2](#code-leakhandle1)
    code. So let’s go through two potential solutions in [Example 11-5](#code-leakhandle2)
    that seem to fix the problem, but still leak in some way!
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-5\. (Still) leaking handlers. This time the goroutines left behind
    eventually stop.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The only difference between this code and `HandleVeryWrong` in [Example 11-2](#code-leakhandle1)
    is that we create a channel with a buffer for one message. This allows the computation
    goroutine to push one message to this channel without waiting for someone to read
    it. If we cancel and wait some time, the “left behind” goroutine will eventually
    finish.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: To make things more efficient, we could even implement a `ComplexComputationWithCtx`
    that accepts context, which cancels computation and is no longer needed.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimization_patterns_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Many context-cancelled functions do not finish immediately when the context
    is cancelled. Perhaps context is checked periodically, or some cleanup might be
    needed to revert cancelled changes. In our case, we simulate cleanup wait time
    with sleep.
  prefs: []
  type: TYPE_NORMAL
- en: 'The examples in [Example 11-5](#code-leakhandle2) provide some progress, but
    unfortunately, they still technically leak. In some ways, the leak is only temporary,
    but it can still cause problems for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Unaccounted resource usage.
  prefs: []
  type: TYPE_NORMAL
- en: If we used the `Handle_AlsoWrong` function for request A, then A would cancel.
    As a result, the `ComplexComputation` would accidentally allocate a lot of memory
    after `Handle_AlsoWrong` finished—it would create a confusing situation. Furthermore,
    all observability tools would indicate that a spike of memory happened after request
    A finished, so it would be a false perception that request A is not correlated
    to the memory problem.
  prefs: []
  type: TYPE_NORMAL
- en: Accounting problems can have big consequences on the future scalability of our
    program. For example, imagine that a cancelled request usually takes 200 ms to
    finish. That’s not true—if we accounted for all computations, we would see it’s
    200 ms with, e.g., 1 second for `ComplexComputation` cleanup latency. This calculation
    is very important when predicting resource usage for certain traffic given certain
    machine resources.
  prefs: []
  type: TYPE_NORMAL
- en: We can run out of resources sooner.
  prefs: []
  type: TYPE_NORMAL
- en: Such “left behind” goroutines can still cause OOM as the usage is non-deterministic.
    Continuous runs and cancels can still give the impression that the server is ready
    to schedule another request, and keep adding leaked asynchronous jobs, which can
    eventually starve the program. This situation fits in the leak definition.
  prefs: []
  type: TYPE_NORMAL
- en: Are we sure they finished?
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, leaving behind goroutines gives us no visibility on how long they
    run and if they finished in all edge cases. Perhaps there is a bug that gets them
    stuck at some point.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, I would highly suggest never leaving behind goroutines in your
    code. Fortunately, [Example 11-3](#code-leakhandletest) marks all three functions
    (`Handle_VeryWrong`, `Handle_Wrong`, and `Handle_AlsoWrong`) as leaking, which
    is usually what we want. To fix the leak completely, we can, in our case, always
    wait for the result channel, as presented in [Example 11-6](#code-leakhandle3).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-6\. Version of [Example 11-2](#code-leakhandle1) that is not leaking
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Always reading from the channel allows us to wait for the goroutine stop. We
    also respond to cancel as quickly as possible, thanks to propagating proper context
    to `ComplexComputationWithCtx`.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, be careful when you benchmark concurrent code. Always wait
    in each `b.N` iteration for what you want to define as “an operation.” A common
    leak in benchmarking code with the solution is presented in [Example 11-7](#code-leakbenchmark).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-7\. Showcase of a common leak in benchmarking concurrent code
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we want to benchmark concurrent `ComplexComputation`. Scheduling two
    goroutines might find some interesting slowdowns if any resources are shared between
    those functions. However, these benchmark results are completely wrong. My machine
    shows `1860 ns/op`, but if we look carefully, we will see we don’t wait for any
    of those goroutines to complete. As a result, we only measure the latency needed
    to schedule two goroutines per operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: To measure the latency of two concurrent computations, we have to wait for their
    completion, perhaps with `sync.WaitGroup`. This benchmark shows a much more realistic
    `2000339135 ns/op` (two seconds per operation) result.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimization_patterns_CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: We can also use `goleak` on our benchmarks to verify against leaks! However,
    we need to have a benchmark-specific filter due to this [issue](https://oreil.ly/VTE9t).
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, control your goroutine lifecycle for reliable efficiency now and
    in the future! Ensure the goroutine lifecycle as a reasonable optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Reliably Close Things
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This might be obvious, but if we create some object that is supposed to be
    closed after use, we should ensure we don’t forget or ignore this. We have to
    be extra careful if we create an instance of some `struct` or use a function,
    and we see some kind of “closer,” for example:'
  prefs: []
  type: TYPE_NORMAL
- en: It returns `cancel` or `close` closure, e.g., [`context.WithTimeout`](https://oreil.ly/lmvQd)
    or [`context.WithCancel`](https://oreil.ly/aVkMY).^([9](ch11.html#idm45606817523552))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The returned object has a method with closing, cancelling, or stopping-like
    semantics, e.g., [`io.ReaderCloser.Close()`](https://oreil.ly/7Lyfs), [`time.Timer.Stop()`](https://oreil.ly/V7ba8),
    or TearDown.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some functions do not have a closer method but have a dedicated closing or deleting
    package-level function, e.g., the corresponding “releasing” function for [`os.Create`](https://oreil.ly/a2nt4)
    or [`os.Mkdir`](https://oreil.ly/klgKo) is [`os.Remove`](https://oreil.ly/DPNIA).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we have such a situation, assume the worst: if we don’t call that function
    at the end of using that object, bad things will happen. Some goroutine will not
    finish, some memory will be kept referenced, or worse, our data will not bet saved
    (e.g., in case of `os.File.Close()`). We should try to be vigilant. When we use
    a new abstraction, we should check if it has any closers. Unfortunately, there
    are no linters that would point out if we forgot to call them.^([10](ch11.html#idm45606817514192))'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, that isn’t everything. We can’t just defer a call to `Close`.
    Typically, it also returns the error, which might mean the close could not happen,
    and this situation has to be handled. For example, `os.Remove` failed because
    of permission issues and the file was not removed. If we cannot exit the application,
    retry, or handle the error, we should at least be aware of this potential leak.
  prefs: []
  type: TYPE_NORMAL
- en: Does it mean that `defer` statements are less useful, and we have to have that
    `if err != nil` boilerplate for all closers? Not really. This is when I would
    suggest using the [`errcapture`](https://oreil.ly/ucTUB) and [`logerrcapture`](https://oreil.ly/vb2vn)
    packages. See [Example 11-8](#code-deferclose).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-8\. Examples of closing files with `defer`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Never ignore errors. Especially on a file close, which often flushes some of
    our writes to disk only on `Close`, we lose data on an error.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, we don’t need to give up on the amazing Go `defer` logic. Using
    `errcapture`, we can return an error if `f.Close` returns an error. If `doWithFile_CaptureCloseErr`
    returns an error and we do `Close`, the potential close error will be appended
    to the returned one. This is possible thanks to the return argument `(err error`)
    of this function. This pattern will not work without it!
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimization_patterns_CO8-4)'
  prefs: []
  type: TYPE_NORMAL
- en: We can also log the close error if we can’t handle it.
  prefs: []
  type: TYPE_NORMAL
- en: If we see any project I was involved in (and influenced to impact patterns like
    this), I use `errcapture` in all functions that return errors, and I can defer
    them—a clean and reliable way to avoid some leaks.
  prefs: []
  type: TYPE_NORMAL
- en: Another common example of when we forget to close things is error cases. Suppose
    we have to open a set of files for later use. Making sure we close them is not
    always trivial, as shown in [Example 11-9](#code-errclose).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-9\. Closing files in error cases
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This is often difficult to notice, but if we create more resources that have
    to be closed, or we want to close them in a different function, `defer` can’t
    be used. This is normally fine, but if we want to create three files and we have
    an error when opening the second one, we are leaking resources for the first nonclosed
    file! We cannot just return the files opened so far from `openMultiple_Wrong`
    and an error because the consistent flow is to ignore anything returned if there
    was an error. We typically have to close the already opened file to avoid leaks
    and confusion.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The solution is typically creating a short helper that will iterate over appended
    closers and close them. For example, we use the [`merrors`](https://oreil.ly/icRMt)
    package for convenient error append, because we want to know if any new error
    happened in any `Close` call.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, closing things is very important and considered a good optimization.
    Of course, no single pattern or linter would prevent us from all mistakes, but
    we can do a lot to reduce that risk.
  prefs: []
  type: TYPE_NORMAL
- en: Exhaust Things
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make things more complex, certain implementations require us to do more work
    to release all resources fully. For example, an [`io.Reader`](https://oreil.ly/HR89x)
    implementation might not give the `Close` method, but it might assume that all
    bytes will be read fully. On the other hand, some implementations might have a
    `Close` method, yet still expect us to “exhaust” the reader for efficient use.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular implementations that have such behavior are the [`http.Request`](https://oreil.ly/3Gq9j)
    and [`http.Response`](https://oreil.ly/3L02L) body `io.ReadCloser` from the standard
    library. The problem is shown in [Example 11-10](#code-exhaust).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-10\. An example of the inefficiency of `http/net` Client caused by
    a wrongly handled HTTP response
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine we are designing a function that handles an HTTP response from a [`http.Client.Get`](https://oreil.ly/uB0Vd)
    request. `Get` clearly mentions that the “caller should close resp.Body when done
    reading from it.” This `handle​R⁠esp_Wrong` is wrong because it leaks two goroutines:'
  prefs: []
  type: TYPE_NORMAL
- en: One doing `net/http.(*persistConn).writeLoop`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second doing `net/http.(*persistConn).readLoop`, which is visible when we
    run `BenchmarkClient` with the `goleak`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO10-2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `handleResp_StillWrong` is better, as we stop the main leak. However, we
    still don’t read bytes from the body. We might not need them, but the `net/http`
    implementations can block the TCP connection if we don’t fully exhaust the body.
    Unfortunately, this is not well-known information. It is briefly mentioned in
    the [`http.Client.Do`](https://oreil.ly/RegPv) method description: “If the Body
    is not both read to EOF and closed, the Client’s underlying RoundTripper (typically
    Transport) may not be able to re-use a persistent TCP connection to the server
    for a subsequent ‘keep-alive’ request.”'
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimization_patterns_CO10-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we read until the EOF (end of file), representing the end of whatever
    we are reading. For this reason we created convenient helpers like `ExhaustClose`
    from [`errcapture`](https://oreil.ly/4LhOs) or [`logerrcapture`](https://oreil.ly/XRxyA)
    that do exactly this.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_optimization_patterns_CO10-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Client runs some goroutines for each TCP connection we want to keep alive and
    reuse. We can close them using `CloseIdleConnection` to detect any leaks our code
    might introduce.
  prefs: []
  type: TYPE_NORMAL
- en: I wish structures like `http.Response.Body` were easier to use. The close and
    exhaust need for the body are important and should be used as a reasonable optimization.
    `handleResp_Wrong` fails the `BenchmarkClient` with a leak error. The `handleResp_StillWrong`
    does not leak any goroutine, so the leak test passes. The “leak” is on a different
    level, the TCP level, with the TCP connection being unable to reuse, which can
    cost us extra latency and insufficient file descriptors.
  prefs: []
  type: TYPE_NORMAL
- en: We can see its impact with the results of the `BenchmarkClient` benchmark in
    [Example 11-10](#code-exhaust). On my machine, it takes 265 ms to call `http://google.com`
    with `handleResp_StillWrong`. For the version that cleans all resources in `handleResp_Better`,
    it takes only 188 ms, which is 29% faster!^([11](ch11.html#idm45606816330528))
  prefs: []
  type: TYPE_NORMAL
- en: The need for exhaust is also visible in `http.HandlerFunc` code. We should always
    ensure our server implementation exhausts and closes the `http.Request` body.
    Otherwise, we will have the same problem as in [Example 11-10](#code-exhaust).
    Similarly, this can be true for all sorts of iterators; for example, a [Prometheus
    storage can have a `ChunkSeriesSet` iterator](https://oreil.ly/voRFc). Some implementations
    can leak or overuse resources if we forget to iterate through all items until
    `Next()` equals false.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, always check the implementation for those nontrivial edge cases.
    Ideally, we should design our implementations to have obvious efficiency guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now dive into the pre-allocation technique I mentioned in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-Allocate If You Can
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I mentioned pre-allocation in [“Optimized Code Is Not Readable”](ch01.html#ch-eff-s-readable)
    as a reasonable optimization. I showed how easy it is to pre-allocate a slice
    with `make` in [Example 1-4](ch01.html#code-prea2) as an optimization to `append`.
    Generally, we want to reduce the amount of work that code has to do to resize
    or allocate new items if we know the code has to do it eventually.
  prefs: []
  type: TYPE_NORMAL
- en: The `append` example is important, but there are more examples. It turns out
    that almost every container implementation that cares about efficiency has some
    easier pre-allocation methods. See the ones in [Example 11-11](#code-prea3) with
    explanations.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-11\. Examples of pre-allocation for some common types
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO11-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume we know the size we want to grow the containers up front.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO11-2)'
  prefs: []
  type: TYPE_NORMAL
- en: '`make` with slices allows us to grow the capacity of the underlying arrays
    to the given size. Thanks to the proactive growth of the array with `make`, the
    loop with `append` is much cheaper in CPU time and memory allocation. This is
    because `append` does not need to resize the array when it’s too small.'
  prefs: []
  type: TYPE_NORMAL
- en: Resizing is quite naive. It simply creates a new, bigger array and copies all
    elements. A certain heuristic also tells how many new slices are grown. This heuristic
    was recently [changed](https://oreil.ly/6uIHH), but it will still allocate and
    copy a few times until it extends to our expected one million elements. In our
    case, the same logic is 8 times faster with pre-allocation and allocates 16 MB
    instead of 88 MB of memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimization_patterns_CO11-3)'
  prefs: []
  type: TYPE_NORMAL
- en: We can also pre-allocate the slice’s capacity and length. Both `slice` and `slice2`
    will have the same elements. Both ways are almost equally efficient, so we use
    one that fits more functionally to what we need to do. However, with `slice2`,
    we are using all array elements, whereas in `slice`, we can grow it to be bigger
    but end up using a smaller number if needed.^([12](ch11.html#idm45606816088384))
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_optimization_patterns_CO11-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Map can be created using `make` with an optional number representing its capacity.
    If we know the size up front, it’s more efficient for Go to create the required
    internal data structure with up-front sizes. The efficiency results show the difference—on
    my machine, with pre-allocation, such map initialization takes 87 ms, without
    179 ms! The total allocated space with pre-allocation is 57 MB, without 123 MB.
    However, map insertion can still allocate some memory, just much smaller than
    pre-allocation.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_optimization_patterns_CO11-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Various buffers and builders offer the `Grow` function that also pre-allocates.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding example is actually something I use very often during almost every
    coding session. Pre-allocation usually takes the extra line of code, but it is
    a fantastic, more readable pattern. If you are still not convinced that you won’t
    have a lot of situations when you know the size up front for the slice, let’s
    talk about `io.ReadAll`. We use [`io.ReadAll`](https://oreil.ly/TN7bt) (previously
    [`ioutil.ReadAll`](https://oreil.ly/nt1oT)) functions in the Go community a lot.
    Did you know you can optimize it significantly by pre-allocating the internal
    byte slice if you know the size up front? Unfortunately, `io.ReadAll` does not
    have a `size` or `capacity` argument, but there is a simple way to optimize it,
    as presented in [Example 11-12](#code-prea4).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-12\. Examples of ReadAll optimizations with the benchmark
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO12-1)'
  prefs: []
  type: TYPE_NORMAL
- en: One way of simulating `ReadAll` is by creating a pre-allocated buffer and using
    `io.Copy` to copy all bytes.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO12-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Even more efficient is pre-allocating a byte slice and using `ReadFull`, which
    is similar. `ReadAll` does not use the `io.EOF` error sentinel if everything is
    read, so we need special handling for it.
  prefs: []
  type: TYPE_NORMAL
- en: The results, presented in [Example 11-13](#code-prea4-res), speak for themselves.
    The `ReadAll2` using `io.ReadFull` is over eight times faster and allocates five
    times less memory for our one million byte slice.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-13\. Results of the benchmark in [Example 11-12](#code-prea4)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `io.ReadAll` optimization is very often possible in our Go code. Especially
    when dealing with HTTP code, the request or response headers often offer a `Content-Length`
    header that allows pre-allocations.^([13](ch11.html#idm45606815217568)) The preceding
    examples represent only a small subset of types and abstractions that allow pre-allocation.
    Check the documentation and code of the type we use if we can average eager allocations
    for better efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is one more amazing pre-allocation pattern I would like you to
    know. Consider a simple, singly linked list. If we implement it using pointers,
    and if we know we will insert millions of new elements on that list, is there
    a way to pre-allocate things for efficiency? Turns out there might be, as shown
    in [Example 11-14](#code-prea5).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-14\. Basic pre-allocation of linked list elements
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO13-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This line makes this linked list a bit special. We maintain a pool of objects
    in the form of one slice.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO13-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to the pool, we can implement our own `Grow` method, which will allocate
    a pool of many `Node` objects within one allocation. Generally, it’s way faster
    to allocate one large `[]Node` than millions of `*Node`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimization_patterns_CO13-3)'
  prefs: []
  type: TYPE_NORMAL
- en: During the insert, we can check if we have room in our pool and take one element
    from it instead of allocating an individual `Node`. This implementation can be
    expanded to be more robust, e.g., for subsequent growth, if we hit the capacity
    limit.
  prefs: []
  type: TYPE_NORMAL
- en: If we benchmarked the insertion of one million elements using the preceding
    linked list, we would see that the insertion takes four times less time with one
    eager allocation and the same space with just one allocation instead of one million.
  prefs: []
  type: TYPE_NORMAL
- en: The simple pre-allocation with slices and maps presented in [Example 11-11](#code-prea3)
    have almost no downsides, so they can be treated as reasonable optimizations.
    The pre-allocation presented in [Example 11-14](#code-prea5), on the other hand,
    should be done with care, deliberately, and with benchmarks as it’s not without
    trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: First, the problem is that potential deletion logic or allowing the `Grow` call
    multiple times is not trivial to implement. The second issue is that a single
    `Node` element is now connected to a very large single memory block. Let’s dive
    into this problem in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Overusing Memory with Arrays
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you probably know, slices are very powerful in Go. They offer [robust flexibility
    for using arrays](https://oreil.ly/YhOdH) that is used daily in the Go community.
    But with power and flexibility comes responsibility. There are many cases where
    we might end up overusing memory, which some might call a “memory leak.” The main
    problem is that those cases will never appear in [“Go Benchmarks”](ch08.html#ch-obs-micro-go),
    because it’s related to garbage collection and will not release memory we thought
    could be released. Let’s explore this problem in [Example 11-15](#code-overuse-mem),
    which tests potential deletion in `SinglyLinkedList` introduced in [Example 11-14](#code-prea5).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-15\. Reproducing memory overuse for a linked list that used pre-allocation
    in [Example 11-14](#code-prea5)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO14-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s add deletion logic to the linked list, which removes the given element.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO14-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Using a microbenchmark to assess the efficiency of `Delete` would show us that
    when `Grow` was used, the deletion was only marginally faster. However, to showcase
    the memory overuse problem, we would need the macrobenchmarks test (see [“Macrobenchmarks”](ch08.html#ch-obs-macro)).
    Alternatively, we can write a brittle interactive test as we did here.^([14](ch11.html#idm45606814784544))
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimization_patterns_CO14-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Notice we are trying our best for the GC to remove the deleted node. However,
    we `nil` the `pool` variable, so the slice we used to create all nodes in the
    list is not referenced anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_optimization_patterns_CO14-4)'
  prefs: []
  type: TYPE_NORMAL
- en: We use a manual trigger for the GC and print of the heap, which is not very
    reliable generally as it contains allocations from background runtime work. However,
    it’s good enough here to show us the problem. The pre-allocated list showed 15,818.5
    KB in one of the runs, and 15,813.0 KB for the run without `Grow`. Don’t look
    at the difference between those, but how this value changed for pre-allocated.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_optimization_patterns_CO14-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s remove all but one element.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_optimization_patterns_CO14-6)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a perfect world, we would expect to hold only memory for one `Node`, right?
    This is the case for the non-pre-allocated list—189.85 KB on the heap. On the
    other hand, for the pre-allocated list, we can observe a certain problem: the
    heap is still big, with 15,831.2 KB on it!'
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_optimization_patterns_CO14-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Only after all the elements do we see a small heap size for both cases (around
    190 KB for both).
  prefs: []
  type: TYPE_NORMAL
- en: This problem is important to understand, and we have it every time we work with
    structs with arrays. The representation of what happens when all but one element
    is deleted in both cases is shown in [Figure 11-1](#img-opt-overuse-mem).
  prefs: []
  type: TYPE_NORMAL
- en: '![efgo 1101](assets/efgo_1101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-1\. The heap’s state with references with one node in the list. On
    the left, created without a pool, on the right with it.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When we allocate an individual object, we see that it receives its own memory
    block that can be managed in isolation. If we use pooling or subslicing (e.g.,
    `buf[1:2]`) from a bigger slice, the GC will see that the big memory block for
    continuous memory used by the array is referenced. It’s not smart enough to see
    that only 1% of it is used and could be “clipped.”
  prefs: []
  type: TYPE_NORMAL
- en: The solution is to avoid pooling or come up with a more advanced pool that can
    be grown or shrunk (maybe even automatically). For example, if half of the objects
    are deleted, we can “clip” the array behind our linked list nodes. Alternatively,
    we can add the on-demand `ClipMemory` method, as presented in [Example 11-16](#code-overuse-mem2).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-16\. Example implementation of clipping too-big memory block
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO15-1)'
  prefs: []
  type: TYPE_NORMAL
- en: At this moment, we get rid of the reference to the old `[]Node` slice and create
    a smaller one.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO15-2)'
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in [Figure 11-1](#img-opt-overuse-mem), there are still other references
    to bigger memory blocks from each element in the list. So we need to perform a
    copy using a new pool of objects to ensure the GC can remove that old bigger pool.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimization_patterns_CO15-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s not forget about the last pointer, `l.head`, which would otherwise still
    point to the old memory block.
  prefs: []
  type: TYPE_NORMAL
- en: We can now use the `ClipMemory` when we delete some items to resize the underlying
    memory block.
  prefs: []
  type: TYPE_NORMAL
- en: As presented in [Example 11-15](#code-overuse-mem), the overuse of memory is
    more common than we might think. However, we don’t need such specific pooling
    to experience it. Subslicing and using clever zero copy functions like in [Example 10-4](ch10.html#code-sum3)
    (`zeroCopyToString`) are very much prone to this problem.^([15](ch11.html#idm45606814531920))
  prefs: []
  type: TYPE_NORMAL
- en: This section is not to demotivate you from pre-allocating things, subslicing,
    or experimenting with reusing byte slices. Rather it’s a reminder to always keep
    in mind how Go manages memory (as discussed in [“Go Memory Management”](ch05.html#ch-hw-go-mem))
    when we attempt to do more advanced things with slices and underlying arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that Go benchmarking does not cover memory usage characteristics, as
    mentioned in [“Microbenchmarks Versus Memory Management”](ch08.html#ch-obs-micro-mem).
    Move to the [“Macrobenchmarks”](ch08.html#ch-obs-macro) level to verify all efficiency
    aspects if you suspect you are affected by this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Since we mentioned pooling, let’s dive into the last section. What are the other
    ways to reuse and pool memory in Go? It turns out that sometimes not pooling anything
    might be better!
  prefs: []
  type: TYPE_NORMAL
- en: Memory Reuse and Pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Memory reuse allows using the same memory blocks for subsequent operations.
    If the operation we perform requires a bigger `struct` or `slice` and we perform
    a lot of them in a quick sequence, it’s wasteful to allocate a new memory block
    every time because:'
  prefs: []
  type: TYPE_NORMAL
- en: Allocation of memory with guaranteed zero-ing of the memory block takes CPU
    time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We put more work into the GC, so more CPU cycles are used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GC is eventual, so our maximum heap size can grow uncontrollably.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I already presented some memory reuse techniques in [Example 10-8](ch10.html#code-sum6),
    using a small buffer to process files chunk by chunk. Then, in [Example 11-14](#code-prea5),
    I showed how we could allocate one bigger memory block at once and use that as
    our pool of objects.
  prefs: []
  type: TYPE_NORMAL
- en: The logic of reusing objects, especially byte slices, is often enabled by many
    popular implementations, such as `io.CopyBuffer` or `io.ReadFull`. Even our `Sum6Reader​(r
    ⁠io.Reader, buf []byte)` from [Example 10-8](ch10.html#code-sum6) allows further
    reuse of the buffer. However, memory reuse is not always so easy. Consider the
    following example of byte slice reuse in [Example 11-17](#code-reuse1).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-17\. Simple buffering or byte slice
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO16-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Because our logic uses `append`, we need to zero the length of the slice while
    reusing the same underlying array for efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO16-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We can simulate no buffer by simply passing `nil`. Fortunately, Go handles nil
    slices in the operations like `buf[:0]` or `append([]byte(nil), 'a')`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimization_patterns_CO16-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Reusing the buffer is better in this case. On my machine, benchmarks show that
    each operation with reused buffer is almost two times faster and allocates zero
    bytes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding example looks excellent, but the real code contains complications
    and edge cases. Two main problems sometimes block us from implementing such naive
    memory reuse, as in [Example 11-17](#code-reuse1):'
  prefs: []
  type: TYPE_NORMAL
- en: We know the buffer size will be similar for most operations, but we don’t know
    the exact number. This can be easily fixed by passing an empty buffer and reusing
    the grown underlying array from the first operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We might run the `processUsingBuffer` code concurrently at some point. Sometimes
    with four workers, sometimes with one thousand, sometimes with one. In this case,
    we could implement this by maintaining a static number of buffers. The number
    could be the maximum goroutines we want to run concurrently or less with some
    locking. This obviously can have a lot of waste if the number of goroutines is
    dynamically changing and is sometimes zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For those reasons, the Go team came up with the [`sync.Pool`](https://oreil.ly/BAQwU)
    structure that performs a particular form of memory pooling. It’s important to
    understand that memory pooling is not the same as typical caching.
  prefs: []
  type: TYPE_NORMAL
- en: 'The type that Brad Fitzpatrick requested [`sync.Pool`] is actually a pool:
    A set of interchangeable values where it doesn’t matter which concrete value you
    get out, because they’re all identical. You wouldn’t even notice when, instead
    of getting a value from the pool, you get a newly created one. Caches, on the
    other hand, map keys to concrete values.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dominik Honnef, [“What’s Happening in Go Tip”](https://oreil.ly/z6AUf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The `sync.Pool` from the standard library is implemented purely as a very short,
    temporary cache for the same type of free memory blocks that last until more or
    less the next GC invocation. It uses quite smart logic that makes it thread-safe
    yet avoids locking as much as possible for efficient access. The main idea behind
    `sync.Pool` is to reuse memory that the GC did not yet release. Since we keep
    those memory blocks around until eventual GC, why not make them accessible and
    useful? The example of using `sync.Pool` in [Example 11-17](#code-reuse1) is presented
    in [Example 11-18](#code-reuse2).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-18\. Simple buffering using `sync.Pool`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO17-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`sync.Pool` pools an object of the given type, so we must cast it to the type
    we put or create. When `Get` is involved, we either allocate a new object or use
    one of the pooled ones.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO17-2)'
  prefs: []
  type: TYPE_NORMAL
- en: To use the pool effectively, we need to put back the object to reuse. Remember
    to never put back the object you are still using to avoid races!
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimization_patterns_CO17-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The `New` closure specifies how a new object will be created.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_optimization_patterns_CO17-4)'
  prefs: []
  type: TYPE_NORMAL
- en: For our example, the implementation with `sync.Pool` is very efficient. It’s
    over 2 times faster than without reuse, with an average of 2 KB of space allocated
    versus 5 MB allocated per operation from code that does not reuse the buffer.
  prefs: []
  type: TYPE_NORMAL
- en: While results look very promising, pooling using `sync.Pool` is a more advanced
    optimization that can bring more efficiency bottlenecks than optimizations if
    wrongly used. The first problem is that, as with any other complex structure that
    works with slices, using it is prone to errors. Consider the code with benchmark
    in [Example 11-19](#code-reuse3).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-19\. Common, hard-to-spot bug while using `sync.Pool` and `defer`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO18-1)'
  prefs: []
  type: TYPE_NORMAL
- en: There is a bug in this function that defies the point of using `sync.Pool`—`Get`
    will always allocate an object in our case. Can you spot it?
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that the `Put` might be deferred to the correct time, but its
    argument is evaluated at the moment of the `defer` schedule. As a result, the
    `buf` variable we are putting might point to a different slice if `append` will
    have to grow it.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO18-2)'
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the benchmark will show that this `processUsingPool_Wrong` operation
    is twice as slow as the `alloc` case in [Example 11-17](#code-reuse1) that always
    allocates. Using `sync.Pool` to only `Get` and never `Put` is slower than straight
    allocation (`make([]byte)` in our case).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the real difficulty comes from the specific `sync.Pool` characteristic:
    it only pools objects for a short duration, which is not reflected by our typical
    microbenchmark like in [Example 11-18](#code-reuse2). We can see the difference
    if we trigger GC manually in our benchmark, done for demonstration in [Example 11-20](#code-reuse3-gc).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-20\. Common, hard-to-spot bug while using `sync.Pool` and `defer`,
    triggering GC manually
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO19-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The second surprise comes from the fact that in our initial benchmarks, the
    `process*` operations are performed quickly, one after another. However, on a
    macro level that might not be true. This is fine for `processUsingBuffer`. If
    the GC runs once or twice in the meantime for our simple buffered solution, the
    allocation and latency (adjusted with GC latency) stay the same because we keep
    the memory references in our `buf` variable. The next `processUsingBuffer` will
    be as fast as always.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimization_patterns_CO19-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This is not the case for the standard pool. After two GC runs, the `sync.Pool`
    is, by design, fully cleaned from all objects,^([16](ch11.html#idm45606813445248))
    which results in performance worse than `alloc` in [Example 11-17](#code-reuse1).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it’s fairly easy to make mistakes using `sync.Pool`. The fact
    that it does not preserve the pool after garbage collection might be beneficial
    in cases where we don’t want to keep pooled objects for a longer duration. However,
    in my experience, it makes it very hard to work with due to nondeterministic behavior
    caused by the combination of nontrivial `sync.Pool` implementation with an even
    more complex GC schedule.
  prefs: []
  type: TYPE_NORMAL
- en: 'To show the potential damage when `sync.Pool` is applied to the wrong workloads,
    let’s try to optimize the memory use of the `labeler` service from [“Go e2e Framework”](ch08.html#ch-obs-macro-example)
    using optimized buffered code from [Example 10-8](ch10.html#code-sum6) and four
    different buffering techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '`no-buffering`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Sum6Reader` without buffering—always allocates a new buffer.'
  prefs: []
  type: TYPE_NORMAL
- en: '`sync-pool`'
  prefs: []
  type: TYPE_NORMAL
- en: With `sync.Pool`.
  prefs: []
  type: TYPE_NORMAL
- en: '`gobwas-pool`'
  prefs: []
  type: TYPE_NORMAL
- en: With [`gobwas/pool`](https://oreil.ly/VZjYW) that maintains multiple buckets
    of `sync.Pool`. In theory, it should work well for byte slices that might require
    different buffer sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '`static-buffers`'
  prefs: []
  type: TYPE_NORMAL
- en: With four static buffers that offer a buffer for a maximum of four goroutines.
  prefs: []
  type: TYPE_NORMAL
- en: The main problem is that the [Example 10-8](ch10.html#code-sum6) workload might
    not look immediately like a wrong fit. The small allocation of `make([]byte, 8*1024)`
    per operation is the only one we make during the computation, so pooling to save
    the total memory usage might feel like a valid choice. The microbenchmark also
    shows amazing results. The benchmarks perform sequential `Sum6` operations on
    two different files (50% of the time, we use files with 10 million numbers, 50%
    with 100 million). The results are shown in [Example 11-21](#code-labeler2-micro).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-21\. The microbenchmark results with one hundred iterations that
    compare labeler `labelObject` logic using [Example 10-8](ch10.html#code-sum6)
    and four different buffering versions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimization_patterns_CO20-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The bucketed pool is slightly more memory intensive, but this is expected, as
    two separate pools are maintained. However, ideally, we expect to see larger benefits
    from that split on a larger scale.
  prefs: []
  type: TYPE_NORMAL
- en: We see that the `sync.Pool` version and static buffer are winning in terms of
    memory allocations. The latency is more or less similar, given most of [Example 10-8](ch10.html#code-sum6)
    is spent on integer parsing, not allocating the buffer.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, on the macro level, for a 5-minute test per version with 2 virtual
    users in `k6s` performing a sum on 10 million lines and then 100 million line
    files, we see that the reality is different than what [Example 11-21](#code-labeler2-micro)
    showed. What’s good is that the `labeler` without buffering allocates significantly
    more (3.3 GB in total) during that load than other versions (500 MB on average),
    as visible in [Figure 11-2](#img-labeler2-alloc).
  prefs: []
  type: TYPE_NORMAL
- en: '![efgo 1102](assets/efgo_1102.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-2\. The Parca Graph for the total memory allocated during macrobenchmark
    from heap profiles. Four lines indicate runs of four different versions in order:
    `no-buffering`, `sync-pool`, `gobwas-pool`, and `static-buffers`.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: However, it seems that such allocations are not a huge problem for the GC, as
    the simplest, no buffering solution `labelObject1` has similar average latency
    to others (same CPU usage as well), but also the lowest maximum heap usage, as
    visible in [Figure 11-3](#img-labeler2-use).
  prefs: []
  type: TYPE_NORMAL
- en: '![efgo 1103](assets/efgo_1103.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-3\. The Prometheus Graph for the heap size during the macrobenchmark.
    Four lines indicate runs of four different versions in order: `no-buffering`,
    `sync-pool`, `gobwas-pool`, and `static-buffers`.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can reproduce the whole experiment thanks to [the `e2e` framework code
    in the example repo](https://oreil.ly/9vDNZ). The results were not satisfying,
    but the experiment can give us a lot of lessons:'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing allocations might be the easiest way to improve latency and memory
    efficiency, but not always! Clearly, in this case, higher allocations were better
    than pooling. One reason is that the `Sum6` in [Example 10-8](ch10.html#code-sum6)
    was already heavily optimized. The CPU profile of `Sum6` in [Example 10-8](ch10.html#code-sum6)
    clearly shows that allocation is not a latency bottleneck. Secondly, the slower
    allocation pace caused the GC to kick in less often, allowing generally higher
    maximum memory usage. Additional `GOGC` tuning might have helped here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The microbenchmarking does not always show the full picture. So always assess
    efficiency on multiple levels to be sure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `sync.Pool` helps the most with allocation latency, not with maximum memory
    usage, as our goal here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Optimization Journey Can Be a Roller Coaster!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes we achieve improvement, and sometimes we spend a few days on change
    that can’t be merged. We all learn every day, try things, and sometimes fail.
    What’s most important is to fail early, so the less efficient version is not accidentally
    released to our users!
  prefs: []
  type: TYPE_NORMAL
- en: 'The main issue of this experiment is that the `sync.Pool` is not designed for
    the type of workload that `labeler` represents. The `sync.Pool` have very specific
    use cases. Use it when:'
  prefs: []
  type: TYPE_NORMAL
- en: You want to reuse large or extreme amounts of objects to reduce the latency
    of those allocations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don’t care about the object content, just its memory blocks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to reuse those objects from multiple goroutines, which can vary in
    number.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to reuse objects between quick computations that frequently happen
    (maximum one GC cycle away).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, `sync.Pool` works great when we want to pool objects for an [extremely
    fast pseudorandom generator](https://oreil.ly/9mvAE). The HTTP servers use [many
    different pools of bytes](https://oreil.ly/TpzMN) to reuse bytes for reading from
    the network.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, in my experience, the `sync.Pool` is overused. The perception
    is that the `sync.Pool` is in the standard library, so it must be handy, but that
    isn’t always true. The `sync.Pool` has a very narrow use case, and there are high
    chances it’s not what we want.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, I prefer simple optimization first. The more clever the optimization
    is, the more vigilant we should be and the more benchmarking effort we should
    make. The `sync.Pool` structure is one of the more complex solutions. I would
    recommend looking at easier solutions first, e.g., a simple static reusable buffer
    of memory, as in [Example 11-17](#code-reuse1). My recommendation is to avoid
    `sync.Pool` until you are sure your workloads match the use cases mentioned previously.
    In most cases, after reduced work and allocations, adding `sync.Pool` will only
    make your code less efficient, brittle, and harder to assess its efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That’s it. You made it to the end of this book, congratulations! I hope it was
    a fantastic and valuable journey. I know it was for me!
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps, if you have made it this far, the world of pragmatic, efficient software
    is much more accessible for you than it was before opening this book. Or perhaps
    you see how all the details on how we write our code and design our algorithms
    can impact the software efficiency, which can translate to real cost in the long
    run.
  prefs: []
  type: TYPE_NORMAL
- en: In some ways, this is extremely exciting. With one deliberate change and the
    right observability tools to assess it, we can sometimes save millions of dollars
    for our employer, or enable use cases or customers that were not possible before.
    But, on the other hand, it is quite scary how easy it is to waste that money on
    silly mistakes like leaking a few goroutines or not pre-allocating some slices
    on critical paths.
  prefs: []
  type: TYPE_NORMAL
- en: My advice for you, if you are more on the “scared” side, is…​to relax! Remember
    that nothing in the world is perfect, and our code can’t be perfect either. It’s
    good to know in what direction to turn to for perfection, but as the saying goes,
    [“Perfect is the enemy of good”](https://oreil.ly/OogZF), and there has to be
    a moment when the software is “good enough.” In my opinion, this is the key difference
    between the professional, pragmatic, everyday efficiency practices I wanted to
    teach you here and Donald Knuth’s “premature optimization is the root of all evil”
    world. This is also why my book is called *Efficient Go* and not *Ultra-Performance,
    Super Fast Go*.
  prefs: []
  type: TYPE_NORMAL
- en: I think the pragmatic car mechanic profession could be a good comparison to
    the pragmatic efficiency-aware software developer (sorry for my car analogies!).
    Imagine a passionate and experienced mechanical engineer with huge experience
    in building F1 cars—one of the fastest racing automobiles in the world. Imagine
    they work at the auto workshop, and a customer goes there with some standard saloon
    car that has an oil leak. Even with the greatest knowledge about making the car
    extremely fast, the pragmatic mechanic would fix the oil leak, double-check the
    whole car if there was anything wrong with it, and that’s it. However, if the
    mechanic starts to tune the customer’s car for faster acceleration, better air
    efficiency, and braking performance, you can imagine the customer would not be
    satisfied. Better car performance would probably make the customer happy, but
    this always comes with an extreme bill for work hours, expensive parts, and delayed
    time to repair.
  prefs: []
  type: TYPE_NORMAL
- en: Follow the same rules as you would expect from your mechanic. Do what’s needed
    to be done to satisfy functional and efficiency goals. This is not being lazy;
    it’s being pragmatic and professional. No optimization is premature if we do this
    within the premise of requirements.
  prefs: []
  type: TYPE_NORMAL
- en: That’s why my second piece of advice is to always set some goals. Look how (in
    some sense) “easy” it was to assess if the `Sum` optimizations in [Chapter 10](ch10.html#ch-opt)
    were acceptable or not. One of the biggest mistakes I made in most of my software
    projects was to ignore or procrastinate on setting clear, ideally written, data-driven
    goals for the project’s expected efficiency. Even if it’s obvious, note, “I expect
    this functionality to finish in one minute.” You can iterate on better requirements
    later on! Without clear goals, every optimization is potentially premature.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, my third bit of advice is to invest in good observability tools. I
    was lucky that during my daily job for the last few years, the teams I worked
    with delivered observability software. Furthermore, those observability tools
    are *free* in open source, and every reader of this book can install them right
    now. I can’t imagine not having the tools mentioned in [Chapter 6](ch06.html#ch-observability).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, I also see, as a tech leader of [the CNCF interest group
    observability](https://oreil.ly/yJKg4), and speaker and attendee of technical
    conferences, how many developers and organizations don’t use observability tools.
    They either don’t observe their software or don’t use those tools correctly! That
    is why it’s very hard for those individuals or organizations to pragmatically
    improve the efficiency of their programs.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t get distracted by overhyped solutions and vendors who promise shiny observability
    solutions for a high price.^([18](ch11.html#idm45606813281824)) Instead, I would
    recommend starting small with open source monitoring and observability solutions
    like [Prometheus](https://oreil.ly/2Sa3P), [Loki](https://oreil.ly/Fw9I3), [OpenSearch](https://oreil.ly/RohpZ),
    [Tempo](https://oreil.ly/eZ2Gy), or [Jaeger](https://oreil.ly/q5O8u)!
  prefs: []
  type: TYPE_NORMAL
- en: Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout this book, we went through all the elements required to become effective
    with the efficiency development of Go if required. Particularly:'
  prefs: []
  type: TYPE_NORMAL
- en: We discussed motivation for efficient programs and introduction in [Chapter 1](ch01.html#ch-efficiency-matters).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We walked through the foundational aspects of Go in [Chapter 2](ch02.html#ch-go).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discussed challenges, optimizations, RAER, and TFBO in [Chapter 3](ch03.html#ch-efficiency).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I explained the two most important resources we optimize for: the CPU in [Chapter 4](ch04.html#ch-hardware)
    and memory in [Chapter 5](ch05.html#ch-hardware2). I also mentioned latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discussed observability and common instrumentation in [Chapter 6](ch06.html#ch-observability).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We walked through data-driven efficiency analysis, complexities, and reliability
    of experiments in [Chapter 7](ch07.html#ch-observability2).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discussed benchmarking in [Chapter 8](ch08.html#ch-benchmarking).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I introduced the topic of profiling, which helps with bottleneck analysis in
    [Chapter 9](ch09.html#ch-observability3).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we optimized various code examples in [Chapter 10](ch10.html#ch-opt)
    and summarized common patterns in [Chapter 11](#ch-opt2).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, as with everything, there is always more to learn if you are interested!
  prefs: []
  type: TYPE_NORMAL
- en: First, I skipped some aspects of the Go language that were not strictly related
    to the efficiency topic. To learn more about those, I would recommend reading
    [“Practical Go Lessons”](https://oreil.ly/VnFms) authored by Maximilien Andile
    and…​practicing writing Go programs for realistic goals for work or as a fun side
    project.^([19](ch11.html#idm45606813255952))
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, hopefully, I enabled you to understand the underlying mechanisms
    of the resources you are optimizing for. One of the next steps to becoming better
    at software efficiency is to learn more about other resources we commonly optimize
    for, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: Disk
  prefs: []
  type: TYPE_NORMAL
- en: We use disk storage every day in our Go programs. The way OS handles reads or
    writes to it can be similarly complex, as you saw in [“OS Memory Management”](ch05.html#ch-hw-memory-os).
    Understanding disk storage better (e.g., the [SSD](https://oreil.ly/3mjc6) characteristics)
    will make you a better developer. If you are curious about the alternative optimizations
    to disk access, I would also recommend reading about the [`io_uring` interface
    that comes with the new Linux kernels](https://oreil.ly/Sxagc). It might allow
    you to build even better concurrency for your Go programs using a lot of disk
    access.
  prefs: []
  type: TYPE_NORMAL
- en: Network
  prefs: []
  type: TYPE_NORMAL
- en: Reading more about the network constraints like latency, bandwidth, and different
    protocols will make you more aware of how to optimize your Go code that is constrained
    by network limitations.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs and FPGA
  prefs: []
  type: TYPE_NORMAL
- en: For more on offloading some computations to external devices like [GPUs](https://oreil.ly/yEi43)
    or [programmable hardware](https://oreil.ly/1dPXO), I would recommend [cu](https://oreil.ly/T8q9A),
    which uses the popular [CUDA API](https://oreil.ly/PXZhH) for the NVIDIA GPUs,
    or this [guide](https://oreil.ly/v3dty) to run Go on Apple M1 GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thirdly, while I might add more optimization examples in the next editions
    of this book, the list will never be complete. This is because some developers
    might want to try many more or less extreme optimizations for some specific part
    of their programs. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Something I wanted to talk about but could not fit into this book is the importance
    of error path and [instrumentation efficiency](https://oreil.ly/2IoAP). Choosing
    efficient interfaces for your metrics, logging, tracing, and profiling instrumentations
    can be important.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory alignment and [struct padding optimizations](https://oreil.ly/r1aJn)
    with tools like [`structslop`](https://oreil.ly/IuWGN).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using more efficient [string encodings](https://oreil.ly/ALPOm).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partial encoding and decoding of common formats like [protobuf](https://oreil.ly/gzswU).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removal of bound checks (BCE), e.g., from [arrays](https://oreil.ly/uOHmo).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Branchless Go coding, optimizing for [the CPU branch predictions](https://oreil.ly/v9eNk).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Array of structs versus structs of arrays and loop fusion and fission](https://oreil.ly/SxPUA).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, try to run different languages from Go to offload some performance-sensitive
    logic, for example, running [Rust from Go](https://oreil.ly/vp5V3), or in the
    future, [Carbon](https://oreil.ly/ZO3Zn) from Go! Let’s not forget about something
    much more common: running [Assembly from Go](https://oreil.ly/eLZKW) for efficiency
    reasons.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, all examples in this book are available at the [*https://github.com/efficientgo/examples*](https://github.com/efficientgo/examples)
    open source repository. Give feedback, contribute, and learn together with others.
  prefs: []
  type: TYPE_NORMAL
- en: Everybody learns differently, so try what helps you the most. However, I strongly
    recommend practicing the software of your choice using the practices you learned
    in this book. Try to set reasonable efficiency goals and try to optimize them.^([20](ch11.html#idm45606813221312))
  prefs: []
  type: TYPE_NORMAL
- en: 'You are also welcome to use and contribute to other Go tools I maintain in
    the open source: [*https://github.com/efficientgo/core*](https://github.com/efficientgo/core),
    [*https://github.com/efficientgo/e2e*](https://github.com/efficientgo/e2e), [*https://github.com/prometheus/prometheus*](https://github.com/prometheus/prometheus),
    and more!^([21](ch11.html#idm45606813214720))'
  prefs: []
  type: TYPE_NORMAL
- en: Join our [“Efficient Go” Discord Community](https://oreil.ly/cNnt2), and feel
    free to give feedback on the book, ask additional questions, or find new friends!
  prefs: []
  type: TYPE_NORMAL
- en: Massive thanks to all (see [“Acknowledgments”](preface01.html#thanks)) who directly
    or indirectly helped to create this book. Thanks to those who mentored me to where
    I am now!
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for buying and reading my book. See you in the open source! :)
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch11.html#idm45606819377840-marker)) I spoke about this problem at the
    [GitHub Global Maintainers Summit](https://oreil.ly/z6YHe).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch11.html#idm45606819373392-marker)) This list was inspired by Chapter
    4 in *Writing Efficient Programs* by Jon Louis Bentley.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch11.html#idm45606819974288-marker)) There’s a reason some people call
    caches [“a memory leak you don’t know about yet”](https://oreil.ly/KNQP3).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch11.html#idm45606819332528-marker)) See a nice blog post about those
    [here](https://oreil.ly/KrVnG).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch11.html#idm45606819295376-marker)) For example, in [the Prometheus project
    we removed](https://oreil.ly/WFbrk) the manual GC trigger when code conditions
    changed a little. That decision was based on micro- and macrobenchmarks discussed
    in [Chapter 7](ch07.html#ch-observability2).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch11.html#idm45606819276864-marker)) The reason is that we might reuse
    the same code in a more long-living scenario, where a leak might have much bigger
    consequences.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch11.html#idm45606819271728-marker)) Unless we disabled it using the `GOGC=off`
    environment variable.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch11.html#idm45606819262800-marker)) For that, we could use tools that
    [analyze the dumped core](https://oreil.ly/iTXhz), but they aren’t very accessible
    at the moment, so I would not recommend them.
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch11.html#idm45606817523552-marker)) Yes! If we don’t invoke the returned
    `context.CancelContext` function, it will keep a goroutine running forever (when
    `WithContext` was used) or until the timeout (`WithTimeout`).
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch11.html#idm45606817514192-marker)) I have only seen linters that check
    some basic things like if the code closed [request body](https://oreil.ly/DpSLY),
    or [sql statements](https://oreil.ly/EVB8M). There is room to contribute more
    of those, e.g., [in the `semgrep-go` project](https://oreil.ly/WfmyC).
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch11.html#idm45606816330528-marker)) Which is quite interesting, considering
    we do more work in our code. We read through all bytes of the HTML returned by
    Google. Yet, it’s faster as we create fewer TCP connections.
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch11.html#idm45606816088384-marker)) This is often used when we know
    only the worst-case `size`. Sometimes it’s worth growing it to the worst case,
    even if we use less in the end. See [“Overusing Memory with Arrays”](#ch-basic-subslice).
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch11.html#idm45606815217568-marker)) For example, this is what we did
    in [Thanos](https://oreil.ly/8nWCH) some time ago.
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch11.html#idm45606814784544-marker)) This is great as a quick showcase,
    but does not work well as a reliable efficiency assessment.
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch11.html#idm45606814531920-marker)) In the Prometheus project ecosystem,
    we experienced such a problem many times. For example, chunk pooling caused us
    to keep arrays that were way bigger than required, so we introduced [the `Compact`
    method](https://oreil.ly/ORx1C). In Thanos, I introduced a (probably too) clever
    [`ZLabel` construct](https://oreil.ly/Z3Q8n) that avoided expensive copy of strings
    for metric labels. It turned out to be beneficial for cases when we were not keeping
    the label strings for longer. For example, it was better to perform when we did
    [a lazy copy](https://oreil.ly/5o6sH).
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch11.html#idm45606813445248-marker)) If you are interested in the specific
    implementation details, check out [this amazing blog post](https://oreil.ly/oMh6I).
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch11.html#idm45606813301392-marker)) Interestingly enough, `sync.Pool`
    was proposed to be named `sync.Cache` initially and have cache semantics.
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch11.html#idm45606813281824-marker)) And be vigilant when someone offers
    shiny observability for a low price. It is often less cheap in practice, given
    how much data we usually have to pass through those systems.
  prefs: []
  type: TYPE_NORMAL
- en: ^([19](ch11.html#idm45606813255952-marker)) My recommendation is to [avoid following
    only tutorials](https://oreil.ly/5YDe6). If you are out of your comfort zone and
    have to think on your own, you learn.
  prefs: []
  type: TYPE_NORMAL
- en: ^([20](ch11.html#idm45606813221312-marker)) If you are interested, I would like
    to invite you to our yearly [efficiency-coding-advent](https://oreil.ly/OPPXh),
    where we try to solve [coding challenges around Christmas time](https://oreil.ly/10gGv)
    with an efficient approach.
  prefs: []
  type: TYPE_NORMAL
- en: ^([21](ch11.html#idm45606813214720-marker)) You can find all the projects I
    maintain (or used to maintain) on [my website](https://oreil.ly/0af14).
  prefs: []
  type: TYPE_NORMAL
