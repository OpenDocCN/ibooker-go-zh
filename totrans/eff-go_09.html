<html><head></head><body><section data-pdf-bookmark="Chapter 9. Data-Driven Bottleneck Analysis" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch-observability3">&#13;
<h1><span class="label">Chapter 9. </span>Data-Driven Bottleneck Analysis</h1>&#13;
&#13;
<blockquote>&#13;
<p>Programmers are usually notoriously bad at guessing which parts of the code are the primary consumers of the resources. It is all too common for a programmer to modify a piece of code expecting see a huge time savings and then to find that it makes no difference at all because the code was rarely executed.</p>&#13;
<p data-type="attribution"> Jon Louis Bentley, <i>Writing Efficient Programs</i></p></blockquote>&#13;
&#13;
<p><a data-primary="bottleneck analysis" data-type="indexterm" id="ix_ch09-asciidoc0"/>One <a data-primary="Bentley, Jon Louis" data-secondary="on programmers’ assessment of resource consumption" data-type="indexterm" id="idm45606826058736"/>of the key steps to improving the efficiency of our Go programs is to know where is the main source of the latency or resource usage you want to improve. Therefore, we should make a conscious effort to first focus on the code parts that contribute the most (the bottleneck or hot spot) to get the biggest value for our &#13;
<span class="keep-together">optimizations.</span></p>&#13;
&#13;
<p>It is very tempting to use our experience in software development to estimate what part of the code is the most expensive or too slow to compute. We might have already seen similar code fragments causing efficiency problems in the past. For example, “Oh, I worked with linked lists in Go, it was so slow, this must be it!” or “We create a lot of new slices here, I think this is our bottleneck, let’s reuse some.” We might still remember the pain or stress it might have caused. Unfortunately, those feelings-based conclusions are often wrong. Every program, use case, and environment is different. The software might struggle in other places. It’s essential to uncover that part quickly and reliably so we know where to spend our optimization efforts.</p>&#13;
&#13;
<p>Fortunately, we don’t need to guess. We can gather appropriate data! Go provides and integrates very rich tools we can use for bottleneck analysis. We will start our journey with the <a data-type="xref" href="#ch-obs-cause">“Root Cause Analysis, but for Efficiency”</a> that introduces some of them. Then, I will introduce you to <a data-type="xref" href="#ch-obs-profiling">“Profiling in Go”</a>, where you will learn about the <code>pprof</code> ecosystem. This profiling foundation is quite popular, yet it isn’t easy to understand its results if you don’t know the basics. The tooling, reports, and views are poorly documented, so I will spend a few sections describing the principles and common representations. In <a data-type="xref" href="#ch-obs-pprof-obtain">“Capturing the Profiling Signal”</a>, you will learn how to instrument and collect profiles. In <a data-type="xref" href="#ch-obs-prof-common">“Common Profile Instrumentation”</a>, I will explain a few important existing profiles we can use right now in Go. Finally, we go through some <a data-type="xref" href="#ch-obs-tricks">“Tips and Tricks”</a>, including the recently popular technique called <a data-type="xref" href="#ch-obs-cont-profiling">“Continuous Profiling”</a>!</p>&#13;
&#13;
<p>This is one of those chapters where I learned a lot while researching and preparing the content. This is why I am even more excited to share that knowledge with you! Let’s start with root cause analysis and its connection to bottleneck analysis.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Root Cause Analysis, but for Efficiency" data-type="sect1"><div class="sect1" id="ch-obs-cause">&#13;
<h1>Root Cause Analysis, but for Efficiency</h1>&#13;
&#13;
<p><a data-primary="bottleneck analysis" data-secondary="as root cause analysis for efficiency" data-secondary-sortas="root cause" data-type="indexterm" id="idm45606826047840"/>The bottleneck analysis process is no different from the <a href="https://oreil.ly/3MhUA">causal analysis</a> or <a href="https://oreil.ly/KNqVV">root cause analysis</a> engineers perform after system incidents or failed tests. In fact, efficiency problems cause many of those incidents, e.g., HTTP requests timing out as the CPUs were saturated. As a result, it’s best if we equip ourselves with similar mindsets and tools during bottleneck analysis of our system or program.</p>&#13;
&#13;
<p>For more complex systems with multiple processes, the investigation might be quite involved with many symptoms,<sup><a data-type="noteref" href="ch09.html#idm45606826044080" id="idm45606826044080-marker">1</a></sup> red herrings,<sup><a data-type="noteref" href="ch09.html#idm45606826043392" id="idm45606826043392-marker">2</a></sup> or even multiple bottlenecks.</p>&#13;
&#13;
<p>The tools in <a data-type="xref" href="ch06.html#ch-observability">Chapter 6</a> are always invaluable for bottleneck analysis. With metrics around resource usage, we can narrow down when and which process allocated or used the most memory or CPU time, etc. With detailed logging, we could provide extra latency measurements for each stage. With tracing, we can analyze the request path and find which process and sometimes program function<sup><a data-type="noteref" href="ch09.html#idm45606826040352" id="idm45606826040352-marker">3</a></sup> contribute the most to the latency of the whole operation.</p>&#13;
&#13;
<p>The other naive way is trial-and-error flow. We can always manually experiment by disabling certain code parts one by one to check if we can reproduce that efficiency error or not. However, for large systems, this is likely to be infeasible in practice. There might be a better way to determine the main contributor to the extensive resource usage or high latency. Something that, in seconds, can tell us the exact code line responsible for it.</p>&#13;
&#13;
<p>That convenient signal is called <em>profiling</em>, and it’s often described as the fourth pillar of observability. Let’s explore profiling in detail in the next section.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Profiling in Go" data-type="sect1"><div class="sect1" id="ch-obs-profiling">&#13;
<h1>Profiling in Go</h1>&#13;
<blockquote>&#13;
<p>Profiling is a form of dynamic code analysis. You capture characteristics of the application as it runs, and then you use this information to identify how to make your application faster and more efficient.</p>&#13;
<p data-type="attribution">“Profiling Concepts,” <a href="https://oreil.ly/okyge">Google Cloud Documentation</a></p></blockquote>&#13;
&#13;
<p><a data-primary="bottleneck analysis" data-secondary="profiling in Go" data-seealso="profiling" data-type="indexterm" id="ix_ch09-asciidoc1"/><a data-primary="profiling" data-type="indexterm" id="ix_ch09-asciidoc2"/>Profiling is a perfect concept for representing the exact usage of something (e.g., elapsed time, CPU time, memory, goroutines, or rows in the database) caused by a specific code line in a program. Depending on what we look for, we can compare the contribution of something for different code lines or grouped by functions<sup><a data-type="noteref" href="ch09.html#idm45606826031328" id="idm45606826031328-marker">4</a></sup> or files.</p>&#13;
&#13;
<p>In my experience, profiling is one of the most mature debugging methods in the Go community. It’s rich, efficient, and accessible to everyone, with the Go standard library providing six profile implementations out of the box, community-created ones, and easy-to-build custom ones. What’s amazing is that all these profiles might have different meanings and are related to different resources, but their representation follows the same convention and format. This means that no matter if you want to explore heap (see <a data-type="xref" href="#ch-obs-pprof-heap">“Heap”</a>), goroutine (see <a data-type="xref" href="#ch-obs-pprof-goroutine">“Goroutine”</a>), &#13;
<span class="keep-together">or CPU</span> (see <a data-type="xref" href="#ch-obs-pprof-cpu">“CPU”</a>), you can use the same visualization and analysis &#13;
<span class="keep-together">tools and</span> patterns.</p>&#13;
&#13;
<p>Without a doubt, many thanks should go to the <a href="https://oreil.ly/jBj18"><code>pprof</code> project</a> (“pprof” stands for performance profiles). There are many profilers out there. We have <a href="https://oreil.ly/M08S8"><code>perf_events</code> (<code>perf</code> tool)</a> for Linux, <a href="https://oreil.ly/JJ8Gp"><code>hwpmc</code></a> for FreeBSD, <a href="https://oreil.ly/hUm9r">DTrace</a>, and much more. What’s special about <code>pprof</code> is that it establishes a common representation, file format, and visualization tooling for profiling data. This means you can use any of the preceding tools, or implement a profiler in Go from scratch and use the same tooling and semantics for analyzing those profiles.</p>&#13;
<div data-type="note" epub:type="note"><h1>Profiler</h1>&#13;
<p><a data-primary="profiler (definition)" data-type="indexterm" id="idm45606826019040"/>A profiler is a piece of software that can collect the stack traces and usage of a certain resource (or time) and then save it into a profile. Configured, installed, or instrumented profiler can be called profiling instrumentation.</p>&#13;
</div>&#13;
&#13;
<p>Let’s dive into <code>pprof</code> in the next section.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="pprof Format" data-type="sect2"><div class="sect2" id="ch-obs-pprof">&#13;
<h2>pprof Format</h2>&#13;
<blockquote>&#13;
<p>The original <code>pprof</code> tool was a Perl script developed internally at Google. Based on the copyright header, development might go back to 1998. It was first released in 2005 as part of <code>gperftools</code>, and added to the Go project in 2010. In 2014 the Go project replaced the Perl based version of the pprof tool with a Go implementation by Raul Silvera that was already used inside of Google at this point. This implementation was re-released as a standalone project in 2016. Since then the Go project has been vendoring a copy of the upstream project, updating it on a regular basis.</p>&#13;
<p data-type="attribution">Felix Geisendörfer, <a href="https://oreil.ly/FmOz8">“Go’s pprof Tool and Format”</a></p>&#13;
</blockquote>&#13;
&#13;
<p><a data-primary="pprof format" data-seealso="profiling" data-type="indexterm" id="ix_ch09-asciidoc3"/><a data-primary="profiling" data-secondary="pprof format" data-type="indexterm" id="ix_ch09-asciidoc4"/>Many <a data-primary="Geisendörfer, Felix, on original pprof tool" data-type="indexterm" id="idm45606826008320"/>programming languages like Go and <a href="https://oreil.ly/0maaM">C++</a>, and tools like Linux <a href="https://oreil.ly/PTJFN"><code>perf</code></a> can leverage the <code>pprof</code> format, so it’s worth learning about it more. To truly understand profiling, let’s quickly create our custom profiling to track currently opened files in our Go program. There is a limit to how many file descriptors the program can hold simultaneously. If our program encounters such a problem, the file descriptor profiling might be beneficial to find what part of the program is responsible for opening the largest number of descriptors.<sup><a data-type="noteref" href="ch09.html#idm45606826005440" id="idm45606826005440-marker">5</a></sup></p>&#13;
&#13;
<p>For such basic profiling, we don’t need to implement any <code>pprof</code> encoding or tracking code. Instead, we can use a simple <a href="https://oreil.ly/f2OkA"><code>runtime/pprof.Profile</code> struct</a> that the standard library implements. It allows for creating profiles that record counts and sources of the currently used objects of the desired type. <code>pprof.Profile</code> is very simple and a bit limited,<sup><a data-type="noteref" href="ch09.html#idm45606826001264" id="idm45606826001264-marker">6</a></sup> but it’s perfect to start our journey with profiling.</p>&#13;
&#13;
<p class="less_space pagebreak-before">The basic profiler example is presented in <a data-type="xref" href="#code-pprof-fd">Example 9-1</a>.</p>&#13;
<div data-type="example" id="code-pprof-fd">&#13;
<h5><span class="label">Example 9-1. </span>Implementing file descriptor profiling using <code>pprof.Profile</code> functionality</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kn">package</code><code class="w"> </code><code class="nx">fd</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kn">import</code><code class="w"> </code><code class="p">(</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"os"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"runtime/pprof"</code><code class="w">&#13;
</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">var</code><code class="w"> </code><code class="nx">fdProfile</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="nx">pprof</code><code class="p">.</code><code class="nx">NewProfile</code><code class="p">(</code><code class="s">"fd.inuse"</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO1-1" id="co_data_driven_bottleneck_analysis_CO1-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
&#13;
</code><code class="c1">// File is a wrapper on os.File that tracks file descriptor lifetime.</code><code class="w">&#13;
</code><code class="kd">type</code><code class="w"> </code><code class="nx">File</code><code class="w"> </code><code class="kd">struct</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="o">*</code><code class="nx">os</code><code class="p">.</code><code class="nx">File</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="c1">// Open opens a file and tracks it in the `fd` profile`.</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">Open</code><code class="p">(</code><code class="nx">name</code><code class="w"> </code><code class="kt">string</code><code class="p">)</code><code class="w"> </code><code class="p">(</code><code class="o">*</code><code class="nx">File</code><code class="p">,</code><code class="w"> </code><code class="kt">error</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">f</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">os</code><code class="p">.</code><code class="nx">Open</code><code class="p">(</code><code class="nx">name</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">if</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">!=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="k">return</code><code class="w"> </code><code class="kc">nil</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">fdProfile</code><code class="p">.</code><code class="nx">Add</code><code class="p">(</code><code class="nx">f</code><code class="p">,</code><code class="w"> </code><code class="mi">2</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO1-2" id="co_data_driven_bottleneck_analysis_CO1-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
    </code><code class="k">return</code><code class="w"> </code><code class="o">&amp;</code><code class="nx">File</code><code class="p">{</code><code class="nx">File</code><code class="p">:</code><code class="w"> </code><code class="nx">f</code><code class="p">}</code><code class="p">,</code><code class="w"> </code><code class="kc">nil</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="c1">// Close closes files and updates profile.</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="p">(</code><code class="nx">f</code><code class="w"> </code><code class="o">*</code><code class="nx">File</code><code class="p">)</code><code class="w"> </code><code class="nx">Close</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="kt">error</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">defer</code><code class="w"> </code><code class="nx">fdProfile</code><code class="p">.</code><code class="nx">Remove</code><code class="p">(</code><code class="nx">f</code><code class="p">.</code><code class="nx">File</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO1-3" id="co_data_driven_bottleneck_analysis_CO1-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
    </code><code class="k">return</code><code class="w"> </code><code class="nx">f</code><code class="p">.</code><code class="nx">File</code><code class="p">.</code><code class="nx">Close</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="c1">// Write saves the profile of the currently open file</code><code class="w">&#13;
</code><code class="c1">// descriptors into a file in pprof format.</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">Write</code><code class="p">(</code><code class="nx">profileOutPath</code><code class="w"> </code><code class="kt">string</code><code class="p">)</code><code class="w"> </code><code class="kt">error</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">out</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">os</code><code class="p">.</code><code class="nx">Create</code><code class="p">(</code><code class="nx">profileOutPath</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">if</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">!=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="k">return</code><code class="w"> </code><code class="nx">err</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">if</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">fdProfile</code><code class="p">.</code><code class="nx">WriteTo</code><code class="p">(</code><code class="nx">out</code><code class="p">,</code><code class="w"> </code><code class="mi">0</code><code class="p">)</code><code class="p">;</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">!=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO1-4" id="co_data_driven_bottleneck_analysis_CO1-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
        </code><code class="nx">_</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="nx">out</code><code class="p">.</code><code class="nx">Close</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">        </code><code class="k">return</code><code class="w"> </code><code class="nx">err</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">return</code><code class="w"> </code><code class="nx">out</code><code class="p">.</code><code class="nx">Close</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO1-1" id="callout_data_driven_bottleneck_analysis_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p><code>pprof.NewProfile</code> is designed to be used as a global variable. It registers profiles with the provided name, which has to be unique. In this example, I use the <code>fd.inuse</code> name to indicate the profile tracks in-use file descriptors.</p>&#13;
&#13;
<p>Unfortunately, this global registry convention has a few downsides. If you import two packages that create profiles you don’t want to use, or they register profiles with common names, our program will panic. On the other hand, the global pattern allows us to use <code>pprof.Lookup("fd.inuse")</code> to get the created profile from different packages. It also automatically works with the <code>net/http/pprof</code> handler, explained in <a data-type="xref" href="#ch-obs-pprof-obtain">“Capturing the Profiling Signal”</a>. For our example, it works fine, but I would usually not recommend using global conventions for any serious custom profiler.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO1-2" id="callout_data_driven_bottleneck_analysis_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>To record living file descriptors, we offer an <code>Open</code> function that mimics the <code>os.Open</code> function. It opens a file and records it. It also wraps the <code>os.File</code>, so we know when it’s closed. The <code>Add</code> method records the object. The second argument tells how many calls to skip in the stack trace. The stack trace is used to record the location of the profile in the further <code>pprof</code> format.</p>&#13;
&#13;
<p>I decided to use the <code>Open</code> function as the reference to sample creation, so I have to skip two stack frames.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO1-3" id="callout_data_driven_bottleneck_analysis_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>We can remove the object when the file is closed. Note I am using the same inner <code>*os.File</code>, so the <code>pprof</code> package can track and find the object I opened.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO1-4" id="callout_data_driven_bottleneck_analysis_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Standard Go profiles offer a <code>WriteTo</code> method that writes bytes of a full <code>pprof</code> file into a provided writer. However, we typically want to save it to the file, so I added the <code>Write</code> method.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Many standard profiles, like those mentioned later in <a data-type="xref" href="#ch-obs-prof-common">“Common Profile Instrumentation”</a>, are transparently instrumented. For example, we don’t have to allocate memory differently to see it in the heap profile (see <a data-type="xref" href="#ch-obs-pprof-heap">“Heap”</a>). For custom profiles like ours, a profiler has to be manually instrumented in our program. For example, I created <code>TestApp</code> that simulates an app that opens exactly 112 files. The code using <a data-type="xref" href="#code-pprof-fd">Example 9-1</a> is presented in <a data-type="xref" href="#code-pprof-fd-usage">Example 9-2</a>.</p>&#13;
<div data-type="example" id="code-pprof-fd-usage">&#13;
<h5><span class="label">Example 9-2. </span><code>TestApp</code> code instrumented with <code>fd.inuse</code> profiling saves the profile at the end to the <code>fd.pprof</code> file</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kn">package</code><code class="w"> </code><code class="nx">main</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="c1">// import "github.com/efficientgo/examples/pkg/profile/fd"</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">type</code><code class="w"> </code><code class="nx">TestApp</code><code class="w"> </code><code class="kd">struct</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">files</code><code class="w"> </code><code class="p">[</code><code class="p">]</code><code class="nx">io</code><code class="p">.</code><code class="nx">ReadCloser</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="p">(</code><code class="nx">a</code><code class="w"> </code><code class="o">*</code><code class="nx">TestApp</code><code class="p">)</code><code class="w"> </code><code class="nx">Close</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">for</code><code class="w"> </code><code class="nx">_</code><code class="p">,</code><code class="w"> </code><code class="nx">cl</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="k">range</code><code class="w"> </code><code class="nx">a</code><code class="p">.</code><code class="nx">files</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">_</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="nx">cl</code><code class="p">.</code><code class="nx">Close</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="c1">// TODO: Check error. </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO2-2" id="co_data_driven_bottleneck_analysis_CO2-1"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">a</code><code class="p">.</code><code class="nx">files</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="nx">a</code><code class="p">.</code><code class="nx">files</code><code class="p">[</code><code class="p">:</code><code class="mi">0</code><code class="p">]</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="p">(</code><code class="nx">a</code><code class="w"> </code><code class="o">*</code><code class="nx">TestApp</code><code class="p">)</code><code class="w"> </code><code class="nx">open</code><code class="p">(</code><code class="nx">name</code><code class="w"> </code><code class="kt">string</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">f</code><code class="p">,</code><code class="w"> </code><code class="nx">_</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">fd</code><code class="p">.</code><code class="nx">Open</code><code class="p">(</code><code class="nx">name</code><code class="p">)</code><code class="w"> </code><code class="c1">// TODO: Check error. </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO2-1" id="co_data_driven_bottleneck_analysis_CO2-2"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">    </code><code class="nx">a</code><code class="p">.</code><code class="nx">files</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="nb">append</code><code class="p">(</code><code class="nx">a</code><code class="p">.</code><code class="nx">files</code><code class="p">,</code><code class="w"> </code><code class="nx">f</code><code class="p">)</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="p">(</code><code class="nx">a</code><code class="w"> </code><code class="o">*</code><code class="nx">TestApp</code><code class="p">)</code><code class="w"> </code><code class="nx">OpenSingleFile</code><code class="p">(</code><code class="nx">name</code><code class="w"> </code><code class="kt">string</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">a</code><code class="p">.</code><code class="nx">open</code><code class="p">(</code><code class="nx">name</code><code class="p">)</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="p">(</code><code class="nx">a</code><code class="w"> </code><code class="o">*</code><code class="nx">TestApp</code><code class="p">)</code><code class="w"> </code><code class="nx">OpenTenFiles</code><code class="p">(</code><code class="nx">name</code><code class="w"> </code><code class="kt">string</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="mi">10</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">a</code><code class="p">.</code><code class="nx">open</code><code class="p">(</code><code class="nx">name</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="p">(</code><code class="nx">a</code><code class="w"> </code><code class="o">*</code><code class="nx">TestApp</code><code class="p">)</code><code class="w"> </code><code class="nx">Open100FilesConcurrently</code><code class="p">(</code><code class="nx">name</code><code class="w"> </code><code class="kt">string</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">wg</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">sync</code><code class="p">.</code><code class="nx">WaitGroup</code><code class="p">{</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">wg</code><code class="p">.</code><code class="nx">Add</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="mi">10</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="k">go</code><code class="w"> </code><code class="kd">func</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">            </code><code class="nx">a</code><code class="p">.</code><code class="nx">OpenTenFiles</code><code class="p">(</code><code class="nx">name</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">            </code><code class="nx">wg</code><code class="p">.</code><code class="nx">Done</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">        </code><code class="p">}</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">wg</code><code class="p">.</code><code class="nx">Wait</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">main</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">a</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="o">&amp;</code><code class="nx">TestApp</code><code class="p">{</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">defer</code><code class="w"> </code><code class="nx">a</code><code class="p">.</code><code class="nx">Close</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="c1">// No matter how many files we opened in the past...</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="mi">10</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">a</code><code class="p">.</code><code class="nx">OpenTenFiles</code><code class="p">(</code><code class="s">"/dev/null"</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO2-3" id="co_data_driven_bottleneck_analysis_CO2-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
        </code><code class="nx">a</code><code class="p">.</code><code class="nx">Close</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="c1">// ...after the last Close, only files below will be used in the profile.</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">f</code><code class="p">,</code><code class="w"> </code><code class="nx">_</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">fd</code><code class="p">.</code><code class="nx">Open</code><code class="p">(</code><code class="s">"/dev/null"</code><code class="p">)</code><code class="w"> </code><code class="c1">// TODO: Check error.</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">a</code><code class="p">.</code><code class="nx">files</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="nb">append</code><code class="p">(</code><code class="nx">a</code><code class="p">.</code><code class="nx">files</code><code class="p">,</code><code class="w"> </code><code class="nx">f</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">a</code><code class="p">.</code><code class="nx">OpenSingleFile</code><code class="p">(</code><code class="s">"/dev/null"</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">a</code><code class="p">.</code><code class="nx">OpenTenFiles</code><code class="p">(</code><code class="s">"/dev/null"</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">a</code><code class="p">.</code><code class="nx">Open100FilesConcurrently</code><code class="p">(</code><code class="s">"/dev/null"</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">if</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">fd</code><code class="p">.</code><code class="nx">Write</code><code class="p">(</code><code class="s">"fd.pprof"</code><code class="p">)</code><code class="p">;</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">!=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO2-4" id="co_data_driven_bottleneck_analysis_CO2-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
        </code><code class="nx">log</code><code class="p">.</code><code class="nx">Fatal</code><code class="p">(</code><code class="nx">err</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO2-2" id="callout_data_driven_bottleneck_analysis_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>We open the file using our <code>fd.Open</code> function, which starts recording it in the profile as a side effect of opening the file.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO2-1" id="callout_data_driven_bottleneck_analysis_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>We always need to ensure the file will be closed when we don’t need it anymore. This saves resources (like file descriptor) and more importantly, flushes any buffered writes and records that the file is no longer used.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO2-3" id="callout_data_driven_bottleneck_analysis_CO2-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>To demonstrate our profiling works, we first open 10 files and close them, repeated 10 times. We use <em>/dev/null</em> as our dummy file for testing purposes.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO2-4" id="callout_data_driven_bottleneck_analysis_CO2-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Finally, we create 110 files using methods that are chained in some way. Then we take a snapshot of this situation in the form of our <code>fd.inuse</code> profile. I use the <em>.pprof</em> file extension for this file (Go documentation uses <em>.prof</em>), but technically it’s a gzipped (compressed using <code>gzip</code> program) protobuf file, so the <em>.pb.gz</em> file extension is often used. Use whatever you find more readable.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>What’s happening in the code in <a data-type="xref" href="#code-pprof-fd-usage">Example 9-2</a> might seem straightforward. In practice, however, the complexity of our Go program might cause us to wonder what piece of code creates so many files that are not closed. The data saved in the created <em>fd.pprof</em> should give us an answer to this question. We refer to the <code>pprof</code> format in the Go community as simply a gzipped <a href="https://oreil.ly/2Lgbl">protobuf</a> (binary format) file. The format is typed with the schema defined in the <code>.proto</code> language and officially defined in &#13;
<span class="keep-together"><a href="https://oreil.ly/CiEKb"><em>google/pprof</em> project’s <em>proto</em> file</a>.</span></p>&#13;
&#13;
<p>To learn the <code>pprof</code> schema and its primitives quickly, let’s look at what the <em>fd.pprof</em> file produced in <a data-type="xref" href="#code-pprof-fd-usage">Example 9-2</a> could store. The high-level representation of the open (in use) and total file descriptors diagram is presented in <a data-type="xref" href="#img-obs-prof-pprof">Figure 9-1</a>.</p>&#13;
&#13;
<p><a data-type="xref" href="#img-obs-prof-pprof">Figure 9-1</a> shows what objects are stored in <code>pprof</code> format and a few core fields those objects contain (there are more). As you might notice, this format is designed for efficiency, with many indirections (referencing other things via integer IDs). I skipped that detail on the diagram for simplicity, but all strings are also referenced as integers with the string table for <a href="https://oreil.ly/KT4UY">interning</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-prof-pprof">&#13;
<img alt="efgo 0901" src="assets/efgo_0901.png"/>&#13;
<h6><span class="label">Figure 9-1. </span>The high-level representation of open (in use) and total file descriptors in <code>pprof</code> format</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-primary="pprof format" data-secondary="Profile child objects" data-type="indexterm" id="ix_ch09-asciidoc5"/><code>pprof</code> format starts with the single root object called <code>Profile</code>, which contains the following child objects:</p>&#13;
<dl>&#13;
<dt><code>Mappings</code></dt>&#13;
<dd>&#13;
<p>Not every program has debugging symbols inside the binary. For example, in <a data-type="xref" href="ch04.html#ch-hw-compilation">“Understanding Go Compiler”</a>, we mentioned that Go has them by default to provide human-readable stack traces that refer to source code. However, someone compiling binary might remove this information to make the binary size much smaller. If there are no symbols, the <code>pprof</code> file can be used with addresses of stack frames (locations). Those addresses will then be dynamically translated to the exact source code line by further tooling in a process called <a href="https://oreil.ly/zcZKa">symbolization</a>. Mapping allows specifying how addresses are mapped to the binary if it’s dynamically provided in a later step.</p>&#13;
&#13;
<p>Unfortunately, if you need a binary file, it has to be built from the same source code version and architecture from which we gathered profiles. This is usually very tricky. For example, when we obtain profiles from remote services (more on that in <a data-type="xref" href="#ch-obs-pprof-obtain">“Capturing the Profiling Signal”</a>), we most likely won’t have &#13;
<span class="keep-together">the same</span> binary on the machine where we analyze the profiles.</p>&#13;
&#13;
<p>Fortunately, we can store all required metadata in the <code>pprof</code> profile, so no symbolization is needed. This is what’s used for standard profiles in Go from <a href="https://oreil.ly/qONe8">Go 1.9</a>, so I will skip explaining the symbolization techniques.</p>&#13;
</dd>&#13;
<dt><code>Locations</code></dt>&#13;
<dd>&#13;
<p>Locations are code lines (or their addresses). For convenience, a location can point to a function it was defined in and the source code filename. Location essentially represents a stack frame.</p>&#13;
</dd>&#13;
<dt><code>Functions</code></dt>&#13;
<dd>&#13;
<p>Functions structures hold metadata about functions in which locations are defined. They are only filled if debug symbols were present in the binary.</p>&#13;
</dd>&#13;
<dt><code>ValueTypes</code></dt>&#13;
<dd>&#13;
<p>This tells how many dimensions we have in our profiles. Each location can be responsible for using (contributing to usage of) some values. Value types define the unit and what that value means. Our <a data-type="xref" href="#code-pprof-fd">Example 9-1</a> profile has only the <code>fd.inuse</code> type, because the current, simplistic <code>pprof.Profile</code> does not allow putting more dimensions; but for demonstration, <a data-type="xref" href="#img-obs-prof-pprof">Figure 9-1</a> has two types representing total count and current count.</p>&#13;
<div data-type="note" epub:type="note"><h1>Contributions</h1>&#13;
<p>The <code>pprof</code> format profile does not limit what the profile value means. It’s up to the implementation to define the measured value semantics. For example, in <a data-type="xref" href="#code-pprof-fd">Example 9-1</a>, I defined it as the number of open files present at the moment of the profile snapshot. For other <a data-type="xref" href="#ch-obs-prof-common">“Common Profile Instrumentation”</a>, the value means something else: the time spent on CPU, allocated bytes, or the number of goroutines executing in a specific location. Always clarify what your profile values mean!</p>&#13;
&#13;
<p>Generally, most profile values tell us how much each part of our code uses a certain resource or time. That’s why I stick to the <em>contribution</em> verb when explaining profile values on &#13;
<span class="keep-together">samples.</span></p>&#13;
</div>&#13;
</dd>&#13;
<dt><code>Samples</code></dt>&#13;
<dd>&#13;
<p>The measurement or measured contribution by a given stack trace of some value for a given value type. To represent a stack trace (call sequence), a sample lists all location IDs starting from the top of the stack trace. The important detail is that the sample has to have the exact number of values equal to the number (and order) of value types we have defined. We can also attach labels to samples. For example, we could attach the example filename that was open in that stack trace. <a data-type="xref" href="#ch-obs-pprof-heap">“Heap”</a> uses it to show average allocation size.</p>&#13;
</dd>&#13;
<dt>Further metadata</dt>&#13;
<dd>&#13;
<p>Information like when the profile was captured, data tracking duration (if applicable), and some filtering information can also be in the profile object. One of the most important fields is the <code>period</code> field, which tells us if the profile was sampled or not. We track all the instrumented <code>Open</code> calls in <a data-type="xref" href="#code-pprof-fd-usage">Example 9-2</a>, so we have <code>period</code> equal to one.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>With all those components, the <code>pprof</code> data model is very well designed with the profiling data that describes any aspect of our software. It also works well with statistical profiles, which capture the data from a small portion of all the things that<a data-startref="ix_ch09-asciidoc5" data-type="indexterm" id="idm45606825286480"/> &#13;
<span class="keep-together">happened.</span></p>&#13;
&#13;
<p>In <a data-type="xref" href="#code-pprof-fd-usage">Example 9-2</a>, tracking opened files does not pose too much overhead to the &#13;
<span class="keep-together">application.</span> Perhaps in extreme production cases calling <code>Add</code> and <code>Remove</code>, and mapping objects on every file open and closed, might slow down some critical paths. &#13;
<span class="keep-together">However, the</span> situation is much worse with complex profiles like <a data-type="xref" href="#ch-obs-pprof-cpu">“CPU”</a> &#13;
<span class="keep-together">or <a data-type="xref" href="#ch-obs-pprof-heap">“Heap”</a>.</span> For the CPU profile that profiles the use of the CPU by our program, it’s impractical (and impossible) to track what exact instruction was executed in every single cycle. This is because, for every cycle, we would need to capture a stack trace and record it in memory, which, as we learned in <a data-type="xref" href="ch04.html#ch-hardware">Chapter 4</a>, can take hundreds of CPU cycles alone.</p>&#13;
&#13;
<p>This is why the CPU profile has to be sampled. This is similar to other profiles, like memory. As you will learn in <a data-type="xref" href="#ch-obs-pprof-heap">“Heap”</a>, we sample it because tracking all individual allocations would add significant overhead and slow down all allocations in our program.</p>&#13;
&#13;
<p>Fortunately, even with highly sampled profiles, profiling is extremely useful. By design, profiling is primarily used for bottleneck analysis. By definition, the bottleneck is something that uses most of some resources or time. This means that no matter if we capture 100%, 10%, or even 1% of events that use, e.g., the CPU time, statistically, the code that uses the most CPU should still be at the top with the largest usage number. This is why the more expensive profiles will always be sampled in some way, which allows Go developers to safely pre-enable profiles in almost all our programs. It also enables the continuous profiling practices discussed in <a data-type="xref" href="#ch-obs-cont-profiling">“Continuous Profiling”</a>.</p>&#13;
<div data-type="note" epub:type="note"><h1>Statistical Profiles Are Not 100% Precise</h1>&#13;
<p>In the sampled profile, you can miss some portion of the &#13;
<span class="keep-together">contributions.</span></p>&#13;
&#13;
<p>Profilers like Go have a sophisticated <a href="https://oreil.ly/DrfIA">scaling mechanism that attempts to find</a> the probability of missing the allocations and adjust for it, which usually is precise enough.</p>&#13;
&#13;
<p>Yet, those are only approximations. We can sometimes miss some code locations with smaller allocations on our profiles. Sometimes the real allocation is a little larger or smaller than estimated.</p>&#13;
&#13;
<p>Make sure to check the <code>period</code> information in the <code>pprof</code> profiles (explained in <a data-type="xref" data-xrefstyle="select:nopage" href="#ch-obs-profiling-res">“go tool pprof Reports”</a>), and be aware of the sampling in your profiles to reach the right conclusions. Don’t be surprised and worried that your benchmarked allocation numbers do not exactly match the numbers in the profile. We can be entirely certain about absolute numbers only when we obtain a profile with a period equal to one (100% samples).<a data-startref="ix_ch09-asciidoc4" data-type="indexterm" id="idm45606825408144"/><a data-startref="ix_ch09-asciidoc3" data-type="indexterm" id="idm45606825407440"/></p>&#13;
</div>&#13;
&#13;
<p>With the fundamentals of the <code>pprof</code> standard explained, let’s look at what we can do with such a <em>.pprof</em> file. Fortunately, we have plenty of tools that understand this format and help us analyze the profiling data.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="go tool pprof Reports" data-type="sect2"><div class="sect2" id="ch-obs-profiling-res">&#13;
<h2>go tool pprof Reports</h2>&#13;
&#13;
<p><a data-primary="go tool pprof reports" data-type="indexterm" id="ix_ch09-asciidoc6"/><a data-primary="pprof format" data-secondary="go tool pprof reports" data-type="indexterm" id="ix_ch09-asciidoc7"/><a data-primary="profiling" data-secondary="go tool pprof reports" data-type="indexterm" id="ix_ch09-asciidoc8"/>There are many tools (and websites!) out there you can use to parse and analyze <code>pprof</code> profiles. Thanks to a clear schema, you can also easily write your own tool. However, the most popular one out there is the <code>google/pprof</code> project, which implements the <a href="https://oreil.ly/lGZJG"><code>pprof</code> CLI tool</a> for this purpose. The same tool is also <a href="https://oreil.ly/pbDk3">vendored in the Go project</a>, which allows us to use it through the Go CLI. For example, we can report all the <code>pprof</code> relevant fields in semi-human readable format using the <code>go tool pprof -raw fd.pprof</code> command, as presented in <a data-type="xref" href="#code-obs-prof-fdraw">Example 9-3</a>.</p>&#13;
<div data-type="example" id="code-obs-prof-fdraw">&#13;
<h5><span class="label">Example 9-3. </span>Raw debug output of the .pprof file using the Go CLI</h5>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting"><code>go tool pprof -raw fd.pprof&#13;
PeriodType: fd.inuse count&#13;
Period: 1 </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO3-1" id="co_data_driven_bottleneck_analysis_CO3-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
Time: 2022-07-29 15:18:58.76536008 +0200 CEST&#13;
Samples:&#13;
fd.inuse/count&#13;
        100: 1 2&#13;
         10: 1 3 4&#13;
          1: 5 4&#13;
          1: 6 4&#13;
Locations&#13;
1: 0x4b237b M=1 main.(*TestApp).open example/main.go:23 s=0&#13;
    main.(*TestApp).OpenTenFiles example/main.go:33 s=0&#13;
2: 0x4b25cd M=1 main.(*TestApp).Open100FilesConcurrently.func1 (...)&#13;
3: 0x4b283a M=1 main.main example/main.go:64 s=0&#13;
4: 0x435b51 M=1 runtime.main /go1.18.3/src/runtime/proc.go:250 s=0&#13;
5: 0x4b26f2 M=1 main.main example/main.go:60 s=0&#13;
6: 0x4b2799 M=1 main.(*TestApp).open example/main.go:23 s=0&#13;
    main.(*TestApp).OpenSingleFile example/main.go:28 s=0&#13;
    main.main example/main.go:63 s=0&#13;
Mappings&#13;
1: 0x400000/0x4b3000/0x0 /tmp/go-build3464577057/b001/exe/main  [FN]</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO3-1" id="callout_data_driven_bottleneck_analysis_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The <code>-raw</code> output is <a href="https://oreil.ly/juE75">currently the best way</a> to discover what sampling (<code>period</code>) was used when capturing the profile. Using it with the <code>head</code> utility lets us see the first few rows containing that information, which is useful for large profiles, for example, <code>go tool pprof -raw fd.pprof | head</code>.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>The raw output can reveal some basic information about the data contained by the profile, and it helped create the diagram in <a data-type="xref" href="#img-obs-prof-pprof">Figure 9-1</a>. However, there are much better ways to analyze bigger profiles. For example, if you run <code>go tool pprof fd.pprof</code>, it will enter an interactive mode that lets you inspect different locations and generate various reports. We won’t cover this mode in this book because there is a much better way these days that does almost all the interactive mode can—the web viewer!</p>&#13;
&#13;
<p>The most common way to run a web viewer is to run a local server on your machine via the Go CLI. Use the <code>-http</code> flag to specify the address with the port to listen on. For example, running the <code>go tool pprof -http :8080 fd.pprof</code> <sup><a data-type="noteref" href="ch09.html#idm45606825325680" id="idm45606825325680-marker">7</a></sup> command will open the web viewer website<sup><a data-type="noteref" href="ch09.html#idm45606825324096" id="idm45606825324096-marker">8</a></sup> in your browser showing the profile obtained in <a data-type="xref" href="#code-pprof-fd-usage">Example 9-2</a>. The first page you would see is a directed graph rendered based on the given <code>fd.pprof</code> profile (see <a data-type="xref" href="#ch-obs-profiling-res-graph">“Graph”</a>). But before we get there, let’s get familiar with the top navigation menu available<sup><a data-type="noteref" href="ch09.html#idm45606825431504" id="idm45606825431504-marker">9</a></sup> in the web interface, shown in <a data-type="xref" href="#img-obs-prof-nav">Figure 9-2</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-prof-nav">&#13;
<img alt="efgo 0902" src="assets/efgo_0902.png"/>&#13;
<h6><span class="label">Figure 9-2. </span>The top navigation on the <code>pprof</code> web interface</h6>&#13;
</div></figure>&#13;
&#13;
<p>From the left, the top gray overlay menu has the following buttons and inputs:<sup><a data-type="noteref" href="ch09.html#idm45606825248624" id="idm45606825248624-marker">10</a></sup></p>&#13;
<dl>&#13;
<dt>VIEW</dt>&#13;
<dd>&#13;
<p>Allows you to choose different views (reports) of the same profiling data. We will go through all six view types in the subsections below. They all show profiles from a slightly different angle and have a purpose; you might favor different ones. They are generated from the location hierarchy (stack trace) that can be reconstructed from the samples in <a data-type="xref" href="#img-obs-prof-pprof">Figure 9-1</a>.</p>&#13;
</dd>&#13;
<dt>SAMPLE</dt>&#13;
<dd>&#13;
<p>This menu option is not present in <a data-type="xref" href="#img-obs-prof-nav">Figure 9-2</a> because we only have one sample value type (<code>fd.inuse</code> type with <code>count</code> unit), but for profiles with more types, the SAMPLE menu allows us to choose what sample type we want to use (we can use one at a time). This is commonly present on heap profiles.</p>&#13;
</dd>&#13;
<dt>REFINE</dt>&#13;
<dd>&#13;
<p>This menu works only in the Graph and Top views (see <a data-type="xref" href="#ch-obs-profiling-res-graph">“Graph”</a> and <a data-type="xref" href="#ch-obs-profiling-res-top">“Top”</a>). It allows filtering the Graph or Top views to certain locations of interest: nodes in the graph and rows in the top table. It is especially useful for very complex profiles with hundreds or more locations. To use it, click on one or more Graph nodes or rows in the Top table to select the locations. Then click REFINE and choose if you want to focus, ignore, hide, or show them.</p>&#13;
&#13;
<p>Focus and Ignore control the visibility of samples that go through a selected node or row, allowing you to focus on or ignore full stack traces. Hide and Show control only the node or row’s visibility without impacting samples.</p>&#13;
&#13;
<p>The same filtering can be applied using <code>-focus</code> and other flags in the <a href="https://oreil.ly/OVQLC"><code>go tool pprof</code> CLI</a>. Additionally, the REFINE &gt; Reset option brings us back to a &#13;
<span class="keep-together">nonfiltered</span> view, and if you change to a view that does not support refined options, it only persists in the Focus value.</p>&#13;
<div class="tip5" data-type="tip">&#13;
<p>Focus and Ignore are incredibly useful when you want to find the exact contribution of a certain code path. On the other hand, you can use Hide and Show when you want to present the graph to somebody or as documentation for a clearer &#13;
<span class="keep-together">picture.</span></p>&#13;
&#13;
<p>Don’t use those options if you’re trying to mentally correlate your code with the profile, as you can get easily confused, especially at the start of your profiling journey.</p>&#13;
</div>&#13;
</dd>&#13;
</dl>&#13;
<dl>&#13;
<dt>CONFIG</dt>&#13;
<dd>&#13;
<p>The refinement settings you used from the REFINE option are saved in the URL. However, you can save these settings to a special, named configuration (as well as a zoom option for the Graph view). Click CONFIG &gt; &#13;
<span class="keep-together">Save As …​,</span> then choose the configuration you will be using. The <code>Default</code> configuration works like REFINE &gt; Reset. The configuration is saved under <a href="https://oreil.ly/nWfnq"><em>&lt;os.UserConfigDir&gt;/pprof/settings.json</em></a>. On my Linux machine, it is in <em>~/.config/pprof/settings.json</em>. This option also works only on the Top and Graph views and automatically changes to Default if you change to any other view.</p>&#13;
</dd>&#13;
<dt>DOWNLOAD</dt>&#13;
<dd>&#13;
<p>This option downloads the same profile you used in <code>go tool pprof</code>. It is useful if someone exposes the web viewer on the remote server and you want to save the remote profile.</p>&#13;
</dd>&#13;
<dt>Search regexp</dt>&#13;
<dd>&#13;
<p>You can search for samples of interest using the <a href="https://oreil.ly/c0vAq">RE2 regular expression</a> syntax by the location’s function name, filename, or object name. This sets the Focus option in the REFINE menu. In some views, like Top, Graph, and os.ReadFile, the interface also highlights matched samples as you write the expression.</p>&#13;
</dd>&#13;
<dt>The binary name and sample type</dt>&#13;
<dd>&#13;
<p>In the right-hand corner is a link with the chosen binary name and sample value type. You can click this menu item to open a small pop-up with quick statistics about the profile, view, and options we are running with. For example, <a data-type="xref" href="#img-obs-prof-nav">Figure 9-2</a> shows what you see when you click on that link with some REFINE options on.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Before diving into the different views available in the <code>pprof</code> tool, we have to understand important concepts of Flat and Cumulative (Cum for short) values for certain location granularity.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<p>Every <code>pprof</code> view shows Flat and Cumulative values for one or more locations:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Flat represents a certain node’s <em>direct</em> responsibility for resource or time usage.</p>&#13;
</li>&#13;
<li>&#13;
<p>Cumulative is a sum of <em>direct</em> in <em>indirect</em> contributions. Indirect means that the locations did not create any resource (or were not used anytime) directly, but may have invoked one or more functions that did.</p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
&#13;
<p>Using code examples is best to explain those definitions in detail. Let’s use part of the <code>main()</code> function from <a data-type="xref" href="#code-pprof-fd-usage">Example 9-2</a> presented in <a data-type="xref" href="#code-pprof-fd-usage2">Example 9-4</a>.</p>&#13;
<div data-type="example" id="code-pprof-fd-usage2">&#13;
<h5><span class="label">Example 9-4. </span>Snippet of <a data-type="xref" href="#code-pprof-fd-usage">Example 9-2</a> explaining Flat and Cumulative values</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">func</code><code class="w"> </code><code class="nx">main</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO4-1" id="co_data_driven_bottleneck_analysis_CO4-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
    </code><code class="c1">// ...</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">f</code><code class="p">,</code><code class="w"> </code><code class="nx">_</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">fd</code><code class="p">.</code><code class="nx">Open</code><code class="p">(</code><code class="s">"/dev/null"</code><code class="p">)</code><code class="w"> </code><code class="c1">// TODO: Check error. </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO4-2" id="co_data_driven_bottleneck_analysis_CO4-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="w">    </code><code class="nx">a</code><code class="p">.</code><code class="nx">files</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="nb">append</code><code class="p">(</code><code class="nx">a</code><code class="p">.</code><code class="nx">files</code><code class="p">,</code><code class="w"> </code><code class="nx">f</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO4-3" id="co_data_driven_bottleneck_analysis_CO4-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
&#13;
    </code><code class="nx">a</code><code class="p">.</code><code class="nx">OpenSingleFile</code><code class="p">(</code><code class="s">"/dev/null"</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">a</code><code class="p">.</code><code class="nx">OpenTenFiles</code><code class="p">(</code><code class="s">"/dev/null"</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO4-4" id="co_data_driven_bottleneck_analysis_CO4-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
&#13;
    </code><code class="c1">// ...</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO4-1" id="callout_data_driven_bottleneck_analysis_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Profiling is tightly coupled with a stack trace representing a call sequence that led to a certain sample, so in our case, opening files. However, we could aggregate all samples going through the <code>main()</code> function to learn more. In this case, the <code>main()</code> function Flat number of open files is 1, Cum is 12. This is because, in the main function, we directly open only one file (via <code>fd.Open</code>);<sup><a data-type="noteref" href="ch09.html#idm45606824757280" id="idm45606824757280-marker">11</a></sup> the rest were opened via chained (descendant) functions.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO4-2" id="callout_data_driven_bottleneck_analysis_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>From our <em>fd.pprof</em> profile, we could find that this code line Flat value is 1 and Cum is 1. It directly opens one file and does not contribute indirectly to any more file descriptor usage.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO4-3" id="callout_data_driven_bottleneck_analysis_CO4-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p><code>append</code> does not contribute to any sample. Therefore, no sample should include this code line.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO4-4" id="callout_data_driven_bottleneck_analysis_CO4-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>The code line that invokes the <code>a.OpenSingleFile</code> method has a Flat value of 0 and a Cum of 1. Similarly, the <code>a.OpenTenFiles</code> method Flat value is 0 and Cum is 10. Both directly in the moment of the CPU touching this program line do not create (yet) any files.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>I find the Flat and Cum names quite confusing, so I will use the direct and cumulative terms in further content. Both numbers are beneficial to compare what parts of the code contribute to the resource usage (or time used). The cumulative number helps us understand what flow is more expensive, whereas the direct value tells us the source of the potential bottleneck.</p>&#13;
&#13;
<p>Let’s walk through the different views and see how we can use them to analyze the <em>fd.pprof</em> file obtained in <a data-type="xref" href="#code-pprof-fd-usage">Example 9-2</a>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Top" data-type="sect3"><div class="sect3" id="ch-obs-profiling-res-top">&#13;
<h3>Top</h3>&#13;
&#13;
<p><a data-primary="go tool pprof reports" data-secondary="Top report" data-type="indexterm" id="ix_ch09-asciidoc9"/>First on the VIEW list, the Top report shows a table of statistics per location grouped by functions. The view for the <em>fd.pprof</em> file is presented in <a data-type="xref" href="#img-obs-prof-top">Figure 9-3</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-prof-top">&#13;
<img alt="efgo 0903" src="assets/efgo_0903.png"/>&#13;
<h6><span class="label">Figure 9-3. </span>The Top view is sorted by the direct value</h6>&#13;
</div></figure>&#13;
&#13;
<p>Each row represents direct and cumulative contributions of open files for the single function, which, as we learned from <a data-type="xref" href="#code-pprof-fd-usage2">Example 9-4</a>, aggregates the usage of one or multiple lines within that function. This is called function granularity, which can be configured by URL or CLI flag.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45606825183632">&#13;
<h5>Choose Your Granularity</h5>&#13;
<p><a data-primary="go tool pprof reports" data-secondary="function granularity flags" data-type="indexterm" id="idm45606825182464"/>Certain views like Top, Graph, and Flame Graph allow us to group locations by file, function, or not group at all (grouping by line or address). This means that one entry, row, or graph node will group contributions from all lines within a single function or file.</p>&#13;
&#13;
<p>You can choose the granularity by using one of the following flags in the <code>go tool pprof</code> command: <code>-functions</code> (default option), &#13;
<span class="keep-together"><code>-files</code>,</span> <code>-lines</code>, or <code>-address</code>. Similarly, you can set this using the URL parameter <code>?g=&lt;granularity&gt;</code>.</p>&#13;
&#13;
<p>Usually, function granularity is low enough, especially given low sampling rates (e.g., in the CPU profile). However, switching to line granularity effectively tells us exactly which code line contributes to the resource we profile for and where to find it. If the function has a nonzero direct contribution value, you might want to check what exact part of the function is a bottleneck!</p>&#13;
</div></aside>&#13;
&#13;
<p>We already defined the values represented by the Flat and Cum columns. Other columns in this view are:</p>&#13;
<dl>&#13;
<dt>Flat%</dt>&#13;
<dd>&#13;
<p>The percentage of the row’s direct contributions to the program’s total contributions. In our case, 99.11% of the open file descriptors were created directly by the <code>open</code> method (111 out of 112).</p>&#13;
</dd>&#13;
<dt>Sum%</dt>&#13;
<dd>&#13;
<p>The third column is the percentage of all direct values from the top to the current flow to the total contributions. For instance, the 2 top rows are directly responsible for all 112 file descriptors. This statistic allows us to narrow down to the functions that might matter the most for our bottleneck analysis.</p>&#13;
</dd>&#13;
<dt>Cum%</dt>&#13;
<dd>&#13;
<p>The percentage of the cumulative contribution of the row to the total &#13;
<span class="keep-together">contributions.</span></p>&#13;
<div data-type="warning" epub:type="warning"><h1>Be Careful When Goroutines Are Involved</h1>&#13;
<p><a data-primary="go tool pprof reports" data-secondary="goroutines and" data-type="indexterm" id="idm45606825169472"/>The cumulative value can be misleading in some cases with goroutines. For example, <a data-type="xref" href="#img-obs-prof-top">Figure 9-3</a> indicated that <code>runtime.main</code> cumulatively opened 12 files. However, from <a data-type="xref" href="#code-pprof-fd-usage">Example 9-2</a> you can find that it also executes the <code>Open100Fil⁠esConcurrently</code> method, which then executes <code>Open100FilesConcurrently.func1</code> (anonymous function) as a new goroutine. I would expect a link from <code>runtime.main</code> to <code>Open100FilesConcurrently.func1</code> in the Graph, and the cumulative value of <code>runtime.main</code> to be 112.</p>&#13;
&#13;
<p>The problem is that stack traces of each goroutine in Go are always separate. Therefore, there is no relation between goroutines in which goroutine created which one, which will be clear when we look at the goroutine profiles in <a data-type="xref" href="#ch-obs-pprof-goroutine">“Goroutine”</a>. We must keep this in mind while analyzing our program’s bottleneck.</p>&#13;
</div>&#13;
</dd>&#13;
<dt>Name and Inlined</dt>&#13;
<dd>&#13;
<p>The function name for the location and whether it was inlined during compilation. In <a data-type="xref" href="#code-pprof-fd-usage">Example 9-2</a>, both <code>open</code> and <code>OpenSingleFile</code> were simple enough for compiler to inline them to the parent functions. You can represent the situation from the binary (after inline) by adding the <code>-noinlines</code> flag to the <code>pprof</code> command or by adding the <code>?noinlines=t</code> URL parameter. Seeing the situation before inlining is still recommended to map what happened to the source code more easily.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The sorting order of rows in our Top table is by direct contribution, but we can change it with the <code>-cum</code> flag to order by cumulative values. We can also click on each header in the table to trigger different sorting in this view.</p>&#13;
&#13;
<p>The Top view might be the simplest and fastest way to find the functions (or files or lines, depending on the chosen granularity) directly or cumulatively responsible for using resources or time you are profiling for. The downside is that it does not tell us the exact link between those rows, which would tell us which code flow (full stack trace) might have triggered the usage. For such cases, it might be worth using the Graph view explained in the next section.<a data-startref="ix_ch09-asciidoc9" data-type="indexterm" id="idm45606824702944"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Graph" data-type="sect3"><div class="sect3" id="ch-obs-profiling-res-graph">&#13;
<h3>Graph</h3>&#13;
&#13;
<p><a data-primary="go tool pprof reports" data-secondary="Graph view" data-type="indexterm" id="ix_ch09-asciidoc10"/><a data-primary="Graph view" data-type="indexterm" id="ix_ch09-asciidoc11"/>The Graph view is the first thing you see when opening the <code>pprof</code> tool web interface. This is not without reason—humans work better if things <a href="https://oreil.ly/VElUH">are visualized</a> than if we have to parse and visualize all in our brain from the text report. This is my favorite view as well, especially for profiles obtained from less familiar code bases.</p>&#13;
&#13;
<p>To render the Graph view, the <code>pprof</code> tool generates a graphical <a href="https://oreil.ly/hzglQ">directed acyclic graph (DAG)</a> from the provided profile in the <a href="https://oreil.ly/HiRV9">DOT</a> format. We can then use the <code>-dot</code> flag with <code>go tool pprof</code>, and use other rendering tools or render it to the format we want with the <code>-svg</code>, <code>-png</code>, <code>-jpg</code>, <code>-gif</code>, or <code>-pdf</code> formats. On the other hand, we have the <code>-http</code> option that generates a temporary graphic using the <code>.svg</code> format and starts the web browser from it. From the browser, we can see the <code>.svg</code> visualization in the Graph view and use the interactive REFINE options explained before: zoom in, zoom out, and move around through the graph. The example Graph view from our <em>fd.pprof</em> format is presented in <a data-type="xref" href="#img-obs-prof-graph">Figure 9-4</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-prof-graph">&#13;
<img alt="efgo 0904" src="assets/efgo_0904.png"/>&#13;
<h6><span class="label">Figure 9-4. </span>The Graph view of <a data-type="xref" href="#code-pprof-fd-usage">Example 9-2</a> with function granularity</h6>&#13;
</div></figure>&#13;
&#13;
<p>What I love about this view is that it clearly represents the relation (hierarchy) of different execution parts of your program regarding resource or time usage. While it might be tempting, you cannot move nodes around. You can only hide or show them using the REFINE options. Hovering over a node also shows the full package name or code line.</p>&#13;
&#13;
<p>On top of that, every aspect of this graph <a href="https://oreil.ly/GQbNn">has its meaning</a>, which helps to find the most expensive parts. Let’s go through the graph attributes:</p>&#13;
<dl class="less_space pagebreak-before">&#13;
<dt>Node</dt>&#13;
<dd>&#13;
<p>Each node represents the contribution of a function for the currently opened files. This is why the first part of the text in the node shows the Go package and function (or method). We would see the code line or file if we chose a different granularity. The second part of the node shows the direct and cumulative values. If any of the values are nonzero, we see that the percentage of that value to the total contributions. For example, in <a data-type="xref" href="#img-obs-prof-graph">Figure 9-4</a> we see the <code>main.main()</code> node (on the right) confirms the number we found in <a data-type="xref" href="#code-pprof-fd-usage2">Example 9-4</a>. Using <code>pprof</code>, we recorded 1 direct contribution and 12 cumulative ones in that function. The color and size tell us something too:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The size of the node represents direct contributions. The bigger the node, the more resource or time it used directly.</p>&#13;
</li>&#13;
<li>&#13;
<p>The border and fill color represent cumulative values. The normal color is gold. Large positive cumulative numbers make the node red. Cumulative values close to zero cause the node to be gray.</p>&#13;
</li>&#13;
</ul>&#13;
</dd>&#13;
<dt>Edge</dt>&#13;
<dd>&#13;
<p>Each edge represents the call path between functions (files or lines). The call does not need to be direct. For example, if you use the <code>REFINE</code> option, you can hide multiple nodes that were called between two, causing the edge to show an indirect link. The value on the edge represents the cumulative contributions of that code path. The <code>inline</code> word next to the number tells us that the call pointed to by edge was inlined into the caller. Other characteristics matter as well:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The weight of the edge indicates cumulative contributions by a path. The thicker the edge, the more resources were used.</p>&#13;
</li>&#13;
<li>&#13;
<p>The color shows the same. Normally an edge is gold. Larger positive values color an edge red, close to zero to gray.</p>&#13;
</li>&#13;
<li>&#13;
<p>A dashed edge indicates that some connected locations were removed, e.g., because of a node limit.<sup><a data-type="noteref" href="ch09.html#idm45606824674864" id="idm45606824674864-marker">12</a></sup></p>&#13;
</li>&#13;
</ul>&#13;
</dd>&#13;
</dl>&#13;
<div data-type="warning" epub:type="warning"><h1>Some Nodes Might Be Hidden!</h1>&#13;
<p>Don’t be surprised if you don’t see every contribution to the resource you profile in the Graph view. As I mentioned before, most of the profiles are sampled. This means that statistically, the locations that contribute a little might be missed in the resulting profile.</p>&#13;
&#13;
<p>The second reason is the node limit in the <code>pprof</code> viewer. By default, it does not show more <a href="https://oreil.ly/Wcwsu">than 80 nodes</a> for readability. You can change that limit using the <code>-nodecount</code> flag.</p>&#13;
&#13;
<p>Finally, the <code>-edgefraction</code> and <code>-nodefraction</code> settings hide the edges and nodes with the fraction of direct contribution to the total contribution lower than the specified value. By <a href="https://oreil.ly/oVfrt">default</a> it is 0.005 (0.5%) for node fraction and 0.001 (0.1%) for edge fraction.</p>&#13;
</div>&#13;
&#13;
<p>With theory aside, what can we learn from the <code>pprof</code> Graph view? This view is perfect for learning about efficiency bottlenecks and how to find their source. From <a data-type="xref" href="#img-obs-prof-graph">Figure 9-4</a> we can immediately see that the biggest cumulative contributor is <code>Open100FilesConcurrently</code>, which seems to be a new goroutine since it is not connected to the <code>runtime/main</code> function. It might be a good idea to optimize that path first. The most open files come from <code>OpenTenFiles</code> and <code>open</code>. This tells us that it’s a critical path for the efficiency of this resource. If some new functionality required creating an additional file on every <code>open</code> call, we would see a significant growth in opened file descriptors by our Go program.</p>&#13;
&#13;
<p>The Graph view is an excellent method to understand how your application’s different functionalities impact your program’s resource usage. It is especially important for more complex programs with large dependencies your team did not create. As it turns out, it is easy to misunderstand the right way of using the library you depend on. Unfortunately, this also means that there will be a lot of function names or code lines you don’t recognize or don’t understand. See <a data-type="xref" href="#img-obs-prof-graph2">Figure 9-5</a>, taken from the optimized <code>Sum</code> we optimize in <a data-type="xref" href="ch10.html#ch-opt-latency-concurrency-example">“Optimizing Latency Using Concurrency”</a>.</p>&#13;
&#13;
<p>This result also proves the importance of the skill of switching between different granularity. It’s as easy as adding to a URL <code>?g=lines</code> to switch to line granularity—it’s way more effective than reopening <code>go tool pprof</code> with the <code>-lines</code> flag.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-prof-graph2">&#13;
<img alt="efgo 0905" src="assets/efgo_0905.png"/>&#13;
<h6><span class="label">Figure 9-5. </span>Snippet of the Graph view of the CPU profile taken from <a data-type="xref" href="ch10.html#code-sum-concurrent1">Example 10-10</a> with line granularity</h6>&#13;
</div></figure>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45606824656432">&#13;
<h5>Don’t Be Afraid of Unknowns!</h5>&#13;
<p>It’s normal to feel a bit anxious if you see your Go program’s Graph profile for the first time. For example, it isn’t uncommon to see various runtime functions. We don’t need to always have an idea of what they do exactly, but we can always find that information if we want to!</p>&#13;
&#13;
<p>Build your confidence in being able to dive into any new function or code line that matters. In <a data-type="xref" href="#img-obs-prof-graph2">Figure 9-5</a>, the <code>runtime.newproc</code> function was one of the biggest bottlenecks for CPU time. Instinct might tell us it has something to do with creating a new goroutine (kind of new process), but it’s relatively easy to confirm:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>A quick Google search for <code>runtime.newproc github</code> or Peek, Source, and &#13;
<span class="keep-together">Disassemble</span> views gives us the exact <a href="https://oreil.ly/3tgSz">code line of <code>newproc</code></a>. From this, we can try to read the comment or code and figure out what this function is responsible for (not always trivial).</p>&#13;
</li>&#13;
<li>&#13;
<p>The Top view or Graph view with line granularity tells the exact line where this contribution starts. As presented in <a data-type="xref" href="#img-obs-prof-graph2">Figure 9-5</a>, it is triggered by <a href="https://oreil.ly/YlASl">line 120</a>, which clearly shows creation of the <a data-startref="ix_ch09-asciidoc11" data-type="indexterm" id="idm45606824646832"/><a data-startref="ix_ch09-asciidoc10" data-type="indexterm" id="idm45606824646128"/>goroutine!</p>&#13;
</li>&#13;
</ul>&#13;
</div></aside>&#13;
&#13;
<p>Following the Graph view, we have the latest addition to the <code>pprof</code> tool—the Flame Graph view, which many members of the Go community prefer. So let’s dive into it.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Flame Graph" data-type="sect3"><div class="sect3" id="ch-obs-profiling-res-flame">&#13;
<h3>Flame Graph</h3>&#13;
&#13;
<p><a data-primary="Flame Graph view" data-type="indexterm" id="ix_ch09-asciidoc12"/><a data-primary="go tool pprof reports" data-secondary="Flame Graph view" data-type="indexterm" id="ix_ch09-asciidoc13"/><a data-primary="Icicle (Flame) Graph view" data-type="indexterm" id="ix_ch09-asciidoc14"/>The <a data-primary="Gregg, Brendan, on Flame Graph" data-type="indexterm" id="idm45606824639024"/>Flame Graph (sometimes also called the Icicle Graph) view in <code>pprof</code> is inspired by Brendan Gregg’s <a href="https://oreil.ly/sKFbH">work</a>, focused initially on CPU profiling.</p>&#13;
<blockquote>&#13;
<p>A flame graph visualizes a collection of stack traces (aka call stacks), shown as an adjacency diagram with an inverted icicle layout. Flame graphs are commonly used to visualize CPU profiler output, where stack traces are collected using sampling.</p>&#13;
<p data-type="attribution">Brendan Gregg, <a href="https://oreil.ly/RAsrK">“The Flame Graphs”</a></p></blockquote>&#13;
&#13;
<p>The Flame Graph report rendered from <em>fd.pprof</em> is presented in <a data-type="xref" href="#img-obs-prof-flame">Figure 9-6</a>.</p>&#13;
<div data-type="note" epub:type="note"><h1>Color and Order of Segments Usually Do Not Matter</h1>&#13;
<p>This depends on the tool that renders the Flame Graph, but for the <code>pprof</code> tool, both color and order do not have any meaning here. The segments are typically sorted by the location name or label value.</p>&#13;
</div>&#13;
&#13;
<figure><div class="figure" id="img-obs-prof-flame">&#13;
<img alt="efgo 0906" src="assets/efgo_0906.png"/>&#13;
<h6><span class="label">Figure 9-6. </span>The Flame Graph view of <a data-type="xref" href="#code-pprof-fd-usage">Example 9-2</a> with a function granularity</h6>&#13;
</div></figure>&#13;
&#13;
<p>The <code>pprof</code> is an inverted version of the original Flame Graph, where each significant code flow forms a separate icicle. The main attribute that matters here is the width of the rectangular segment, which represents the node from the Graph view—function in our case. The wider the block, the larger the cumulative contribution it is &#13;
<span class="keep-together">responsible</span> for. You can hover over individual segments to see their absolute and percentage cumulative values. Click on each block to focus the view on the given code path.</p>&#13;
&#13;
<p>Instead of edges, we can follow call hierarchy by looking at what’s above the current segment. Don’t focus too much on the height of the icicle—it only shows how complex (deep) the call stack is. It’s the width that matters here.</p>&#13;
&#13;
<p>In some way, a Flame Graph is often favored by more advanced engineers because it’s more compact. It allows a pragmatic insight into the biggest bottlenecks of the system. It immediately shows the percentage of all resources that each code path contributed. At a glance, in <a data-type="xref" href="#img-obs-prof-flame">Figure 9-6</a> we can quickly tell without any interactivity that <code>Open100FilesConcurrently.func1</code> is the major bottleneck of opened files with approximately 90% of resources used by it. The Flame Graph is also excellent to show if there is any major bottleneck. On some occasions, a lot of small contributors might together generate a large usage. A Flame Graph will tell us about this situation immediately. Note that similar to the <a data-type="xref" href="#img-obs-prof-graph">Figure 9-4</a> view, it can drop many nodes from the view. The number of dropped nodes is presented if you click the binary name at the top right corner.</p>&#13;
&#13;
<p>Any of the three views we discussed—Top, Graph, or Flame Graph—should be the first point of interest to find the biggest bottleneck in our program efficiency. Remember about sampling, switching granularity to learn more, and focusing your time on the biggest bottlenecks first. However, three more views are worth briefly mentioning: Peek, Source, and Disassemble. Let’s look at them in the next section.<a data-startref="ix_ch09-asciidoc14" data-type="indexterm" id="idm45606824622080"/><a data-startref="ix_ch09-asciidoc13" data-type="indexterm" id="idm45606824621376"/><a data-startref="ix_ch09-asciidoc12" data-type="indexterm" id="idm45606824620704"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Peek, Source, and Disassemble" data-type="sect3"><div class="sect3" id="ch-obs-profiling-res-other">&#13;
<h3>Peek, Source, and Disassemble</h3>&#13;
&#13;
<p>The other three views—Peek, Source, and Disassemble—are not affected by the granularity option. They all show the raw line or address level of locations, which is especially useful if you want to go back to your source code to focus on your code optimization inside your favorite IDE.</p>&#13;
&#13;
<p><a data-primary="go tool pprof reports" data-secondary="Peek view" data-type="indexterm" id="idm45606824617184"/><a data-primary="Peek view" data-type="indexterm" id="idm45606824616208"/>The Peek view provides a table similar to the Top view. The only difference is that each code line shows all direct callers and the usage distribution in the Call and Calls% columns. It helps in cases with many callers where you want to narrow down the code path that contributes the most.</p>&#13;
&#13;
<p><a data-primary="go tool pprof reports" data-secondary="Source view" data-type="indexterm" id="idm45606824614848"/><a data-primary="Source view" data-type="indexterm" id="idm45606824613872"/>One of my favorite tools is the Source view. It shows the exact code line in the context of the program source code. In addition, it shows the few lines before and after. Unfortunately, the output is not ordered, so you have to use previous views to know what function or code line you want to focus on, and use the Search feature to focus on what you want. For example, we could see direct and cumulative contributions of <code>Open100FilesConcurrently</code> directly mapped to the code line in our code, as presented in <a data-type="xref" href="#img-obs-prof-source">Figure 9-7</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-prof-source">&#13;
<img alt="efgo 0907" src="assets/efgo_0907.png"/>&#13;
<h6><span class="label">Figure 9-7. </span>The Source view of <a data-type="xref" href="#code-pprof-fd-usage">Example 9-2</a> focused on the &#13;
<span class="keep-together"><code>Open100FilesConcurrently</code></span> search</h6>&#13;
</div></figure>&#13;
&#13;
<p>For me, there is something special in the Source view. Seeing the open file descriptors, allocation points, CPU time, etc., directly mapped to a code statement in your source code gives a bigger understanding and awareness than seeing lines as a bunch of boxes in <a data-type="xref" href="#img-obs-prof-graph">Figure 9-4</a>. For the standard library code, or when you provide a binary (as mentioned for the Disassemble view), you can also click on a function to display its assembly code!</p>&#13;
&#13;
<p>The Source view is incredibly useful when attempting to estimate the <a data-type="xref" href="ch07.html#ch-hw-complexity">“Complexity Analysis”</a> of the code we profile. I recommend using the Source view if you can’t fully wrap your head around the part of the code that uses the resource and why.</p>&#13;
&#13;
<p><a data-primary="Disassemble view" data-type="indexterm" id="idm45606824604272"/><a data-primary="go tool pprof reports" data-secondary="Disassemble view" data-type="indexterm" id="idm45606824603568"/>Finally, the Disassemble view is useful for advanced profiling. It provides the Source view, but at the assembly level (see <a data-type="xref" href="ch04.html#ch-hw-assembly">“Assembly”</a>). It allows checking compilation details around the problematic code. This view requires a provided binary built from the same source code as the program you took the profile from. For &#13;
<span class="keep-together">example,</span> for my case with the <em>fd.inuse</em> file, I have to provide a statically built binary via a path using <code>go tool pprof -http :8080 pkg/profile/fd/example/main fd.pprof</code>.<sup><a data-type="noteref" href="ch09.html#idm45606824599744" id="idm45606824599744-marker">13</a></sup></p>&#13;
<div data-type="warning" epub:type="warning">&#13;
<p>Currently, no mechanism will check if you are using the correct program binary for the profile you analyze. Therefore, the results might be, by accident, correct or totally wrong. The result in the error case is nondeterministic, so ensure you provide the correct binary!</p>&#13;
</div>&#13;
&#13;
<p>The <code>pprof</code> tool is an amazing way to confirm, in a data-driven way, your initial guesses about the efficiency of your application and what causes the potential problems. The amazing thing about the skills you acquired in this section is that the mentioned text and visual representations of the <code>pprof</code> profiles are not only used by the native <code>pprof</code> tooling. Similar views and techniques are used among many other profiling tools and paid vendor services, like <a href="https://oreil.ly/HowVb">Polar Signals</a>, <a href="https://oreil.ly/Ru0Hu">Grafana Phlare</a> <a href="https://oreil.ly/mJu6V">Google Profiler</a>, <a href="https://oreil.ly/WF9fG">Datadog’s Continuous Profiler</a>, <a href="https://oreil.ly/eKyK7">Pyroscope project</a>, and more!</p>&#13;
&#13;
<p>It is also quite likely that your Go IDE<sup><a data-type="noteref" href="ch09.html#idm45606824589952" id="idm45606824589952-marker">14</a></sup> supports rendering and gathering <code>pprof</code> profiles out of the box. Using IDE is great as it can integrate directly into your source code and enable smooth navigation through locations. However, I prefer <code>go tool pprof</code> and <code>pprof</code> tool-based cloud projects like <a href="https://oreil.ly/2PKkx">the Parca project</a> since we often have to profile on the macrobenchmarks level (see <a data-type="xref" href="ch08.html#ch-obs-macro">“Macrobenchmarks”</a>)<a data-startref="ix_ch09-asciidoc8" data-type="indexterm" id="idm45606824584880"/><a data-startref="ix_ch09-asciidoc7" data-type="indexterm" id="idm45606824584176"/><a data-startref="ix_ch09-asciidoc6" data-type="indexterm" id="idm45606824583504"/>.<a data-startref="ix_ch09-asciidoc2" data-type="indexterm" id="idm45606824582704"/><a data-startref="ix_ch09-asciidoc1" data-type="indexterm" id="idm45606824582000"/></p>&#13;
&#13;
<p>With the format and visualization descriptions complete, let’s dive into how to obtain profiles from your Go program.</p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Capturing the Profiling Signal" data-type="sect1"><div class="sect1" id="ch-obs-pprof-obtain">&#13;
<h1>Capturing the Profiling Signal</h1>&#13;
&#13;
<p><a data-primary="bottleneck analysis" data-secondary="capturing the profiling signal" data-type="indexterm" id="ix_ch09-asciidoc15"/><a data-primary="profiling" data-secondary="capturing the profiling signal" data-type="indexterm" id="ix_ch09-asciidoc16"/>Recently we started treating profiling as <a href="https://oreil.ly/zlAis">a fourth observability signal</a>. This is because profiling, in many ways, is very similar to the previously discussed signals in <a data-type="xref" href="ch06.html#ch-observability">Chapter 6</a>, like metrics, logging, and tracing. For example, similar to other signals, we need instrumentation and reliable experiments to obtain meaningful data.</p>&#13;
&#13;
<p>We discussed how to write custom instrumentation in <a data-type="xref" href="#ch-obs-pprof">“pprof Format”</a>, and we will go through common existing profilers available in Go runtime. However, it’s not enough to be able to fetch profiles about various resource usage in our program—we also need to know how to trigger situations that would give us the information about the efficiency bottleneck we want.</p>&#13;
&#13;
<p>Fortunately, we already went through <a data-type="xref" href="ch07.html#ch-obs-rel">“Reliability of Experiments”</a> and <a data-type="xref" href="ch07.html#ch-obs-benchmarking">“Benchmarking Levels”</a> that explained reliable experiments. Profiling practices are designed to integrate with our benchmarking process naturally. This enables a pragmatic optimization workflow that fits well in our TFBO loop (<a data-type="xref" href="ch03.html#ch-conq-eff-flow">“Efficiency-Aware Development Flow”</a>):</p>&#13;
<ol>&#13;
<li>&#13;
<p>We perform a benchmark on the desired level (micro, macro, or production) to assure the efficiency of our program.</p>&#13;
</li>&#13;
<li>&#13;
<p>If we are not happy with the result, we can rerun the same benchmark while also capturing the profile during or at the end of the experiment to find the efficiency bottleneck.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
<div data-type="tip"><h1>Always-On Profiling</h1>&#13;
<p>You can design your workflow to not need to rerun the benchmark for profiling capturing. In <a data-type="xref" href="ch08.html#ch-obs-micro">“Microbenchmarks”</a>, I recommended always capturing your profiles on most of your Go benchmarks. In <a data-type="xref" href="#ch-obs-cont-profiling">“Continuous Profiling”</a>, you will learn how to profile continuously at macro or production levels!</p>&#13;
</div>&#13;
&#13;
<p>Having instrumentation and the right experiment (reusing benchmarks) is great. Still, we also need to learn how to trigger and transfer the profile from the instrumentation of your choice to analysis with the tools you learned in <a data-type="xref" href="#ch-obs-profiling-res">“go tool pprof Reports”</a>.</p>&#13;
&#13;
<p>We need to know the API for the profiler we want to use for that purpose. As we learned in <a data-type="xref" href="ch06.html#ch-observability">Chapter 6</a>, similar to other signals, we generally have two main types of instrumentation: autoinstrumentation and manual. Regarding the former model, there are many ways to obtain profiles about our Go program without adding a single line of code! With technology like <a href="https://oreil.ly/8mqs6">eBPF</a>, we can have instrumentation for virtually any resource usage of our Go program. Many open source projects, start-ups, or established vendors are on the mission to make this space accessible and easier to use.</p>&#13;
&#13;
<p>However, everything is a trade-off. The eBPF is still early technology that works only on Linux. It has some portability challenges across Linux kernel versions and nontrivial maintainability costs. It is also usually a generic solution that will never have the same reliability and ability to provide semantic, application-level profiles as we can now with more manual, in-process profilers. Finally, this is a Go programming language book, so I would love to share how to create, capture, and use native in-process profilers.</p>&#13;
&#13;
<p>The API for using instrumentation depends on the implementation. For example, you can write a profiler that will save a profile on a disk every minute or every time some event occurs (e.g., <a href="https://oreil.ly/xCW7u">when a certain Linux signal is captured</a>). However, generally in the Go community, we can outline three main patterns of triggering and saving profiles:</p>&#13;
<dl>&#13;
<dt>Programmatically triggered</dt>&#13;
<dd>&#13;
<p>Most profilers you will see and use in Go can be manually inserted into your code to save profiles when you want. This is what I used in <a data-type="xref" href="#code-pprof-fd-usage">Example 9-2</a> to capture the <em>fd.pprof</em> file we were analyzing in <a data-type="xref" href="#ch-obs-profiling-res">“go tool pprof Reports”</a>. The typical interface has a signature similar to the <code>WriteTo(w io.Writer) error</code> (used in <a data-type="xref" href="#code-pprof-fd">Example 9-1</a>) that captures samples that were recorded from the beginning of the program run. The profile in <code>pprof</code> format is then written to a writer of your choice (typically a file).</p>&#13;
&#13;
<p>Some profilers set an explicit starting point when the profiler starts recording samples. This is true, for example, for the CPU profiler (see <a data-type="xref" href="#ch-obs-pprof-cpu">“CPU”</a>) that has a signature like <code>StartCPUProfile(w io.Writer) error</code> to start the cycle, and then <code>StopCPUProfile()</code> to end the profiling cycle.</p>&#13;
&#13;
<p>This pattern of using the profiles is great for quick tests in the development environment or when used in the microbenchmarks code (see <a data-type="xref" href="ch08.html#ch-obs-micro">“Microbenchmarks”</a>). Usually, however, developers don’t use it directly. Instead, they often use it as a building block for two other patterns: Go benchmark integrations and HTTP handlers:</p>&#13;
</dd>&#13;
<dt>Go benchmark integrations</dt>&#13;
<dd>&#13;
<p>As presented in an example command I typically use for Go benchmarks in <a data-type="xref" href="ch08.html#code-sum-go-bench-all">Example 8-4</a>, you can fetch all standard profiles from a microbenchmark by specifying flags in the <code>go test</code> tool. Almost all profiles explained in <a data-type="xref" href="#ch-obs-prof-common">“Common Profile Instrumentation”</a> can be enabled using the <code>-memprofile</code>, &#13;
<span class="keep-together"><code>-cpuprofile</code>,</span> <code>-blockprofile</code>, and <code>-mutexprofile</code> flags. No need to put custom code into your benchmark unless you want to trigger the profile at a certain moment. There’s no support for custom profiles at the moment.</p>&#13;
</dd>&#13;
<dt>HTTP handlers</dt>&#13;
<dd>&#13;
<p>Finally, an HTTP server is the most common way to capture profiles for programs at macro and production levels. This pattern is especially useful for backend Go applications, which by default accept HTTP connections for normal use. It’s then fairly easy to add special HTTP handlers for profiling and other monitoring functionalities (e.g., the Prometheus <code>/metrics</code> endpoint). Let’s explore this pattern next.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The standard Go library provides HTTP server handlers for all profilers using the <code>pprof.Profile</code> structure, for example, our <a data-type="xref" href="#code-pprof-fd">Example 9-1</a> profiler or any of the standard profiles explained in <a data-type="xref" href="#ch-obs-prof-common">“Common Profile Instrumentation”</a>. You can add these handlers to your <code>http.Server</code> in a few code lines in your Go program, as presented in <a data-type="xref" href="#code-pprof-http">Example 9-5</a>.</p>&#13;
<div data-type="example" id="code-pprof-http">&#13;
<h5><span class="label">Example 9-5. </span>Creating the HTTP server with debug handlers for custom and standard profilers</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kn">import</code><code class="w"> </code><code class="p">(</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"net/http"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"net/http/pprof"</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/felixge/fgprof"</code><code class="w">&#13;
</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="c1">// ...</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="nx">m</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">http</code><code class="p">.</code><code class="nx">NewServeMux</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO5-1" id="co_data_driven_bottleneck_analysis_CO5-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="nx">m</code><code class="p">.</code><code class="nx">HandleFunc</code><code class="p">(</code><code class="s">"/debug/pprof/"</code><code class="p">,</code><code class="w"> </code><code class="nx">pprof</code><code class="p">.</code><code class="nx">Index</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO5-2" id="co_data_driven_bottleneck_analysis_CO5-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="nx">m</code><code class="p">.</code><code class="nx">HandleFunc</code><code class="p">(</code><code class="s">"/debug/pprof/profile"</code><code class="p">,</code><code class="w"> </code><code class="nx">pprof</code><code class="p">.</code><code class="nx">Profile</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO5-3" id="co_data_driven_bottleneck_analysis_CO5-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
</code><code class="nx">m</code><code class="p">.</code><code class="nx">HandleFunc</code><code class="p">(</code><code class="s">"/debug/fgprof/profile"</code><code class="p">,</code><code class="w"> </code><code class="nx">fgprof</code><code class="p">.</code><code class="nx">Handler</code><code class="p">(</code><code class="p">)</code><code class="p">.</code><code class="nx">ServeHTTP</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO5-4" id="co_data_driven_bottleneck_analysis_CO5-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
&#13;
</code><code class="nx">srv</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">http</code><code class="p">.</code><code class="nx">Server</code><code class="p">{</code><code class="nx">Handler</code><code class="p">:</code><code class="w"> </code><code class="nx">m</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="c1">// Start server...</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO5-1" id="callout_data_driven_bottleneck_analysis_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The <code>Mux</code> structure allows registering HTTP server handlers on specific HTTP paths. Importing <code>_ "net/http/pprof"</code> will register standard profiles in the default global mux (<code>http.DefaultServeMux</code>) by default. However, I always recommend creating a new empty <code>Mux</code> instead of using a global one to be explicit for what paths you are registering. That’s why I register them manually in my &#13;
<span class="keep-together">example.</span></p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO5-2" id="callout_data_driven_bottleneck_analysis_CO5-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The <code>pprof.Index</code> handler exposes a root HTML index page that lists quick statistics and links to profilers registered using <code>pprof.NewProfile</code>. An example view is presented in <a data-type="xref" href="#img-obs-prof-index">Figure 9-8</a>. Additionally, this handler forwards to each profiler referenced by name; for example, <code>/debug/pprof/heap</code> will forward to the heap profiler (see <a data-type="xref" href="#ch-obs-pprof-heap">“Heap”</a>). Finally, this handler adds links to <code>cmdline</code> and <code>trace</code> handlers, which provides further debugging capabilities, and to the &#13;
<span class="keep-together"><code>profile</code></span> registered line below.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO5-3" id="callout_data_driven_bottleneck_analysis_CO5-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>The standard Go CPU is not using <code>pprof.Profile</code>, so we have to register that HTTP path explicitly.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO5-4" id="callout_data_driven_bottleneck_analysis_CO5-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>The same profile-capturing method can be used for third-party profilers, e.g., the profiler for <a data-type="xref" href="#ch-obs-pprof-latency">“Off-CPU Time”</a> called <code>fgprof</code>.</p></dd>&#13;
</dl></div>&#13;
&#13;
<figure><div class="figure" id="img-obs-prof-index">&#13;
<img alt="efgo 0908" src="assets/efgo_0908.png"/>&#13;
<h6><span class="label">Figure 9-8. </span>The served HTML page from the &#13;
<span class="plain">debug/pprof/</span> path of the server created in <a data-type="xref" href="#code-pprof-http">Example 9-5</a></h6>&#13;
</div></figure>&#13;
&#13;
<p>The index page is nice to have if you forget what name the profiler uses or what profilers you have available in your Go program. Notice that our custom <a data-type="xref" href="#code-pprof-fd">Example 9-1</a> profiler is also on this list (<code>fd.inuse</code> with 165 files<sup><a data-type="noteref" href="ch09.html#idm45606824378144" id="idm45606824378144-marker">15</a></sup>), because it was created using <code>pprof.NewProfile</code>. For programs that do not import the <code>fd</code> package that has the code presented in <a data-type="xref" href="#code-pprof-fd">Example 9-1</a>, this index page would miss the <code>fd.inuse</code> line.</p>&#13;
&#13;
<p>A nice debugging page is not the primary purpose of the HTTP handlers. Their fundamental benefit is that a human operator or automation can dynamically capture the profiles from outside, triggering them in the most relevant moments of the macro test, incident, or normal production run. In my experience, I have found four ways of using the profilers via the HTTP protocol:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>You can click on the link for the desired profiler in the HTML page visible in <a data-type="xref" href="#img-obs-prof-index">Figure 9-8</a>, for example, <code>heap</code>. This will open the <code>http://&lt;address&gt;/debug/pprof/heap?debug=1</code> URL that prints the count of samples per stack trace in the current moment—a simplified memory profile in text format.</p>&#13;
</li>&#13;
<li>&#13;
<p>Removing the <code>debug</code> parameter will download the desired profile in <code>pprof</code> format; e.g., the <code>http://&lt;address&gt;/debug/pprof/heap</code> URL in the browser will download the memory profile explained in <a data-type="xref" href="#ch-obs-pprof-heap">“Heap”</a> to a local file. You can then open this file using <code>go tool pprof</code>, as I explained in <a data-type="xref" href="#ch-obs-profiling-res">“go tool pprof Reports”</a>.</p>&#13;
</li>&#13;
<li>&#13;
<p>You can point the <code>pprof</code> tool directly to the profiler URL to avoid the manual process of downloading the file. For example, we can open a web profiler viewer for a memory profile if we run in our terminal <code>go tool pprof -http :8080 http://&lt;address&gt;/debug/pprof/heap</code>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Finally, we can use another server to collect those profiles to a dedicated database periodically, e.g., using the <a href="https://oreil.ly/Ru0Hu">Phlare</a> or <a href="https://oreil.ly/2PKkx">Parca</a> projects explained in <a data-type="xref" href="#ch-obs-cont-profiling">“Continuous Profiling”</a>.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>To sum up, use whatever you find more convenient for the program you are analyzing. Profiling is great for understanding the efficiency of complex production applications in a microservice architecture, so the pattern of the HTTP API for capturing profiles is usually what I use. The Go benchmark profiling is perhaps the most useful for the micro level. The mentioned access patterns are commonly used in the Go community, but it doesn’t mean you can’t innovate and write the capturing flow that will fit to your workflow better.</p>&#13;
&#13;
<p>To explain the view types in <a data-type="xref" href="#ch-obs-profiling-res">“go tool pprof Reports”</a>, <code>pprof</code> format, and custom profilers, I created the simplest possible file descriptor profiling instrumentation (<a data-type="xref" href="#code-pprof-fd">Example 9-1</a>). Fortunately, we don’t need to write our instrumentation to have robust profiling for common machine resources. Go comes with a few standard profilers, well maintained and used by the community and users worldwide. Plus, I will mention a useful bonus profiler from the open source community. Let’s unpack those in the next section.<a data-startref="ix_ch09-asciidoc16" data-type="indexterm" id="idm45606824357632"/><a data-startref="ix_ch09-asciidoc15" data-type="indexterm" id="idm45606824356960"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Common Profile Instrumentation" data-type="sect1"><div class="sect1" id="ch-obs-prof-common">&#13;
<h1>Common Profile Instrumentation</h1>&#13;
&#13;
<p><a data-primary="instrumentation" data-secondary="common profile instrumentation" data-type="indexterm" id="ix_ch09-asciidoc17"/><a data-primary="profiling" data-secondary="common profile instrumentation" data-type="indexterm" id="ix_ch09-asciidoc18"/>In Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.html#ch-hardware">4</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch05.html#ch-hardware2">5</a>, I explained two main resources we have to optimize for—CPU time and memory. I also discussed how those could impact latency. The whole space can be intimidating at first, given the complexity and the concern given in <a data-type="xref" href="ch07.html#ch-obs-rel">“Reliability of Experiments”</a>. This is why it’s critical to understand what common profiling implementations Go has and how to use them. We will start with heap profiling.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Heap" data-type="sect2"><div class="sect2" id="ch-obs-pprof-heap">&#13;
<h2>Heap</h2>&#13;
&#13;
<p><a data-primary="alloc (heap) profile" data-type="indexterm" id="ix_ch09-asciidoc19"/><a data-primary="heap" data-secondary="profile" data-type="indexterm" id="ix_ch09-asciidoc20"/><a data-primary="profiling" data-secondary="heap profile" data-type="indexterm" id="ix_ch09-asciidoc21"/>The <code>heap</code> profile, also sometimes referred to as the <code>alloc</code> profile, provides a reliable way to find the main contributors of the memory allocated on the heap (explained in <a data-type="xref" href="ch05.html#ch-hw-go-mem">“Go Memory Management”</a>). However, similar to the <code>go_​mem⁠stats_heap</code> metric mentioned in <a data-type="xref" href="ch06.html#ch-obs-mem-usage">“Memory Usage”</a>, it only shows memory blocks allocated on the heap, not memory allocated on stack, or custom <code>mmap</code> calls. Still, the heap part of Go program memory usually causes the biggest problem; thus, the heap profile tends to be very useful, in my experience.</p>&#13;
&#13;
<p>You can redirect the heap profile to <code>io.Writer</code> using <a href="https://oreil.ly/kMjqJ"><code>pprof.Lookup​("heap").WriteTo(w, 0)</code></a>, with <code>-memprofile</code> on Go benchmark, or by calling the &#13;
<span class="keep-together"><code>/debug/pprof/heap</code></span> URL with handlers, as in <a data-type="xref" href="#code-pprof-http">Example 9-5</a>.<sup><a data-type="noteref" href="ch09.html#idm45606824335568" id="idm45606824335568-marker">16</a></sup></p>&#13;
&#13;
<p>The memory profiler has to be efficient for it to be feasible for practical purposes. That’s why the <code>heap</code> profiler is sampled and deeply integrated with the <a href="https://oreil.ly/NF1ni">Go runtime allocator flow</a> that is responsible for allocating values, pointers, and memory blocks (see <a data-type="xref" href="ch05.html#ch-hw-allocations">“Values, Pointers, and Memory Blocks”</a>). The sampling can be controlled by <a href="https://oreil.ly/iJaAU">the <code>runtime.MemProfileRate</code> variable</a> (or the <code>GODEBUG=memprofilerate=X</code> environment variable) and is defined as the average number of bytes that must be allocated to record a profile sample. By default, Go records a sample per every 512 KB of allocated memory on the heap.</p>&#13;
<div data-type="tip"><h1>What Memory Profile Rate Should You Choose?</h1>&#13;
<p>I would recommend not changing the default value of 512 KB. It is low enough for practical bottleneck analysis for most Go programs, and cheap enough so we can always have it on.</p>&#13;
&#13;
<p>For more detailed profiling values or to optimize a smaller size of allocations on the critical path, consider changing it to one byte to record all allocations in your program. However, this can impact your application’s latency and CPU time (which will be visible on the CPU profile). Still, it might be fine for your memory-focused benchmark.</p>&#13;
</div>&#13;
&#13;
<p>If you have multiple allocations in a single function, it is often useful to analyze the heap profile in <code>lines</code> granularity (add the <code>&amp;g=lines</code> URL parameter in the web viewer). An example heap profile of <code>labeler</code> in the <code>e2e</code> framework (see <a data-type="xref" href="ch08.html#ch-obs-macro-example">“Go e2e Framework”</a>) is presented in <a data-type="xref" href="#img-obs-prof-heap">Figure 9-9</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-prof-heap">&#13;
<img alt="efgo 0909" src="assets/efgo_0909.png"/>&#13;
<h6><span class="label">Figure 9-9. </span>The zoomed-in Graph view for the <code>heap</code> profile from the <code>labeler</code> <code>Sum</code> code from <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> in <code>alloc_space</code> dimension and <code>lines</code> granularity</h6>&#13;
</div></figure>&#13;
&#13;
<p>The unique aspect of the <code>heap</code> profile is that it has four value (sample) types, which you can choose in a new SAMPLE menu item. The currently selected value type is presented in the top right-hand corner. Each type is useful in a different way:</p>&#13;
<dl>&#13;
<dt><code>alloc_space</code></dt>&#13;
<dd>&#13;
<p>In this mode, the sample value means a total number of allocated bytes by location on the heap since the start of your program. This means that we will see all the memory that was allocated in the past, but most likely is already released by the garbage collection.</p>&#13;
<div data-type="warning" epub:type="warning">&#13;
<p>Don’t be surprised to see huge values here! For example, if the program runs for a longer time and one function allocates 100 KB every minute, it means ~411 GB after 30 days. This looks scary, but the same application might just use a maximum of 10 MB of physical memory during those 30 days.</p>&#13;
</div>&#13;
&#13;
<p>The total historical allocations are great to see in the code that in total allocated the largest amount of bytes in the past, which can lead to problems with the maximum memory used by that program. Even if the allocations made by certain locations were small but very frequent, it might be caused by the impact of the garbage collection (see <a data-type="xref" href="ch05.html#ch-hw-garbage">“Garbage Collection”</a>). The <code>alloc_space</code> is also very useful for spotting past events that allocated large space.</p>&#13;
&#13;
<p>For example, in <a data-type="xref" href="#img-obs-prof-heap">Figure 9-9</a> we see 78.6% of cumulative memory used by the <code>bytes.Split</code> function. This knowledge will be extremely valuable in the example in <a data-type="xref" href="ch10.html#ch-opt-mem-example">“Optimizing Memory Usage”</a>. As we already saw in <a data-type="xref" href="ch08.html#ch-obs-micro-go">“Go Benchmarks”</a>, the number of allocations is way larger than the dataset, so there must be a way to find a less expensive memory solution to splitting a string into lines.</p>&#13;
<div data-type="tip"><h1>Resetting Cumulative Allocations</h1>&#13;
<p>We can’t reset the heap profiler programmatically, for example, to start recording allocations from a certain moment.</p>&#13;
&#13;
<p>However, as you will learn in <a data-type="xref" href="#ch-obs-comp-prof">“Comparing and Aggregating Profiles”</a>, we can perform operations like subtracting the <code>pprof</code> values. So for example, we can capture the heap profile at moment A, then 30 seconds later at moment B, and create a “delta” heap profile that will show what allocation happened during those 30 seconds.</p>&#13;
&#13;
<p>There is also a hidden feature for Go <code>pprof</code> HTTP handlers. When capturing the <code>heap</code> profile, you can add a <code>seconds</code> parameter! For example, with <a data-type="xref" href="#code-pprof-http">Example 9-5</a> you can &#13;
<span class="keep-together">call <code>http://&lt;address&gt;/debug/pprof/heap?seconds=30s</code></span> to remotely capture a delta heap profile!</p>&#13;
</div>&#13;
</dd>&#13;
<dt><code>alloc_objects</code></dt>&#13;
<dd>&#13;
<p>Similar to <code>alloc_space</code>, the value tells us about the number of allocated memory blocks, not the actual space. This is mainly useful for finding the latency bottlenecks caused by frequent allocations.</p>&#13;
</dd>&#13;
</dl>&#13;
<dl class="less_space pagebreak-before">&#13;
<dt><code>inuse_space</code></dt>&#13;
<dd>&#13;
<p>This mode shows the currently allocated bytes on the heap—the allocated memory minus the released memory at each location. This value type is great for &#13;
<span class="keep-together">cases when</span> we want to find the memory bottleneck in a specific moment of the program.<sup><a data-type="noteref" href="ch09.html#idm45606824295584" id="idm45606824295584-marker">17</a></sup></p>&#13;
&#13;
<p>Finally, this mode is excellent for finding memory leaks. The memory that was constantly allocated and never released will stand out in the profile.</p>&#13;
<div data-type="tip"><h1>Finding the Source of the Memory Leaks</h1>&#13;
<p>The <code>heap</code> profile shows the code that allocated memory blocks, not the code (e.g., variables) that currently reference those memory blocks. To discover the latter, we could use the <a href="https://oreil.ly/c4rGl"><code>viewcore</code> utility</a> that analyzes the currently formed heap. This is, however, not trivial.</p>&#13;
&#13;
<p>Instead, try to statically analyze the code path first to find where the created structures might be referred. But even before that, check the <code>goroutine</code> profile in the next section first. We will discuss this problem in <a data-type="xref" href="ch11.html#ch-basic-leaks">“Don’t Leak Resources”</a>.</p>&#13;
</div>&#13;
</dd>&#13;
<dt><code>inuse_objects</code></dt>&#13;
<dd>&#13;
<p>The value shows the current number of allocated memory blocks (objects) on the heap. This is useful to reveal the amount of live objects on the heap, which represents well the amount of work for garbage collection (see <a data-type="xref" href="ch05.html#ch-hw-garbage">“Garbage Collection”</a>). Most of the CPU-bound work of garbage collection is in the mark phase that has to traverse through objects in a heap. So the more we have, the larger the negative impact allocation might be.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Knowing how to use <code>heap</code> profiles is a must-have skill for every Go developer interested in the efficiency of their programs. Focus on the code with the biggest contribution of allocations space. Don’t worry about the absolute numbers that might not correlate with the memory you use with other observability tools (see <a data-type="xref" href="ch06.html#ch-obs-mem-usage">“Memory Usage”</a>). With higher memory profile rates, you see only a portion of the allocations that statically matter.<a data-startref="ix_ch09-asciidoc21" data-type="indexterm" id="idm45606824283600"/><a data-startref="ix_ch09-asciidoc20" data-type="indexterm" id="idm45606824282928"/><a data-startref="ix_ch09-asciidoc19" data-type="indexterm" id="idm45606824282256"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Goroutine" data-type="sect2"><div class="sect2" id="ch-obs-pprof-goroutine">&#13;
<h2>Goroutine</h2>&#13;
&#13;
<p><a data-primary="goroutine profiler" data-type="indexterm" id="ix_ch09-asciidoc22"/><a data-primary="profiling" data-secondary="goroutine profiler" data-type="indexterm" id="ix_ch09-asciidoc23"/>The <code>goroutine</code> profiler can show us how many goroutines are running and what code they are executing. This includes all goroutines waiting on I/O, locks, channels, etc. There is no sampling for this profile—all goroutines except <a href="https://oreil.ly/bg2fB">system goroutines</a> are always captured.<sup><a data-type="noteref" href="ch09.html#idm45606824276208" id="idm45606824276208-marker">18</a></sup></p>&#13;
&#13;
<p>Similar to the <code>heap</code> profile, we can redirect this profile to <code>io.Writer</code> using <code>pprof.Lookup("goroutine").WriteTo(w, 0)</code>, with <code>-goroutineprofile</code> on Go benchmark, or by calling the <code>/debug/pprof/goroutine</code> URL with handlers, as in <a data-type="xref" href="#code-pprof-http">Example 9-5</a>. The overhead of capturing a <code>goroutine</code> profile can be significant for Go programs with a larger number of goroutines or when you care about every 10 ms of your program latency.</p>&#13;
&#13;
<p>The key value of the goroutine profile is to give you an awareness of what most of your code goroutines are doing. In some cases, you might be surprised how many goroutines your program requires to fulfill some functionality. Seeing a large (and perhaps increasing) number of goroutines doing the same thing might indicate a memory leak.</p>&#13;
&#13;
<p>Remember that, as mentioned in <a data-type="xref" href="#img-obs-prof-top">Figure 9-3</a>, for Go developers, by design, there is no link between the new goroutine and the goroutine that created it.<sup><a data-type="noteref" href="ch09.html#idm45606824268336" id="idm45606824268336-marker">19</a></sup> For this reason, the root location we see in the profile is always the first statement or function where the goroutine is called.</p>&#13;
&#13;
<p>The example Graph view for our <code>labeler</code> program is presented in <a data-type="xref" href="#img-obs-prof-goroutine">Figure 9-10</a>. We can see that <code>labeler</code> does not do a lot. In the zoom-out view, we can see there are only 13 goroutines, and none of the locations are the application logic—only profiler goroutine, signal goroutine, and a few HTTP server ones polling connection bytes. This indicates that perhaps the server is waiting on a TCP connection for the incoming request.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-prof-goroutine">&#13;
<img alt="efgo 0910" src="assets/efgo_0910.png"/>&#13;
<h6><span class="label">Figure 9-10. </span>The zoomed-in Graph view for the <code>goroutine</code> profile from the <code>labeler</code> <code>Sum</code> code from <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a></h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-primary="goroutine" data-secondary="common functions found in" data-type="indexterm" id="idm45606824259440"/>Still, <a data-type="xref" href="#img-obs-prof-goroutine">Figure 9-10</a> makes you aware of a few common functions you can typically find in the goroutine view:</p>&#13;
<dl>&#13;
<dt><code>runtime.gopark</code></dt>&#13;
<dd>&#13;
<p>The <a href="https://oreil.ly/Zqf2K"><code>gopark</code></a> is an internal function that keeps the goroutine waiting for the state until an external callback will get it back to work. Essentially it is a way for the runtime scheduler to pause (park) goroutines when they are waiting for things a bit longer—for example, channel communication, network I/O, or sometimes mutex locks.</p>&#13;
</dd>&#13;
<dt><code>runtime.chanrecv</code> and <code>runtime.chansend</code></dt>&#13;
<dd>&#13;
<p>As the name suggests, a goroutine in the <code>chanrecv</code> function is receiving messages or waiting for something to be sent in the channel. Similarly, it is in <code>chansend</code> if it is sending a message or waiting for the channel to have a buffer room.</p>&#13;
</dd>&#13;
<dt><code>runtime.selectgo</code></dt>&#13;
<dd>&#13;
<p>You will see this if the goroutine is waiting or checking cases in the <a href="https://oreil.ly/T52Kg"><code>select</code> statement</a>.</p>&#13;
</dd>&#13;
<dt><code>runtime.netpollblock</code></dt>&#13;
<dd>&#13;
<p>The <a href="https://oreil.ly/5Iw71"><code>netpoll</code> function</a> sets the goroutine to wait until the I/O bytes are received from the network connection.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>As you can see, it’s fairly easy to track the functions’ meaning, even if you are seeing them in your profile for the first time.<a data-startref="ix_ch09-asciidoc23" data-type="indexterm" id="idm45606824246000"/><a data-startref="ix_ch09-asciidoc22" data-type="indexterm" id="idm45606824245264"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="CPU" data-type="sect2"><div class="sect2" id="ch-obs-pprof-cpu">&#13;
<h2>CPU</h2>&#13;
&#13;
<p><a data-primary="CPU resource" data-secondary="profiling CPU usage" data-type="indexterm" id="ix_ch09-asciidoc24"/><a data-primary="profiling" data-secondary="CPU usage" data-type="indexterm" id="ix_ch09-asciidoc25"/>We profile the CPU to find the parts of code that use CPU time the most. Reducing that allows us to reduce the cost of running our program and enable easier system scalability. For the CPU-bound programs, shaving some CPU usage also means reduced latency.</p>&#13;
&#13;
<p>Profiling the CPU usage is proven to be very hard. The first reason for this is that the CPU just does a lot in a single moment—the CPU clocks can perform billions of operations per second. Understanding the full distribution of all the cycles across our program code is hard to track without slowing down significantly. The multi-CPU core programs make this problem even harder.</p>&#13;
&#13;
<p class="fix_tracking">At the time of writing this book, Go 1.19 provides a CPU profiler integrated into the Go runtime. Any CPU profiler adds some overhead, so it can’t just run in the background. We have to start and stop it for the whole process explicitly. Like other profilers, we can do that programmatically through the <code>pprof.StartCPUProfile(w)</code> and <code>pprof.StopCPUProfile()</code> functions. We can use the <code>-cpuprofile</code> flag on Go benchmark or the &#13;
<span class="keep-together"><code>/debug/pprof/profile?seconds=&lt;integer&gt;</code></span> URL with handlers in <a data-type="xref" href="#code-pprof-http">Example 9-5</a>.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>CPU Profile Has Its Start and End</h1>&#13;
<p>Don’t be surprised if the <code>profile</code> HTTP handler does not return the response immediately, as with other profiles! The HTTP handler will start the CPU profiler, run it for the number of seconds provided in the <code>seconds</code> parameter (30 seconds if not specified), and only then return the HTTP request.</p>&#13;
</div>&#13;
&#13;
<p>The current implementation is heavily sampled. When the profiler starts, it schedules the OS-specific timers to interrupt the program execution at the specified rate. On Linux, this means using either <a href="https://oreil.ly/tQNJK"><code>settimer</code></a> or <a href="https://oreil.ly/WdjVW"><code>timer_create</code></a> to set up timers for each OS thread, and in the Go runtime, listening for the <a href="https://oreil.ly/dcQTf"><code>SIGPROF</code></a> signal. The signal interrupts the Go runtime, which then obtains the current stack trace of the goroutine executing on that OS thread. The sample is then queued into a pre-allocated ring buffer, which is then scraped by the <code>pprof</code> writer every 100 milliseconds.<sup><a data-type="noteref" href="ch09.html#idm45606824228720" id="idm45606824228720-marker">20</a></sup></p>&#13;
&#13;
<p>The CPU profiling rate is currently hardcoded<sup><a data-type="noteref" href="ch09.html#idm45606824226864" id="idm45606824226864-marker">21</a></sup> to 100 Hz, so it will record, in theory, one sample from each OS thread every 10 ms of the CPU time (not real time). There are <a href="https://oreil.ly/VXEPO">plans</a> to make this value configurable in the future.</p>&#13;
&#13;
<p>Despite the CPU profile being one of the most popular efficiency workflows, it’s a complex problem to solve. It will serve you well for the typical cases, but it’s not perfect. For example, there are known problems on some OSes like the BSD<sup><a data-type="noteref" href="ch09.html#idm45606824221904" id="idm45606824221904-marker">22</a></sup> and various inaccuracies in <a href="https://oreil.ly/Ar8Up">some specific cases</a>. In the future, we might see some improvements in this space, with <a href="https://oreil.ly/zDSEq">new proposals</a> being currently considered that use <a href="https://oreil.ly/75AHf">hardware-based performance monitor units (PMUs)</a>.</p>&#13;
&#13;
<p>The example CPU profile showing the distribution of CPU time taken by each function for the <code>labeler</code> is presented in <a data-type="xref" href="#img-obs-prof-cpu">Figure 9-11</a>. Given the inaccuracies from the lower sampling rate, the function granularity view might lead to better conclusions.</p>&#13;
&#13;
<figure class="width-90"><div class="figure" id="img-obs-prof-cpu">&#13;
<img alt="efgo 0911" src="assets/efgo_0911.png"/>&#13;
<h6><span class="label">Figure 9-11. </span>The Flame Graph view for the 30-second CPU profile from the <code>labeler</code> <code>Sum</code> code from <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> at <code>functions</code> granularity</h6>&#13;
</div></figure>&#13;
&#13;
<p class="less_space pagebreak-before">The CPU profile comes with two value types:</p>&#13;
<dl>&#13;
<dt>Samples</dt>&#13;
<dd>&#13;
<p>The sample value indicates the number of samples observed at the location.</p>&#13;
</dd>&#13;
<dt>CPU</dt>&#13;
<dd>&#13;
<p>Each sample value represents the CPU time.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>From <a data-type="xref" href="#img-obs-prof-cpu">Figure 9-11</a>, we can see what we have to focus on if we want to optimize CPU time or latency caused by the amount of work by our <code>labeler</code> Go program. From the Flame Graph view, we can outline five major parts:</p>&#13;
<dl>&#13;
<dt><code>io.Copy</code></dt>&#13;
<dd>&#13;
<p>This function used by the code responsible for copying the file from local object storage takes 22.6% of CPU time. Perhaps we could utilize local caching to save that CPU time.</p>&#13;
</dd>&#13;
<dt><code>bytes.Split</code></dt>&#13;
<dd>&#13;
<p>This splits lines in <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> and takes 19.69%, so this function might be checked if there is any way we can split it into lines with less work.</p>&#13;
</dd>&#13;
<dt><code>gcBgMarkWorker</code></dt>&#13;
<dd>&#13;
<p>This function takes 15.6%, which indicates there was a large number of objects alive on the heap. Currently, the GC takes some portion of CPU time for garbage collection.</p>&#13;
</dd>&#13;
<dt><code>runtime.slicebytetostring</code></dt>&#13;
<dd>&#13;
<p>It indicates a nontrivial amount of CPU time (13.4%) is spent converting &#13;
<span class="keep-together">bytes to string.</span> Thanks to the Source view, I could track it to <code>num, err := ⁠strconv.Par⁠seInt(string(line), 10, 64)</code> line. This reveals a straightforward optimization of trying to come up with a function that parses integers directly from the byte slice.</p>&#13;
</dd>&#13;
<dt><code>strconv.ParseInt</code></dt>&#13;
<dd>&#13;
<p>This function uses 12.4% of CPU. We might want to check if there is any unnecessary work or checks we could remove by writing our parsing function (spoiler: there is).</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Turns out, such a CPU profile is valuable even if it is not entirely accurate. We will try the mentioned optimizations in <a data-type="xref" href="ch10.html#ch-opt-latency-example">“Optimizing Latency”</a>.<a data-startref="ix_ch09-asciidoc25" data-type="indexterm" id="idm45606824194176"/><a data-startref="ix_ch09-asciidoc24" data-type="indexterm" id="idm45606824193472"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Off-CPU Time" data-type="sect2"><div class="sect2" id="ch-obs-pprof-latency">&#13;
<h2>Off-CPU Time</h2>&#13;
&#13;
<p><a data-primary="CPU resource" data-secondary="profiling off-CPU time" data-type="indexterm" id="ix_ch09-asciidoc26"/><a data-primary="off-CPU time" data-secondary="profiling" data-type="indexterm" id="ix_ch09-asciidoc27"/><a data-primary="profiling" data-secondary="off-CPU time" data-type="indexterm" id="ix_ch09-asciidoc28"/>It is often forgotten, but the typical goroutines mostly wait for work instead of executing on the CPU. This is why when looking to optimize the latency of our &#13;
<span class="keep-together">program’s</span> functionality, we can’t just look at CPU time.<sup><a data-type="noteref" href="ch09.html#idm45606824186560" id="idm45606824186560-marker">23</a></sup> For all programs, especially the I/O-bound ones, your process might take a lot of time sleeping or waiting. Specifically, we can define four categories that compose the entire program execution, presented in <a data-type="xref" href="#img-obs-prof-walltime">Figure 9-12</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-prof-walltime">&#13;
<img alt="efgo 0912" src="assets/efgo_0912.png"/>&#13;
<h6><span class="label">Figure 9-12. </span>The process execution time composition<sup><a data-type="noteref" href="ch09.html#idm45606824182112" id="idm45606824182112-marker">24</a></sup></h6>&#13;
</div></figure>&#13;
&#13;
<p>The first observation is that the total execution time is longer than the wall time, so real time elapsed when executing this program. It’s not because computers can slow time somehow; it’s because all Go programs are multithreaded (or even multigoroutines in Go), so the total measured execution time will always be longer than real time. We can outline four categories of execution time:</p>&#13;
<dl>&#13;
<dt>CPU time</dt>&#13;
<dd>&#13;
<p>The time our program actively spent using CPU, as explained in <a data-type="xref" href="#ch-obs-pprof-cpu">“CPU”</a>.</p>&#13;
</dd>&#13;
</dl>&#13;
<dl>&#13;
<dt>Block time</dt>&#13;
<dd>&#13;
<p>The mutex time, plus the time our process spent waiting for Go channel communication (e.g., <code>&lt;-ctx.Done()</code>, as discussed in <a data-type="xref" href="ch04.html#ch-hw-concurrency">“Go Runtime Scheduler”</a>), so all synchronization primitives. We can profile that time using the <code>block</code> profiler. It’s not enabled by default, so we need to turn it on by setting a nonzero block profiling rate using <a href="https://oreil.ly/GwjwY"><code>runtime.SetBlockProfileRate(int)</code></a>. This specifies the number of nanoseconds spent blocked for one blocking event sample. Then we can use <code>pprof.Lookup</code> in Go, <code>-blockprofile</code> in Go benchmark, or the <code>/debug/pprof/block</code> HTTP handler to capture <code>contention</code> and <code>delay</code> value types.</p>&#13;
</dd>&#13;
<dt>Mutex time</dt>&#13;
<dd>&#13;
<p>The time spent on lock contentions (e.g., the time spent in <a href="https://oreil.ly/chnpS"><code>sync.RWMutex.Lock</code></a>). Like block profile, it’s disabled by default and can be enabled with <a href="https://oreil.ly/oIg45"><code>runtime.SetMutexProfileFraction(int)</code></a>. Fraction specifies that <code>1/<em>&lt;fraction&gt;</em></code> lock contentions should be tracked. Similarly, we can use <code>pprof.Lookup</code> in Go, <code>-mutexprofile</code> in Go benchmark, or the &#13;
<span class="keep-together"><code>/debug/pprof/mutex</code></span> HTTP handler to capture <code>mutex</code> and <code>delay</code> value types.</p>&#13;
</dd>&#13;
<dt>Untracked off-CPU time</dt>&#13;
<dd>&#13;
<p>The goroutines that are sleeping, waiting for CPU time, I/O (e.g., from disk, network, or external device), syscalls, and so on are not tracked by any standard profiling tool. To discover the impact of that latency, we need to use different tools as explained next.</p>&#13;
</dd>&#13;
</dl>&#13;
<div data-type="note" epub:type="note"><h1>Do We Have to Measure or Find Bottlenecks in Off-CPU Time?</h1>&#13;
<p><a data-primary="bottleneck analysis" data-secondary="in off-CPU time" data-secondary-sortas="off-CPU" data-type="indexterm" id="idm45606824160704"/><a data-primary="off-CPU time" data-secondary="bottleneck analysis in" data-type="indexterm" id="idm45606824159456"/>Program threads spend a lot of time off-CPU. This is why the main reason your program is slow might not be its CPU time. For example, suppose the execution of your program takes 20 seconds, but it waits 19 seconds on an answer from the database. In that case, we might want to look at bottlenecks in the database (or mitigate the database slowness in our code) instead of optimizing the CPU time.</p>&#13;
</div>&#13;
&#13;
<p>Generally, it is recommended to use tracing to find the bottlenecks in the wall time (latency) of our functionality. Especially, distributed tracing allows us to narrow down our optimization focus to what takes the most time in the request of functionality flow. Go has built-in <a href="https://oreil.ly/pKeI1">tracing instrumentation</a>, but it only instruments Go runtime, not our application code. However, we discussed basic tracing instrumentation compatible with the cloud-native standards like <a href="https://oreil.ly/sPiw9">OpenTelemetry</a> to achieve application-level tracing.</p>&#13;
&#13;
<p><a data-primary="Full Go Profiler" data-type="indexterm" id="idm45606824155152"/>There is also an amazing profiler called the <a href="https://oreil.ly/4WWHN">Full Go Profiler (<code>fgprof</code>)</a> out there focused on tracking both CPU and off-CPU time. While <a href="https://oreil.ly/ri1Kb">it’s not</a> officially recommended yet and has <a href="https://oreil.ly/8Lk9t">known limitations</a>, I found it very useful, depending on what kind of Go program I analyze. The <code>fgprof</code> profile can be exposed using the HTTP handler mentioned in <a data-type="xref" href="#code-pprof-http">Example 9-5</a>. The example view of the <code>fgprof</code> profile for <code>labeler</code> service is presented in <a data-type="xref" href="#img-obs-prof-fgprof">Figure 9-13</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-prof-fgprof">&#13;
<img alt="efgo 0913" src="assets/efgo_0913.png"/>&#13;
<h6><span class="label">Figure 9-13. </span>The Flame Graph view for the 30-seconds-<code>fgprof</code> profile from the <code>labeler</code> <code>Sum</code> code from <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> at <code>functions</code> granularity</h6>&#13;
</div></figure>&#13;
&#13;
<p>From the profile, we can quickly tell that for most of the wall time, the <code>labeler</code> service is simply waiting for the signal interrupt or HTTP requests! If we are interested in improving the maximum rate of the incoming requests that <code>labeler</code> can serve, we can quickly find that  <code>labeler</code> is not the problem, but rather the testing client is not sending requests fast enough.<sup><a data-type="noteref" href="ch09.html#idm45606824142096" id="idm45606824142096-marker">25</a></sup></p>&#13;
&#13;
<p>To sum up, in this section, I presented the most common profiler implementations that are used<sup><a data-type="noteref" href="ch09.html#idm45606824139264" id="idm45606824139264-marker">26</a></sup> in the Go community. There are also tons of closed-box monitoring profilers like Linux <code>perf</code> and <code>eBPF</code>-based profiles, but they are outside the scope of this book. I prefer the ones I mentioned as they are free (open source!), explicit, and relatively easy to use and understand<a data-startref="ix_ch09-asciidoc28" data-type="indexterm" id="idm45606824135824"/><a data-startref="ix_ch09-asciidoc27" data-type="indexterm" id="idm45606824135120"/><a data-startref="ix_ch09-asciidoc26" data-type="indexterm" id="idm45606824134448"/>.<a data-startref="ix_ch09-asciidoc18" data-type="indexterm" id="idm45606824133648"/><a data-startref="ix_ch09-asciidoc17" data-type="indexterm" id="idm45606824132944"/></p>&#13;
&#13;
<p>Let’s now look at some lesser-known tools and practices I found useful when profiling Go programs.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tips and Tricks" data-type="sect1"><div class="sect1" id="ch-obs-tricks">&#13;
<h1>Tips and Tricks</h1>&#13;
&#13;
<p><a data-primary="bottleneck analysis" data-secondary="tips and tricks" data-type="indexterm" id="ix_ch09-asciidoc29"/>There are three more advanced yet incredibly useful tricks for profiling I would love you to know. These helped me analyze software bottlenecks even more effectively. So let’s go through them!</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Sharing Profiles" data-type="sect2"><div class="sect2" id="ch-obs-share">&#13;
<h2>Sharing Profiles</h2>&#13;
&#13;
<p><a data-primary="bottleneck analysis" data-secondary="sharing profiles" data-type="indexterm" id="idm45606824127024"/><a data-primary="profiling" data-secondary="sharing profiles" data-type="indexterm" id="idm45606824126048"/>Typically, we don’t work on software projects alone. Instead, we are in a  bigger team, which shares responsibilities and reviews each other’s code. Sharing is caring, so similar to <a data-type="xref" href="ch08.html#ch-obs-micro-share">“Sharing Benchmarks with the Team (and Your Future Self)”</a>, we should focus on presenting our bottlenecks results and findings with team members or other interested parties.</p>&#13;
&#13;
<p>We download or check multiple <code>pprof</code> profiles in the typical workflow. In theory, we could name them descriptively to avoid confusion and send them to each other using any file-sharing solution like Google Drive or Slack. This, however, tends to be cumbersome because the recipient has to download the <code>pprof</code> file and run <code>go tool pprof</code> locally to analyze.</p>&#13;
&#13;
<p>Another option is to share a screenshot of the profile, but we have to choose some partial view, which can be cryptic for others. Perhaps others would like to analyze the profile using a different view or value type. Maybe they want to find the sampling rate or narrow the profile down to some code path. With just a screenshot, you are missing all of those interactive capabilities.</p>&#13;
&#13;
<p>Fortunately, some websites allow us to save <code>pprof</code> files for others or our future self and analyze them without downloading that profile. For example, the <a href="https://oreil.ly/HowVb">Polar Signals</a> company hosts an entirely free <a href="https://pprof.me"><em>pprof.me</em></a> website that allows exactly that. You can upload your profile (note that it will be shared publicly!) and share the link with team members, who can analyze it using common <code>go tools pprof</code> reports views (see <a data-type="xref" href="#ch-obs-profiling-res">“go tool pprof Reports”</a>). I use it all the time with my team.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Continuous Profiling" data-type="sect2"><div class="sect2" id="ch-obs-cont-profiling">&#13;
<h2>Continuous Profiling</h2>&#13;
&#13;
<p><a data-primary="bottleneck analysis" data-secondary="continuous profiling" data-type="indexterm" id="ix_ch09-asciidoc30"/><a data-primary="continuous profiling" data-type="indexterm" id="ix_ch09-asciidoc31"/><a data-primary="profiling" data-secondary="continuous" data-type="indexterm" id="ix_ch09-asciidoc32"/>In the open source ecosystem, continuous profiling was perhaps one of the most popular topics in 2022. It means automatically collecting useful profiles from our Go program at every configured interval instead of being manually triggered.</p>&#13;
&#13;
<p>In many cases, the efficiency problem happens somewhere in the remote environment where the program is running. Perhaps it happened in the past in response to some event that is now hard to reproduce. Continuous profiling tools allow us to have our profiling “always on” and retrospectively look at profiles from the past.<a data-primary="Branczyk, Frederic" data-secondary="on continuous profiling" data-type="indexterm" id="idm45606824110528"/></p>&#13;
<blockquote class="pagebreak-before">&#13;
<p class="less_space">Say you see an increase in resource usage – say, CPU usage. And then you take a one-time profile to try to figure out what’s using more resources. Continuous profiling is essentially doing this all the time. (...) When you have all this data over time, you can compare the entire lifetime of a version of a process to a newly rolled-out version. Or you can compare two different points in time. Let’s say there’s a CPU or memory spike. We can actually understand what was different in our processes down to the line number. It’s super powerful, and it’s an extension of the other tools already useful in observability, but it shines a different light on our running programs.</p>&#13;
<p data-type="attribution">Frederic Branczyk, <a href="https://oreil.ly/Jp9gQ">“Grafana’s Big Tent: Continuous Profiling with Frederic Branczyk”</a></p>&#13;
</blockquote>&#13;
&#13;
<p>Continuous profiling emerged in the cloud-native open source community as the fourth observability signal, but it’s not new. The concept was introduced first in 2010 by the <a href="https://oreil.ly/FbHY8">“Google-Wide Profiling: A Continuous Profiling Infrastructure For Data Centers” research paper</a> by Gang Ren et al., which proved that profiling can be used against production workloads continuously without the major overhead, and helped in efficiency optimizations at Google.</p>&#13;
&#13;
<p>We have recently seen open source projects that made this technology more accessible. I have personally used the continuous profiling tool for a couple of years already to profile our Go services, and I love it!</p>&#13;
&#13;
<p><a data-primary="Parca project" data-type="indexterm" id="ix_ch09-asciidoc33"/>You can quickly set up continuous profiling using the open source <a href="https://oreil.ly/X8003">Parca project</a>. In many ways, it is similar to the <a href="https://oreil.ly/2Sa3P">Prometheus project</a>. Parca is a single binary Go program that periodically captures profiles using the HTTP handlers we discussed in <a data-type="xref" href="#ch-obs-pprof-obtain">“Capturing the Profiling Signal”</a> and stores them in a local database. Then we can search for profiles, download them, or even use the embedded <code>tool pprof</code> like a viewer to analyze them.</p>&#13;
&#13;
<p>You can use it anywhere: set up continuous profiling on your production, remote environment, or macrobenchmarking environment that might run in the cloud or on your laptop. It might not make sense on a microbenchmarks level, as we run tests in the smallest possible scope, which can be profiled for the full duration of the benchmark (see <a data-type="xref" href="ch08.html#ch-obs-micro">“Microbenchmarks”</a>).</p>&#13;
&#13;
<p>Adding continuous profiling with Parca to our <code>labeler</code> macrobenchmark in <a data-type="xref" href="ch08.html#code-macrobench">Example 8-19</a> requires only a few lines of code and a simple YAML configuration, as presented in <a data-type="xref" href="#code-parca">Example 9-6</a>.</p>&#13;
<div data-type="example" id="code-parca">&#13;
<h5><span class="label">Example 9-6. </span>Starting continuous profiling container in <a data-type="xref" href="ch08.html#code-macrobench">Example 8-19</a> between <code>labeler</code> creation and <code>k6</code> script execution</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="nx">labeler</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="o">...</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="nx">parca</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">e2e</code><code class="p">.</code><code class="nx">NewInstrumentedRunnable</code><code class="p">(</code><code class="nx">e</code><code class="p">,</code><code class="w"> </code><code class="s">"parca"</code><code class="p">)</code><code class="p">.</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">WithPorts</code><code class="p">(</code><code class="kd">map</code><code class="p">[</code><code class="kt">string</code><code class="p">]</code><code class="kt">int</code><code class="p">{</code><code class="s">"http"</code><code class="p">:</code><code class="w"> </code><code class="mi">7070</code><code class="p">}</code><code class="p">,</code><code class="w"> </code><code class="s">"http"</code><code class="p">)</code><code class="p">.</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">Init</code><code class="p">(</code><code class="nx">e2e</code><code class="p">.</code><code class="nx">StartOptions</code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">Image</code><code class="p">:</code><code class="w"> </code><code class="s">"ghcr.io/parca-dev/parca:main-4e20a666"</code><code class="p">,</code><code class="w"> </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO6-1" id="co_data_driven_bottleneck_analysis_CO6-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
        </code><code class="nx">Command</code><code class="p">:</code><code class="w"> </code><code class="nx">e2e</code><code class="p">.</code><code class="nx">NewCommand</code><code class="p">(</code><code class="s">"/bin/sh"</code><code class="p">,</code><code class="w"> </code><code class="s">"-c"</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">          </code><code class="s">`cat &lt;&lt; EOF &gt; /shared/data/config.yml &amp;&amp; \&#13;
    /parca --config-path=/shared/data/config.yml&#13;
object_storage: </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO6-2" id="co_data_driven_bottleneck_analysis_CO6-2"><img alt="2" src="assets/2.png"/></a><code class="s">&#13;
  bucket:&#13;
    type: "FILESYSTEM"&#13;
    config:&#13;
      directory: "./data"&#13;
scrape_configs: </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO6-3" id="co_data_driven_bottleneck_analysis_CO6-3"><img alt="3" src="assets/3.png"/></a><code class="s">&#13;
- job_name: "%s"&#13;
  scrape_interval: "15s"&#13;
  static_configs:&#13;
    - targets: [ '`</code><code class="o">+</code><code class="nx">labeler</code><code class="p">.</code><code class="nx">InternalEndpoint</code><code class="p">(</code><code class="s">"http"</code><code class="p">)</code><code class="o">+</code><code class="s">`' ]&#13;
  profiling_config:&#13;
    pprof_config: </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO6-4" id="co_data_driven_bottleneck_analysis_CO6-4"><img alt="4" src="assets/4.png"/></a><code class="s">&#13;
      fgprof:&#13;
        enabled: true&#13;
        path: /debug/fgprof/profile&#13;
        delta: true&#13;
EOF&#13;
`</code><code class="p">)</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">User</code><code class="p">:</code><code class="w">      </code><code class="nx">strconv</code><code class="p">.</code><code class="nx">Itoa</code><code class="p">(</code><code class="nx">os</code><code class="p">.</code><code class="nx">Getuid</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">Readiness</code><code class="p">:</code><code class="w"> </code><code class="nx">e2e</code><code class="p">.</code><code class="nx">NewTCPReadinessProbe</code><code class="p">(</code><code class="s">"http"</code><code class="p">)</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="p">)</code><code class="w">&#13;
</code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">t</code><code class="p">,</code><code class="w"> </code><code class="nx">e2e</code><code class="p">.</code><code class="nx">StartAndWaitReady</code><code class="p">(</code><code class="nx">parca</code><code class="p">)</code><code class="p">)</code><code class="w">&#13;
</code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">t</code><code class="p">,</code><code class="w"> </code><code class="nx">e2einteractive</code><code class="p">.</code><code class="nx">OpenInBrowser</code><code class="p">(</code><code class="s">"http://"</code><code class="o">+</code><code class="nx">parca</code><code class="p">.</code><code class="nx">Endpoint</code><code class="p">(</code><code class="s">"http"</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_data_driven_bottleneck_analysis_CO6-5" id="co_data_driven_bottleneck_analysis_CO6-5"><img alt="5" src="assets/5.png"/></a><code class="w">&#13;
&#13;
</code><code class="nx">k6</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="o">...</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO6-1" id="callout_data_driven_bottleneck_analysis_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The <a href="https://oreil.ly/f0IJo"><code>e2e</code> framework</a> runs all workloads in the container, so we do that for the Parca server. We use the container image build from <a href="https://oreil.ly/ETsNV">the official project page</a>.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO6-2" id="callout_data_driven_bottleneck_analysis_CO6-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The basic configuration of the Parca server has two parts. The first is object storage configuration: where we want to store Parca’s database internal data files. Parca uses <a href="https://oreil.ly/A9y23">FrostDB columnar storage</a> to store debugging information and profiles. To make it easy, we can use the local filesystem as our most basic object storage.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO6-3" id="callout_data_driven_bottleneck_analysis_CO6-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>The second important configuration is the scrape configuration that allows us to put certain endpoints as targets to profile capturing. In our case, I only put the <code>labeler</code> HTTP endpoint on the local network. I also specified to get the profile every 15 seconds. For always-on production use, I would recommend larger intervals, e.g., one minute.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO6-4" id="callout_data_driven_bottleneck_analysis_CO6-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>The common profile—like a heap, CPU, goroutine block, and mutex—<a datatype="link" href="https://oreil.ly/pcZmg">are enabled by default</a>. However, we have to manually allow other profiles, like the <code>fgprof</code> profile discussed in <a data-type="xref" href="#ch-obs-pprof-latency">“Off-CPU Time”</a>.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_bottleneck_analysis_CO6-5" id="callout_data_driven_bottleneck_analysis_CO6-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Once Parca starts, we can use the <code>e2einteractive</code> package to open the Parca UI to explore viewer-like presentations of our profiles during or after the <code>k6</code> script finishes.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Thanks to continuously profiling, we don’t need to wait until our benchmark (using the <code>k6</code> load tester) finishes—we can jump to our UI straightaway to see profiles every 15 seconds, live! Another great thing about continuous profiling is that we can extract metrics from the sum of all sample values taken from each profile over time. For example, Parca can give us a graph of heap memory usage for the <code>labeler</code> container over time, taken from periodic <code>heap</code> <code>inuse_alloc</code> profiles (discussed in <a data-type="xref" href="#img-obs-prof-heap">Figure 9-9</a>). The result, presented in <a data-type="xref" href="#img-obs-prof-parca-graph">Figure 9-14</a>, should have values very close to the <code>go_memstats_heap_total</code> metric mentioned in <a data-type="xref" href="ch06.html#ch-obs-mem-usage">“Memory Usage”</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-prof-parca-graph">&#13;
<img alt="efgo 0914" src="assets/efgo_0914.png"/>&#13;
<h6><span class="label">Figure 9-14. </span>Screenshot of Parca UI result showing the <code>labeler</code> <a data-type="xref" href="#img-obs-prof-heap">Figure 9-9</a> <code>inuse_alloc</code> profiles over time</h6>&#13;
</div></figure>&#13;
&#13;
<p>You can now click on samples in the graph, representing the moment of taking the profile snapshot. Thanks to continuous form, you can choose the time that interests you the most, perhaps the moment when the memory usage was the highest! Once clicked, the Flame Graph of that specific profile appears, as presented in <a data-type="xref" href="#img-obs-prof-parca-graph2">Figure 9-15</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-prof-parca-graph2">&#13;
<img alt="efgo 0915" src="assets/efgo_0915.png"/>&#13;
<h6><span class="label">Figure 9-15. </span>Screenshot of Parca UI Flame Graph (called Icicle Graph in Parca) when you click the specific profile from <a data-type="xref" href="#img-obs-prof-graph">Figure 9-4</a></h6>&#13;
</div></figure>&#13;
&#13;
<p>The Parca maintainers decided to use a different visual style for the Flame Graphs than the <code>go tool pprof</code> tool in <a data-type="xref" href="#ch-obs-profiling-res-flame">“Flame Graph”</a>. However, as many other tools in the profiling space, it uses the same semantics. This means we can use our analyzing skills from <code>go tool pprof</code> specifics with different UIs like Parca.</p>&#13;
&#13;
<p>In the profile view, we can download the <code>pprof</code> file we selected. We can share the profile as discussed in <a data-type="xref" href="#ch-obs-share">“Sharing Profiles”</a>, filter view, or choose different views. We also see a Flame Graph representing the function’s contributions to the live objects in a heap for the selected time. We could not easily capture that manually. In <a data-type="xref" href="ch08.html#img-macrobench-heap">Figure 8-5</a>, I captured the profile after the interesting event happened, so I had to use <code>alloc_space</code> that shows the total allocations from when the program started. For a long-living process, this view might be very noisy and show situations that I am not interested in. Even worse, the process might have restarted after certain events, like panics or OOMs. Doing such a heap profile after restart will tell us nothing. A similar problem occurs with every other profile that only shows the current or specific moment, like goroutines, CPUs, or our custom file descriptor profile.</p>&#13;
&#13;
<p>This is where continuous profiling proves to be extremely helpful. It allows us to have profiles captured whenever an interesting event occurs, so we can quickly jump into the UI and analyze for efficiency bottlenecks. For example, in <a data-type="xref" href="#img-obs-prof-parca-graph2">Figure 9-15</a>, we can see the <code>bytes.Split</code> as the function that uses the most memory on the heap at the current moment.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Overhead of Continuous Profiling</h1>&#13;
<p>Capturing on-demand profiles has some overhead to the running Go program. However, capturing multiple profiles periodically makes this overhead continuous throughout the application run, so ensure your profilers do not cause your efficiency to drop below the expected level.</p>&#13;
&#13;
<p>Try to understand the overhead of profiling in your programs. The standard default Go profilers aim to not add more than 5% of the CPU overhead for a single process. You can control that by changing the continuous profiling interval or the sampling of profiles. It is also useful to profile <a href="https://oreil.ly/yAACa">only one of many of the same replicas in large deployments</a> to amortize collection cost.</p>&#13;
&#13;
<p>In our <a href="https://oreil.ly/6CSV7">infrastructure at Red Hat</a>, we run continuous profiling always on with a one-minute interval, and we keep only a few days’ worth of profiles.</p>&#13;
</div>&#13;
&#13;
<p>To sum up, I recommend continuous profiling on live Go programs that you know might need continuous efficiency improvement in the future. Parca is one open source example, but there are other projects or vendors<sup><a data-type="noteref" href="ch09.html#idm45606823825248" id="idm45606823825248-marker">27</a></sup> that allow you to do the same. Just be careful, as profiling might be addictive<a data-startref="ix_ch09-asciidoc33" data-type="indexterm" id="idm45606823821072"/>!<a data-startref="ix_ch09-asciidoc32" data-type="indexterm" id="idm45606823820304"/><a data-startref="ix_ch09-asciidoc31" data-type="indexterm" id="idm45606823819600"/><a data-startref="ix_ch09-asciidoc30" data-type="indexterm" id="idm45606823818928"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Comparing and Aggregating Profiles" data-type="sect2"><div class="sect2" id="ch-obs-comp-prof">&#13;
<h2>Comparing and Aggregating Profiles</h2>&#13;
&#13;
<p><a data-primary="bottleneck analysis" data-secondary="comparing/aggregating profiles" data-type="indexterm" id="idm45606823817056"/><a data-primary="profiling" data-secondary="comparing/aggregating profiles" data-type="indexterm" id="idm45606823816016"/>The <code>pprof</code> format has one more interesting characteristic. By design, it allows certain aggregations or comparison for multiple profiles:</p>&#13;
<dl>&#13;
<dt>Subtracting profiles</dt>&#13;
<dd>&#13;
<p>You can subtract one profile from another. This is useful to reduce noise and narrow down to the event or component you care about. For example, you can have a heap profile from one run of your Go program when you load tested simultaneously with some <code>A</code> and <code>B</code> events. Then, you can subtract the heap second profile you have from the same Go program that was load tested with only the <code>B</code> event to check what the impact was purely from the <code>A</code> event. The <code>go tool pprof</code> allows you to subtract one profile from another using the <code>-base</code> flag—for example, <code>go tool pprof heap-AB.pprof -base heap-B.pprof</code>.</p>&#13;
</dd>&#13;
</dl>&#13;
<dl class="less_space pagebreak-before">&#13;
<dt>Comparing profiles</dt>&#13;
<dd>&#13;
<p><a href="https://oreil.ly/NHfZP">The comparison</a> is similar to subtracting; instead of removing matching sample values, it provides negative or positive delta numbers between profiles. This is useful to measure the change of the contribution of a particular function before and after optimization. You can also use <code>go tool pprof</code> to compare your profiles using <code>-diff_base</code>.</p>&#13;
</dd>&#13;
<dt>Merging profiles</dt>&#13;
<dd>&#13;
<p>It is less known in the community, but you can merge multiple profiles into one! The merging functionality allows us to combine profiles representing the current situation. For example, we could take dozens of short CPU profiles into a single profile of all the CPU work across a longer duration. Or perhaps we could merge multiple heap profiles to the aggregate profile of all heap objects from multiple time points.</p>&#13;
&#13;
<p>The <code>go tool pprof</code> does not support this. However, you can write your own Go program that does it using the <a href="https://oreil.ly/bvoSL"><code>google/pprof/profile.Merge</code> function</a>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>I wasn’t using these mechanics very often because I was easily confused with multiple local <code>pprof</code> files when working with the <code>go tool pprof</code> tool. This changed when I started working with more advanced profiling tools like Parca. As you can see in <a data-type="xref" href="#img-obs-prof-parca-graph">Figure 9-14</a>, there is a Compare button to compare two particular profiles, and a Merge button to combine all profiles from the focused time range into one profile. With the UI, it is much easier to select what profiles you want to compare or aggregate, and how!<a data-startref="ix_ch09-asciidoc29" data-type="indexterm" id="idm45606823798768"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45606823797968">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Profiling space for Go might be nuanced, but it’s not that difficult to utilize once you know the basics. In this chapter, we went through all the profiling aspects from the common profilers, through capturing patterns and <code>pprof</code> format, to standard visualization techniques. Finally, we touched on advanced techniques like continuous profiling, which I recommend trying.</p>&#13;
<div data-type="tip"><h1>Profile First, Ask Questions Later</h1>&#13;
<p>I would suggest using profiling in any shape that fits in your daily optimization workflow. Ask questions like what is causing the slowdown or high resource usage in your code only after you have already captured the profiles from your program.</p>&#13;
</div>&#13;
&#13;
<p class="less_space pagebreak-before">I believe this is not the end of the innovations in this space. Thanks to common efficient profiling formats like <code>pprof</code> that allow interoperability across different tools and profilers, we will see more tools, UI, useful visualizations, or even correlations with different observability signals mentioned in <a data-type="xref" href="ch06.html#ch-observability">Chapter 6</a>.</p>&#13;
&#13;
<p>Furthermore, more eBPF profiles are emerging in the open source ecosystem, making profiling cheaper and more uniform across programming languages. So be open-minded and try different techniques and tools to find out what works best for you, your team, or your organization.<a data-startref="ix_ch09-asciidoc0" data-type="indexterm" id="idm45606823791520"/></p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45606826044080"><sup><a href="ch09.html#idm45606826044080-marker">1</a></sup> A symptom is an effect we see caused by some underlying situation, e.g., OOM is a symptom of the Go program requiring more memory than allowed. The problem with symptoms is that they often look like a root cause, but there might be an underlying bottleneck causing them. For example, the high memory usage of a process that caused the OOM might look like a root cause, but it can as well be just a symptom of a different issue if it was caused by a dependency not processing requests fast enough.</p><p data-type="footnote" id="idm45606826043392"><sup><a href="ch09.html#idm45606826043392-marker">2</a></sup> A <a href="https://oreil.ly/5AKbS">red herring</a> is an unexpected behavior that turns out to not be a problem to the general topic of our investigation. For example, while investigating the higher latency of our requests, it might be concerning to see the debug log “started handling request” in our application and not see a “finished request” for hours. It often turns out that the “finish” log message we might expect was not implemented, or we just dropped it in our logging system. Things often can mislead us; that’s why we should be clean and explicit without observability and program flows to mislead us when we need to find the problem fast.</p><p data-type="footnote" id="idm45606826040352"><sup><a href="ch09.html#idm45606826040352-marker">3</a></sup> Usually, tracing does not provide a full stack trace, just the most important functionalities. This is to limit overhead and cost of tracing.</p><p data-type="footnote" id="idm45606826031328"><sup><a href="ch09.html#idm45606826031328-marker">4</a></sup> Or methods, but that is treated in Go in the same way. Especially in this chapter, I will use the term <em>function</em> very often, and I mean both Go functions and methods.</p><p data-type="footnote" id="idm45606826005440"><sup><a href="ch09.html#idm45606826005440-marker">5</a></sup> Such a profiler was already proposed in the Go community to be included in the standard library. However, for now, <a href="https://oreil.ly/YZoiR">the idea was rejected</a> by the Go team as you can, in theory, track opened files thanks to the memory profile focused on allocations from <code>os.Open</code>.</p><p data-type="footnote" id="idm45606826001264"><sup><a href="ch09.html#idm45606826001264-marker">6</a></sup> With <code>pprof.Profile</code>, we can only track objects. We cannot profile advanced things like past object creation, I/O usage, etc. We also can’t customize what is in the resulted <code>pprof</code> file, like extra labels, custom sampling, other value types, etc. Such custom profiling requires more code, but it is still relatively easy to implement thanks to Go packages like <a href="https://oreil.ly/DgeqN"><code>github.com/google/pprof/profile</code></a>.</p><p data-type="footnote" id="idm45606825325680"><sup><a href="ch09.html#idm45606825325680-marker">7</a></sup> The <code>:8080</code> is shorthand for <code>0.0.0.0:8080</code>, so listening on all network interfaces of your machine.</p><p data-type="footnote" id="idm45606825324096"><sup><a href="ch09.html#idm45606825324096-marker">8</a></sup> To run this command or generate graphs, you need to install the <a href="http://www.graphviz.org"><code>graphviz</code> tool</a> on your machine.</p><p data-type="footnote" id="idm45606825431504"><sup><a href="ch09.html#idm45606825431504-marker">9</a></sup> This guide is for the web interface from Go 1.19. There are no hints that it will change, but the <code>pprof</code> tool may be enhanced or updated in subsequent versions of Go.</p><p data-type="footnote" id="idm45606825248624"><sup><a href="ch09.html#idm45606825248624-marker">10</a></sup> You can also hover over each menu item, and after three seconds a short help pop-up will appear.</p><p data-type="footnote" id="idm45606824757280"><sup><a href="ch09.html#idm45606824757280-marker">11</a></sup> From the perspective of the profiling, the direct (Flat) contribution is decided by instrumentation implementation. Our custom code in <a data-type="xref" href="#code-pprof-fd">Example 9-1</a> treats the <code>fd.Open</code> function as the moment the file descriptor was opened. Different profiling implementations might define the moment of “use” differently (moment of the allocation, use of CPU time, waiting for lock opening, etc.).</p><p data-type="footnote" id="idm45606824674864"><sup><a href="ch09.html#idm45606824674864-marker">12</a></sup> The REFINE hidden option keeps the line solid.</p><p data-type="footnote" id="idm45606824599744"><sup><a href="ch09.html#idm45606824599744-marker">13</a></sup> Note that currently there are some bugs in this view in <code>pprof</code>. When you are missing binary, the UI shows <code>no matches found for regexp:</code>. Search also does not work, but you can use the built-in browser search to find what you want (e.g., using Ctrl+F).</p><p data-type="footnote" id="idm45606824589952"><sup><a href="ch09.html#idm45606824589952-marker">14</a></sup> For example, the plugin in <a href="https://oreil.ly/eaooe">VSCode</a> or <a href="https://oreil.ly/YT9cs">GoLand</a>.</p><p data-type="footnote" id="idm45606824378144"><sup><a href="ch09.html#idm45606824378144-marker">15</a></sup> Funny enough, the 165 number is excessive. Making this screenshot gave me the insight that I have a bug in the <code>labeler</code> code. I was not closing the temporary file.</p><p data-type="footnote" id="idm45606824335568"><sup><a href="ch09.html#idm45606824335568-marker">16</a></sup> The same profile is also available via <code>/debug/pprof/alloc</code>. The only difference is that the <code>alloc</code> profile has <code>alloc_space</code> as the default value type.</p><p data-type="footnote" id="idm45606824295584"><sup><a href="ch09.html#idm45606824295584-marker">17</a></sup> Unfortunately, given I took the snapshot when the load test finished, the current amount of spaces contributed by code toward the heap is minimal and does not represent any interesting event that happened in the past. You will see this value type being more useful in <a data-type="xref" href="#ch-obs-cont-profiling">“Continuous Profiling”</a>.</p><p data-type="footnote" id="idm45606824276208"><sup><a href="ch09.html#idm45606824276208-marker">18</a></sup> See the excellent <a href="https://oreil.ly/U8tCN">goroutine profiler overview</a>.</p><p data-type="footnote" id="idm45606824268336"><sup><a href="ch09.html#idm45606824268336-marker">19</a></sup> Technically speaking, the Go scheduler <a href="https://oreil.ly/g3tl2">records that information</a>. It can be exposed to us when stack is retrieved with <code>GODEBUG=tracebackancestors=X</code>.</p><p data-type="footnote" id="idm45606824228720"><sup><a href="ch09.html#idm45606824228720-marker">20</a></sup> See the <a href="https://oreil.ly/8vy83">proposal for the next iteration of the potential CPU profiler</a> for a detailed description.</p><p data-type="footnote" id="idm45606824226864"><sup><a href="ch09.html#idm45606824226864-marker">21</a></sup> Technically speaking, there is one very hacky way of setting different profiling CPU rates. You can call <a href="https://oreil.ly/M8HwB"><code>runtime.SetCPUProfileRate()</code></a> with the rate you want right before <code>pprof.StartCPUProfile(w)</code>. The <code>pprof.StartCPUProfile(w)</code> will try to override the rate, but it will fail due to <a href="https://oreil.ly/8JBxX">the bug</a>. Change the rate only if you know what you are doing—100 Hz is usually a good default. Values higher than 250–500 Hz are not supported by most of the OS timers anyway.</p><p data-type="footnote" id="idm45606824221904"><sup><a href="ch09.html#idm45606824221904-marker">22</a></sup> See <a href="https://oreil.ly/E0W5v">this issue</a> for a currently known list of OSes with certain problems.</p><p data-type="footnote" id="idm45606824186560"><sup><a href="ch09.html#idm45606824186560-marker">23</a></sup> In fact, even CPU time includes waiting for a memory fetch, as discussed in <a data-type="xref" href="ch04.html#ch-hw-mem-wall">“CPU and Memory Wall Problem”</a>. This is, however, included in the CPU profile.</p><p data-type="footnote" id="idm45606824182112"><sup><a href="ch09.html#idm45606824182112-marker">24</a></sup> This view is heavily inspired by the <a href="https://oreil.ly/nwVwF">Felix’s great guide</a>.</p><p data-type="footnote" id="idm45606824142096"><sup><a href="ch09.html#idm45606824142096-marker">25</a></sup> This can be confirmed in <a data-type="xref" href="ch08.html#code-macrobench">Example 8-19</a> code, where the <code>k6s</code> script has only one user that waits 500 ms between HTTP calls.</p><p data-type="footnote" id="idm45606824139264"><sup><a href="ch09.html#idm45606824139264-marker">26</a></sup> I skipped the <code>threadcreate</code> profile present in the Go <code>pprof</code> package as it’s known to be broken <a href="https://oreil.ly/b8MpS">since 2013</a> with little priority to be fixed in the future.</p><p data-type="footnote" id="idm45606823825248"><sup><a href="ch09.html#idm45606823825248-marker">27</a></sup> <a href="https://oreil.ly/Ru0Hu">Phlare</a>, <a href="https://oreil.ly/eKyK7">Pyroscope</a>, <a href="https://oreil.ly/OGoVR">Google Cloud Profiler</a>, <a href="https://oreil.ly/urVE0">AWS CodeGuru Profiler</a>, or <a href="https://oreil.ly/El7zq">Datadog continuous profiler</a>, to name a few.</p></div></div></section></body></html>