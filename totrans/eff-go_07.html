<html><head></head><body><section data-pdf-bookmark="Chapter 7. Data-Driven Efficiency Assessment" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch-observability2">&#13;
<h1><span class="label">Chapter 7. </span>Data-Driven Efficiency Assessment</h1>&#13;
&#13;
&#13;
<p><a data-primary="efficiency assessment" data-type="indexterm" id="ix_ch07-asciidoc0"/>You learned how to observe our Go program using different observability signals in the previous chapter. We discussed how to transform those signals to numeric values, or metrics, to effectively observe and assess the latency and resource consumption of the program.</p>&#13;
&#13;
<p>Unfortunately, knowing how to measure the current or maximum consumption or latency for running a program does not guarantee the correct assessment of the overall program efficiency for our application. What we are missing here is the experiment part, which might be the most challenging part of optimization generally: how to trigger situations that are worth measuring with the observability tools mentioned in <a data-type="xref" href="ch06.html#ch-observability">Chapter 6</a>!</p>&#13;
<div data-type="note" epub:type="note"><h1>The Definition of Measuring</h1>&#13;
<p><a data-primary="measuring (definition)" data-type="indexterm" id="idm45606830423600"/>I find the verb “to measure” very imprecise. I have seen this word overused to describe two things: the process of performing an experiment and gathering numeric data from it.</p>&#13;
&#13;
<p>In this book, every time you read about the “measuring” process, I follow the definition used in <a href="https://oreil.ly/5PRMp">metrology (the science of measurement)</a>. I precisely mean the process of using the instruments to quantify what is happening now (e.g., the latency of the event, or how much memory it required) or what happened in a given time window. Everything that leads to this event that we measure (simulated by us in a benchmark or occurring naturally) is a separate topic, discussed in this chapter.</p>&#13;
</div>&#13;
&#13;
<p>In this chapter, I will introduce you to the art of experimentation and measurement for efficiency purposes. I will mainly focus on data-driven assessment, more commonly known as benchmarking. This chapter will help you understand the best &#13;
<span class="keep-together">practices</span> before we jump to writing benchmarking code in <a data-type="xref" href="ch08.html#ch-benchmarking">Chapter 8</a>. These practices will also be invaluable in <a data-type="xref" href="ch09.html#ch-observability3">Chapter 9</a>, which focuses on profiling.</p>&#13;
&#13;
<p>I start with complexity analysis as a less empirical way of assessing the efficiency of our solutions. Then, I will explain benchmarking in <a data-type="xref" href="#ch-obs-bench-intro">“The Art of Benchmarking”</a>. We will compare it to functional testing and clarify the common stereotype that claims “benchmarks always lie.”</p>&#13;
&#13;
<p>Later in <a data-type="xref" href="#ch-obs-rel">“Reliability of Experiments”</a>, we will move to the reliability aspect of our experiments for both benchmarking and profiling purposes. I will provide the ground rules to avoid wasting time (or money) by gathering bad data and making wrong conclusions.</p>&#13;
&#13;
<p>Finally, in <a data-type="xref" href="#ch-obs-benchmarking">“Benchmarking Levels”</a>, I will introduce you to the full landscape of benchmark strategies. In the previous chapters, I already used benchmarks to provide data that explained the behavior of CPU or memory resources. For example, in <a data-type="xref" href="ch02.html#ch-go-tooling">“Consistent Tooling”</a>, I mentioned that the Go tooling provides a standard benchmarking framework. But the benchmarking skill I want to teach you in this chapter goes beyond that, and it is just one tool of many discussed in <a data-type="xref" href="ch08.html#ch-obs-micro">“Microbenchmarks”</a>. There are many different ways of assessing the efficiency of our Go code. Knowing when to use what is key.</p>&#13;
&#13;
<p>Let’s start by introducing the benchmarking tests and what the critical aspects of those are.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Complexity Analysis" data-type="sect1"><div class="sect1" id="ch-hw-complexity">&#13;
<h1>Complexity Analysis</h1>&#13;
&#13;
<p><a data-primary="complexity analysis" data-type="indexterm" id="ix_ch07-asciidoc1"/><a data-primary="efficiency assessment" data-secondary="complexity analysis" data-type="indexterm" id="ix_ch07-asciidoc2"/>We don’t always have the luxury of having empirical data that guides us through the efficiency of a certain solution. Your idea of a better system or algorithm might not be implemented yet and would require a lot of effort to do so before we could benchmark it. Additionally, I mentioned the need for complexity estimation in <a data-type="xref" href="ch03.html#example-defining-raer">“Example of Defining RAER”</a>.</p>&#13;
&#13;
<p>This might feel contradictory to what we learned in <a data-type="xref" href="ch03.html#ch-conq-challenges">“Optimization Challenges”</a> (“programmers are notoriously bad at estimating exact resource consumption”), but sometimes engineers rely on theoretical analysis to assess the program. One example is when we assess optimizations on the algorithm level (from <a data-type="xref" href="ch03.html#ch-conq-opt-levels">“Optimization Design Levels”</a>). Developers and scientists often use complexity analysis to compare and decide what algorithm might fit better to solve certain problems with certain constraints. More specifically, they use asymptotic notations (commonly known as “Big O” complexities). Most likely, you have heard about them, as they are commonly asked about during any software engineering interview.</p>&#13;
&#13;
<p>However, to fully understand asymptotic notations, you must know what “estimated” efficiency complexity means and what it looks like!</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="“Estimated” Efficiency Complexity" data-type="sect2"><div class="sect2" id="idm45606830402512">&#13;
<h2>“Estimated” Efficiency Complexity</h2>&#13;
&#13;
<p><a data-primary="“estimated” efficiency complexity" data-primary-sortas="estimated" data-type="indexterm" id="ix_ch07-asciidoc3"/><a data-primary="complexity analysis" data-secondary="“estimated” efficiency complexity" data-secondary-sortas="estimated" data-type="indexterm" id="ix_ch07-asciidoc4"/>I mentioned in <a data-type="xref" href="ch03.html#ch-conq-req">“Resource-Aware Efficiency Requirements”</a> that we can represent the CPU time or consumption of any resources as a mathematical function related to specific input parameters. Typically, we talk about <em>runtime</em> complexity, which tells us about the CPU time required to perform a certain operation using a particular piece of code and environment. However, we also have <em>space</em> complexity, which can describe the required memory, disk space, or other space requirements for that operation.</p>&#13;
&#13;
<p>For example, let’s take our <code>Sum</code> function from <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a>. I can prove that such code has estimated space complexity (representing heap allocations) of the following function, where <em>N</em> is a number of integers in the input file:</p>&#13;
<div data-type="equation">&#13;
<math>&#13;
  <mrow>&#13;
    <mi>s</mi>&#13;
    <mi>p</mi>&#13;
    <mi>a</mi>&#13;
    <mi>c</mi>&#13;
    <mi>e</mi>&#13;
    <mo>(</mo>&#13;
    <mi>N</mi>&#13;
    <mo>)</mo>&#13;
    <mspace width="3.33333pt"/>&#13;
    <mo>=</mo>&#13;
    <mo>(</mo>&#13;
    <mn>848</mn>&#13;
    <mo>+</mo>&#13;
    <mn>3</mn>&#13;
    <mo>.</mo>&#13;
    <mn>6</mn>&#13;
    <mo>*</mo>&#13;
    <mi>N</mi>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mo>(</mo>&#13;
    <mn>24</mn>&#13;
    <mo>+</mo>&#13;
    <mn>24</mn>&#13;
    <mo>*</mo>&#13;
    <mi>N</mi>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mo>(</mo>&#13;
    <mn>2</mn>&#13;
    <mo>.</mo>&#13;
    <mn>8</mn>&#13;
    <mo>*</mo>&#13;
    <mi>N</mi>&#13;
    <mo>)</mo>&#13;
    <mspace width="0.222222em"/>&#13;
    <mi>b</mi>&#13;
    <mi>y</mi>&#13;
    <mi>t</mi>&#13;
    <mi>e</mi>&#13;
    <mi>s</mi>&#13;
    <mo>=</mo>&#13;
    <mn>872</mn>&#13;
    <mo>+</mo>&#13;
    <mn>30</mn>&#13;
    <mo>.</mo>&#13;
    <mn>4</mn>&#13;
    <mo>*</mo>&#13;
    <mi>N</mi>&#13;
    <mspace width="0.222222em"/>&#13;
    <mi>b</mi>&#13;
    <mi>y</mi>&#13;
    <mi>t</mi>&#13;
    <mi>e</mi>&#13;
    <mi>s</mi>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Knowing detailed complexity is great, but typically it’s impossible or hard to find the true complexity function because there are too many variables. We can, however, try to estimate those, especially for more deterministic resources like memory allocation, by simplifying the variables. For example, the preceding equation is only an estimation with a simplified function that takes only one parameter—the number of integers. Of course, this code also depends on the size of integers, but I assumed the integer is ~3.6 bytes long (statistic from my test input).</p>&#13;
<div data-type="warning" epub:type="warning"><h1>“Estimated” Complexity</h1>&#13;
<p><a data-primary="asymptotic complexity, “estimated” efficiency complexity versus" data-type="indexterm" id="idm45606830365664"/><a data-primary="Big O asymptotic complexity" data-secondary="“estimated” efficiency complexity versus" data-secondary-sortas="estimated" data-type="indexterm" id="idm45606830364912"/>As I try to teach you in this book—be precise with the wording.</p>&#13;
&#13;
<p>I was so wrong for all those years, thinking that complexity always means Big O asymptotic complexity. Turns out <a href="https://oreil.ly/LG5qb">the complexity exists too</a> and can be very useful in some cases. At least we should be aware it exists!</p>&#13;
&#13;
<p>Unfortunately, it’s easy to confuse it with asymptotic complexity, so I would propose calling the one that cares about constants—the “estimated” complexity.</p>&#13;
</div>&#13;
&#13;
<p>How did I find this complexity equation? It wasn’t trivial. I had to analyze the source code, do some stack escape analysis, run multiple benchmarks, and use profiling (so all the things you will learn in this and the next two chapters) to discover those &#13;
<span class="keep-together">complexities.</span></p>&#13;
<div data-type="warning" epub:type="warning"><h1>This Is Just an Example!</h1>&#13;
<p>Don’t worry. To assess or optimize your code, you don’t need to perform such detailed complexity analysis, especially in such detail. I did this to show it’s possible and what it gives, but there are more pragmatic ways to assess efficiency quickly and find out the next optimizations. You will see example flows in <a data-type="xref" href="ch10.html#ch-opt">Chapter 10</a>.</p>&#13;
&#13;
<p>Funny enough, at the end of the TFBO flow, when you optimized one part of your program a lot, you might have a detailed awareness of the problem space so that you could find such complexity quickly. However, doing this for every version of your code would be wasteful.</p>&#13;
</div>&#13;
&#13;
<p>It might be useful to explain the process of gathering the complexity and mapping it to the source code, as shown in <a data-type="xref" href="#code-sum-compl">Example 7-1</a>.</p>&#13;
<div data-type="example" id="code-sum-compl">&#13;
<h5><span class="label">Example 7-1. </span>Complexity analysis of <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a></h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">func</code><code class="w"> </code><code class="nx">Sum</code><code class="p">(</code><code class="nx">fileName</code><code class="w"> </code><code class="kt">string</code><code class="p">)</code><code class="w"> </code><code class="p">(</code><code class="nx">ret</code><code class="w"> </code><code class="kt">int64</code><code class="p">,</code><code class="w"> </code><code class="nx">_</code><code class="w"> </code><code class="kt">error</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">   </code><code class="nx">b</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">os</code><code class="p">.</code><code class="nx">ReadFile</code><code class="p">(</code><code class="nx">fileName</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_data_driven_efficiency_assessment_CO1-1" id="co_data_driven_efficiency_assessment_CO1-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
   </code><code class="k">if</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">!=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">      </code><code class="k">return</code><code class="w"> </code><code class="mi">0</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w">&#13;
</code><code class="w">   </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">   </code><code class="k">for</code><code class="w"> </code><code class="nx">_</code><code class="p">,</code><code class="w"> </code><code class="nx">line</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="k">range</code><code class="w"> </code><code class="nx">bytes</code><code class="p">.</code><code class="nx">Split</code><code class="p">(</code><code class="nx">b</code><code class="p">,</code><code class="w"> </code><code class="p">[</code><code class="p">]</code><code class="nb">byte</code><code class="p">(</code><code class="s">"\n"</code><code class="p">)</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_data_driven_efficiency_assessment_CO1-2" id="co_data_driven_efficiency_assessment_CO1-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
      </code><code class="nx">num</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">strconv</code><code class="p">.</code><code class="nx">ParseInt</code><code class="p">(</code><code class="nb">string</code><code class="p">(</code><code class="nx">line</code><code class="p">)</code><code class="p">,</code><code class="w"> </code><code class="mi">10</code><code class="p">,</code><code class="w"> </code><code class="mi">64</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_data_driven_efficiency_assessment_CO1-3" id="co_data_driven_efficiency_assessment_CO1-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
      </code><code class="k">if</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">!=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">         </code><code class="k">return</code><code class="w"> </code><code class="mi">0</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w">&#13;
</code><code class="w">      </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">      </code><code class="nx">ret</code><code class="w"> </code><code class="o">+=</code><code class="w"> </code><code class="nx">num</code><code class="w">&#13;
</code><code class="w">   </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">   </code><code class="k">return</code><code class="w"> </code><code class="nx">ret</code><code class="p">,</code><code class="w"> </code><code class="kc">nil</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_data_driven_efficiency_assessment_CO1-1" id="callout_data_driven_efficiency_assessment_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>We can attach the 848 + 3.6 * <em>N</em> part of the complexity equation to the operation of reading the file content into memory. The test input I used is very stable—the integers have a different number of digits, but on average they have 2.6 digits. Adding a new line (<code>\n</code>) character means every line has approximately 3.6 bytes. Since <code>ReadFile</code> returns a byte array with the content of the input file, we can say that our program requires exactly 3.6 * <em>N</em> bytes for the byte array pointed to by the <code>b</code> slice. The constant amount of 848 bytes comes from various objects allocated on the heap in the <code>os.ReadFile</code> function—for example, the slice value for <code>b</code> (24 bytes), which escaped the stack. To discover that constant, it was enough to benchmark with an empty file and profile it.</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_efficiency_assessment_CO1-2" id="callout_data_driven_efficiency_assessment_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>As you will learn in <a data-type="xref" href="ch10.html#ch-opt">Chapter 10</a>, the <code>bytes.Split</code> is quite expensive when it comes to both allocations and runtime latency. However, we can attribute most of the allocations to this part, so to the 24 + 24 * <em>N</em> complexity part. It’s the “majority” because it’s the largest constant (24) multiplied by the input size. The reason is the allocation needed to return the <a href="https://oreil.ly/Be0OF"><code>[][]byte</code></a> data structure. While we don’t copy the underlying byte arrays (we share it with the buffer from <code>os.ReadFile)</code>, the <em>N</em> allocated empty <code>[]byte</code> slices require 24 * <em>N</em> of the heap in total, plus the 24 for the <code>[][]byte</code> slice header. This is a huge allocation if <em>N</em> is on the order of billions (22 GB for a billion integers).</p></dd>&#13;
<dt><a class="co" href="#co_data_driven_efficiency_assessment_CO1-3" id="callout_data_driven_efficiency_assessment_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Finally, as we learned in <a data-type="xref" href="ch05.html#ch-hw-allocations">“Values, Pointers, and Memory Blocks”</a> and as we will uncover in <a data-type="xref" href="ch10.html#ch-opt-latency-example-strcopy">“Optimizing runtime.slicebytetostring”</a>, we allocate on this line a lot too. It’s not visible at first, but the memory required for <code>string(line)</code> (which is always a copy) is escaping to heap.<sup><a data-type="noteref" href="ch07.html#idm45606830136704" id="idm45606830136704-marker">1</a></sup> This attributes to the 2.8 * <em>N</em> part of the complexity because we do this conversion N times for 2.6 digits on average. The source of the remaining 0.2 * <em>N</em> is unknown.<sup><a data-type="noteref" href="ch07.html#idm45606830134048" id="idm45606830134048-marker">2</a></sup></p></dd>&#13;
</dl></div>&#13;
&#13;
<p>I hope that with this analysis, you see what complexity means. Perhaps you already see how useful it is to know. Maybe you already see many optimization opportunities, which we will try in <a data-type="xref" href="ch10.html#ch-opt">Chapter 10</a>!<a data-startref="ix_ch07-asciidoc4" data-type="indexterm" id="idm45606830132000"/><a data-startref="ix_ch07-asciidoc3" data-type="indexterm" id="idm45606830131296"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Asymptotic Complexity with Big O Notation" data-type="sect2"><div class="sect2" id="ch-hw-algo-bigo">&#13;
<h2>Asymptotic Complexity with Big O Notation</h2>&#13;
&#13;
<p><a data-primary="asymptotic complexity with Big O notation" data-type="indexterm" id="ix_ch07-asciidoc5"/><a data-primary="Big O asymptotic complexity" data-type="indexterm" id="ix_ch07-asciidoc6"/><a data-primary="complexity analysis" data-secondary="asymptotic complexity with Big O notation" data-type="indexterm" id="ix_ch07-asciidoc7"/>The asymptotic complexity ignores the overheads of the implementation, particularly hardware or environment. Instead, it focuses on <a href="https://oreil.ly/MR0Jz">asymptotic mathematical analysis</a>: how fast runtime or space demands grow in relation to the input size. This allows algorithm classifications based on their scalability, which usually matters for the researchers who search for algorithms solving complex problems (which usually require enormous inputs). For example, in <a data-type="xref" href="#img-opt-bigo">Figure 7-1</a>, we see a small overview of typical functions and an opinionated assessment of what’s typically bad and what’s good complexity for the algorithm. Note that “bad” complexity here doesn’t mean there are algorithms that do better—there are some problems that can’t be done in a faster way.</p>&#13;
&#13;
<figure><div class="figure" id="img-opt-bigo">&#13;
<img alt="efgo 0701" src="assets/efgo_0701.png"/>&#13;
<h6><span class="label">Figure 7-1. </span>Big O complexity chart from <a class="bare" href="https://www.bigocheatsheet.com"><em class="hyperlink">https://www.bigocheatsheet.com</em></a>. Shading indicates the opinionated rates of efficiency for usual problems.</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-primary="Knuth, Donald" data-secondary="complexity notations" data-type="indexterm" id="idm45606830119856"/>We usually use Big O notation to represent asymptotic complexity. To my knowledge, it was Donald Knuth who attempted to clearly define three notations (O, Ω, Θ)<sup><a data-type="noteref" href="ch07.html#idm45606830118784" id="idm45606830118784-marker">3</a></sup> in <a href="https://oreil.ly/yeFpW">his article from 1976</a>.</p>&#13;
<blockquote>&#13;
<p>Verbally, O(f(n)) can be read as “order at most f(n)”; Ω(f(n)) as “order at least f(n)”; Θ(f(n)) as “order exactly f(n)”.</p>&#13;
<p data-type="attribution">Donald Knuth, <a href="https://oreil.ly/yeFpW">“Big Omicron and Big Omega and Big Theta”</a></p>&#13;
</blockquote>&#13;
&#13;
<p>The phrase “in order of f(<em>N</em>)” means that we are not interested in the exact complexity numbers but rather the approximation:</p>&#13;
<dl>&#13;
<dt>The upper bound (O)</dt>&#13;
<dd>&#13;
<p>Big Oh means the function can’t be asymptotically worse than <code>f(n)</code>. It is also sometimes used to reflect the worst-case scenario if other input characteristics matter (e.g., in a sorting problem, we usually talk about a number of elements, but sometimes it matters if the input is already sorted).</p>&#13;
</dd>&#13;
<dt>The tight bound (Θ)</dt>&#13;
<dd>&#13;
<p>Big Theta represents the exact asymptotic function or, sometimes, the average, typical case.</p>&#13;
</dd>&#13;
<dt>The lower bound (Ω)</dt>&#13;
<dd>&#13;
<p>Big Omega means the function can’t be asymptotically better than <code>f(n)</code>. It also sometimes represents the best case.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>For example, the <a href="https://oreil.ly/a2jhF">quicksort</a> sorting algorithm has the best and average runtime complexity (depending on how input is sorted and where we choose the pivot point) of the <em>N</em> * log<em>N</em>, so <em>Ω</em>(<em>N</em> * log<em>N</em>) and <em>Θ</em>(<em>N</em> * log<em>N</em>), even though the worst case is <em>O</em>(<em>N</em><sup>2</sup>).</p>&#13;
<div data-type="note" epub:type="note"><h1>The Industry Is Not Always Using Big O Notation Properly</h1>&#13;
<p><a data-primary="Big Theta" data-type="indexterm" id="idm45606830099680"/>Generally, during interviews, discussions, and tutorials, you would see people using Big Oh (<em>O</em>) where Big Theta (<em>Θ</em>) should be used to describe a typical case. For example, we often say quicksort is &#13;
<span class="keep-together"><em>O</em>(<em>N</em> * log<em>N</em>),</span> which is not true, but in many instances we would accept that answer. Perhaps people try to make this space more accessible by simplifying this topic. I will try to be more precise here, but you can always swap <em>Θ</em> with <em>O</em> (but not in the opposite direction).</p>&#13;
</div>&#13;
&#13;
<p>For our algorithm in <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a>, the asymptotic space complexity is linear:</p>&#13;
<div data-type="equation">&#13;
<math>&#13;
  <mrow>&#13;
    <mi>s</mi>&#13;
    <mi>p</mi>&#13;
    <mi>a</mi>&#13;
    <mi>c</mi>&#13;
    <mi>e</mi>&#13;
    <mo>(</mo>&#13;
    <mi>N</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>872</mn>&#13;
    <mo>+</mo>&#13;
    <mn>30</mn>&#13;
    <mo>.</mo>&#13;
    <mn>4</mn>&#13;
    <mo>*</mo>&#13;
    <mi>N</mi>&#13;
    <mspace width="0.222222em"/>&#13;
    <mi>b</mi>&#13;
    <mi>y</mi>&#13;
    <mi>t</mi>&#13;
    <mi>e</mi>&#13;
    <mi>s</mi>&#13;
    <mo>=</mo>&#13;
    <mi>Θ</mi>&#13;
    <mo>(</mo>&#13;
    <mn>1</mn>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>Θ</mi>&#13;
    <mo>(</mo>&#13;
    <mi>N</mi>&#13;
    <mo>)</mo>&#13;
    <mspace width="0.222222em"/>&#13;
    <mi>b</mi>&#13;
    <mi>y</mi>&#13;
    <mi>t</mi>&#13;
    <mi>e</mi>&#13;
    <mi>s</mi>&#13;
    <mo>=</mo>&#13;
    <mi>Θ</mi>&#13;
    <mo>(</mo>&#13;
    <mi>N</mi>&#13;
    <mo>)</mo>&#13;
    <mspace width="0.222222em"/>&#13;
    <mi>b</mi>&#13;
    <mi>y</mi>&#13;
    <mi>t</mi>&#13;
    <mi>e</mi>&#13;
    <mi>s</mi>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>In asymptotic analysis, constants like 1, 872, and 30.2 do not matter, even though in practice, it might matter if our code allocates 1 MB (<em>Θ</em>(<em>N</em>)) or 30.4 MB.</p>&#13;
&#13;
<p>Note that we don’t need precise complexity to figure out the asymptotic one. That’s the point: precise complexity depends on too many variables, especially when it comes to runtime complexity. Generally, we can learn to find the theoretical asymptotic complexity based on algorithm pseudocode or description. It takes some practice, but imagine we don’t have <a data-type="xref" href="#code-sum-compl">Example 7-1</a> implemented; instead, we design an algorithm. For example, the naive algorithm for the sum of all integers in the file can be described as follows:</p>&#13;
<ol>&#13;
<li>&#13;
<p>We read the file’s content into memory, which has <em>Θ</em>(<em>N</em>) of asymptotic space complexity, where <em>N</em> is the number of integers or lines. As we read N lines, this also has <em>Θ</em>(<em>N</em>) runtime complexity.</p>&#13;
</li>&#13;
<li>&#13;
<p>We split the content into subslices. If we do it in place, this means <em>Θ</em>(<em>N</em>). Otherwise, in theory, it is <em>Θ</em>(1). This is an interesting one, as we saw in precise &#13;
<span class="keep-together">complexity</span> that despite doing this in place, the overhead is 24 * <em>N</em>, which suggests <em>Θ</em>(<em>N</em>). In both cases, the runtime complexity is <em>Θ</em>(<em>N</em>), as we have to go through &#13;
<span class="keep-together">all lines.</span></p>&#13;
</li>&#13;
<li>&#13;
<p>For every subslice (space complexity <em>Θ</em>(1) and runtime <em>Θ</em>(<em>N</em>)):</p>&#13;
<ol>&#13;
<li>&#13;
<p>We parse the integer. Technically this needs no extra space on the heap, assuming the integers can be kept on the stack. The runtime of this should also be <em>Θ</em>(1) if we relate to the number of lines and the number of digits is limited.</p>&#13;
</li>&#13;
<li>&#13;
<p>We add the parsed value into a temporary variable containing a partial sum: <em>Θ</em>(1) runtime and <em>Θ</em>(1) space.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>With such analysis, we can tell that the space complexity is <em>Θ</em>(<em>N</em>) + <em>Θ</em>(1) + <em>Θ</em>(<em>N</em>) * <em>Θ</em>(1), so <em>Θ</em>(<em>N</em>). I also mentioned runtime complexity in step 2, which combines into <em>Θ</em>(<em>N</em>) + <em>Θ</em>(<em>N</em>) + <em>Θ</em>(<em>N</em>) * <em>Θ</em>(1), so also linear <em>Θ</em>(<em>N</em>).</p>&#13;
&#13;
<p>Generally, such a <code>Sum</code> algorithm is fairly easy to assess asymptotically, but this is not trivial in many cases. It takes some practice and experience. I would love it if some automatic tools detected such complexity. There were interesting <a href="https://oreil.ly/0h9ff">attempts</a> in the past, but in practice, they are too expensive.<sup><a data-type="noteref" href="ch07.html#idm45606830040464" id="idm45606830040464-marker">4</a></sup> Perhaps there is a way to implement some algorithm that assesses pseudocode for its complexity, but it’s our job now!<a data-startref="ix_ch07-asciidoc7" data-type="indexterm" id="idm45606830039472"/><a data-startref="ix_ch07-asciidoc6" data-type="indexterm" id="idm45606830038800"/><a data-startref="ix_ch07-asciidoc5" data-type="indexterm" id="idm45606830038128"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Practical Applications" data-type="sect2"><div class="sect2" id="ch-hw-algo-practical">&#13;
<h2>Practical Applications</h2>&#13;
&#13;
<p><a data-primary="complexity analysis" data-secondary="practical applications" data-type="indexterm" id="ix_ch07-asciidoc8"/>Frankly speaking, I was always skeptical about the “complexity” topic. Perhaps I missed the lectures about it at my university,<sup><a data-type="noteref" href="ch07.html#idm45606830034224" id="idm45606830034224-marker">5</a></sup> but I was always disappointed when somebody asked me to determine the complexity of some algorithm. I was convinced that it is only used to trick candidates during technical interviews and has almost no use in practical software development.</p>&#13;
&#13;
<p>The first problem was imprecision—when people asked me to determine complexity, they meant asymptotic complexity in Big O notation. Furthermore, what’s the point of Big O if, during paid work, I could usually search an element in the array with the linear algorithm instead of a hashmap, and still the code would be fast enough in most cases? Moreover, more experienced developers were rejecting my merge requests because my fancy linked list with better insertion complexity could be just a simpler array with <code>appends</code>. Finally, I was learning about all those fast algorithms with incredible asymptotic complexity that are not used in practice because of hidden constant costs or other caveats.<sup><a data-type="noteref" href="ch07.html#idm45606830032352" id="idm45606830032352-marker">6</a></sup></p>&#13;
&#13;
<p>I think most of my frustration came from misunderstandings and misuses stemming from the industry’s stereotypes and simplifications. I am especially surprised that <a href="https://oreil.ly/1yxqH">not a few engineers</a> are willing to perform such “estimated” complexity. Perhaps we often feel demotivated or overwhelmed by how hard it is to estimate beyond asymptotic complexity. For me, reading old programming books was eye-opening—some of them use both complexities in most of their optimization examples!<a data-primary="Bentley, Jon Louis" data-secondary="on Pascal running time" data-type="indexterm" id="idm45606830029392"/></p>&#13;
<blockquote>&#13;
<p>The main <code>for</code> loop of the program is executed <code>N-1</code> times, and contains an inner loop that is itself executed <code>N</code> times; the total time required by the program will therefore be dominated by a term proportional to <code>N^2</code>. The Pascal running time of Fragment A1 was observed to by approximately 47.0N^2 microseconds.</p>&#13;
<p data-type="attribution">Jon Louis Bentley, <em>Writing Efficient Programs</em></p>&#13;
</blockquote>&#13;
&#13;
<p>When you try to assess or optimize algorithm and code that requires better efficiency, being aware of its estimated complexity and asymptotic complexity has a real value. Let’s go through some use cases.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="If you know precise complexity, you don’t need to measure to know expected resource requirements" data-type="sect3"><div class="sect3" id="idm45606830023840">&#13;
<h3>If you know precise complexity, you don’t need to measure to know expected resource requirements</h3>&#13;
&#13;
<p>In practice, we rarely have precise complexity from the start, but imagine someone giving us such complexity. This gives an enormous win for tasks like capacity planning, where you need to find out the cost of running your system under various loads (e.g., different inputs).</p>&#13;
&#13;
<p>For example, how much memory does the naive implementation of <code>Sum</code> use in <a data-type="xref" href="#code-sum-compl">Example 7-1</a>? It turns out that without any benchmark, I could use the space complexity of 872 + 30.4 * <em>N</em> bytes to tell that for various input sizes, for example:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>For 1 million integers, my code would need 30,400,872 bytes, so 30.4 MB if we use the <a href="https://oreil.ly/SYcm8">1,000 multiplier, not the 1,024</a>.<sup><a data-type="noteref" href="ch07.html#idm45606830017600" id="idm45606830017600-marker">7</a></sup></p>&#13;
</li>&#13;
<li>&#13;
<p>For 2 million integers, it would need 60.8 MB.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This can be confirmed if we would perform a quick microbenchmark (don’t worry, I will explain how to perform benchmarks here and in <a data-type="xref" href="ch08.html#ch-benchmarking">Chapter 8</a>). Results are presented in <a data-type="xref" href="#code-sum-bench2">Example 7-2</a>.</p>&#13;
<div data-type="example" id="code-sum-bench2">&#13;
<h5><span class="label">Example 7-2. </span>Benchmark allocation result for <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> with one million elements and two million elements input, respectively</h5>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">name (alloc/op)    Sum1M        Sum2M&#13;
Sum                30.4MB ± 0%  60.8MB ± 0%&#13;
&#13;
name (alloc/op)    Sum1M        Sum2M&#13;
Sum                800k ± 0%    1600k ± 0%</pre></div>&#13;
&#13;
<p>Based on just those two results, our space complexity is fairly accurate.<sup><a data-type="noteref" href="ch07.html#idm45606829986592" id="idm45606829986592-marker">8</a></sup></p>&#13;
<div data-type="tip">&#13;
<p>It’s unlikely you can always find the full, accurate, real complexity. However, usually it’s enough to have a very high-level estimation of this complexity, e.g., 30 * <em>N</em> bytes would be detailed enough space complexity for our <code>Sum</code> function in <a data-type="xref" href="#code-sum-compl">Example 7-1</a>.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="It tells us if there is any easy optimization to our code" data-type="sect3"><div class="sect3" id="idm45606830004944">&#13;
<h3>It tells us if there is any easy optimization to our code</h3>&#13;
&#13;
<p>Sometimes we don’t need detailed empirical data to know we have efficiency problems.<sup><a data-type="noteref" href="ch07.html#idm45606830003600" id="idm45606830003600-marker">9</a></sup> This is great because such techniques can tell us how easy it is to optimize our program further. Such a quick efficiency assessment is something I would love you to know before we move into heavy benchmarking.</p>&#13;
&#13;
<p>For example, when I wrote the naive implementation of the <code>Sum</code> in <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a>, I expected to write an algorithm with <em>Θ</em>(<em>N</em>) space (asymptotic) complexity. However, I expected it to have around 3.5 * <em>N</em> of the real complexity because I read the whole file content to memory. Only when I ran benchmarks that gave me output like <a data-type="xref" href="#code-sum-bench2">Example 7-2</a> did I realize how poor my naive implementation was, with almost 10 times more memory usage than expected (30.5 MB). This expected estimation of the real complexity versus the resulting one is typically a good indication that there might be some trivial optimization if we have to improve the efficiency.</p>&#13;
&#13;
<p>Secondly, if my algorithm space Big O complexity is linear, it is already a bad sign for such simple functionality. My algorithm will use an extreme amount of memory for huge inputs. Depending on requirements, that might be fine or it might mean real issues if we want to scale this application.<sup><a data-type="noteref" href="ch07.html#idm45606829995728" id="idm45606829995728-marker">10</a></sup> If not a problem right now, the maximum expected input size should be acknowledged and documented as it might be a surprise to somebody who will be using this function in the future!</p>&#13;
&#13;
<p>Finally, suppose the measurements are totally off the expected complexity of the algorithm. In that case, it might signal a <a href="https://oreil.ly/ZNB5s">memory leak</a>, which is often easy to fix if you have the right tools (as we will discuss in <a data-type="xref" href="ch11.html#ch-basic-leaks">“Don’t Leak Resources”</a>).</p>&#13;
<div data-type="tip"><h1>Three Clear Indications We Are Wasting Memory Space</h1>&#13;
<ul>&#13;
<li>&#13;
<p><a data-primary="memory resource" data-secondary="waste indications" data-type="indexterm" id="idm45606829970928"/>The difference between the theoretical space complexity (asymptotic and estimated) and the reality measured with a benchmark can immediately tell you if something is not as expected.</p>&#13;
</li>&#13;
<li>&#13;
<p>Significant space complexity depending on the user (or caller) input is a bad sign that might mean future scalability &#13;
<span class="keep-together">problems.</span></p>&#13;
</li>&#13;
<li>&#13;
<p>If, with time, the total memory used by the program constantly grows and never goes down, it most likely indicates a memory leak.</p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="It helps us assess ideas for a better algorithm as an optimization" data-type="sect3"><div class="sect3" id="idm45606829966752">&#13;
<h3>It helps us assess ideas for a better algorithm as an optimization</h3>&#13;
&#13;
<p>Another amazing use case for complexities is quickly assessing algorithmic optimizations without implementing them. For our <code>Sum</code> example, we don’t need extreme algorithmic skills to know that we don’t need to buffer the whole file in memory. If we want to save memory, we should be able to have a small buffer for parsing purposes. Let’s describe an improved algorithm:</p>&#13;
<ol>&#13;
<li>&#13;
<p>We open the input file without reading anything.</p>&#13;
</li>&#13;
<li>&#13;
<p>We create a 4 KB buffer, so we need at least 4 KB of memory, which is still a constant amount (<em>Θ</em>(1)).</p>&#13;
</li>&#13;
<li>&#13;
<p>We read the file in 4 KB chunks. For every chunk:</p>&#13;
<ol>&#13;
<li>&#13;
<p>We parse the number.</p>&#13;
</li>&#13;
<li>&#13;
<p>We add it to a temporary partial sum.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>Such an improved algorithm, in theory, should give us the space complexity of ~4 KB, so <em>O</em>(1). As a result, our <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> could use 7,800 times less space for 1 &#13;
<span class="keep-together">million</span> integers! So we can tell without implementation that such optimization on an algorithmic level would be very beneficial, and you will see it in action in <a data-type="xref" href="ch10.html#ch-opt-mem-example">“Optimizing Memory Usage”</a>.</p>&#13;
&#13;
<p>Doing such complexity analysis can quickly assess your ideas for improvement without needing the full TFBO loop!</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Worse Is Sometimes Better!</h1>&#13;
<p>If we decide to implement the algorithm with better asymptotic or theoretical complexity, don’t forget to assess it at the code level using benchmarks! When designing an algorithm, we often optimize for asymptotic complexity, but when we write code, we optimize the constants of that asymptotic complexity.</p>&#13;
&#13;
<p>Without good measurements, you might implement a good algorithm in terms of Big O complexity, but with the inefficient code, make efficiency optimizations instead of improvement!</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="It tells us where the bottleneck is and what part of the algorithm is critical" data-type="sect3"><div class="sect3" id="idm45606829952432">&#13;
<h3>It tells us where the bottleneck is and what part of the algorithm is critical</h3>&#13;
&#13;
<p>Finally, a quick look at the detailed space complexity, especially when mapped to the source code as in <a data-type="xref" href="#code-sum-compl">Example 7-1</a>, is a great way to determine the efficiency bottleneck. We can see that the constant 24 is the biggest one, and it comes from the <code>bytes.Split</code> function that we will optimize first in <a data-type="xref" href="ch10.html#ch-opt">Chapter 10</a>. In practice, however, profiling can yield data-driven results much faster, so we will focus on this method in <a data-type="xref" href="ch09.html#ch-observability3">Chapter 9</a>.</p>&#13;
&#13;
<p>To sum up, the wider knowledge about the complexity and ability to mix basic measurements with theoretical asymptotic taught me that complexities could be useful. It can be an excellent tool for more theoretical efficiency assessment if used correctly. However, as you can see, the real value is when we mix empirical measurements with theory.<a data-startref="ix_ch07-asciidoc8" data-type="indexterm" id="idm45606829947056"/> With this in mind, let’s learn more about benchmarking!<a data-startref="ix_ch07-asciidoc2" data-type="indexterm" id="idm45606829946224"/><a data-startref="ix_ch07-asciidoc1" data-type="indexterm" id="idm45606829945552"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Art of Benchmarking" data-type="sect1"><div class="sect1" id="ch-obs-bench-intro">&#13;
<h1>The Art of Benchmarking</h1>&#13;
&#13;
<p><a data-primary="benchmarks/benchmarking" data-secondary="data-driven efficiency assessment and" data-type="indexterm" id="ix_ch07-asciidoc9"/><a data-primary="efficiency assessment" data-secondary="benchmarking" data-type="indexterm" id="ix_ch07-asciidoc10"/>Assessing efficiency is essential in the TFBO flow, represented by step 4 in <a data-type="xref" href="ch03.html#img-opt-flow">Figure 3-5</a>. Such evaluation of our code, algorithm, or system is generally a complex problem, achievable in many ways. For example, we discussed assessing efficiency on the algorithm level through research, static analysis, and Big O notations for runtime &#13;
<span class="keep-together">complexity.</span></p>&#13;
&#13;
<p class="less_space pagebreak-before">We can assess a lot by performing a theoretical analysis and estimating code efficiency. Still, in many cases, the most reliable way is to get our hands dirty, run some code, and see things in action. As we learned in <a data-type="xref" href="ch03.html#ch-conq-challenges">“Optimization Challenges”</a>, we are bad at estimating the resource consumption of our code, so empirical assessments allow us to reduce the number of guesses in our evaluations.<sup><a data-type="noteref" href="ch07.html#idm45606829937408" id="idm45606829937408-marker">11</a></sup> Ideally, we assume nothing and verify the efficiency using special testing processes that test efficiency instead of correctness. We call those tests <em>benchmarks</em>.</p>&#13;
<div data-type="note" epub:type="note"><h1>Benchmarking Versus Stress and Load Tests</h1>&#13;
<p><a data-primary="benchmarks/benchmarking" data-secondary="stress/load tests versus" data-type="indexterm" id="idm45606829934176"/>There are many alternative names for benchmarking, such as stress tests, performance tests, and load tests. However, since they generally mean the same, for consistency, I will use benchmarking in this book.</p>&#13;
</div>&#13;
&#13;
<p>Generally, benchmarking is an effective efficiency assessment method for our software or systems. In abstract, the process of benchmarking is composed of four core parts, which we describe logically as a simple function:</p>&#13;
<div data-type="equation">&#13;
<math>&#13;
  <mrow>&#13;
    <mi>B</mi>&#13;
    <mi>e</mi>&#13;
    <mi>n</mi>&#13;
    <mi>c</mi>&#13;
    <mi>h</mi>&#13;
    <mi>m</mi>&#13;
    <mi>a</mi>&#13;
    <mi>r</mi>&#13;
    <mi>k</mi>&#13;
    <mo>=</mo>&#13;
    <mi>N</mi>&#13;
    <mo>*</mo>&#13;
    <mo>(</mo>&#13;
    <mi>E</mi>&#13;
    <mi>x</mi>&#13;
    <mi>p</mi>&#13;
    <mi>e</mi>&#13;
    <mi>r</mi>&#13;
    <mi>i</mi>&#13;
    <mi>m</mi>&#13;
    <mi>e</mi>&#13;
    <mi>n</mi>&#13;
    <mi>t</mi>&#13;
    <mo>+</mo>&#13;
    <mi>M</mi>&#13;
    <mi>e</mi>&#13;
    <mi>a</mi>&#13;
    <mi>s</mi>&#13;
    <mi>u</mi>&#13;
    <mi>r</mi>&#13;
    <mi>e</mi>&#13;
    <mi>m</mi>&#13;
    <mi>e</mi>&#13;
    <mi>n</mi>&#13;
    <mi>t</mi>&#13;
    <mi>s</mi>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>C</mi>&#13;
    <mi>o</mi>&#13;
    <mi>m</mi>&#13;
    <mi>p</mi>&#13;
    <mi>a</mi>&#13;
    <mi>r</mi>&#13;
    <mi>i</mi>&#13;
    <mi>s</mi>&#13;
    <mi>o</mi>&#13;
    <mi>n</mi>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>At the core of any benchmarking, we have the experimentations and measurements cycle:</p>&#13;
<dl>&#13;
<dt>Experiment</dt>&#13;
<dd>&#13;
<p><a data-primary="experiment (definition)" data-type="indexterm" id="idm45606829907200"/>The act of simulating a specific functionality of our software to learn about its efficiency behavior. We can scope that experiment to a single Go function or Go structure or even complex, distributed systems. For example, if your team develops the web server, it might mean starting a web server and performing a single HTTP request with realistic data that the user would use.</p>&#13;
</dd>&#13;
<dt>Measurement</dt>&#13;
<dd>&#13;
<p>In <a data-type="xref" href="ch06.html#ch-observability">Chapter 6</a>, we discussed getting accurate measurements for latency and the consumption of various resources. It’s vital to reliably observe our software during the entire experiment to make meaningful conclusions when it ends. For our web server example, this might mean measuring the latency of the operations on various levels (e.g., client and server latencies), as well as the memory consumption of our web server.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Now the unique part of our benchmarking process is that the experiment and measurements cycle has to be performed <em>N</em> times with the comparison phase at the end:</p>&#13;
<dl>&#13;
<dt>The number of test iterations (N)</dt>&#13;
<dd>&#13;
<p><em>N</em> is the number of test iterations we must perform to build enough confidence in the results. The exact number of runs depends on many factors, which we will discuss in <a data-type="xref" href="#ch-obs-rel">“Reliability of Experiments”</a>. Generally, the more iterations we do, the better. In many cases, we have to balance between higher confidence and cost or wait time of a too large number of iterations.</p>&#13;
</dd>&#13;
<dt>Comparison</dt>&#13;
<dd>&#13;
<p>Finally, in the benchmarking definition, we have <a href="https://oreil.ly/kzNR3">the comparison aspect</a>, which allows us to learn what’s improving the efficiency of our software, what’s hindering it, and how far we are from the expectations (RAER).</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>In many ways, you might notice that benchmarking is similar to the testing we do to verify correctness (referred to later as functional testing). As a result, many testing practices apply to benchmarking. Let’s look at that next.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Comparison to Functional Testing" data-type="sect2"><div class="sect2" id="ch-obs-bench-intro-fun">&#13;
<h2>Comparison to Functional Testing</h2>&#13;
&#13;
<p><a data-primary="benchmarks/benchmarking" data-secondary="functional testing versus" data-type="indexterm" id="ix_ch07-asciidoc11"/><a data-primary="functional testing, benchmarking versus" data-type="indexterm" id="ix_ch07-asciidoc12"/>Comparison to something we are familiar with is one of the best ways to learn. So, let’s compare benchmarking to functional testing. Is there anything we can reuse in terms of methodology or practices? You will learn in this chapter that we can share many things between functional tests and benchmarking. For example, there are a few similar aspects:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Best practices for forming test cases (e.g., <a href="https://oreil.ly/Sw9qB">edge cases</a>), <a href="https://oreil.ly/Q3bXD">table-driven testing</a>, and regression testing</p>&#13;
</li>&#13;
<li>&#13;
<p>Splitting tests into <a href="https://oreil.ly/tvaMk">unit, integration, e2e</a>, and testing in production (more on that in <a data-type="xref" href="#ch-obs-benchmarking">“Benchmarking Levels”</a>)</p>&#13;
</li>&#13;
<li>&#13;
<p>Automation for continuous testing</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Unfortunately, we have to also be aware of significant differences. With benchmarks:</p>&#13;
<dl>&#13;
<dt>We have to have different <a href="https://oreil.ly/me3cM">test cases and test data</a>.</dt>&#13;
<dd>&#13;
<p>It might be tempting, but we cannot reuse the same test data (input parameters, potential fake, test data in a database, etc.) as we used for our unit or integrations tests meant for correctness tests. This is because the goals are different. In correctness tests, we tend to focus on different <a href="https://oreil.ly/Sw9qB">edge cases</a> from a functional perspective (e.g., failure modes). Whereas in efficiency tests, the edge cases are usually focused on triggering different efficiency issues (e.g., big requests versus many small requests). We will discuss these<a data-primary="Bentley, Jon Louis" data-secondary="on test data choices" data-type="indexterm" id="idm45606829882192"/> in <a data-type="xref" href="#ch-obs-rel-repro">“Reproducing Production”</a>.</p>&#13;
</dd>&#13;
</dl>&#13;
<blockquote><p>For most systems, though, the programmer should monitor the program on input data that is typical of the data the program will encounter in production. Note that usual test data often does not meet this requirement: while test data is chosen to exercise all parts of the code, profiling [and benchmarking] data should be chosen for its <span class="keep-together">“typicality.”</span></p><p data-type="attribution">Jon Louis Bentley, <i>Writing Efficient Programs</i></p></blockquote>&#13;
<dl>&#13;
<dt>Embrace the performance nondeterminism</dt>&#13;
<dd>&#13;
<p>Modern software and hardware consist of layers of complex optimizations. This can cause nondeterministic conditions to change while performing our benchmarks, which might mean that the results will also be nondeterministic. We will expand on this in <a data-type="xref" href="#ch-obs-rel">“Reliability of Experiments”</a>, but this is why we &#13;
<span class="keep-together">usually</span> repeat test iteration cycles hundreds if not thousands of times (our <em>N</em> component) to increase confidence in our observations. The main goal here is to figure out how repeatable our benchmark is. If the variance is too high, we know we cannot trust the results and must mitigate the variance. This is why we rely on statistics in our benchmarks, which helps a lot, but also makes it easy to mislead others and ourselves.<a data-primary="Cramblitt, Bob, on repeatability" data-type="indexterm" id="idm45606829873328"/><a data-primary="repeatability (definition)" data-type="indexterm" id="idm45606829872560"/></p>&#13;
<blockquote>&#13;
<p>Repeatability: Ensuring that the same operations are benchmarked on all configurations and that metrics are repeatable over many test runs. Rule of thumb is a variation of up to 5% is generally acceptable.</p>&#13;
<p data-type="attribution">Bob Cramblitt, <a href="https://oreil.ly/ghvJ7">“Lies, Damned Lies, and Benchmarks: What Makes a Good Performance Metric”</a></p>&#13;
</blockquote>&#13;
</dd>&#13;
<dt>It is more expensive to write and run</dt>&#13;
<dd>&#13;
<p>As you can imagine, the number of iterations we have to perform increases the running cost and complexity of performing the benchmark, both the compute cost and developer time spent on creating those and waiting. But that is not the only additional cost compared to correctness tests. To trigger efficiency problems, especially for large systems load tests, we have to exhaust different systems capacities, which means buying a lot of computing power just for the sake of tests.</p>&#13;
&#13;
<p>This is why we have to focus on a pragmatic optimization process where we only care about efficiency where necessary. There are also ways to be smart and avoid full-scale macrobenchmarks by using tactical microbenchmarks of isolated functions, as discussed in <a data-type="xref" href="#ch-obs-benchmarking">“Benchmarking Levels”</a>.</p>&#13;
</dd>&#13;
<dt>Expectations are less specific</dt>&#13;
<dd>&#13;
<p>Correctness tests always end up with some assertions. For example, in Go tests, we check if the result of the functions has the expected value. If not, we use <code>t.Error</code> or <code>t.Fail</code> to indicate the test should fail (or one-liners like <a href="https://oreil.ly/ncVhq"><code>testutil.Ok</code></a> or <a href="https://oreil.ly/uH1F5"><code>testutil.Equals</code></a>).</p>&#13;
&#13;
<p>It would be amazing if we could do the same when benchmarking—asserting if the latency and resource consumption are not exceeding the RAER. Unfortunately, we cannot just do <code>if maxMemoryConsumption &lt; 200 * 1024 * 1024</code> at the end of a microbenchmark. The typical high variance of the results, challenges in isolating the latency and resource consumption to just one functionality we test, and other problems mentioned in <a data-type="xref" href="#ch-obs-rel">“Reliability of Experiments”</a> make it hard to automate the assertion process. Typically, there has to be human or very complex anomaly detection or assertion software to understand whether the results are acceptable. Hopefully, we will see more tools that make it easier in the future.</p>&#13;
&#13;
<p>To make things harder, we might have a RAER for bigger APIs and functionalities. But if the RAER says the latency of the whole HTTP request should be &#13;
<span class="keep-together">lower than</span> the 20s, what does that mean for the single Go function involved in this request (out of thousands)? How much latency should we expect in &#13;
<span class="keep-together">microbenchmarks</span> used by this function? There is no good answer.</p>&#13;
<div data-type="tip"><h1>We Focus More on Relative Results than Absolute Numbers!</h1>&#13;
<p>In benchmarks, we usually don’t assert absolute values. Instead, we focus on comparing results to some baseline (e.g., the previous benchmark before our code change). This way, we know if we improved or negatively affected the efficiency of a single component without looking at the big picture. This is usually enough on the unit microbenchmarks level.</p>&#13;
</div>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>With the basic concept of benchmarking explained, let’s address the elephant in the room in the next section—the stereotype that associates benchmarks with lies. Unfortunately, there are <a href="https://oreil.ly/yotxL">solid reasons for this relation</a>. Let’s unpack this and see how we can tell if we can trust the benchmarks that we or<a data-startref="ix_ch07-asciidoc12" data-type="indexterm" id="idm45606829854240"/><a data-startref="ix_ch07-asciidoc11" data-type="indexterm" id="idm45606829853568"/> &#13;
<span class="keep-together">others do.</span></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Benchmarks Lie" data-type="sect2"><div class="sect2" id="idm45606829852080">&#13;
<h2>Benchmarks Lie</h2>&#13;
&#13;
<p><a data-primary="benchmarks/benchmarking" data-secondary="cheating/lying stereotype" data-type="indexterm" id="idm45606829850704"/>There is an extension to a <a href="https://oreil.ly/xULP5">famous phrase</a> that states that we can order the following words from the best to worst: “lies, damn lies, <a data-primary="Carlton, Alexander, on performance benchmarks" data-type="indexterm" id="idm45606829848816"/>and &#13;
<span class="keep-together">benchmarks.”</span></p>&#13;
<blockquote>&#13;
<p>This interest in performance has not gone unnoticed by the computer vendors. Just about every vendor promotes their product as being faster or having better “bang for the buck.” All of this performance marketing begs the question: “How can these competitors all be the fastest?” The truth is that computer performance is a complex phenomenon, and who is fastest all depends upon the particular simplifications being employed to present a particular simplistic conclusion.</p>&#13;
<p data-type="attribution">Alexander Carlton, <a href="https://oreil.ly/WClsq">“Lies, Damn Lies, and Benchmarks”</a></p>&#13;
</blockquote>&#13;
&#13;
<p>Cheating in benchmarks is indeed widespread. The efficiency results through benchmarks have significant importance in a competitive market. Users have too many choices to make, so simplifying the comparison to a simple question, “which is the fastest solution?” or “which one is the most scalable?” is common among decision-makers. As a result, benchmarking became <a href="https://oreil.ly/4NAVh">a gamification system that is cheated on</a>. The fact that efficiency assessment is very complex to get right and expensive to reproduce makes it easy to get away with a misleading conclusion. There are many examples of companies, vendors, and individuals lying in benchmarks.<sup><a data-type="noteref" href="ch07.html#idm45606829843312" id="idm45606829843312-marker">12</a></sup> However, it is essential to highlight that not all cases are done intentionally or with malicious intent. For better or worse, in most cases, the author did not purposely report misleading results. It’s only natural to get tricked by <a href="https://oreil.ly/jPxnA">statistical fallacies</a> and paradoxes that are counterintuitive to the human brain.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Benchmarks Don’t Lie; We Just Misinterpret the Results!</h1>&#13;
<p><a data-primary="benchmarks/benchmarking" data-secondary="misinterpretation of results" data-type="indexterm" id="idm45606829837312"/>There are many ways we can make wrong conclusions from benchmarks. If done accidentally, it can have severe consequences—usually a big waste of time and money. If done intentionally…​well, lies have short legs. :)</p>&#13;
&#13;
<p>We can be misled by benchmarks due to human mistakes, benchmarks performed under conditions irrelevant to us and our problem, or simply statistical error. The benchmark results themselves don’t lie; we might have just measured the wrong thing!</p>&#13;
&#13;
<p>The solution is to be a mindful consumer or developer of those benchmarks, plus learn the basics of data science. We will discuss common mistakes and solutions in <a data-type="xref" href="#ch-obs-rel">“Reliability of Experiments”</a>.</p>&#13;
</div>&#13;
&#13;
<p>To overcome some biases that are naturally happening in the benchmarks, industries often come up with some standards and certifications. For example, to ensure fair fuel economy efficiency assessments, <a href="https://oreil.ly/gKOc2">all light-duty vehicles in the US are required to have their economy results tested by the US Environmental Protection Agency (EPA)</a>. Similarly, in Europe, in response to the 40% gap between the fuel economy carmakers’ tests and reality, <a href="https://oreil.ly/LPUXj">the EU adopted the Worldwide Harmonized Light-Duty Vehicle Test Cycle and Procedure</a>. For hardware and software, many independent organizations design consistent benchmarks for specific requirements. <a href="https://oreil.ly/tkV6O">SPEC</a> and <a href="https://oreil.ly/ngRKu">Percona HammerDB</a> are two examples out of many.</p>&#13;
&#13;
<p>To overcome both lies and honest mistakes, we must focus on understanding what factors make benchmarks unreliable and what we can do to improve that quality. It’s foundational knowledge explaining many benchmark practices we will discuss in <a data-type="xref" href="ch08.html#ch-benchmarking">Chapter 8</a>. Let’s do that in the next section.<a data-startref="ix_ch07-asciidoc10" data-type="indexterm" id="idm45606829828576"/><a data-startref="ix_ch07-asciidoc9" data-type="indexterm" id="idm45606829827904"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Reliability of Experiments" data-type="sect1"><div class="sect1" id="ch-obs-rel">&#13;
<h1>Reliability of Experiments</h1>&#13;
&#13;
<p><a data-primary="efficiency assessment" data-secondary="reliability of experiments" data-type="indexterm" id="ix_ch07-asciidoc13"/>The TFBO cycle takes time. No matter on what level we assess and optimize efficiency, in all cases, it is necessary to spend a nontrivial amount of time on implementing benchmarks, executing them, interpreting results, finding bottlenecks, and trying new optimizations. It is frustrating if all or part of our efforts are wasted due to unreliable assessments.</p>&#13;
&#13;
<p>As mentioned when explaining benchmarking lies, there are many reasons why benchmarks are prone to misleading us. There are a set of common challenges it’s useful to be aware of.</p>&#13;
<div data-type="note" epub:type="note"><h1>The Same Applies to Bottleneck Analysis!</h1>&#13;
<p>In this chapter, we might be discussing benchmarks, so experiments mainly allow us to measure our efficiency (latency or resource consumption), but similar reliability concerns can be applied to other experiments or measurements around efficiency. For example, profiling our Go programs to find bottlenecks, discussed in <a data-type="xref" href="ch09.html#ch-observability3">Chapter 9</a>.</p>&#13;
</div>&#13;
&#13;
<p>We can outline three common challenges to the reliability of benchmarks: human errors, the relevance of our experiments to the production environment, and the nondeterministic efficiency of modern computers. Let’s go through these in the next sections.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Human Errors" data-type="sect2"><div class="sect2" id="ch-obs-rel-err">&#13;
<h2>Human Errors</h2>&#13;
&#13;
<p><a data-primary="benchmarks/benchmarking" data-secondary="human error and" data-type="indexterm" id="ix_ch07-asciidoc14"/><a data-primary="human error, experiment reliability and" data-type="indexterm" id="ix_ch07-asciidoc15"/><a data-primary="reliability of experiments" data-secondary="human error and" data-type="indexterm" id="ix_ch07-asciidoc16"/>Optimizations and benchmarking routines, as it stands today, involve a lot of manual work from developers. We need to run experiments with different algorithms and code, while caring about reproducing production and performance nondeterminism. Due to the manual nature, this is prone to human error.</p>&#13;
&#13;
<p>It’s easy to get lost in what optimizations we already tried, what code you added for debugging purposes, and what is meant to be saved. It is also easy to get confused about what version of code the benchmarking results belong to and what assumptions you already proved wrong.</p>&#13;
&#13;
<p class="less_space pagebreak-before">Many problems with our benchmarks tend to be caused by our sloppiness and lack of organization. Unfortunately, I am guilty of many of those mistakes too! For example, when I thought I was benchmarking optimization X, I discarded it after seeing no significant difference in benchmarking results. Only some hours later did I notice I tested the wrong code, and optimization X was helpful!</p>&#13;
&#13;
<p>Fortunately, there are some ways to reduce those risks:</p>&#13;
<dl>&#13;
<dt>Keep it simple.</dt>&#13;
<dd>&#13;
<p>Try to iterate with code changes related to efficiency in the smallest iterations possible. If you try to optimize multiple elements of your code simultaneously, it most likely will obfuscate your benchmark results. You might miss that one of those optimizations limits the efficiency of the aspect you are interested in.</p>&#13;
&#13;
<p>Similarly, try to isolate complex parts into smaller separate parts you can optimize and assess separately (divide and conquer).</p>&#13;
</dd>&#13;
<dt>Know what version of software you are benchmarking.</dt>&#13;
<dd>&#13;
<p>It might be trivial, but it’s worth repeating—use <a href="https://oreil.ly/P0eoP">software versioning</a>! If you try different optimizations, commit them in separate commits and distribute them across separate branches so you can get back to previous versions if needed. Don’t lose your optimization effort by forgetting to commit your work at the end of the day.<sup><a data-type="noteref" href="ch07.html#idm45606829807072" id="idm45606829807072-marker">13</a></sup></p>&#13;
&#13;
<p>This also means you have to be strict about what version of code you just benchmarked. Even a small reorder of seemingly unrelated statements might impact your code’s efficiency, so always benchmark your programs in atomic iterations. This also includes all dependencies your code needs, for example, those outlined in your <em>go.mod</em> file.</p>&#13;
</dd>&#13;
<dt>Know what version of benchmark you are using.</dt>&#13;
<dd>&#13;
<p>Furthermore, remember to version the code of the benchmark test itself! Avoid comparing results between different benchmark implementations, even if the change was minor (adding an extra check).</p>&#13;
&#13;
<p>Scripting scripts to execute those benchmarks with the same configuration and versioning those is also a great way not to get lost. In <a data-type="xref" href="ch08.html#ch-benchmarking">Chapter 8</a>, I  mention some best practices around declarative ways to share benchmark options for your future self and others on your team.</p>&#13;
</dd>&#13;
<dt>Keep your work well organized and structured.</dt>&#13;
<dd>&#13;
<p>Make notes, design your own consistent workflow, and be explicit in what version of code you experimented with. Track the dependency versions, and track all benchmarking results explicitly in a consistent way. Finally, be clear in communicating your findings with others.</p>&#13;
&#13;
<p>Your code should also be clean during different code attempts. Keep all best practices like <a href="https://oreil.ly/S887r">DRY</a>, don’t keep commented out code, isolate state between tests, etc.</p>&#13;
</dd>&#13;
<dt>Be skeptical about “too good to be true” benchmarking results.</dt>&#13;
<dd>&#13;
<p>If you can’t explain why your code is suddenly quicker or uses fewer resources, you most certainly did something wrong while benchmarking. It is tempting to celebrate, accept it, and move on without double-checking.</p>&#13;
&#13;
<p>Check common issues like if your benchmark test cases trigger errors instead of successful runs (mentioned in <a data-type="xref" href="ch08.html#ch-obs-micro-corr">“Test Your Benchmark for Correctness!”</a>), or perhaps the compiler optimized your microbenchmark away (discussed &#13;
<span class="keep-together">in <a data-type="xref" href="ch08.html#ch-obs-micro-comp">“Compiler Optimizations Versus Benchmark”</a>).</span></p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>A little bit of laziness in our work is healthy.<sup><a data-type="noteref" href="ch07.html#idm45606829792768" id="idm45606829792768-marker">14</a></sup> However, laziness at the wrong moment might significantly increase the number of unknowns and risks to the already difficult subject of program efficiency optimizations.<a data-startref="ix_ch07-asciidoc16" data-type="indexterm" id="idm45606829791088"/><a data-startref="ix_ch07-asciidoc15" data-type="indexterm" id="idm45606829790448"/><a data-startref="ix_ch07-asciidoc14" data-type="indexterm" id="idm45606829789776"/></p>&#13;
&#13;
<p>Now let’s look at the second key element of reliable benchmarks, relevance.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Reproducing Production" data-type="sect2"><div class="sect2" id="ch-obs-rel-repro">&#13;
<h2>Reproducing Production</h2>&#13;
&#13;
<p><a data-primary="benchmarks/benchmarking" data-secondary="relevance" data-type="indexterm" id="ix_ch07-asciidoc17"/><a data-primary="production" data-secondary="reproducing in experiments" data-type="indexterm" id="ix_ch07-asciidoc18"/><a data-primary="reliability of experiments" data-secondary="reproducing production" data-type="indexterm" id="ix_ch07-asciidoc19"/>It might be obvious, but we don’t optimize software so it can run faster or consume fewer resources on our development machine.<sup><a data-type="noteref" href="ch07.html#idm45606829782464" id="idm45606829782464-marker">15</a></sup> We optimize to ensure the software has efficient enough execution for the target destinations that matter for our business, so-called <em>production</em>.</p>&#13;
&#13;
<p>Production might mean a production server environment you deploy if you build a backend application, or a customer device like a PC, laptop, or smartphone if you build an end-user application. Therefore, we can significantly improve the quality of our efficiency assessment for all benchmarks by enhancing their relevance. We can do that by trying our best to simulate (reproduce) situations and environmental conditions of production. Particularly:</p>&#13;
<dl class="less_space pagebreak-before">&#13;
<dt>Production conditions</dt>&#13;
<dd>&#13;
<p>The characteristics of a production environment. For example, how much RAM and what kind of CPU the production machines will have dedicated for our program. What OS version does it have? What versions and kinds of dependencies will our program use?</p>&#13;
</dd>&#13;
<dt>Production workload</dt>&#13;
<dd>&#13;
<p>The data our program will work with and the behavior of the user traffic it has to handle.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Perhaps the first thing we should do is to gather requirements around the software target destination, ideally in written form in our RAER. Without it, we can’t correctly assess the efficiency of our software. Similarly, if you see benchmarks done by a vendor or independent entity, you should check if the benchmark conditions match your production and requirements. Typically, they don’t, and to fully trust it, we should try to reproduce such a benchmark on our side.</p>&#13;
&#13;
<p>Assuming we roughly know what the target production for our software looks like, we might start designing our benchmark flow, test data, and cases. The bad news is that it’s impossible to fully reproduce every aspect of production in our development or testing environment. There will always be differences and unknowns. There are many reasons why production will be different:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Even if we run the same kind and version of the OS as production, it is impossible to reproduce the dynamic state of the OS, which impacts efficiency. In fact, we cannot fully reproduce this state between two runs on the same local machine! This challenge is often called nondeterministic performance, and we will discuss it in <a data-type="xref" href="#ch-obs-rel-unkn">“Performance Nondeterminism”</a>.</p>&#13;
</li>&#13;
<li>&#13;
<p>It’s often too expensive to reproduce all kinds of production workloads that can happen (e.g., forking all production traffic and putting it through testing &#13;
<span class="keep-together">clusters).</span></p>&#13;
</li>&#13;
<li>&#13;
<p>When developing an end-user application, there are too many permutations of different hardware, dependency software versions, and situations. For example, imagine you create an Android app—tons of smartphone models could potentially run your software, even if we would limit ourselves to smartphones made in the last two years.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The good news is that we don’t need to reproduce all aspects of production. Instead, it’s often enough to represent key characteristics of the products that might limit our workloads. We might know about it from the start of development—but with time, experiments, and macrobenchmarks (see <a data-type="xref" href="ch08.html#ch-obs-macro">“Macrobenchmarks”</a>), or even production—you will learn what matters.</p>&#13;
&#13;
<p class="less_space pagebreak-before">For example, imagine you develop Go code responsible for uploading local files to a remote server, and the users notice unacceptable latency when uploading a large file. Based on that, our benchmark to reproduce this should:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Focus on test cases that involve big files. Don’t try to optimize a large number of small files, all different error cases, and potential encryption layers if that doesn’t represent what production users are using the most. Instead, be pragmatic and focus with benchmarks on what your goal is now.</p>&#13;
</li>&#13;
<li>&#13;
<p>Be mindful that your local benchmarks are not reproducing potential network latencies and behavior you will see in production. A bug in your code might cause resource leaks only in case of a slow network, which might be hard to reproduce on your machine. For these optimizations, it’s worth moving &#13;
<span class="keep-together">with benchmarks</span> to different levels, as explained in <a data-type="xref" href="#ch-obs-benchmarking">“Benchmarking Levels”</a>.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Simulating the “characteristics” of production does not necessarily mean the same dataset and workload that will exist on production! For our earlier example, you don’t need to create 200 GB test files and benchmark your program with them. In many cases, you can start with relatively large files like 5 MB, then 10 MB, and together with complexity analysis, deduce what will happen at the 200 GB level. This will allow you to optimize those cases much faster and cheaper.</p>&#13;
<blockquote>&#13;
<p>Typically it would be too difficult and inefficient to attempt to exactly reproduce a specific workload. A benchmark is usually an abstraction of a workload. It is necessary, in this process of abstracting a workload into a benchmark, to capture the essential aspects of the workload and represent them in a way that maps accurately.</p>&#13;
<p data-type="attribution">Alexander Carlton, “Lies, Damn Lies, and Benchmarks”</p>&#13;
</blockquote>&#13;
&#13;
<p>To sum up, when trying to assess the efficiency or reproduce efficiency regressions, be mindful of the differences between your testing setup and production. Not all of them are worth reproducing, but the first step is to know about those differences and how they can impact the reliability of our benchmarks! Let’s now look at what else we can do to improve the confidence of our benchmarking experiments.<a data-startref="ix_ch07-asciidoc19" data-type="indexterm" id="idm45606829759312"/><a data-startref="ix_ch07-asciidoc18" data-type="indexterm" id="idm45606829758608"/><a data-startref="ix_ch07-asciidoc17" data-type="indexterm" id="idm45606829757936"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Performance Nondeterminism" data-type="sect2"><div class="sect2" id="ch-obs-rel-unkn">&#13;
<h2>Performance Nondeterminism</h2>&#13;
&#13;
<p><a data-primary="benchmarks/benchmarking" data-secondary="noise problems" data-type="indexterm" id="ix_ch07-asciidoc20"/><a data-primary="noise (performance nondeterminism)" data-type="indexterm" id="ix_ch07-asciidoc21"/><a data-primary="nondeterministic performance (noise)" data-type="indexterm" id="ix_ch07-asciidoc22"/><a data-primary="performance nondeterminism (noise)" data-type="indexterm" id="ix_ch07-asciidoc23"/><a data-primary="reliability of experiments" data-secondary="performance nondeterminism (noise)" data-type="indexterm" id="ix_ch07-asciidoc24"/>Perhaps the biggest challenge with efficiency optimizations is the “nondeterministic performance” of modern computers. It means so-called noise, so the variance in our experiment results is because of the high complexity of all layers that impacts the efficiency we learned about in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.html#ch-hardware">4</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch05.html#ch-hardware2">5</a>. As a result, efficiency characteristics are often unpredictable and highly fragile to environmental side effects.</p>&#13;
&#13;
<p class="less_space pagebreak-before">For example, let’s consider a single statement in the Go code, an <code>a += 4</code>. No matter what conditions this code is executed in, assuming we are the only user of memory used by the <code>a</code> variable, the result of <code>a += 4</code> is always deterministic—a value of <code>a</code> plus <code>4</code>. This is because, in almost all cases, it is hard to impact correctness. You can put the computer in extreme heat or cold, you can shake it, you can schedule millions of simultaneous processes in the OS, and you can use any version of CPU that exists with any supported type of operating system that supports that hardware. Unless you do something extreme like influencing the electric signal in the memory, or you put the computer out of power, that <code>a += 4</code> operation will always give us the same result.</p>&#13;
&#13;
<p>Now let’s imagine we are interested to learn how our <code>a += 4</code> operation contributes to the latency in the bigger program. At first glance, the latency assessment should be simple—this requires a single CPU instruction (e.g., <a href="https://oreil.ly/Vv83D"><code>ADDQ</code></a>) and a single CPU register, so the amortized cost should be as fast as your CPU frequency, so, for example, an average of 0.3 ns for 3 GHz CPU.</p>&#13;
&#13;
<p>In practice, however, overheads are never amortized and never static within a single run, making that statement latency highly nondeterministic. As we learned in <a data-type="xref" href="ch04.html#ch-hardware">Chapter 4</a>, if we don’t have the data in the registers, the CPU has to fetch it from L-caches, which might take one nanosecond. If L-caches contain data the CPU needs, our single statement might take 50 ns. Suppose the OS is busy running millions of other processes; our single statement might take milliseconds. Notice that we are talking about a single instruction! On a larger scale, if this noise builds, we can accumulate variance measurable in seconds.</p>&#13;
&#13;
<p>Be mindful. Almost everything can impact the latency of our operations. Busy OS, different versions of hardware elements, and even differences in manufactured CPUs from the same company might mean different latency measurements. Ambient temperature near a laptop’s CPU or battery modes can trigger thermal scaling of our CPU frequency up and down. In extreme cases, even screaming at your computer can impact the efficiency!<sup><a data-type="noteref" href="ch07.html#idm45606829739808" id="idm45606829739808-marker">16</a></sup> The more complexity and layers we have when running our programs, the more fragile our efficiency measurements. Similar problems apply to remote devices, personal computers, and public cloud providers (e.g., AWS or &#13;
<span class="keep-together">Google)</span> that use shared infrastructure with virtualization like containers or virtual machines.<sup><a data-type="noteref" href="ch07.html#idm45606829737312" id="idm45606829737312-marker">17</a></sup></p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45606829735616">&#13;
<h5>Compressible Versus Noncompressible Resources</h5>&#13;
<p><a data-primary="compressible resources" data-type="indexterm" id="idm45606829734448"/><a data-primary="noncompressible resources" data-type="indexterm" id="idm45606829733744"/><a data-primary="resources, compressible versus noncompressible" data-type="indexterm" id="idm45606829733008"/>All efficiency aspects have some nondeterminism, but some resources are more predictive than others. Typically, it is correlated to the categorization known as how compressible resources are. Compression refers to the consequences of the saturation of certain resources (what happens when you don’t have enough of the resource).</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The latency and I/O throughput of CPU time, memory or disk access, and network bandwidth are compressible. So if we have too many processes demanding CPU time, we can slow down execution, but eventually, we will execute all the scheduled work. This means we won’t see machines crashing due to CPU saturation, but it also results in highly dynamic latency results.</p>&#13;
</li>&#13;
<li>&#13;
<p>The space and allocation aspect of the resource, like memory or disk space used, is noncompressible on its own. As we learned in <a data-type="xref" href="ch05.html#ch-hardware2">Chapter 5</a>, if the program needs more memory space than the OS has, it has to crash the process or the whole system in most cases. There are mitigations like using space of different mediums instead (OS swap) and compressing the data we want to save, but used space can’t compress automatically. This might feel like a challenge, but it is beneficial for benchmarking and measurement purposes—behavior is more deterministic.</p>&#13;
</li>&#13;
</ul>&#13;
</div></aside>&#13;
&#13;
<p>The fragility of efficiency assessment is so common that we have to expect it in every benchmarking attempt. Therefore, we have to embrace it and embed mitigations to those risks into our tools.</p>&#13;
&#13;
<p>The first thing you might want to do before mitigating nondeterministic performance is to check if this problem impacts your benchmarks. Verify the repeatability of your test by calculating the variance of your results (e.g., using standard deviation). I will explain a good tool for that in <a data-type="xref" href="ch08.html#ch-obs-micro-res">“Understanding the Results”</a>, but often you can see it in plain sight.</p>&#13;
&#13;
<p>For example, if you run the experiment once and see it finish in 4.05 seconds, and other runs vary from 3.01 to 6.5 seconds, your efficiency assessment might not be accurate. On the other hand, if the variance is low, you can be more confident about the relevance of your benchmarks. Thus, check the repeatability of your benchmark first.</p>&#13;
<div data-type="note" epub:type="note"><h1>Don’t Overuse the Statistics</h1>&#13;
<p><a data-primary="statistics, risks in overusing" data-type="indexterm" id="idm45606829723680"/>It is tempting to accept high variance and either remove the extreme results (outliers) or take the mean (average) of all your results. You can apply very complex statistics to find some efficiency numbers with <a href="https://oreil.ly/594nD">some probability</a>. Increasing benchmark runs can also make your average numbers more stable, thus giving you a bit more confidence.</p>&#13;
&#13;
<p>In practice, there are better ways to try first to mitigate stability. Statistics are great where we can’t perform a stable measurement, or we can’t verify all samples (e.g., we cannot poll all humans on Earth to find out how many smartphones are used). While benchmarking, we have more control over stability than we might initially think.</p>&#13;
</div>&#13;
&#13;
<p>There are many best practices we can follow to ensure our efficiency measurements will be more reliable by reducing the potential nondeterministic performance effects:</p>&#13;
<dl>&#13;
<dt>Ensure the stable state of the machine you benchmark on.</dt>&#13;
<dd>&#13;
<p>For most benchmarks that rely on comparisons, it matters less what conditions we benchmark in as long as they are stable (the state of the machine does not change during or between benchmarks). Unfortunately, three mechanics typically get in the way of machine stability:</p>&#13;
<dl>&#13;
<dt>Background threads</dt>&#13;
<dd>&#13;
<p><a data-primary="background threads, as source of noise" data-type="indexterm" id="idm45606829716880"/>As you learned in <a data-type="xref" href="ch04.html#ch-hardware">Chapter 4</a>, it’s hard to isolate processes on machines. Even a single, seemingly small process can make your OS and hardware busy enough to change your efficiency measurements. For example, you might be surprised how much memory and CPU time one browser tab or Slack application might use. On public clouds, it’s even more hidden as we might see processes impacting us from different virtual OSes we don’t own.</p>&#13;
</dd>&#13;
<dt>Thermal scaling</dt>&#13;
<dd>&#13;
<p><a data-primary="thermal scaling, as source of noise" data-type="indexterm" id="idm45606829713632"/>The temperature of high-end CPUs increases significantly under load. The CPUs are designed to sustain relatively hot temperatures like 80–110°C, but there are limits. If the fans cannot cool the hardware fast enough, the OS or the firmware will limit the CPU cycles to avoid component meltdown. Especially with remote devices like laptops or smartphones, it’s easy to trigger thermal scaling when the ambient temperature is high, your device is in the sunlight, or something is obstructing the cooling fans.</p>&#13;
</dd>&#13;
<dt>Power management</dt>&#13;
<dd>&#13;
<p>Similarly, devices can limit the hardware speed to reduce power consumption. This is typically seen on laptops and smartphones with battery-saving modes.</p>&#13;
</dd>&#13;
</dl>&#13;
</dd>&#13;
</dl>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45606829710864">&#13;
<h5>For Most Cases, It’s Enough to Maintain Simple Stability Best Practices</h5>&#13;
<p><a data-primary="hardware" data-secondary="stability best practices" data-type="indexterm" id="idm45606829709696"/>To reduce machine instability, you could go extreme and buy a dedicated bare-metal server that only runs OS and your benchmarks. In addition, you could turn off all software updates and all advanced thermal and power management components and keep your server specially cooled. However, for practical efficiency benchmarking, following a few reasonable practices is usually enough to avoid those problems, all while still using your developer device for testing for the quick feedback loop. For example, when benchmarking:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Try to keep your machine relatively idle, don’t actively browse the internet, and avoid running multiple benchmarks at the same time.<sup><a data-type="noteref" href="ch07.html#idm45606829707312" id="idm45606829707312-marker">18</a></sup> Close your messaging apps like Slack or Discord or any other programs that might become active during the benchmark. Literally just typing on characters in my IDE editor while performing tests usually impacts my benchmarking results 10%!</p>&#13;
</li>&#13;
<li>&#13;
<p>If you use a laptop as your benchmarking machine, keep your laptop connected to power during benchmarks.</p>&#13;
</li>&#13;
<li>&#13;
<p>Similarly, don’t keep the laptop on your lap or your bed (e.g., on the pillow) when benchmarking. This blocks the fans from pulling the hot air out, which can trigger thermal scaling!</p>&#13;
</li>&#13;
</ul>&#13;
</div></aside>&#13;
<dl>&#13;
<dt>Be extra vigilant on shared infrastructure.</dt>&#13;
<dd>&#13;
<p><a data-primary="shared infrastructure" data-type="indexterm" id="idm45606829701072"/>Buying a dedicated virtual machine on a stable cloud provider for benchmarking is not a bad idea. We mentioned noisy neighbor problems, but if done right, the cloud can be sometimes more durable than your desktop machine running various interactive software during benchmarks.</p>&#13;
&#13;
<p>When using cloud resources, ensure you choose the best possible, strict Quality of Service (QoS) contract with the provider. For example, avoid cheaper <a href="https://oreil.ly/Nu5C6">burstable</a> or preemptible virtual machines, which by design are prone to infrastructure instabilities and noisy neighbors.</p>&#13;
&#13;
<p><a data-primary="CI (Continuous Integration) pipeline, noise problems and" data-type="indexterm" id="idm45606829698464"/><a data-primary="Continuous Integration (CI) pipeline, noise problems and" data-type="indexterm" id="idm45606829697712"/>Avoid Continuous Integration (CI) pipelines, especially those from free tiers like <a href="https://oreil.ly/RcKXR">GitHub Action</a> or other providers. While they remain a convenient and cheap option, they are designed for correctness testing that has to eventually finish (not as fast as physically possible) and scale dynamically to the user demands to minimize costs. This doesn’t provide strict and stable resource allocations required for benchmarks.</p>&#13;
</dd>&#13;
<dt>Be mindful of benchmark machine limits.</dt>&#13;
<dd>&#13;
<p>Be aware of your machine spec. For example, if your laptop has only 6 CPU cores (12 virtual cores with Hyper-Threading), don’t implement benchmark cases that require the <code>GOMAXPROCS</code> to be larger than the CPUs you have available for test. Furthermore, it might make sense to benchmark with only four CPUs for six physical core CPUs on your general-purpose machine to ensure spare room for OS and background processes.<sup><a data-type="noteref" href="ch07.html#idm45606829694064" id="idm45606829694064-marker">19</a></sup></p>&#13;
&#13;
<p>Similarly, be mindful of the limits of other resources, like memory. For example, don’t run benchmarks that use close to a maximum capacity of RAM, as memory pressure, faster garbage collection, and memory trashing might slow down all threads on the machine, including the OS!</p>&#13;
</dd>&#13;
<dt>Run the experiment longer.</dt>&#13;
<dd>&#13;
<p>One of the easiest ways to reduce variance between benchmark runs is to run the benchmark a bit longer. This allows us to minimize the benchmarking overhead that we might see at the beginning of our benchmarks (e.g., CPU cache warm-up phase). This also statistically gives us more confidence that the average latency or resource consumption metric shows the authentic pattern of the current efficiency level. This method takes time and depends on nontrivial statistics, prone to statistical fallacies, so use it with care and ideally try the suggestions mentioned before.</p>&#13;
</dd>&#13;
</dl>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45606829690032">&#13;
<h5>Avoid Comparing Efficiency with Older Experiment Results!</h5>&#13;
<p><a data-primary="benchmarks/benchmarking" data-secondary="avoiding efficiency comparisons with older experiment results" data-type="indexterm" id="idm45606829688784"/><a data-primary="efficiency assessment" data-secondary="avoiding comparisons with older experiment results" data-type="indexterm" id="idm45606829687760"/>Put an expiration date on all benchmark results. It is tempting to save benchmarking results after testing one version of your code for later. Then we switch our work focus for a few days, perhaps go on holiday, and get back to optimization flow after a few days or weeks. Resist resuming your benchmarking flow by benchmarking a version with optimization and comparing it with days- or weeks-old benchmarking results stored somewhere in your filesystem.</p>&#13;
&#13;
<p>Chances are that things have changed. For example, your system got upgraded, different processes run on your machine, or there is a different load in your clusters. You also risk other human errors, as it’s easy to forget all the past details and environmental conditions you ran in. Solution? Repeat your past benchmarks on demand or invest in continuous benchmarking practices that will do that for you.<sup><a data-type="noteref" href="ch07.html#idm45606829685792" id="idm45606829685792-marker">20</a></sup></p>&#13;
</div></aside>&#13;
&#13;
<p>To sum up, be mindful of potential human errors that can lead to confusion. Do care about the relevance of your experiments to the production end goal you and your development team have. Finally, measure the repeatability of your experiments to assess if you can rely on their results. Of course, there will always be some discrepancy between benchmark runs or between benchmark runs and production setup. Still, with these recommendations, you should be able to reduce them to a safe 2–5% variance <a data-startref="ix_ch07-asciidoc24" data-type="indexterm" id="idm45606829683328"/><a data-startref="ix_ch07-asciidoc23" data-type="indexterm" id="idm45606829682656"/><a data-startref="ix_ch07-asciidoc22" data-type="indexterm" id="idm45606829681984"/><a data-startref="ix_ch07-asciidoc21" data-type="indexterm" id="idm45606829681312"/><a data-startref="ix_ch07-asciidoc20" data-type="indexterm" id="idm45606829680640"/>level.</p>&#13;
&#13;
<p>Perhaps you came to this chapter to learn how to perform Go benchmarks. I can’t wait to explain to you step-by-step how to perform those in the next chapter! However, the Go benchmarks are not all we have in our empirical assessment arsenal. Therefore, it’s essential to learn when to choose the Go benchmarks and when to fall back on different benchmarking methods. I will outline that in the next section.<a data-startref="ix_ch07-asciidoc13" data-type="indexterm" id="idm45606829678992"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Benchmarking Levels" data-type="sect1"><div class="sect1" id="ch-obs-benchmarking">&#13;
<h1>Benchmarking Levels</h1>&#13;
&#13;
<p><a data-primary="benchmarks/benchmarking" data-secondary="levels of" data-type="indexterm" id="ix_ch07-asciidoc25"/><a data-primary="efficiency assessment" data-secondary="benchmarking levels" data-type="indexterm" id="ix_ch07-asciidoc26"/>In <a data-type="xref" href="ch06.html#ch-observability">Chapter 6</a>, we discussed finding latency and resource usage metrics that will allow us reliable measurements. But in the previous section, we learned that this might be only half of the success. By definition, benchmarking requires an experimentation stage that will trigger a certain situation or state of the application, which is valuable to measure.</p>&#13;
&#13;
<p>There is something simpler worth mentioning before we start with experiments. The naive and probably simplest solution to assess the efficiency of, e.g., a new release of our software, is to give it to our customers and collect our metrics during the “production” use. This is great because we don’t need to simulate or reproduce anything. Essentially the customer is performing the “experiment” part on our software, and we just measure their experience. We could call it “monitoring” at the source or “production monitoring.” Unfortunately, there are some challenges:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Computer systems are complex. As we learned in <a data-type="xref" href="#ch-obs-rel-repro">“Reproducing Production”</a>, the efficiency depends on many environmental factors. To truly assess whether our new software versions have better or worse efficiency, we must know about all those “measurement” conditions. However, it is not economical to gather all this information when it runs on client machines.<sup><a data-type="noteref" href="ch07.html#idm45606829669440" id="idm45606829669440-marker">21</a></sup> Without it, we cannot derive any meaningful conclusions. On top of that, many users would opt out of any reporting capabilities, meaning we are even more unaware of what happened.</p>&#13;
</li>&#13;
<li>&#13;
<p>Even if we gather that observability information, it isn’t guaranteed that a situation causing problems will ever occur again. There is no guarantee that the customer will perform all the steps to reproduce the old problem. Statistically, all meaningful situations will happen at some point, but that eventual timing is too long in practice. For example, imagine that one HTTP request to a particular &#13;
<span class="keep-together"><code>/compute</code></span> path was causing efficiency problems. We fixed it and deployed it to production. What if no one used this particular path for the next two weeks? The feedback loop can be very long here.</p>&#13;
</li>&#13;
</ul>&#13;
<div data-type="note" epub:type="note"><h1>Feedback Loop</h1>&#13;
<p><a data-primary="feedback loops" data-type="indexterm" id="idm45606829664112"/>The feedback loop is a cycle that starts from the moment of making changes to our code and ends with observations around these changes.</p>&#13;
&#13;
<p>The longer this loop is, the more expensive development is. The frustration of developers is also often underestimated. In extreme cases, it will inevitably result in developers taking shortcuts by ignoring important testing or benchmarking practices.</p>&#13;
&#13;
<p>To overcome this, we must invest in practices that will give us as much reliable feedback as possible in the shortest time.</p>&#13;
</div>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Finally, it is often too late if we rely on our users to “benchmark” our software. If it’s too slow, we might have already lost their trust. This can be mitigated by <a href="https://oreil.ly/seUXz">canary rollouts</a> and feature flags,<sup><a data-type="noteref" href="ch07.html#idm45606829659856" id="idm45606829659856-marker">22</a></sup> but still, ideally, we catch efficiency issues before releasing our software to production.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Production monitoring is critical, especially when your software runs 24 hours, 7 days a week. Even more, manual monitoring, like observing efficiency trends and user feedback in your bug tracker, is also useful for the last step of efficiency assessment. Things do slip through the testing strategies we are discussing here, so it makes sense to keep production monitoring as a last verification resort. But as a standalone efficiency assessment, production monitoring is quite limited.</p>&#13;
&#13;
<p class="less_space pagebreak-before">Fortunately, we have more testing options that help to verify efficiency. Without further ado, let’s go through the different levels of efficiency testing. If we would put all of them on a single graph that compares them based on the required effort to implement and maintain and the effectiveness of the individual test, it could look like <a data-type="xref" href="#img-obs-meas">Figure 7-2</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-meas">&#13;
<img alt="efgo 0702" src="assets/efgo_0702.png"/>&#13;
<h6><span class="label">Figure 7-2. </span>Types of efficiency and correctness test methods with respect to difficulty to set up and maintain them (horizontal axis) versus how effective a singular test of a given type is in practice (vertical axis)</h6>&#13;
</div></figure>&#13;
&#13;
<p>Which of the methods presented in <a data-type="xref" href="#img-obs-meas">Figure 7-2</a> are used by mature software projects and companies? The answer is all of them. Let me explain.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Benchmarking in Production" data-type="sect2"><div class="sect2" id="ch-obs-benchmarking-prod">&#13;
<h2>Benchmarking in Production</h2>&#13;
&#13;
<p><a data-primary="benchmarks/benchmarking" data-secondary="in production" data-secondary-sortas="production" data-type="indexterm" id="idm45606829648560"/><a data-primary="production" data-secondary="benchmarking in" data-type="indexterm" id="idm45606829647312"/>Following <a href="https://oreil.ly/5NUiw">testing in production practice</a>, we could use a live production system to assess efficiency. It might mean hiring “test drivers” (beta users) who will run our software on their devices and create real usage and report issues. Benchmarking in production is also very useful when your company sells the software you develop as a SaaS. For these cases, it is as easy as creating automation (e.g., a batch job or microservice) that periodically or after every rollout benchmarks the cluster using a predefined set of test cases that mimic real user functionalities (e.g., HTTP requests that simulate user traffic). Especially since you control the production environment, you can mitigate the downsides of production monitoring. You can be aware of environmental conditions, revert quickly, use feature flags, perform canary deployments, and so on.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Benchmarking in Production Has Limited Use</h1>&#13;
<p>Unfortunately, there are many challenges to this testing practice:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>It’s easier when you run your software as a SaaS. Otherwise, it’s much harder as the developers can’t quickly revert or fix potential impacts.</p>&#13;
</li>&#13;
<li>&#13;
<p>You have to ensure Quality of Service (QoS). This means you cannot do benchmarking with extreme payloads, as you need to ensure you don’t impact—e.g., cause Denial of Service (DoS)—your production environment.</p>&#13;
</li>&#13;
<li>&#13;
<p>The feedback loop is quite long for developers in such a model. For example, you need to release your software fully to benchmark it.</p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
&#13;
<p>On the other hand, if you are fine with those limitations, as presented in <a data-type="xref" href="#img-obs-meas">Figure 7-2</a>, benchmarking in production might be the most effective and reliable testing strategy. It is ultimately the closest we can get to real production usage, which reduces the risk of inaccurate results. The effort of creating and maintaining such tests is relatively small, assuming we already have production monitoring. We don’t need to simulate data, environment, dependencies, etc. We can reuse the existing monitoring tools you need to keep the cluster up.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Macrobenchmarks" data-type="sect2"><div class="sect2" id="ch-obs-benchmarking-macro">&#13;
<h2>Macrobenchmarks</h2>&#13;
&#13;
<p><a data-primary="macrobenchmarks/macrobenchmarking" data-secondary="about" data-type="indexterm" id="idm45606829637056"/>Testing or benchmarking in production is reliable, but spotting problems at that point is expensive. That’s why the industry introduced testing in earlier stages of development. The benefit is that we can assess the efficiency with just prototypes, which can be produced much quicker. We call the tests on this level &#13;
<span class="keep-together">“macrobenchmarks.”</span></p>&#13;
&#13;
<p>Macrobenchmarks provide a great balance between good reliability of such tests and faster feedback loop compared to benchmarking in production. In practice, it means building your Go program and benchmarking it in a simulated environment with all required dependencies. For example, for client-side applications, it might mean buying some example client devices (e.g., smartphones if we build the mobile application). Then for some application releases, reinstall your Go program on those devices and thoroughly benchmark it (ideally with some automated suite).</p>&#13;
&#13;
<p>For SaaS-like use cases, it might mean creating copies of production clusters, commonly called “testing” or “staging” environments. Then, to assess efficiency, build your Go program, deploy how you would in production, and benchmark it. We will also discuss more straightforward methods like using an <a href="https://oreil.ly/f0IJo"><code>e2e</code> framework</a> that you can run on a single development machine without complex orchestration systems like Kubernetes. I will explain those two methods briefly in <a data-type="xref" href="ch08.html#ch-obs-macro">“Macrobenchmarks”</a>.</p>&#13;
&#13;
<p>There are many benefits of macrobenchmarking:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>They are highly reliable and effective (yet not as much as benchmarking in &#13;
<span class="keep-together">production).</span></p>&#13;
</li>&#13;
<li>&#13;
<p>You can delegate such macrobenchmarking to independent QA engineers because you can treat your Go program as a “closed box” (previously known as a “black box”—no need to understand how it is implemented).</p>&#13;
</li>&#13;
<li>&#13;
<p>You don’t impact production with anything you do.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The downside of this approach, as shown in <a data-type="xref" href="#img-obs-meas">Figure 7-2</a>, is the effort of building and maintaining such a benchmark suite. Typically, it means complex configuration or code to automate all of it. Additionally, in many cases, any functional changes to our Go program mean we must rebuild parts of the complex macrobenchmarking system. As a result, such macrobenchmarks are viable for more mature projects with stable APIs. On top of that, the feedback loop is still quite long. We also must limit how many benchmarks we can do at once. Naturally, we have a limited number of those testing clusters that we share with other team members for cost efficiency. This means we have to coordinate those benchmarks.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Microbenchmarks" data-type="sect2"><div class="sect2" id="ch-obs-benchmarking-micro">&#13;
<h2>Microbenchmarks</h2>&#13;
&#13;
<p><a data-primary="microbenchmarks/microbenchmarking" data-type="indexterm" id="ix_ch07-asciidoc27"/>Fortunately, there is a way to have more agile benchmarks! We can follow the pattern of <a href="https://oreil.ly/ZFxiG">divide and conquer</a> for optimizations. Instead of looking at the efficiency of the whole system or the Go program, we treat our program in an open box (previously known as a “white box”) manner and divide program functionality into smaller parts. We can then use the profiling we will learn in <a data-type="xref" href="ch09.html#ch-observability3">Chapter 9</a> to identify parts that contribute the most to the efficiency of the whole solution (e.g., use the most CPU or memory resource or add the most to the latency). We can then assess the efficiency of the program’s most “expensive” part by writing small unit tests like microbenchmarks just for this small part in isolation. The Go language provides a native benchmarking framework that you can run with the same tool as unit tests: <code>go test</code>. We will discuss using this practice in <a data-type="xref" href="ch08.html#ch-obs-micro">“Microbenchmarks”</a>.</p>&#13;
&#13;
<p>Microbenchmarks are probably the most fun to write because they are very agile and provide rapid feedback about the efficiency of our Go function, algorithm, or structure. You can quickly run those benchmarks on your (even small!) developer machine, often without going out of your favorite IDE. You can implement such a benchmark test in 10 minutes, execute it in the next 20 minutes, and then tear it down or change it entirely. It is cheap to make, cheap to iterate, like a unit test. You can also treat it as a more reusable development tool—write more complex &#13;
<span class="keep-together">microbenchmarks</span> that will work as acceptance benchmarks for a small part of the code the whole team can use.</p>&#13;
&#13;
<p>Unfortunately, with agility comes many trade-offs. For example, suppose you wrongly identify the efficiency bottleneck of your program. In that case, you might be celebrating that your local microbenchmarks for some parts of the program take only 200 ms. However, when your program is deployed, it might still cause efficiency problems (and violate the RAER). On top of that, some problems are only visible when you run all the code components together (similar to integration tests). The choice of test data is also nontrivial. In many cases, it is impossible to mimic dependencies in a way that makes sense to reproduce certain efficiency problems, so we have to make some assumptions.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>When Microbenchmarking, Don’t Forget About the Big Picture</h1>&#13;
<p>It is not uncommon to perform easy, deliberate optimizations on the part of code that is a bottleneck and see a major improvement. For example, after optimization, our microbenchmarks might indicate that instead of 400 MB, our function now allocates only 2 MB per operation. After thinking about that part of the code, you might have plenty of other ideas about optimizations for that 2 MB of allocations! So you might be tempted to learn and optimize that.</p>&#13;
&#13;
<p>This is a risk. It’s easy to fixate on raw numbers from a single microbenchmark and go into the optimization rabbit hole, introducing more complexity and spending valuable engineering time.</p>&#13;
&#13;
<p>In this case, we should most likely be happy with the massive, 200x improvement, and do all it takes to get it deployed. If we want to further improve the performance of the path we were looking at, it’s not unlikely that the bottleneck of the code path we were testing has now moved somewhere else!<a data-startref="ix_ch07-asciidoc27" data-type="indexterm" id="idm45606829615632"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What Level Should You Use?" data-type="sect2"><div class="sect2" id="idm45606829614672">&#13;
<h2>What Level Should You Use?</h2>&#13;
&#13;
<p><a data-primary="benchmarks/benchmarking" data-secondary="determining appropriate level" data-type="indexterm" id="idm45606829613392"/>As you might have already noticed, there is no “best” benchmark type. Each stage has its purpose and is needed. Every solid software project should eventually have some microbenchmarks, have some macro ones, and potentially benchmark some portion of functionalities in production. This can be confirmed by just looking at some open source projects. There are many examples, but just to pick two:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a data-primary="Prometheus" data-secondary="benchmarking suites" data-type="indexterm" id="idm45606829610912"/>The <a href="https://oreil.ly/FwnBN">Prometheus project</a> has dozens of &#13;
<span class="keep-together">microbenchmarks</span> and a semiautomated, dedicated <a href="https://oreil.ly/QqwrL">macrobenchmark suite</a> that deploys instances of the Prometheus program in Google Cloud and benchmarks them. Many Prometheus users also test and gather efficiency data directly from production clusters.</p>&#13;
</li>&#13;
<li>&#13;
<p><a data-primary="Vitess project" data-type="indexterm" id="idm45606829606784"/>The <a href="https://oreil.ly/tcGNV">Vitess project</a> uses <a href="https://oreil.ly/cLr6f">microbenchmarks written in Go</a> as well. On top of that, the Vitess project maintains <a href="https://oreil.ly/pxtPO">macrobenchmarks</a>. Amazingly, it builds automation that runs both types of benchmarks nightly, with results reported on <a href="https://oreil.ly/8RMw6">the dedicated website</a>. This is an exceptional best-practice example.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>What benchmarks to add to the software projects you work on, and when, depends on needs and maturity. Be pragmatic with adding benchmarks. No software needs numerous benchmarks in the early development cycle. When APIs are unstable and detailed requirements are changing, the benchmark will need to change as well. In fact, it can be harmful to the project if we spend time on writing (and later maintaining) benchmarks for a project that hasn’t yet functionally proven its usefulness.</p>&#13;
&#13;
<p>Follow this (intelligently) lazy approach instead:</p>&#13;
<ol>&#13;
<li>&#13;
<p>If the stakeholder is unhappy with visible efficiency problems, perform the &#13;
<span class="keep-together">bottleneck</span> analysis explained in <a data-type="xref" href="ch09.html#ch-observability3">Chapter 9</a> on production and add &#13;
<span class="keep-together">microbenchmarks</span> (see <a data-type="xref" href="ch08.html#ch-obs-micro">“Microbenchmarks”</a>) to the part that is a bottleneck. When optimized, another part will likely be a bottleneck, so new tests must be added. Do this until you are happy with the efficiency, or it’s too difficult or expensive to optimize the program further. It will grow organically.</p>&#13;
</li>&#13;
<li>&#13;
<p>When a formal RAER is established, it might be useful to ensure that you test efficiency more end to end. Then you might want to invest in the manual, then automatic, macrobenchmarks (see <a data-type="xref" href="ch08.html#ch-obs-macro">“Macrobenchmarks”</a>).</p>&#13;
</li>&#13;
<li>&#13;
<p>If you truly care about accurate and pragmatic tests, and you control your “production” environment (applicable for SaaS software), consider benchmarking in production.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
<div data-type="warning" epub:type="warning"><h1>Don’t Worry About “Benchmark” Code Coverage!</h1>&#13;
<p><a data-primary="code coverage" data-type="indexterm" id="idm45606829592320"/>For functional testing, it’s popular to measure the quality of the project by ensuring the <a href="https://oreil.ly/Sfde9">test code coverage</a> is high.<sup><a data-type="noteref" href="ch07.html#idm45606829590720" id="idm45606829590720-marker">23</a></sup></p>&#13;
&#13;
<p>Never try to measure how many parts of your program have benchmarks! Ideally, you should only implement benchmarks for the critical places you want to optimize because the data indicates they are (or were) the bottleneck.</p>&#13;
</div>&#13;
&#13;
<p>With this theory, you should know what benchmarking levels are available to you and why there is no silver bullet. Still, benchmarks are in the code of our software efficiency story, and the Go language is no different here. We can’t optimize without experimenting and measuring. However, be mindful of the time spent in this phase. Writing, maintaining, and performing benchmarks takes time, so follow the lazy approach and add benchmarks on an appropriate level on demand and only if needed.<a data-startref="ix_ch07-asciidoc26" data-type="indexterm" id="idm45606829587552"/><a data-startref="ix_ch07-asciidoc25" data-type="indexterm" id="idm45606829586848"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45606829585792">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>The reliability issues of these tests are perhaps one of the biggest reasons developers, product managers, and stakeholders de-scope efficiency efforts. Where do you think I found all those little best practices to improve reliability? At the beginning of my engineering career, I spent numerous hours on careful load testing and benchmarks with my team, only to realize it meant nothing as we missed a critical element of the environment. For example, our synthetic workloads were not providing a realistic load.</p>&#13;
&#13;
<p>Such cases can discourage even professional developers and product managers. Unfortunately, this is where we typically prefer to pay more for waste computing rather than invest in optimization efforts. That’s why it’s critically important to ensure the experiment, load tests, and scale tests we do are as reliable as possible to achieve our efficiency goals faster!</p>&#13;
&#13;
<p>In this chapter, you learned the foundations behind reliable efficiency assessment through empirical experiments we call benchmarks.</p>&#13;
&#13;
<p>We discussed the basic complexity analysis that can help optimize our journey. I mentioned the difference between benchmark testing and functional testing and why benchmarks lie if we misinterpret them. You learned common reliability problems that I found truly important during experimentation cycles and the levels of benchmarks commonly spotted in the industry.<a data-startref="ix_ch07-asciidoc0" data-type="indexterm" id="idm45606829582160"/></p>&#13;
&#13;
<p>We are finally ready to learn how to implement those benchmarks on all levels mentioned above, so let’s jump right into it!</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45606830136704"><sup><a href="ch07.html#idm45606830136704-marker">1</a></sup> This is fixed for this particular <code>ParseInt</code> function in Go 1.20 thanks to an amazing <a href="https://oreil.ly/KLIVM">improvement</a>, but you might be surprised by it in any other function!</p><p data-type="footnote" id="idm45606830134048"><sup><a href="ch07.html#idm45606830134048-marker">2</a></sup> It only shows up when we do lots of string copies in our programs. Perhaps it comes from some internal byte pools?</p><p data-type="footnote" id="idm45606830118784"><sup><a href="ch07.html#idm45606830118784-marker">3</a></sup> Those “O-notations” are respectively called Big O or Oh, Omega, and Theta. He also defines “o-notations” (o, ω), which <a href="https://oreil.ly/S44PO">means strict upper or lower bound</a>, so “this function grows slower than <code>f(N)</code>, but not exactly <code>f(N)</code>.” In practice, we don’t use o-notations very often.</p><p data-type="footnote" id="idm45606830040464"><sup><a href="ch07.html#idm45606830040464-marker">4</a></sup> I would categorize them as “brute force”—they do many benchmarks with different inputs and try to approximate the growth function.</p><p data-type="footnote" id="idm45606830034224"><sup><a href="ch07.html#idm45606830034224-marker">5</a></sup> I wouldn’t be surprised—I had a full-time job in IT from the second year of my computer science studies.</p><p data-type="footnote" id="idm45606830032352"><sup><a href="ch07.html#idm45606830032352-marker">6</a></sup> For example, quicksort has worse complexity than other algorithms, yet on average it is the fastest. Or the matrix multiplication algorithm like <a href="https://oreil.ly/q9jhn">Coppersmith-Winograd</a> has a big constant coefficient hidden by the Big O notation, which makes it only worth doing for matrices that are too big for our modern computers.</p><p data-type="footnote" id="idm45606830017600"><sup><a href="ch07.html#idm45606830017600-marker">7</a></sup> Be careful: different tools use different conversions; e.g., <code>pprof</code> uses the 1,024 multiplier, and the <code>benchstat</code> uses the 1,000 multiplier.</p><p data-type="footnote" id="idm45606829986592"><sup><a href="ch07.html#idm45606829986592-marker">8</a></sup> I was very surprised that we can construct such accurate space complexity and have such accurate memory benchmarking and profiling up to every byte on the heap. Kudos to the Go community and <code>pprof</code> community for that hard work!</p><p data-type="footnote" id="idm45606830003600"><sup><a href="ch07.html#idm45606830003600-marker">9</a></sup> This does not mean we should immediately fix those! Instead, always optimize if you know the problem will affect your goals, e.g., user satisfaction or RAER requirements.</p><p data-type="footnote" id="idm45606829995728"><sup><a href="ch07.html#idm45606829995728-marker">10</a></sup> Sometimes, there are relatively easy ways to change our code to stream and use <a href="https://oreil.ly/p6YDD">external memory</a> algorithms that ensure stable memory usage.</p><p data-type="footnote" id="idm45606829937408"><sup><a href="ch07.html#idm45606829937408-marker">11</a></sup> Unfortunately, we still have to guess a little bit—more on that in <a data-type="xref" href="#ch-obs-rel">“Reliability of Experiments”</a>. Nothing will get us 100% assurance. Yet benchmarking is probably the best we have as developers for ensuring the software we develop is efficient enough.</p><p data-type="footnote" id="idm45606829843312"><sup><a href="ch07.html#idm45606829843312-marker">12</a></sup> For example, <a href="https://oreil.ly/WNF1z">car makers cheating on emission benchmarks</a> and <a href="https://oreil.ly/sf80C">phone vendors cheating on hardware benchmarks</a> (which sometimes results with a ban from the popular <a href="https://oreil.ly/8M4ey">Geekbench</a> listing). In the software world, we have a constant battle between various vendors through <a href="https://oreil.ly/RmytC">unfair benchmarks</a>. Whoever creates them is often one of the fastest on the results list.</p><p data-type="footnote" id="idm45606829807072"><sup><a href="ch07.html#idm45606829807072-marker">13</a></sup> Some good IDEs also have additional <a href="https://oreil.ly/Ytdi0">local history</a> if you forgot to commit your changes in your <code>git</code>  <span class="keep-together">repository.</span></p><p data-type="footnote" id="idm45606829792768"><sup><a href="ch07.html#idm45606829792768-marker">14</a></sup> <a href="https://oreil.ly/u8IDm">Laziness is actually good</a> for engineers! But it has to be pragmatic, productive, and reasonable laziness toward the efficiency of our work, not purely based on our emotions in the given moment.</p><p data-type="footnote" id="idm45606829782464"><sup><a href="ch07.html#idm45606829782464-marker">15</a></sup> Unless we write software for fellow developers that runs on similar hardware.</p><p data-type="footnote" id="idm45606829739808"><sup><a href="ch07.html#idm45606829739808-marker">16</a></sup> The engineer Brendan Gregg <a href="https://oreil.ly/vI8Rl">demonstrated</a> how screaming at server hard drive disks severely impacts their I/O latency due to vibrations.</p><p data-type="footnote" id="idm45606829737312"><sup><a href="ch07.html#idm45606829737312-marker">17</a></sup> The situation where one workload from a totally different virtual machine impacts our workload is commonly called <a href="https://oreil.ly/cLRrD">a noisy neighbor situation</a>. It is a serious issue that cloud providers continuously fight, with better or worse results depending on the offering and provider.</p><p data-type="footnote" id="idm45606829707312"><sup><a href="ch07.html#idm45606829707312-marker">18</a></sup> This is why you won’t see me explaining the microbenchmark options like <a href="https://oreil.ly/S74VY">RunParallel</a>. In general, running multiple benchmark functions in parallel can distort the results. Therefore, I recommend avoiding this option.</p><p data-type="footnote" id="idm45606829694064"><sup><a href="ch07.html#idm45606829694064-marker">19</a></sup> You can also fully dedicate CPU cores to your benchmark; consider the <a href="https://oreil.ly/dCLzw"><code>cpuset</code> tool</a>.</p><p data-type="footnote" id="idm45606829685792"><sup><a href="ch07.html#idm45606829685792-marker">20</a></sup> I had this problem when writing <a data-type="xref" href="ch10.html#ch-opt">Chapter 10</a>. I ran some benchmarks in one go on a relatively cold day. Next week there was a heat wave in the UK. I could not continue my optimization effort while reusing the past benchmarking results on such a hot day, as all my code was running 10% slower! I had to redo all the experiments to compare the implementations fairly.</p><p data-type="footnote" id="idm45606829669440"><sup><a href="ch07.html#idm45606829669440-marker">21</a></sup> In some way, this is why selling your product as a SaaS is so appealing in software. Your “production” is on your premises, making it easier to control the experience of the users and validate some efficiency  <span class="keep-together">optimizations.</span></p><p data-type="footnote" id="idm45606829659856"><sup><a href="ch07.html#idm45606829659856-marker">22</a></sup> Feature flags are configuration options that can be changed dynamically without restarting the service—typically through an HTTP call. This allows reverting new functionality quicker, which helps with testing or benchmarking in production. For feature flags I rely on the excellent <a href="https://oreil.ly/rfuh2"><code>go-flagz</code></a> library. I would also pay close attention to the new CNCF project <a href="https://oreil.ly/7Bsiw">OpenFeature</a>, which is meant to provide more standard interface in this space.</p><p data-type="footnote" id="idm45606829590720"><sup><a href="ch07.html#idm45606829590720-marker">23</a></sup> I am personally not a big fan of this approach. Not every part of the code is equally important to test, and not everything is worth testing. On top of that, <a href="https://oreil.ly/NnjCD">engineers tend to gamify this system</a> by writing tests only to improve the coverage, instead on focusing on finding potential problems with the code in the fastest possible way (reducing cost of development).</p></div></div></section></body></html>