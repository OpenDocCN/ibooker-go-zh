<html><head></head><body><section data-pdf-bookmark="Chapter 6. Efficiency Observability" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch-observability">&#13;
<h1><span class="label">Chapter 6. </span>Efficiency Observability</h1>&#13;
&#13;
&#13;
<p><a data-primary="observability" data-type="indexterm" id="ix_ch06-asciidoc0"/>In <a data-type="xref" href="ch03.html#ch-conq-eff-flow">“Efficiency-Aware Development Flow”</a>, you learned to follow the TFBO (test, fix, benchmark, and optimize) flow to validate and achieve the required efficiency results with the least effort. Around the elements of the efficiency phase, observability takes one of the key roles, especially in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch07.html#ch-observability2">7</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch09.html#ch-observability3">9</a>. We focus on that phase in <a data-type="xref" href="#img-obs-intro">Figure 6-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-intro">&#13;
<img alt="efgo 0601" src="assets/efgo_0601.png"/>&#13;
<h6><span class="label">Figure 6-1. </span>An excerpt from <a data-type="xref" href="ch03.html#img-opt-flow">Figure 3-5</a> focusing on the part that requires good &#13;
<span class="keep-together">observability</span></h6>&#13;
</div></figure>&#13;
&#13;
<p class="less_space pagebreak-before">In this chapter, I will explain the required observability and monitoring tools for this part of the flow. First, we will learn what observability is and what problems it solves. Then, we will discuss different observability signals, typically divided into logs, tracing, metrics, and, recently, profiles. Next, we will explain the first three signals in <a data-type="xref" href="#ch-obs-signals">“Example: Instrumenting for Latency”</a>, which takes latency as an example of the efficiency information we might want to measure (profiling is explained in <a data-type="xref" href="ch09.html#ch-observability3">Chapter 9</a>). Last but not least, we will go through the specific semantics and sources of metrics related to our program efficiency in <a data-type="xref" href="#ch-obs-semantics">“Efficiency Metrics Semantics”</a>.</p>&#13;
<div data-type="tip"><h1>You Can’t Improve What You Don’t Measure!</h1>&#13;
<p>This quote, often attributed to Peter Drucker, is a key to improving anything: business revenues, car efficiency, family budget, body fat, or <a href="https://oreil.ly/eKiIR">even happiness</a>.</p>&#13;
&#13;
<p>Especially when it comes to invisible waste that our inefficient software is producing, we can say that it’s impossible to optimize software without assessing and measuring before and after the change. Every decision must be data driven, as our guesses in this virtual space are often wrong.</p>&#13;
</div>&#13;
&#13;
<p>With no further ado, let’s learn how to measure the efficiency of our software in the easiest possible way—with the concept the industry calls observability.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Observability" data-type="sect1"><div class="sect1" id="ch-obs-observability">&#13;
<h1>Observability</h1>&#13;
&#13;
<p><a data-primary="observability" data-secondary="basics" data-type="indexterm" id="ix_ch06-asciidoc1"/>To control software efficiency, we first need to find a structured and reliable way to measure the latency and resource usage of our Go applications. The key is to count these as accurately as possible and present them at the end as easy to understand numeric values. This is why for consumption measurements, we sometimes (not always!) use a “metric signal,” which is a pillar of the essential software (or system) characteristics called observability.</p>&#13;
<div data-type="note" epub:type="note"><h1>Observability</h1>&#13;
<p><a data-primary="observability" data-secondary="definition" data-type="indexterm" id="idm45606832731056"/>In the cloud-native infrastructure world, we often talk about the observability of our applications. Unfortunately, observability is a very overloaded word.<sup><a data-type="noteref" href="ch06.html#idm45606832729952" id="idm45606832729952-marker">1</a></sup> It can be summarized as follows: an ability to deduce the state of a system inferred from external signals.</p>&#13;
&#13;
<p>The external signals the industry uses nowadays can be generally categorized into four types: metrics, logs, traces, and profiling.<sup><a data-type="noteref" href="ch06.html#idm45606832725840" id="idm45606832725840-marker">2</a></sup></p>&#13;
</div>&#13;
&#13;
<p>Observability is a huge topic nowadays as it can help us in many situations while developing and operating our software. Observability patterns allow us to debug failures or unexpected behaviors of our programs, find root causes of incidents, monitor healthiness, alert on unforeseen situations, perform billing, measure <a href="https://oreil.ly/hsdXJ">SLIs (service level indicators)</a>, run analytics, and much more. Naturally, we will focus only on the parts of observability that will help us ensure that our software efficiency matches our requirements (the RAERs mentioned in <a data-type="xref" href="ch03.html#ch-conq-req-formal">“Efficiency Requirements Should Be Formalized”</a>). So what is an observability signal?<a data-primary="Sridharan, Cindy, on observability signals" data-type="indexterm" id="idm45606832722800"/></p>&#13;
<blockquote>&#13;
 <ul><li>Metrics are a numeric representation of data measured over intervals of time. Metrics can harness the power of mathematical modeling and prediction to derive knowledge of the behavior of a system over intervals of time in the present and future.</li>&#13;
<li>An event log is an immutable, timestamped record of discrete events that happened over time. Event logs in general come in three forms but are fundamentally the same: a timestamp and a payload of some context.</li>&#13;
<li class="fix_tracking">A trace is a representation of a series of causally related distributed events that encode the end-to-end request flow through a distributed system. Traces are a representation of logs; the data structure of traces looks almost like that of an event log. A single trace can provide visibility into both the path traversed by a request as well as the structure of a request.</li></ul>&#13;
<p data-type="attribution">Cindy Sridharan, <a href="https://oreil.ly/YrSIE"><i>Distributed Systems Observability</i></a> (O’Reilly, 2018)</p></blockquote>&#13;
&#13;
<p>Generally, all those signals can be used to observe our Go applications’ latency and resource consumption for optimization purposes. For example, we can measure the latency of a specific operation and expose it as a metric. We can send that value encoded into a log line or trace annotations (e.g., <a href="https://oreil.ly/V5sQ6">“baggage”</a> items). We can calculate latency by subtracting the timestamps of two log lines—when the operation started and when it finished. We can use trace spans, which track the latency of a span (individual unit of work done) by design.</p>&#13;
&#13;
<p>However, whatever we use to deliver that information to us (via metric-specific tools, logs, traces, or profiles), in the end, it has to have metric semantics. We need to derive information to a numeric value so we can gather it over time; subtract; find max, min, or average; and aggregate over dimensions. We need the information to visualize and analyze. We need it to allow tools to reactively alert us when required, potentially build further automation that will consume it, and compare other metrics. This is why an efficiency discussion will mostly navigate through metric aggregations: the tail latency of our application, maximum memory usage over time, etc.</p>&#13;
&#13;
<p>As we discussed, to optimize anything, you have to start measuring it, so the &#13;
<span class="keep-together">industry has</span> developed many metrics and instruments to capture the usage of &#13;
<span class="keep-together">various resources.</span> The process of observing or measuring always starts with the &#13;
<span class="keep-together">instrumentation.</span></p>&#13;
<div data-type="note" epub:type="note"><h1>Instrumentation</h1>&#13;
<p><a data-primary="instrumentation (definition)" data-type="indexterm" id="idm45606832711920"/>Instrumentation is a process of adding or enabling instruments for our code that will expose the observability signals we need.</p>&#13;
</div>&#13;
&#13;
<p><a data-primary="instrumentation" data-secondary="forms of" data-type="indexterm" id="idm45606832710496"/>Instrumentation can have many forms:</p>&#13;
<dl>&#13;
<dt>Manual instrumentation</dt>&#13;
<dd>&#13;
<p>We can add a few statements to our code that import a Go module that generates an observability signal (for example, <a href="https://oreil.ly/AoWkJ">Prometheus client for metrics</a>, <a href="https://oreil.ly/adTO3">go-kit logger</a>, or <a href="https://oreil.ly/o7uYH">a tracing</a> library) and hook it to the operations we do. Of course, this requires modifying our Go code, but it usually leads to more personalized and rich signals with more context. Usually, it represents <a href="https://oreil.ly/qMjUP">open box</a> information because we can collect information tailored to the program functionality.</p>&#13;
</dd>&#13;
<dt>Autoinstrumentation</dt>&#13;
<dd>&#13;
<p>Sometimes instrumentation means installing (and configuring) a tool that &#13;
<span class="keep-together">can derive</span> useful information by looking at outside effects. For example, a &#13;
<span class="keep-together">service mesh</span> gathers observability by looking at HTTP requests and responses, or a tool hooks to the operating system and gathers information through &#13;
<span class="keep-together"><a href="https://oreil.ly/aCe6S">cgroups</a></span> or <a href="https://oreil.ly/QjxV9">eBPF</a>.<sup><a data-type="noteref" href="ch06.html#idm45606832699376" id="idm45606832699376-marker">3</a></sup> Autoinstrumentation does not require changing and rebuilding code and usually represents <a href="https://oreil.ly/UO0gK">closed box information</a>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>On top of that, it’s helpful to categorize instrumentation based on the granularity of the information:</p>&#13;
<dl>&#13;
<dt>Capturing raw events</dt>&#13;
<dd>&#13;
<p><a data-primary="instrumentation" data-secondary="granularity-based classification" data-type="indexterm" id="idm45606832694832"/>Instrumentation in this category will try to deliver a separate piece of information for each event in our process. For example, suppose we would like to know how many and what errors are happening in all HTTP requests served by our process. In that case, we could have instrumentation that delivers a separate piece of information about each request (e.g., as a log line). Furthermore, this information usually has some metadata about its context, like the status code, user IP, timestamp, and the process and code statement in which it happened (target metadata).</p>&#13;
&#13;
<p>Once ingested to some observability backend, such raw data is very rich in context and, in theory, allows any ad hoc analysis. For example, we can scan through all events to find an average number of errors or the percentile distributions (more on that in <a data-type="xref" href="#ch-obs-latency">“Latency”</a>). We can navigate to every individual error representing a single event to inspect it in detail. Unfortunately, this kind of data is generally the most expensive to use, ingest, and store. We often risk an inaccuracy here since it’s likely we’ll miss an individual event or two. In extreme cases, it requires complex skills and automation for big data and data mining explorations to find the information you want.</p>&#13;
</dd>&#13;
<dt>Capturing aggregated information</dt>&#13;
<dd>&#13;
<p>We can capture pre-aggregated data instead of raw events. Every piece of information delivered by such instrumentation represents certain information about a group of events. In our HTTP server example, we could count successful and failed requests, and periodically deliver that information. Before forwarding this information, we could go even further and pre-calculate the error ratio inside our code. It’s worth mentioning that this kind of information also requires metadata, so we can summarize, aggregate further, compare, and analyze those aggregated pieces of information.</p>&#13;
&#13;
<p>Pre-aggregated instrumentation forces Go processes or autoinstrumentation tools to do more work, but the results are generally easier to use. On top of &#13;
<span class="keep-together">this, because</span> of the smaller amount of data, the complexity of the instrumentation, signal delivery, and backend is lower, thereby increasing reliability and &#13;
<span class="keep-together">decreasing</span> cost significantly. There are trade-offs here as well. We lose some information (commonly called the cardinality). The decision of what information to prebuild is made up front, and is coded into instrumentation. If you suddenly have different questions to be answered (e.g., how many errors an individual user had across your processes) and your instrumentation was not set to pre-aggregate that information, you have to change it, which takes time and resources. Yet if you roughly know what you will be asking for ahead of time, aggregated type of information is an amazing win and a more pragmatic approach.<sup><a data-type="noteref" href="ch06.html#idm45606832688544" id="idm45606832688544-marker">4</a></sup></p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Last but not least, generally speaking we can design our observability flows into push-and-pull collection models:</p>&#13;
<dl>&#13;
<dt>Push</dt>&#13;
<dd>&#13;
<p><a data-primary="push-based collection model" data-type="indexterm" id="idm45606832685600"/>A system where a centralized remote process collects observability signals from your applications (including your Go programs).</p>&#13;
</dd>&#13;
<dt>Pull</dt>&#13;
<dd>&#13;
<p><a data-primary="pull-based collection model" data-type="indexterm" id="idm45606832683408"/>A system where application processes push the signal to a remote centralized observability system.</p>&#13;
</dd>&#13;
</dl>&#13;
<div data-type="note" epub:type="note"><h1>Push Versus Pull</h1>&#13;
<p>Each of the conventions has its pros and cons. You can push your metrics, logs, and traces, but you can also pull all of them from your process. We can also use a mixed approach, different for each observability signal.</p>&#13;
&#13;
<p>Push versus pull method is sometimes a controversial topic. The industry is polarized as to what is generally better, not only in observability but also for any other architectures. We will discuss the pros and cons in <a data-type="xref" href="#ch-obs-metrics">“Metrics”</a>, but the difficult truth is that both ways can scale equally well, just with different solutions, tools, and best practices.</p>&#13;
</div>&#13;
&#13;
<p>After learning about those three categories, we should be ready to dive further into observability signals. To measure and deliver observability information for efficiency optimizations, we can’t avoid learning more about instrumenting the three common observability signals: logging, tracing, and metrics.<a data-startref="ix_ch06-asciidoc1" data-type="indexterm" id="idm45606832678224"/> In the next section, let’s do that while keeping a practical goal in mind—measuring latency.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Example: Instrumenting for Latency" data-type="sect1"><div class="sect1" id="ch-obs-signals">&#13;
<h1>Example: Instrumenting for Latency</h1>&#13;
&#13;
<p><a data-primary="latency" data-secondary="instrumenting for" data-type="indexterm" id="ix_ch06-asciidoc2"/><a data-primary="observability" data-secondary="instrumenting for latency" data-type="indexterm" id="ix_ch06-asciidoc3"/>All three signals you will learn in this section can be used to build observability that will fit in any of the three categorizations we discussed. Each signal can:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Be manually or autoinstrumented</p>&#13;
</li>&#13;
<li>&#13;
<p>Give aggregated information or raw events</p>&#13;
</li>&#13;
<li>&#13;
<p>Be pulled (collected, tailed, or scraped) from the process or pushed (uploaded)</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Yet every signal—logging, tracing, or metric—might be better or worse fitted in any of those jobs. In this section, we will discuss these predispositions.</p>&#13;
&#13;
<p>The best way to learn how to use observability signals and their trade-offs is to focus on the practical goal. Let’s imagine we want to measure the latency of a specific operation in our code. As mentioned in the introduction, we need to start measuring the latency to assess it and decide if our code needs more optimizations during every optimization iteration. As you will learn in this section, we can get latency results using any of those observability signals. The details around how information is presented, how complex instrumentation is, and so on will help you understand what to choose in your journey. Let’s dive in!</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Logging" data-type="sect2"><div class="sect2" id="ch-obs-logging">&#13;
<h2>Logging</h2>&#13;
&#13;
<p><a data-primary="logging" data-type="indexterm" id="ix_ch06-asciidoc4"/><a data-primary="observability" data-secondary="logging" data-type="indexterm" id="ix_ch06-asciidoc5"/>Logging might be the clearest signal to understand an instrument. So let’s explore the most basic instrumentation that we might categorize as logging to collect latency measurements. Taking basic latency measurements for a single operation in Go code is straightforward, thanks to the standard <a href="https://oreil.ly/t9FDr"><code>time</code> package</a>. Whether you do it by hand or use standard or third-party libraries to obtain latencies, if they are written in Go, they use the pattern presented in <a data-type="xref" href="#code-latency-simplest">Example 6-1</a> using the <code>time</code> package.</p>&#13;
<div data-type="example" id="code-latency-simplest">&#13;
<h5><span class="label">Example 6-1. </span>Manual and simplest latency measurement of a single operation in Go</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kn">import</code><code class="w"> </code><code class="p">(</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"fmt"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"time"</code><code class="w">&#13;
</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">ExampleLatencySimplest</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">xTimes</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">start</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">time</code><code class="p">.</code><code class="nx">Now</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO1-1" id="co_efficiency_observability_CO1-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
        </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">doOperation</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">elapsed</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">time</code><code class="p">.</code><code class="nx">Since</code><code class="p">(</code><code class="nx">start</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO1-2" id="co_efficiency_observability_CO1-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
&#13;
        </code><code class="nx">fmt</code><code class="p">.</code><code class="nx">Printf</code><code class="p">(</code><code class="s">"%v ns\n"</code><code class="p">,</code><code class="w"> </code><code class="nx">elapsed</code><code class="p">.</code><code class="nx">Nanoseconds</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO1-3" id="co_efficiency_observability_CO1-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
&#13;
        </code><code class="c1">// ...</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_efficiency_observability_CO1-1" id="callout_efficiency_observability_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p><code>time.Now()</code> captures the current wall time (clock time) from our operating system clock in the form <code>time.Time</code>. Note the <code>xTime</code>, example variable that specifies the desired number of runs.</p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO1-2" id="callout_efficiency_observability_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>After our <code>cooperation</code> functions finish, we can capture the time between &#13;
<span class="keep-together"><code>start</code> and</span> current time using <code>time.Since(start)</code>, which returns the handy &#13;
<span class="keep-together"><code>time.Duration</code>.</span></p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO1-3" id="callout_efficiency_observability_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>We can leverage such an instrument to deliver our metric sample. For example, we can print the duration in nanoseconds to the standard output using the <code>.Nanoseconds()</code> method.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Arguably, <a data-type="xref" href="#code-latency-simplest">Example 6-1</a> represents the simplest form of instrumentation and observability. We take a latency measurement and deliver it by printing the result into standard output. Given that every operation will output a new line, <a data-type="xref" href="#code-latency-simplest">Example 6-1</a> represents manual instrumentation of raw event information.</p>&#13;
&#13;
<p>Unfortunately, this is a little naive. First of all, as we will learn in <a data-type="xref" href="ch07.html#ch-obs-rel">“Reliability of Experiments”</a>, a single measurement of anything can be misleading. We have to capture more of those—ideally hundreds or thousands for statistical purposes. When we have one process, and only one functionality we want to test or benchmark, <a data-type="xref" href="#code-latency-simplest">Example 6-1</a> will print hundreds of results that we can later analyze. However, to simplify the analysis, we could try to pre-aggregate some results. Instead of logging raw events, we could pre-aggregate using a mathematical average function and output that. <a data-type="xref" href="#code-latency-simplest-aggr">Example 6-2</a> presents a modification of <a data-type="xref" href="#code-latency-simplest">Example 6-1</a> that aggregates events into an easier-to-consume result.</p>&#13;
<div data-type="example" id="code-latency-simplest-aggr">&#13;
<h5><span class="label">Example 6-2. </span>Instrumenting Go to log the average latency of an operation in Go</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">func</code><code class="w"> </code><code class="nx">ExampleLatencyAggregated</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="kd">var</code><code class="w"> </code><code class="nx">count</code><code class="p">,</code><code class="w"> </code><code class="nx">sum</code><code class="w"> </code><code class="kt">int64</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">xTimes</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">start</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">time</code><code class="p">.</code><code class="nx">Now</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">doOperation</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">elapsed</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">time</code><code class="p">.</code><code class="nx">Since</code><code class="p">(</code><code class="nx">start</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">sum</code><code class="w"> </code><code class="o">+=</code><code class="w"> </code><code class="nx">elapsed</code><code class="p">.</code><code class="nx">Nanoseconds</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO2-1" id="co_efficiency_observability_CO2-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
        </code><code class="nx">count</code><code class="o">++</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">        </code><code class="c1">// ...</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">fmt</code><code class="p">.</code><code class="nx">Printf</code><code class="p">(</code><code class="s">"%v ns/op\n"</code><code class="p">,</code><code class="w"> </code><code class="nx">sum</code><code class="o">/</code><code class="nx">count</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO2-2" id="co_efficiency_observability_CO2-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_efficiency_observability_CO2-1" id="callout_efficiency_observability_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Instead of printing raw latency, we can gather a sum and number of operations in the sum.</p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO2-2" id="callout_efficiency_observability_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Those two pieces of information can be used to calculate the accurate average and present that for a group of events instead of the unique latency. For example, one run printed the <code>188324467 ns/op</code> string on my machine.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Given that we stop presenting latency for raw events, <a data-type="xref" href="#code-latency-simplest-aggr">Example 6-2</a> represents a manual, aggregated information observability. This method allows us to quickly get the information we need without complex (and time-consuming) tools analyzing our logging outputs.</p>&#13;
&#13;
<p><a data-primary="Go benchmarks" data-secondary="average latency calculations" data-type="indexterm" id="idm45606832403792"/>This example is how the Go benchmarking tool will do the average latency calculations. We can achieve exactly the same logic as in <a data-type="xref" href="#code-latency-simplest-aggr">Example 6-2</a> using the snippet in <a data-type="xref" href="#code-latency-go-bench">Example 6-3</a> in a file with the <em>_test.go</em> suffix.</p>&#13;
<div data-type="example" id="code-latency-go-bench">&#13;
<h5><span class="label">Example 6-3. </span>Simplest Go benchmark that will measure average latency per operation</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">func</code><code class="w"> </code><code class="nx">BenchmarkExampleLatency</code><code class="p">(</code><code class="nx">b</code><code class="w"> </code><code class="o">*</code><code class="nx">testing</code><code class="p">.</code><code class="nx">B</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">b</code><code class="p">.</code><code class="nx">N</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO3-1" id="co_efficiency_observability_CO3-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
        </code><code class="nx">_</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="nx">doOperation</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_efficiency_observability_CO3-1" id="callout_efficiency_observability_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The <code>for</code> loop with the <code>N</code> variable is essential in the benchmarking framework. It allows the Go framework to try different <code>N</code> values to perform enough test runs to fulfill the configured number of runs or test duration. For example, by default, the Go benchmark runs to fit one second, which is often too short for meaningful output reliability.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Once we run <a data-type="xref" href="#code-latency-go-bench">Example 6-3</a> using <code>go test</code> (explained in detail in <a data-type="xref" href="ch08.html#ch-obs-micro-go">“Go Benchmarks”</a>), it will print certain output. One part of the information is a result line with &#13;
<span class="keep-together">a number</span> of runs and average nanoseconds per operation. One of the runs on my machine gave an output latency of <code>197999371 ns/op</code>, which generally matches the result from <a data-type="xref" href="#code-latency-simplest-aggr">Example 6-2</a>. We can say that the Go benchmark is an autoinstrumentation with aggregated information using logging signals for things like latency.</p>&#13;
&#13;
<p>On top of collecting latency about the whole operation, we can gain a lot of insight from having different granularity of those measurements. For example, we might wish to capture the latency of a few suboperations inside our single operation. Finally, for more complex deployments, when our Go program is part of a &#13;
<span class="keep-together">distributed</span> system, as discussed in <a data-type="xref" href="ch08.html#ch-obs-macro">“Macrobenchmarks”</a>, we have potentially many processes we have to measure across. For those cases, we have to use more sophisticated logging that will give us more metadata and ways to deliver a logging signal, not only by simply printing to a file, but by other means too.</p>&#13;
&#13;
<p><a data-primary="logger pattern" data-type="indexterm" id="idm45606832261456"/>The amount of information we have to attach to our logging signal results in the pattern called a logger in Go (and other programming languages). A logger is a structure that allows us to manually instrument our Go application with logs in the easiest and most readable way. A logger hides complexities like:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Formatting of the log lines.</p>&#13;
</li>&#13;
<li>&#13;
<p>Deciding if we should log or not based on the logging level (e.g., debug, warning, error, or more).</p>&#13;
</li>&#13;
<li>&#13;
<p>Delivering the log line to a configured place, such as the output file. Optionally, more complex, push-based logging delivery is possible to remote backends, which must support back-off retries, authorization, service discovery, etc.</p>&#13;
</li>&#13;
<li>&#13;
<p>Adding context-based metadata and timestamps.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The Go standard library is very rich with many useful utilities, including logging. For example, the <a href="https://oreil.ly/JEUjT"><code>log</code> package</a> contains a simple logger. It can work well for many applications, but it is prone to some usage pitfalls.<sup><a data-type="noteref" href="ch06.html#idm45606832254464" id="idm45606832254464-marker">5</a></sup></p>&#13;
<div data-type="warning" epub:type="warning"><h1>Be Mindful While Using the Go Standard Library Logger</h1>&#13;
<p>There are a few things to remember if you want to use the standard Go logger from the <code>log</code> package:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Don’t use the global <code>log.Default()</code> logger, so <code>log.Print</code> functions, and so on. Sooner or later, it will bite you.</p>&#13;
</li>&#13;
<li>&#13;
<p>Never store or consume <code>*log.Logger</code> directly in your functions and structures, especially when you write a library.<sup><a data-type="noteref" href="ch06.html#idm45606832248016" id="idm45606832248016-marker">6</a></sup> If you do, users will be forced to use a very limited <code>log</code> logger instead of their own logging libraries. Use a custom interface instead (e.g., <a href="https://oreil.ly/tCs2g">go-kit logger</a>), so users can adapt their loggers to what you use in your code.</p>&#13;
</li>&#13;
<li>&#13;
<p>Never use the <code>Fatal</code> method outside the main function. It panics, which should not be your default error handling.</p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
&#13;
<p>To not accidentally get hit by these pitfalls, in the projects I worked on, we decided to use the third-party popular <a href="https://oreil.ly/ziBdb">go-kit</a><sup><a data-type="noteref" href="ch06.html#idm45606832242864" id="idm45606832242864-marker">7</a></sup> logger. An additional advantage of the go-kit logger is that it is easy to maintain some structure. Structure logic is essential to have reliable parsers for automatic log analysis with logging backends like <a href="https://oreil.ly/RohpZ">OpenSearch</a> or <a href="https://oreil.ly/Fw9I3">Loki</a>. To measure latency, let’s go through an example of logger usage in <a data-type="xref" href="#code-latency-log">Example 6-4</a>. Its output is shown in <a data-type="xref" href="#code-latency-log-result">Example 6-5</a>. We use the <a href="https://oreil.ly/vOafG"><code>go-kit</code> module</a>, but other libraries follow similar patterns.</p>&#13;
<div data-type="example" id="code-latency-log">&#13;
<h5><span class="label">Example 6-4. </span>Capturing latency though logging using the <a href="https://oreil.ly/9uCWi"><code>go-kit</code> logger</a></h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kn">import</code><code class="w"> </code><code class="p">(</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"fmt"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"time"</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/go-kit/log"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/go-kit/log/level"</code><code class="w">&#13;
</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">ExampleLatencyLog</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">logger</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">log</code><code class="p">.</code><code class="nx">With</code><code class="p">(</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO4-1" id="co_efficiency_observability_CO4-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
        </code><code class="nx">log</code><code class="p">.</code><code class="nx">NewLogfmtLogger</code><code class="p">(</code><code class="nx">os</code><code class="p">.</code><code class="nx">Stderr</code><code class="p">)</code><code class="p">,</code><code class="w"> </code><code class="s">"ts"</code><code class="p">,</code><code class="w"> </code><code class="nx">log</code><code class="p">.</code><code class="nx">DefaultTimestampUTC</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">xTimes</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">now</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">time</code><code class="p">.</code><code class="nx">Now</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">doOperation</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">elapsed</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">time</code><code class="p">.</code><code class="nx">Since</code><code class="p">(</code><code class="nx">now</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">level</code><code class="p">.</code><code class="nx">Info</code><code class="p">(</code><code class="nx">logger</code><code class="p">)</code><code class="p">.</code><code class="nx">Log</code><code class="p">(</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO4-2" id="co_efficiency_observability_CO4-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
            </code><code class="s">"msg"</code><code class="p">,</code><code class="w"> </code><code class="s">"finished operation"</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">            </code><code class="s">"result"</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">            </code><code class="s">"elapsed"</code><code class="p">,</code><code class="w"> </code><code class="nx">elapsed</code><code class="p">.</code><code class="nx">String</code><code class="p">(</code><code class="p">)</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">        </code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">        </code><code class="c1">// ...</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_efficiency_observability_CO4-1" id="callout_efficiency_observability_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>We initialize the logger. Libraries usually allow you to output the log lines to a file (e.g., standard output or error) or directly push it to some collections tool, e.g., to <a href="https://oreil.ly/pUcmX">fluentbit</a> or <a href="https://oreil.ly/S0aqR">vector</a>. Here we choose to output all logs to standard error<sup><a data-type="noteref" href="ch06.html#idm45606832090240" id="idm45606832090240-marker">8</a></sup> with a timestamp attached to each log line. We also choose to format logs in the human-accessible way with <code>NewLogfmtLogger</code> (still structured so that it can be parsed by software, with space as the delimiter).</p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO4-2" id="callout_efficiency_observability_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>In <a data-type="xref" href="#code-latency-simplest">Example 6-1</a>, we simply printed the latency number. Here we add certain metadata to it to use that information more easily across processes and different operations happening in the system. Notice that we maintain a certain structure. We pass an even number of arguments representing key values. This allows our log line to be structured for easier use by automation. Additionally, we choose <code>level.Info</code>, meaning this log line will be not printed if we choose levels like errors only.</p></dd>&#13;
</dl></div>&#13;
<div data-type="example" id="code-latency-log-result">&#13;
<h5><span class="label">Example 6-5. </span>Example output logs generated by <a data-type="xref" href="#code-latency-log">Example 6-4</a> (wrapped for readability)</h5>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting"><code>level=info ts=2022-05-02T11:30:46.531839841Z msg="finished operation" \&#13;
result="error other" elapsed=83.62459ms </code><a class="co" href="#callout_efficiency_observability_CO5-1" id="co_efficiency_observability_CO5-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
level=info ts=2022-05-02T11:30:46.868633635Z msg="finished operation" \&#13;
result="error other" elapsed=336.769413ms&#13;
level=info ts=2022-05-02T11:30:47.194901418Z msg="finished operation" \&#13;
result="error first" elapsed=326.242636ms&#13;
level=info ts=2022-05-02T11:30:47.51101522Z msg="finished operation" \&#13;
result=null elapsed=316.088166ms&#13;
level=info ts=2022-05-02T11:30:47.803680146Z msg="finished operation" \&#13;
result="error first" elapsed=292.639849ms</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_efficiency_observability_CO5-1" id="callout_efficiency_observability_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Thanks to the log structure, it’s both readable to us and automation can clearly distinguish among different fields like <code>msg</code>, <code>elapsed</code>, <code>info</code>, etc. without expensive and error-prone fuzzy parsing.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Logging with a logger might still be the simplest way to deliver our latency information manually to us. We can tail the file (or use <code>docker log</code> if our Go process was running in Docker, or <code>kubectl logs</code> if we deployed it on Kubernetes) to read those log lines for further analysis. It is also possible to set up an automation that tails those from files or pushes them directly to the collector, adding further information. Collectors can be then configured to push those log lines into free and open source logging backends like <a href="https://oreil.ly/RohpZ">OpenSearch</a>, <a href="https://oreil.ly/Fw9I3">Loki</a>, <a href="https://oreil.ly/EUlts">Elasticsearch</a>, or many of the paid vendors. As a result, you can keep log lines from many processes in a single place, search, visualize, analyze them, or build further automation to handle them as you want.</p>&#13;
&#13;
<p>Is logging a good fit for our efficiency observability? Yes and no. For microbenchmarks explained in <a data-type="xref" href="ch08.html#ch-obs-micro">“Microbenchmarks”</a>, logging is our primary tool &#13;
<span class="keep-together">of measurements</span> because of its simplicity. On the other hand, on a macro level, like <a data-type="xref" href="ch08.html#ch-obs-macro">“Macrobenchmarks”</a>, we tend to use logging for a raw event type of observability, which on such a scale gets very complex and expensive to analyze and keep reliable. Still, because logging is so common, we can find efficiency bottlenecks in a bigger system with logging.</p>&#13;
&#13;
<p>Logging tools are also constantly evolving. For example, many tools allow us to derive metrics from log lines, like Grafana Loki’s <a href="https://oreil.ly/fdoNm">Metric queries inside LogQL</a>. In practice, however, simplicity has its cost. One of the problems stems from the fact that sometimes logs are used directly by humans, and sometimes by automation (e.g., deriving metrics or reacting to situations found in logs). As a result, logs are often unstructured. Even with amazing loggers like go-kit in <a data-type="xref" href="#code-latency-log">Example 6-4</a>, logs are inconsistently structured, making it very hard and expensive to parse for automation. For example, things like inconsistent units (as in <a data-type="xref" href="#code-latency-log-result">Example 6-5</a> for latency measurements), which are great for humans, become almost impossible to derive the value as a metric. Solutions like <a href="https://oreil.ly/Q4wAC">Google mtail</a> try to approach this with custom parsing language. Still, the complexity and ever-changing logging structure make it hard to use this signal to measure our<a data-startref="ix_ch06-asciidoc5" data-type="indexterm" id="idm45606831979952"/><a data-startref="ix_ch06-asciidoc4" data-type="indexterm" id="idm45606831979248"/> code’s &#13;
<span class="keep-together">efficiency.</span></p>&#13;
&#13;
<p>Let’s look at the next observability signal—tracing—to learn in which areas it can help us with our efficiency goals.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tracing" data-type="sect2"><div class="sect2" id="ch-obs-tracing">&#13;
<h2>Tracing</h2>&#13;
&#13;
<p><a data-primary="observability" data-secondary="tracing" data-type="indexterm" id="ix_ch06-asciidoc6"/><a data-primary="tracing" data-secondary="basics" data-type="indexterm" id="ix_ch06-asciidoc7"/>Given the lack of consistent structure in logging, tracing signals emerged to tackle some of the logging problems. In contrast to logging, tracing is a piece of structured information about your system. The structure is built around the transaction, for example, requests-response architecture. This means that things like status codes, the result of the operation, and the latency of operations are natively encoded, thus easier to use by automation and tools. As a trade-off, you need an additional mechanism (e.g., a user interface) to expose this information to humans in a readable way.</p>&#13;
&#13;
<p>On top of that, operations, suboperations, and even cross-process calls (e.g., RPCs) can be linked together, thanks to context propagation mechanisms working well with standard network protocols like HTTP. This feels like a perfect choice for measuring latency for our efficiency needs, right? Let’s find out.</p>&#13;
&#13;
<p>As with logging, there are many different manual instrumentation libraries you can choose from. Popular, open source choices for Go are the <a href="https://oreil.ly/gJeAV">OpenTracing</a> library (currently deprecated but still viable), <a href="https://oreil.ly/uxKoW">OpenTelemetry</a>, or clients from the dedicated tracing vendor. Unfortunately, at the moment of writing, the OpenTelemetry library has a too-complex API to explain in this book, plus it’s still changing, so I started a <a href="https://oreil.ly/rs6fQ">small project called tracing-go</a> that encapsulates the OpenTelemetry client SDK into minimal tracing instrumentation. While tracing-go is my interpretation of the minimal set of tracing functionalities to use, it should teach you the basics of context propagation and span logic. Let’s explore an example manual instrumentation using tracing-go to measure dummy <code>doOperation</code> function latency (and more!) using tracing in <a data-type="xref" href="#code-latency-trace">Example 6-6</a>.</p>&#13;
<div data-type="example" id="code-latency-trace">&#13;
<h5><span class="label">Example 6-6. </span>Capturing latencies of the operation and potential suboperations using <a href="https://oreil.ly/1027d">tracing-go</a></h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kn">import</code><code class="w"> </code><code class="p">(</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"fmt"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"time"</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/bwplotka/tracing-go/tracing"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/bwplotka/tracing-go/tracing/exporters/otlp"</code><code class="w">&#13;
</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">ExampleLatencyTrace</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">tracer</code><code class="p">,</code><code class="w"> </code><code class="nx">cleanFn</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">tracing</code><code class="p">.</code><code class="nx">NewTracer</code><code class="p">(</code><code class="nx">otlp</code><code class="p">.</code><code class="nx">Exporter</code><code class="p">(</code><code class="s">"&lt;endpoint&gt;"</code><code class="p">)</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO6-1" id="co_efficiency_observability_CO6-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
    </code><code class="k">if</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">!=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><code class="cm">/* Handle error... */</code><code class="w"> </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">defer</code><code class="w"> </code><code class="nx">cleanFn</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">xTimes</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">ctx</code><code class="p">,</code><code class="w"> </code><code class="nx">span</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">tracer</code><code class="p">.</code><code class="nx">StartSpan</code><code class="p">(</code><code class="s">"doOperation"</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO6-2" id="co_efficiency_observability_CO6-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
        </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">doOperationWithCtx</code><code class="p">(</code><code class="nx">ctx</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">span</code><code class="p">.</code><code class="nx">End</code><code class="p">(</code><code class="nx">err</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO6-3" id="co_efficiency_observability_CO6-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
&#13;
        </code><code class="c1">// ...</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">doOperationWithCtx</code><code class="p">(</code><code class="nx">ctx</code><code class="w"> </code><code class="nx">context</code><code class="p">.</code><code class="nx">Context</code><code class="p">)</code><code class="w"> </code><code class="kt">error</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">_</code><code class="p">,</code><code class="w"> </code><code class="nx">span</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">tracing</code><code class="p">.</code><code class="nx">StartSpan</code><code class="p">(</code><code class="nx">ctx</code><code class="p">,</code><code class="w"> </code><code class="s">"first operation"</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO6-4" id="co_efficiency_observability_CO6-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
    </code><code class="c1">// ...</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">span</code><code class="p">.</code><code class="nx">End</code><code class="p">(</code><code class="kc">nil</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="c1">// ...</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_efficiency_observability_CO6-1" id="callout_efficiency_observability_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>As with everything, we have to initialize our library. In our example, usually, it means creating an instance of <code>Tracer</code> that is capable of sending the spans that will form traces. We push spans to some collector and eventually to the tracing backend. This is why we have to specify some address to send to. In this example, you could specify a gRPC <code>host:port</code> address of the collector (e.g., <a href="https://oreil.ly/z0Pjt">OpenTelemetry Collector</a>) endpoint that supports the <a href="https://oreil.ly/4IaBd">gRPC OTLP trace protocol</a>.</p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO6-2" id="callout_efficiency_observability_CO6-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>With the tracer, we can create an initial root <code>span</code>. The root means the span that spans the whole transaction. A <code>traceID</code> is created during creation, identifying all spans in the trace. Span represents individual work done. For example, we can add a different name or even baggage items like logs or events. We also get a <code>context.Context</code> instance as part of creation. This Go native context interface can be used to create subspans if our <code>doOperation</code> function will do any subwork pieces worth instrumenting.</p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO6-3" id="callout_efficiency_observability_CO6-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>In the manual instrumentation, we have to tell the tracing provider when the work was done and with what result. In the <code>tracing-go</code> library, we can use <code>end.Stop(&lt;error or nil&gt;)</code> for that. Once you stop the span, it will record the span’s latency from its start, the potential error, and mark itself as ready to be sent asynchronously by <code>Tracer</code>. Tracer exporter implementations usually won’t send spans straightaway but buffer them for batch pushes. <code>Tracer</code> will also check if a trace containing some spans can be sent to the endpoint based on the chosen sampling strategy (more on that later).</p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO6-4" id="callout_efficiency_observability_CO6-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Once you have context with the injected span creator, we can add subspans to it. It’s useful when you want to debug different parts and sequences involved in doing one piece of work.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>One of the most valuable parts of tracing is context propagation. This is what separates distributed tracing from nondistributed signals. I did not reflect this in our examples, but imagine if our operation makes a network call to other microservices. Distributed tracing allows passing various tracing information like <code>traceID</code>, or sampling via a propagation API (e.g., certain encoding using HTTP headers). See a <a href="https://oreil.ly/Qz6lF">related blog post</a> about context propagation. For that to work in Go, you have to add a special middleware or HTTP client with propagation support, e.g., <a href="https://oreil.ly/Rvq6i">OpenTelemetry HTTP transport</a>.</p>&#13;
&#13;
<p>Because of the complex structure, raw traces and spans are not readable by humans. This is why many projects and vendors help users by providing solutions to use tracing effectively. Open source solutions like <a href="https://oreil.ly/CQ1Aq">Grafana Tempo with Grafana UI</a> and <a href="https://oreil.ly/enkG9">Jaeger</a> exist, which offer nice user interfaces and trace collection so you can observe your traces. Let’s look at how our spans from <a data-type="xref" href="#code-latency-trace">Example 6-6</a> look in the latter project. <a data-type="xref" href="#img-obs-tracing">Figure 6-2</a> shows a multitrace search view, and <a data-type="xref" href="#img-obs-spans">Figure 6-3</a> shows what our individual <code>doOperation</code> trace looks like.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-tracing">&#13;
<img alt="efgo 0602" src="assets/efgo_0602.png"/>&#13;
<h6><span class="label">Figure 6-2. </span>View of one hundred operations presented as one hundred traces with their latency results</h6>&#13;
</div></figure>&#13;
&#13;
<figure><div class="figure" id="img-obs-spans">&#13;
<img alt="efgo 0603" src="assets/efgo_0603.png"/>&#13;
<h6><span class="label">Figure 6-3. </span>Click one trace to inspect all of its spans and associated data</h6>&#13;
</div></figure>&#13;
&#13;
<p class="less_space pagebreak-before">Tools and user interfaces can vary, but generally they follow the same semantics I explain in this section. The view in <a data-type="xref" href="#img-obs-tracing">Figure 6-2</a> allows us to search through traces based on their timestamp, durations, service involved, etc. The current search matches our one hundred operations, which are then listed on the screen. A convenient, interactive graph of its latencies is placed, so we can navigate to the operation we want. Once clicked, the view in <a data-type="xref" href="#img-obs-spans">Figure 6-3</a> is presented. In this view, we can see a distribution of spans for this operation. If the operation spans multiple processes and we used network context propagation, all linked spans will be listed here. For &#13;
<span class="keep-together">example,</span> from <a data-type="xref" href="#img-obs-spans">Figure 6-3</a> we can immediately tell that the first operation was responsible for most of the latency, and the last operation introduced the error.</p>&#13;
&#13;
<p>All the benefits of tracing make it an excellent tool for learning the system interactions, debugging, or finding fundamental efficiency bottlenecks. It can also be used for ad hoc verification of system latency measurements (e.g., in our TFBO flow to assess latency). <a data-primary="tracing" data-secondary="downsides of" data-type="indexterm" id="ix_ch06-asciidoc8"/>But unfortunately, there are a few downsides of tracing that you have to be aware of when planning to use it in practice for efficiency or other needs:</p>&#13;
<dl>&#13;
<dt>Readability and maintainability</dt>&#13;
<dd>&#13;
<p>The advantage of tracing is that you can put a huge amount of useful context into your code. In extreme cases, you could potentially be able to rewrite the whole program or even system just by looking at all traces and their emitted spans. But there is a catch. All this manual instrumentation requires code lines. More code lines connected to our existing code increases the complexity of our code, which in turn decreases readability. We also need to ensure that our instrumentation stays updated with ever-changing code.</p>&#13;
&#13;
<p>In practice, the tracing industry tends to prefer autoinstrumentation, which in theory can add, maintain, and hide such instrumentation automatically. Proxies like Envoy (especially with service mesh technologies) are great examples of successful (yet simpler) autoinstrumentation tools for tracing that record the inter-process HTTP calls. But unfortunately, more involved auto-instrumentation is not so easy. The main problem is that the automation has to hook on to some generic path like common database or library operations,  HTTP requests, or syscalls (e.g., through eBPF probes in Linux). Moreover, it is often hard for those tools to understand what more you would like to capture in your application (e.g., the ID of the client in a specific code variable). On top of that, tools like eBPF are pretty unstable and dependent on the kernel version.</p>&#13;
<div data-type="tip"><h1>Hiding Instrumentation Under Abstractions</h1>&#13;
<p><a data-primary="instrumentation" data-secondary="hiding under abstractions" data-type="indexterm" id="idm45606831693488"/>There is a middle ground between manual and fully autonomous instrumentation. We can manually instrument only a few common Go functions and libraries, so all code that uses them will be traced consistently implicitly (automatically!).</p>&#13;
&#13;
<p>For example, we could add a trace for every HTTP or gRPC request to our process. There are already <a href="https://oreil.ly/wZ559">HTTP middlewares</a> and <a href="https://oreil.ly/7gXVF">gRPC interceptors</a> for that purpose.</p>&#13;
</div>&#13;
</dd>&#13;
<dt>Cost and reliability</dt>&#13;
<dd>&#13;
<p>Traces by design fall into the raw event category of observability. This means that tracing is typically more expensive than pre-aggregated equivalents. The reason is the sheer amount of data we send using tracing. Even if we are very moderate with this instrumentation for a single operation, we ideally have dozens of tracing spans. These days, systems have to sustain many QPS (queries per second). In our example, even for 100 QPS, we would generate over 1,000 spans. Each span must be delivered to some backend to be used effectively, with replication on both the ingestion and storage sides. Then you need a lot of computation power to analyze this data to find, for example, average latency across traces or spans. This can easily surpass your price for running the systems without &#13;
<span class="keep-together">observability!</span></p>&#13;
&#13;
<p>The industry is aware of this, and this is why we have tracing sampling, so some decision-making configuration or code decides what data to pass forward and what to ignore. For example, you might want to only collect traces for failed operations or operations that took more than 120 seconds.</p>&#13;
&#13;
<p>Unfortunately, sampling comes with its downsides. For example, it’s challenging to perform tail sampling.<sup><a data-type="noteref" href="ch06.html#idm45606831687072" id="idm45606831687072-marker">9</a></sup> Last but not least, sampling makes us miss some data (similar to profiling). In our latency example, this might mean that the latency we measure represents only part of all operations that happened. Sometimes it might be enough, but it’s easy to <a href="https://oreil.ly/R4gtX">get wrong conclusions with sampling</a>, which might lead to wrong optimization decisions.</p>&#13;
</dd>&#13;
<dt>Short duration</dt>&#13;
<dd>&#13;
<p>We will discuss this in detail in <a data-type="xref" href="#ch-obs-latency">“Latency”</a>, but tracing won’t tell us much when we try to improve very fast functions that last only a few milliseconds or less. Similar to the <code>time</code> package, the span itself introduces some latency. On top of that, adding span for many small operations can add a huge cost to the overall ingestion, storage, and querying of traces.</p>&#13;
&#13;
<p>This is especially visible in streamed algorithms like chunked encodings, compressions, or iterators. If we perform partial operations, we are still often interested in the latency of the sum of all iterations for certain logic. We can’t use tracing for that, as we would need to create tiny spans for every iteration. For those algorithms, <a data-type="xref" href="ch09.html#ch-obs-profiling">“Profiling in Go”</a> yields the best observability.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Despite some downsides, tracing becomes very powerful and even replaces the logging signal in many cases. Vendors and projects add more features, for example, <a href="https://oreil.ly/SSLye">Tempo project’s metric generator</a> that allows recording metrics from traces (e.g., average or tail latency for our efficiency needs). Undoubtedly, tracing would not grow so quickly without the push from the <a href="https://oreil.ly/sPiw9">OpenTelemetry</a> community. Amazing things will come from this community if you are into tracing.</p>&#13;
&#13;
<p>The downsides of one framework are often strengths of other frameworks that choose different trade-offs. For example, many tracing problems come from the fact that it naturally represents raw events happening in the system (that might trigger other events). Let’s now discuss a signal on the opposite spectrum—designed to capture aggregations changing over time<a data-startref="ix_ch06-asciidoc8" data-type="indexterm" id="idm45606831678416"/>.<a data-startref="ix_ch06-asciidoc7" data-type="indexterm" id="idm45606831677584"/><a data-startref="ix_ch06-asciidoc6" data-type="indexterm" id="idm45606831676880"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Metrics" data-type="sect2"><div class="sect2" id="ch-obs-metrics">&#13;
<h2>Metrics</h2>&#13;
&#13;
<p><a data-primary="metrics" data-type="indexterm" id="ix_ch06-asciidoc9"/><a data-primary="observability" data-secondary="metrics" data-type="indexterm" id="ix_ch06-asciidoc10"/>Metrics <a data-primary="metrics" data-secondary="definition" data-type="indexterm" id="idm45606831671520"/>is the observability signal that was designed to observe aggregated information. Such aggregation-oriented metric instrumentations might be the most pragmatic way of solving our efficiency goals. Metrics are also what I used the most in my day-to-day job as a developer and SRE to observe and debug production workloads. In addition, metrics are <a href="https://oreil.ly/x6rNZ">the main signal used for monitoring at Google</a>.</p>&#13;
&#13;
<p><a data-primary="Prometheus" data-secondary="pre-aggregated instrumentation" data-type="indexterm" id="ix_ch06-asciidoc11"/><a data-type="xref" href="#code-latency-metric">Example 6-7</a> shows pre-aggregated instrumentation that can be used to measure latency. This example uses <a href="https://oreil.ly/1r2zw">Prometheus <code>client_golang</code></a>.<sup><a data-type="noteref" href="ch06.html#idm45606831665920" id="idm45606831665920-marker">10</a></sup></p>&#13;
<div data-type="example" id="code-latency-metric">&#13;
<h5><span class="label">Example 6-7. </span>Measuring <code>doOperation</code> latency using the histogram metric with Prometheus <code>client_golang</code></h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kn">import</code><code class="w"> </code><code class="p">(</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"fmt"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"time"</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/prometheus/client_golang/prometheus"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/prometheus/client_golang/prometheus/promauto"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/prometheus/client_golang/prometheus/promhttp"</code><code class="w">&#13;
</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">ExampleLatencyMetric</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">reg</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">prometheus</code><code class="p">.</code><code class="nx">NewRegistry</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO7-1" id="co_efficiency_observability_CO7-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
    </code><code class="nx">latencySeconds</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">promauto</code><code class="p">.</code><code class="nx">With</code><code class="p">(</code><code class="nx">reg</code><code class="p">)</code><code class="p">.</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="nx">NewHistogramVec</code><code class="p">(</code><code class="nx">prometheus</code><code class="p">.</code><code class="nx">HistogramOpts</code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO7-2" id="co_efficiency_observability_CO7-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
        </code><code class="nx">Name</code><code class="p">:</code><code class="w">    </code><code class="s">"operation_duration_seconds"</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">Help</code><code class="p">:</code><code class="w">    </code><code class="s">"Tracks the latency of operations in seconds."</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">Buckets</code><code class="p">:</code><code class="w"> </code><code class="p">[</code><code class="p">]</code><code class="kt">float64</code><code class="p">{</code><code class="mf">0.001</code><code class="p">,</code><code class="w"> </code><code class="mf">0.01</code><code class="p">,</code><code class="w"> </code><code class="mf">0.1</code><code class="p">,</code><code class="w"> </code><code class="mi">1</code><code class="p">,</code><code class="w"> </code><code class="mi">10</code><code class="p">,</code><code class="w"> </code><code class="mi">100</code><code class="p">}</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="p">,</code><code class="w"> </code><code class="p">[</code><code class="p">]</code><code class="kt">string</code><code class="p">{</code><code class="s">"error_type"</code><code class="p">}</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO7-3" id="co_efficiency_observability_CO7-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
&#13;
    </code><code class="k">go</code><code class="w"> </code><code class="kd">func</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">xTimes</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">             </code><code class="nx">now</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">time</code><code class="p">.</code><code class="nx">Now</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">             </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">doOperation</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">             </code><code class="nx">elapsed</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">time</code><code class="p">.</code><code class="nx">Since</code><code class="p">(</code><code class="nx">now</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">             </code><code class="nx">latencySeconds</code><code class="p">.</code><code class="nx">WithLabelValues</code><code class="p">(</code><code class="nx">errorType</code><code class="p">(</code><code class="nx">err</code><code class="p">)</code><code class="p">)</code><code class="p">.</code><code class="w">&#13;
</code><code class="w">                 </code><code class="nx">Observe</code><code class="p">(</code><code class="nx">elapsed</code><code class="p">.</code><code class="nx">Seconds</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO7-4" id="co_efficiency_observability_CO7-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
&#13;
             </code><code class="c1">// ...</code><code class="w">&#13;
</code><code class="w">        </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">http</code><code class="p">.</code><code class="nx">ListenAndServe</code><code class="p">(</code><code class="w">&#13;
</code><code class="w">        </code><code class="s">":8080"</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">promhttp</code><code class="p">.</code><code class="nx">HandlerFor</code><code class="p">(</code><code class="nx">reg</code><code class="p">,</code><code class="w"> </code><code class="nx">promhttp</code><code class="p">.</code><code class="nx">HandlerOpts</code><code class="p">{</code><code class="p">}</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO7-5" id="co_efficiency_observability_CO7-5"><img alt="5" src="assets/5.png"/></a><code class="w">&#13;
    </code><code class="c1">// ...</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_efficiency_observability_CO7-1" id="callout_efficiency_observability_CO7-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Using the Prometheus library always starts with creating a new metric registry.<sup><a data-type="noteref" href="ch06.html#idm45606831400064" id="idm45606831400064-marker">11</a></sup></p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO7-2" id="callout_efficiency_observability_CO7-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The next step is to populate the registry with the metric definitions you want. Prometheus allows a few types of metrics, yet the typical latency measurements for efficiency are best done as histograms. So on top of type, help and histogram buckets are required. We will talk more about buckets and the choice of histograms later.</p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO7-3" id="callout_efficiency_observability_CO7-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>As the last parameter, we define the dynamic dimension of this metric. Here I propose to measure latency for different types of errors (or no error). This is useful as, very often, failures have other timing characteristics.</p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO7-4" id="callout_efficiency_observability_CO7-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>We observe the exact latency with a floating number of seconds. We run all operations in a simplified goroutine, so we can expose metrics while the functionality is performing. The <code>Observe</code> method will add such latency into the histogram of buckets. Notice that we observe this latency for certain errors. We also don’t take an arbitrary error string—we sanitize it to a type using some custom <code>errorType</code> function. This is important because the controlled number of values in the dimension keeps our metric valuable and cheap.</p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO7-5" id="callout_efficiency_observability_CO7-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>The default way to consume those metrics is by allowing other processes (e.g., <a href="https://oreil.ly/2Sa3P">Prometheus server</a>) to pull the current state of the metrics. For example, in this simplified<sup><a data-type="noteref" href="ch06.html#idm45606831336912" id="idm45606831336912-marker">12</a></sup> code we serve those metrics from our registry through an HTTP endpoint on the <code>8080</code> port.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>The Prometheus data model supports four metric types, which are well described in the <a href="https://oreil.ly/mamdO">Prometheus documentation</a>: counters, gauges, histograms, and summaries. There is a reason why I chose a more complex histogram for observing latency instead of a counter or a gauge metric. I explain why in <a data-type="xref" href="#ch-obs-latency">“Latency”</a>. For now, it’s enough to say that histograms allow us to capture distributions of the latencies, which is typically what we need when observing production systems for efficiency and reliability. Such metrics, defined and instrumented in <a data-type="xref" href="#code-latency-metric">Example 6-7</a>, will be represented on an HTTP endpoint, as shown in <a data-type="xref" href="#code-latency-metric-om">Example 6-8</a>.</p>&#13;
<div data-type="example" id="code-latency-metric-om">&#13;
<h5><span class="label">Example 6-8. </span>Sample of the metric output from <a data-type="xref" href="#code-latency-metric">Example 6-7</a> when consumed from the <a href="https://oreil.ly/aZ6GT">OpenMetrics compatible HTTP endpoint</a></h5>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting"><code># HELP operation_duration_seconds Tracks the latency of operations in seconds.&#13;
# TYPE operation_duration_seconds histogram&#13;
operation_duration_seconds_bucket{error_type="",le="0.001"} 0 </code><a class="co" href="#callout_efficiency_observability_CO8-1" id="co_efficiency_observability_CO8-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
operation_duration_seconds_bucket{error_type="",le="0.01"} 0&#13;
operation_duration_seconds_bucket{error_type="",le="0.1"} 1&#13;
operation_duration_seconds_bucket{error_type="",le="1"} 2&#13;
operation_duration_seconds_bucket{error_type="",le="10"} 2&#13;
operation_duration_seconds_bucket{error_type="",le="100"} 2&#13;
operation_duration_seconds_bucket{error_type="",le="+Inf"} 2&#13;
operation_duration_seconds_sum{error_type=""} 0.278675917 </code><a class="co" href="#callout_efficiency_observability_CO8-2" id="co_efficiency_observability_CO8-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
operation_duration_seconds_count{error_type=""} 2</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_efficiency_observability_CO8-1" id="callout_efficiency_observability_CO8-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Each bucket represents a number (counters) of operations that had latency less than or equal to the value specified in <code>le</code>. For example, we can immediately see that we saw two successful operations from the process start. The first was faster than 0.1 seconds; and the second was faster than 1 second, but slower than 0.1 seconds.</p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO8-2" id="callout_efficiency_observability_CO8-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Every histogram also captures a number of observed operations and summarized value (sum of observed latencies, in this case).</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>As mentioned in <a data-type="xref" href="#ch-obs-observability">“Observability”</a>, every signal can be pulled or pushed. However, the Prometheus ecosystem defaults to the pull method for metrics. Not &#13;
<span class="keep-together">the naive</span> pull, though. In the Prometheus ecosystem, we don’t pull a backlog of events or samples like we would when pulling (tailing) traces of logs from, for &#13;
<span class="keep-together">example,</span> a file. Instead, applications serve HTTP payload in the OpenMetrics format (like in <a data-type="xref" href="#code-latency-metric-om">Example 6-8</a>), which is then periodically collected (scraped) by Prometheus servers or Prometheus compatible systems (e.g., Grafana Agent or OpenTelemetry collector). With the Prometheus data model, we scrape the latest information about the process.</p>&#13;
&#13;
<p>To use Prometheus with our Go program instrumented in <a data-type="xref" href="#code-latency-metric">Example 6-7</a>, we have to start the Prometheus server and configure the scrape job that targets the Go process server. For example, assuming we have the code in  <a data-type="xref" href="#code-latency-metric">Example 6-7</a> running, we could use the set of commands shown in <a data-type="xref" href="#code-latency-metric-scrape">Example 6-9</a> to start metric collection.</p>&#13;
<div data-type="example" id="code-latency-metric-scrape">&#13;
<h5><span class="label">Example 6-9. </span>The simplest set of commands to run Prometheus from the terminal to start collecting metrics from <a data-type="xref" href="#code-latency-metric">Example 6-7</a></h5>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code>cat</code><code> </code><code class="s">&lt;&lt; EOF &gt; ./prom.yaml&#13;
scrape_configs:&#13;
- job_name: "local"&#13;
  scrape_interval: "15s" </code><a class="co" href="#callout_efficiency_observability_CO9-1" id="co_efficiency_observability_CO9-1"><img alt="1" src="assets/1.png"/></a><code class="s">&#13;
  static_configs:&#13;
  - targets: [ "localhost:8080" ] </code><a class="co" href="#callout_efficiency_observability_CO9-2" id="co_efficiency_observability_CO9-2"><img alt="2" src="assets/2.png"/></a><code class="s">&#13;
EOF</code><code>&#13;
</code><code>prometheus</code><code> </code><code>--config.file</code><code class="o">=</code><code>./prom.yaml</code><code> </code><a class="co" href="#callout_efficiency_observability_CO9-3" id="co_efficiency_observability_CO9-3"><img alt="3" src="assets/3.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_efficiency_observability_CO9-1" id="callout_efficiency_observability_CO9-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>For my demo purposes, I can limit the <a href="https://oreil.ly/4cPSa">Prometheus configuration</a> to a single scrape job. One of the first decisions is to specify the scrape interval. Typically, it’s around 15–30 seconds for continuous, efficient metric &#13;
<span class="keep-together">collection.</span></p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO9-2" id="callout_efficiency_observability_CO9-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>I also provide a target that points to our tiny instrumented Go program in <a data-type="xref" href="#code-latency-metric">Example 6-7</a>.</p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO9-3" id="callout_efficiency_observability_CO9-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Prometheus is just a single binary written in Go. We install it in <a href="https://oreil.ly/9CxxD">many ways</a>. In the simplest configuration, we can point it to a &#13;
<span class="keep-together">created</span> configuration. When started, the UI will be available on the &#13;
<span class="keep-together"><code>localhost:9090</code>.</span></p></dd>&#13;
</dl></div>&#13;
&#13;
<p>With the preceding setup, we can start analyzing the data using Prometheus APIs. The simplest way is to use the Prometheus query language (PromQL) documented <a href="https://oreil.ly/nY6Yi">here</a> and <a href="https://oreil.ly/jH3nd">here</a>. With Prometheus server started as in <a data-type="xref" href="#code-latency-metric-scrape">Example 6-9</a>, we can use the Prometheus UI and query the data we &#13;
<span class="keep-together">collected.</span></p>&#13;
&#13;
<p>For example, <a data-type="xref" href="#img-obs-metric-buckets">Figure 6-4</a> shows the result of the simple query fetching the latest latency histogram numbers over time (from the moment of the process start) for our <code>operation_duration_seconds</code> metric name that represents successful operations. This generally matches the format we see in <a data-type="xref" href="#code-latency-metric-om">Example 6-8</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-metric-buckets">&#13;
<img alt="efgo 0604" src="assets/efgo_0604.png"/>&#13;
<h6><span class="label">Figure 6-4. </span>PromQL query results for simple query for all <code>operation_duration_​sec⁠onds_bucket</code> metrics graphed in the Prometheus UI</h6>&#13;
</div></figure>&#13;
&#13;
<p class="less_space pagebreak-before">To obtain the average latency of a single operation, we can use certain mathematical operations to divide the rates of <code>operation_duration_seconds_sum</code> by <code>oper⁠ation_duration_seconds_count</code>. We use the <code>rate</code> function to ensure accurate results across many processes and their restart. <code>rate</code> transforms Prometheus counters into a rate per second.<sup><a data-type="noteref" href="ch06.html#idm45606831225328" id="idm45606831225328-marker">13</a></sup> Then we can use <code>/</code> to divide the rates of those metrics. The result of such an average query is presented in <a data-type="xref" href="#img-obs-metric-avg">Figure 6-5</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-metric-avg">&#13;
<img alt="efgo 0605" src="assets/efgo_0605.png"/>&#13;
<h6><span class="label">Figure 6-5. </span>PromQL query results representing average latency captured by the <a data-type="xref" href="#code-latency-metric">Example 6-7</a> instrumentation graphed in the Prometheus UI</h6>&#13;
</div></figure>&#13;
&#13;
<p>With another query, we can check total operations or, even better, check the rate &#13;
<span class="keep-together">per minute</span> of those using the <code>increase</code> function on our <code>operation_duration_​sec⁠onds_count</code> counter, as presented in <a data-type="xref" href="#img-obs-metric-incr">Figure 6-6</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-metric-incr">&#13;
<img alt="efgo 0606" src="assets/efgo_0606.png"/>&#13;
<h6><span class="label">Figure 6-6. </span>PromQL query results representing a rate of operations per minute in our system graphed in the Prometheus UI</h6>&#13;
</div></figure>&#13;
&#13;
<p>There are many other functions, aggregations, and ways of using metric data in the Prometheus ecosystem. We will unpack some of it in later sections.</p>&#13;
&#13;
<p>The amazing part about Prometheus with such a specific scrape technique is that pulling metrics allows our Go client to be ultrathin and efficient. As a result, the Go process does not need to:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Buffer data samples, spans, or logs in memory or on disk</p>&#13;
</li>&#13;
<li>&#13;
<p>Maintain information (and automatically update it!) on where to send potential data</p>&#13;
</li>&#13;
<li>&#13;
<p>Implement complex buffering and persisting logic if the metric backend is down temporarily</p>&#13;
</li>&#13;
<li>&#13;
<p>Ensure a consistent sample push interval</p>&#13;
</li>&#13;
<li>&#13;
<p>Know about any authentication, authorization, or TLS for metric payload</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>On top of that, the observability experience is better when you pull the data in such a way that:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Metric users can easily control the scrape interval, targets, metadata, and recordings from a central place. This makes the metric usage simpler, more pragmatic, and generally cheaper.</p>&#13;
</li>&#13;
<li>&#13;
<p>It is easier to predict the load of such a system, which makes it easier to scale it and react to the situations that require scaling the collection pipeline.</p>&#13;
</li>&#13;
<li>&#13;
<p>Last but not least, pulling metrics allows you to reliably tell your application’s health (if we can’t scrape metrics from it, it is most likely unhealthy or down). We also typically know what sample is the last one for a metric (staleness).<sup><a data-type="noteref" href="ch06.html#idm45606831205328" id="idm45606831205328-marker">14</a></sup></p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>As with everything, there are some trade-offs. Each pulled, tailed, or scraped signal has its downsides. Typical problems of an observability pull-based system include:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>It is generally harder to pull data from short-lived processes (e.g., CLI and batch jobs).<sup><a data-type="noteref" href="ch06.html#idm45606831202864" id="idm45606831202864-marker">15</a></sup></p>&#13;
</li>&#13;
<li>&#13;
<p>Not every system architecture allows ingress traffic.</p>&#13;
</li>&#13;
<li>&#13;
<p>It is generally harder to ensure that all the pieces of information will land safely in a remote place (e.g., this pulling is not suitable for auditing).</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The Prometheus metrics are designed to mitigate downsides and leverage the strength of the pull model. Most of the metrics we use are counters, which means they only increase. This allows Prometheus to skip a few scrapes from the process but still, in the end, have a perfectly accurate number for each metric within larger time windows, like minutes.<a data-startref="ix_ch06-asciidoc11" data-type="indexterm" id="idm45606831198656"/></p>&#13;
&#13;
<p>As mentioned before, in the end, metrics (as numeric values) are what we need when it comes to assessing efficiency. It’s all about comparing and analyzing numbers. This is why a metric observability signal is a great way to gather required information pragmatically. We will use this signal extensively for <a data-type="xref" href="ch08.html#ch-obs-macro">“Macrobenchmarks”</a> and <a data-type="xref" href="ch09.html#ch-obs-cause">“Root Cause Analysis, but for Efficiency”</a>. It’s simple, pragmatic, the ecosystem is huge (you can find metric exporters for almost all kinds of software and hardware), it’s generally cheap, and it works great with both human users and automation (e.g., alerting).</p>&#13;
&#13;
<p class="less_space pagebreak-before">Metric observability signals, especially with the Prometheus data model, fit into aggregated information instrumentation. We discussed the benefits, but some limits and downsides are important to understand. All downsides come from the fact that we generally cannot narrow pre-aggregated data down to a state before aggregation, for example, a single event. We might know with metrics how many requests failed, but we don’t know the exact stack trace, error message, and so on for a singular error that happened. The most granular information we typically have is a type of error (e.g., status code). This makes the surface of possible questions we can ask a metric system smaller than if we would capture all raw events. Another essential characteristic that might be considered a downside is the cardinality of the metrics and the fact that it has to be kept low.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>High Metric Cardinality</h1>&#13;
<p><a data-primary="cardinality" data-type="indexterm" id="idm45606831193792"/><a data-primary="metrics" data-secondary="cardinality" data-type="indexterm" id="idm45606831193088"/>Cardinality means the uniqueness of our metric. For example, imagine in <a data-type="xref" href="#code-latency-metric">Example 6-7</a> we would inject a unique error string instead of the <code>error_type</code> label. Every new label value creates a new, possibly short-lived unique metric. A metric with just a single or a few samples represents more of a raw event, not aggregation over time. Unfortunately, if users try to push event-like information to a system designed for metrics (like Prometheus), it tends to be expensive and slow.</p>&#13;
&#13;
<p>It is very tempting to push more cardinal data to a system designed for metrics. This is because it’s only natural to want to learn more from such cheap and reliable signal-like metrics. Avoid that and keep your cardinality low with metric budgets, recording rules, and allow-list relabeling. Switch to event-based systems like logging and tracing if you wish to capture unique information like exact error messages or the latency for a single, specific operation in the system!</p>&#13;
</div>&#13;
&#13;
<p>Whether gathered from logs, traces, profiles, or metric signals, we already touched on some metrics in previous chapters—for example, CPU core used per second, memory bytes allocated on the heap, or residential memory bytes used per operation. So let’s go through some of those in detail and talk about their semantics, how we should interpret them, potential granularity, and example code that illustrates them using signals you have just learned.</p>&#13;
<div data-type="tip"><h1>There Is No Observability Silver Bullet!</h1>&#13;
<p>Metrics are powerful. Yet as you learned in this chapter, logging and traces also give enormous opportunities to improve the efficiency observability experience with dedicated tools that allow us to derive metrics from them. In this book, you will see me using all of those tools (together with profiling, which we haven’t covered yet) to improve the efficiency of Go programs.<a data-startref="ix_ch06-asciidoc10" data-type="indexterm" id="idm45606831187232"/><a data-startref="ix_ch06-asciidoc9" data-type="indexterm" id="idm45606831186528"/></p>&#13;
&#13;
<p>The pragmatic system captures enough of each of those observability signals that fit your use cases. It’s unlikely to build metric-only, trace-only, or profiling-only systems!<a data-startref="ix_ch06-asciidoc3" data-type="indexterm" id="idm45606831185472"/><a data-startref="ix_ch06-asciidoc2" data-type="indexterm" id="idm45606831184768"/></p>&#13;
</div>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Efficiency Metrics Semantics" data-type="sect1"><div class="sect1" id="ch-obs-semantics">&#13;
<h1>Efficiency Metrics Semantics</h1>&#13;
&#13;
<p><a data-primary="efficiency metrics semantics" data-type="indexterm" id="ix_ch06-asciidoc12"/><a data-primary="metrics" data-secondary="efficiency metrics semantics" data-type="indexterm" id="ix_ch06-asciidoc13"/><a data-primary="observability" data-secondary="efficiency metrics semantics" data-type="indexterm" id="ix_ch06-asciidoc14"/>Observability feels like a vast and deep topic that takes years to grasp and set up. The industry constantly evolves, and creating new solutions does not help. However, it will be easier to understand once we start using observability for a specific goal like the efficiency effort. Let’s talk about exactly which observability bits are essential to start measuring latency and consumption of the resources we care about, e.g., CPU and memory.</p>&#13;
<div data-type="note" epub:type="note"><h1>Metrics As Numeric Value Versus Metric Observability Signal</h1>&#13;
<p><a data-primary="metrics" data-secondary="numeric value versus metric observability signal" data-type="indexterm" id="idm45606831177504"/>In <a data-type="xref" href="#ch-obs-metrics">“Metrics”</a>, we discussed the metric observability signal. Here we discuss specific metric semantics that are useful to capture for efficiency efforts. To clarify, we can capture those specific metrics in various ways. We can use metric observability signals, but we can also derive them from other signals, like logs, traces, and profiling!</p>&#13;
</div>&#13;
&#13;
<p>Two things can define every metric:</p>&#13;
<dl>&#13;
<dt>Semantics</dt>&#13;
<dd>&#13;
<p>What’s the meaning of that number? What do we measure? With what unit? How do we call it?</p>&#13;
</dd>&#13;
<dt>Granularity</dt>&#13;
<dd>&#13;
<p><a data-primary="granularity" data-type="indexterm" id="idm45606831171808"/>How detailed is this information? For example, is it per a unique operation? Is it per a result type of this operation (success versus error)? Per goroutine? Per &#13;
<span class="keep-together">process?</span></p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Metric semantics and granularity both heavily depend on the instrumentation. This section will focus on defining the semantics, granularity, and example instrumentation for the typical metrics we can use to track resource consumption and latency of our software. It is essential to understand the specific measurements we will operate with to work effectively with the benchmark and profiling tools we will learn &#13;
<span class="keep-together">in <a data-type="xref" href="ch07.html#ch-obs-benchmarking">“Benchmarking Levels”</a></span> and <a data-type="xref" href="ch09.html#ch-obs-profiling">“Profiling in Go”</a>. While &#13;
<span class="keep-together">iterating over</span> those semantics, we will uncover common best practices and pitfalls we have to be aware of. Let’s go!</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Latency" data-type="sect2"><div class="sect2" id="ch-obs-latency">&#13;
<h2>Latency</h2>&#13;
&#13;
<p><a data-primary="efficiency metrics semantics" data-secondary="latency" data-type="indexterm" id="ix_ch06-asciidoc15"/><a data-primary="latency" data-secondary="efficiency metrics semantics" data-type="indexterm" id="ix_ch06-asciidoc16"/>If we want to improve how fast our program performs certain operations, we need to measure the latency. Latency means the duration of the operation from the start to either success or failure. Thus, the semantics we need feel pretty simple at first &#13;
<span class="keep-together">glance—we</span> generally want the “amount of time” required to complete our software operation. Our metric will usually have a name containing the words <em>latency</em>, <em>duration</em>, or <em>elapsed</em> with the desired unit. But the devil is in the details, and as you will learn in this section, measuring latency is prone to mistakes.</p>&#13;
&#13;
<p>The preferable unit of the typical latency measurement depends on what kind of operations we measure. If we measure very short operations like compression latency or OS context switch latencies, we must focus on granular nanoseconds. Nanoseconds are also the most granular timing we can count on in typical modern computers. This is why the Go standard library <a href="https://oreil.ly/QGCme"><code>time.Time</code></a> and <a href="https://oreil.ly/9agLb"><code>time.Duration</code></a> structures measure time in nanoseconds.</p>&#13;
&#13;
<p>Generally speaking, the typical measurements of software operations are almost always in milliseconds, seconds, minutes, or hours. This is why it’s often enough to measure latency in seconds, as a floating value, for up to nanoseconds granularity. Using seconds has another advantage: it is a base unit. Using the base unit is often what’s natural and consistent across many solutions.<sup><a data-type="noteref" href="ch06.html#idm45606831155568" id="idm45606831155568-marker">16</a></sup> Consistency is critical here. You don’t want to measure one part of the system in nanoseconds, another in seconds, and another in hours if you can avoid it. It’s easy enough to get confused by our data and have a wrong conclusion without trying to guess a correct unit or writing transformations between those.</p>&#13;
&#13;
<p>In the code examples in <a data-type="xref" href="#ch-obs-signals">“Example: Instrumenting for Latency”</a>, we already mentioned many ways we can instrument latency using various observability signals. Let’s extend <a data-type="xref" href="#code-latency-simplest">Example 6-1</a> in <a data-type="xref" href="#code-latency-simplest-ext">Example 6-10</a> to show important details that ensure latency is measured as reliably as possible.</p>&#13;
<div class="less_space pagebreak-before" data-type="example" id="code-latency-simplest-ext">&#13;
<h5><span class="label">Example 6-10. </span>Manual and simplest latency measurement of a single operation that can error out and has to prepare and tear down phases</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="nx">prepare</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">xTimes</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">start</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">time</code><code class="p">.</code><code class="nx">Now</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO10-1" id="co_efficiency_observability_CO10-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
    </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">doOperation</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">elapsed</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">time</code><code class="p">.</code><code class="nx">Since</code><code class="p">(</code><code class="nx">start</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO10-2" id="co_efficiency_observability_CO10-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
&#13;
    </code><code class="c1">// Capture 'elapsed' value using log, trace or metric...</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">if</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">!=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><code class="cm">/* Handle error... */</code><code class="w"> </code><code class="p">}</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="nx">tearDown</code><code class="p">(</code><code class="p">)</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_efficiency_observability_CO10-1" id="callout_efficiency_observability_CO10-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>We capture the <code>start</code> time as close as possible to the start of our <code>doOperation</code> invocation. This ensures nothing unexpected will get between <code>start</code> and operation start that might introduce unrelated latency, which can mislead the conclusion we might take from this metric further on. This, by design, should exclude any potential preparation or setup we have to do for an operation we measure. Let’s measure those explicitly as another operation. This is also why you should avoid putting any newline (empty line) between <code>start</code> and the invocation of the operation. As a result, the next programmer (or yourself, after some time) won’t add anything in between, forgetting about the instrumentation you added.</p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO10-2" id="callout_efficiency_observability_CO10-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Similarly, it’s important to capture the <code>finish</code> time using the <code>time.Since</code> helper as soon as we finish, so no unrelated duration is captured. For example, similar to excluding <code>prepare()</code> time, we want to exclude any potential close or <code>tearDown()</code> duration. Moreover, if you are an advanced Go programmer, your intuition is always to check errors when some functions finish. This is critical, but we should do that for instrumentation purposes after we capture the latency. Otherwise, we might increase the risk that someone will not notice our instrumentation and will add unrelated statements between what we measure and <code>time.Since</code>. On top of that, in most cases, you want to make sure you measure the latency of both successful and failed operations to understand the complete picture of what your program is doing.</p></dd>&#13;
</dl></div>&#13;
<div data-type="warning" epub:type="warning"><h1>Shorter Latencies Are Harder to Measure Reliably</h1>&#13;
<p><a data-primary="latency" data-secondary="difficulty of measuring shorter latencies" data-type="indexterm" id="idm45606831020128"/>The method for measuring operation latency shown in <a data-type="xref" href="#code-latency-simplest-ext">Example 6-10</a> won’t work well for operations that finish under, let’s say, 0.1 microseconds (100 nanoseconds). This is because the effort of taking the system clock number, allocating variables, and further computing <code>time.Now()</code> and <code>time.Since</code> functions can take its time too, which is significant for such short measurements.<sup><a data-type="noteref" href="ch06.html#idm45606831016880" id="idm45606831016880-marker">17</a></sup> Furthermore, as we will learn in <a data-type="xref" href="ch07.html#ch-obs-rel">“Reliability of Experiments”</a>, every measurement has some variance. The shorter &#13;
<span class="keep-together">latency, the</span> more impactful this noise can be.<sup><a data-type="noteref" href="ch06.html#idm45606831013664" id="idm45606831013664-marker">18</a></sup> This also applies to tracing spans measuring latency.</p>&#13;
</div>&#13;
&#13;
<p>One solution for measuring very fast functions is used by the Go benchmark as presented by <a data-type="xref" href="#code-latency-go-bench">Example 6-3</a>, where we estimate average latency per operation by doing many of them. More on that in <a data-type="xref" href="ch08.html#ch-obs-micro">“Microbenchmarks”</a>.</p>&#13;
<div data-type="note" epub:type="note"><h1>Time Is Infinite; the Software Structures Measuring that Time Are Not!</h1>&#13;
<p><a data-primary="latency" data-secondary="limitations of time/duration measurements" data-type="indexterm" id="idm45606831008448"/>When measuring latency, we have to be aware of the limitations of time or duration measurements in software. Different types can contain different ranges of numeric values, and not all of them can contain negative numbers. For example:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><code>time.Time</code> can only measure time from January 1, 1885<sup><a data-type="noteref" href="ch06.html#idm45606831005584" id="idm45606831005584-marker">19</a></sup> up until 2157.</p>&#13;
</li>&#13;
<li>&#13;
<p>The <code>time.Duration</code> type can measure time (in nanoseconds) approximately between -290 years before your “starting” point and up to 290 years after your “starting” point.</p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
&#13;
<p>If you want to measure things outside of those typical values, you need to extend those types or use your own. Last but not least, Go is prone <a href="https://oreil.ly/MeZ4b">to the leap second problem</a> and time skews of the operating systems. On some systems, the <code>time.Duration</code> (monotonic clock) will also stop if the computer goes to sleep (e.g., laptop or virtual machine suspend), which will lead to wrong measurements, so keep that in mind.</p>&#13;
&#13;
<p class="less_space pagebreak-before"><a data-primary="granularity" data-type="indexterm" id="ix_ch06-asciidoc17"/>We discussed some typical latency metric semantics. Now let’s move to the granularity question. We can decide to measure the latency of operation A or B in our process. We can measure a group of operations (e.g., transaction) or a single suboperation of it. We can gather this data across many processes or look only at one, depending on what we want to achieve.</p>&#13;
&#13;
<p>To make it even more complex, even if we choose a single operation as our granularity to measure latency, that single operation has many stages. In a single process this can be represented by stack trace, but for multiprocess systems with some network communication, we might need to establish additional boundaries.</p>&#13;
&#13;
<p>Let’s take some programs as an example, as the Caddy HTTP web server explained in the previous chapter, with a simple <a href="https://oreil.ly/SHEor">REST</a> HTTP call to retrieve an HTML as our example operation. What latencies should we measure if we install such a Go program in a cloud on production to serve our REST HTTP call to the client (e.g., someone’s browser)? The example granularities we could measure latency for are presented in <a data-type="xref" href="#img-obs-latency-stages">Figure 6-7</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-latency-stages">&#13;
<img alt="efgo 0607" src="assets/efgo_0607.png"/>&#13;
<h6><span class="label">Figure 6-7. </span>Example latency stages we can measure for in our Go web server program communicating with the user’s web browser</h6>&#13;
</div></figure>&#13;
&#13;
<p>We can outline five example stages:</p>&#13;
<dl>&#13;
<dt>Absolute (total) client-side latency</dt>&#13;
<dd>&#13;
<p>The latency measured exactly from the moment the user hits Enter in the URL input in the browser, up until the whole response is retrieved, content is loaded, and the browser renders all.</p>&#13;
</dd>&#13;
<dt>HTTP client-side latency (response time)</dt>&#13;
<dd>&#13;
<p>The latency captured from the moment the first bytes of the HTTP request on the client side are being written to a new or reused TCP connection, up until the client receives all bytes of the response. This excludes everything that happens before (e.g., DNS lookup) or after (rendering HTML and JavaScript in the browser) on the client side.</p>&#13;
</dd>&#13;
<dt>HTTP server-side latency</dt>&#13;
<dd>&#13;
<p>The latency is measured from the moment the server receives the first bytes of the HTTP request from the client, up until the server finishes writing all bytes of the HTTP response. This is typically what we are measuring if we use <a href="https://oreil.ly/Js0NO">the HTTP middlewares pattern</a> in Go.</p>&#13;
</dd>&#13;
<dt>Server-side latency (service time)</dt>&#13;
<dd>&#13;
<p>The latency of server-side computation required to answer the HTTP request, measured without HTTP request parsing and response encoding. Latency is from the moment of having the HTTP request parsed to the moment when we start encoding and sending the HTTP response.</p>&#13;
</dd>&#13;
<dt>Server-side function latency</dt>&#13;
<dd>&#13;
<p>The latency of a single server-side function computation from the moment of invocation, up until the function work is finished and return arguments are in the context of the caller function.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>These are just some of the many permutations we can use to measure latency in our Go programs or systems. Which one should we pick for our optimizations? Which matters the most? It turns out that all of them have their use case. The priority of what latency metric granularity we should use and when depends solely on our &#13;
<span class="keep-together">goals, the</span> accuracy of measurements as explained in <a data-type="xref" href="ch07.html#ch-obs-rel">“Reliability of Experiments”</a>, and the element we want to focus on as discussed in <a data-type="xref" href="ch07.html#ch-obs-benchmarking">“Benchmarking Levels”</a>. To understand the big picture and find the bottleneck, we have to &#13;
<span class="keep-together">measure a few</span> of those different granularities at once. As discussed in <a data-type="xref" href="ch09.html#ch-obs-cause">“Root Cause Analysis, but for Efficiency”</a>, tools like tracing and profiling can help with that.</p>&#13;
<div data-type="tip"><h1>Whatever Metric Granularity You Choose, Understand and Document What You Measure!</h1>&#13;
<p><a data-primary="metrics" data-secondary="documentation of" data-type="indexterm" id="idm45606830977696"/>We waste a lot of time if we take the wrong conclusions from measurements. It is easy to forget or misunderstand what parts of granularity we are measuring. For example, you thought you were measuring server-side latency, but slow client software is introducing latency you felt you didn’t include in your metric. As a result, you might be trying to find a bottleneck on the server side, whereas a potential problem might be in a different process.<sup><a data-type="noteref" href="ch06.html#idm45606830976592" id="idm45606830976592-marker">20</a></sup><a data-startref="ix_ch06-asciidoc17" data-type="indexterm" id="idm45606830975376"/> Understand, document, and be explicit with your instrumentation to avoid those mistakes</p>&#13;
</div>&#13;
&#13;
<p>In <a data-type="xref" href="#ch-obs-signals">“Example: Instrumenting for Latency”</a>, we discussed how we could gather latencies. We mentioned that generally, we use two main measuring methods for efficiency needs in the Go ecosystem. Those two ways are typically the most reliable and cheapest (useful when performing load tests and benchmarks):</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Basic logging using <a data-type="xref" href="ch08.html#ch-obs-micro">“Microbenchmarks”</a> for isolated functionality, &#13;
<span class="keep-together">single</span> process measurements</p>&#13;
</li>&#13;
<li>&#13;
<p>Metrics such as <a data-type="xref" href="#code-latency-metric">Example 6-7</a> for macro measurements that involve larger systems with multiple processes</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Especially in the second case, as mentioned previously, we have to measure latency many times for a single operation to get reliable efficiency conclusions. We don’t have access to raw latency numbers for each operation with metrics—we have to choose some aggregation. In <a data-type="xref" href="#code-latency-simplest-aggr">Example 6-2</a>, we proposed a simple average aggregation mechanism inside instrumentation. With metric instrumentation, this would be trivial to achieve. It’s as easy as creating two counters: one for the <code>sum</code> of latencies and one for the <code>count</code> of operations. We can evaluate collected data with those two metrics into a mean (arithmetic average).</p>&#13;
&#13;
<p><a data-primary="averages, percentiles versus" data-type="indexterm" id="ix_ch06-asciidoc18"/><a data-primary="latency" data-secondary="percentiles versus averages" data-type="indexterm" id="ix_ch06-asciidoc19"/><a data-primary="percentiles, averages versus" data-type="indexterm" id="ix_ch06-asciidoc20"/>Unfortunately, the average is too naive an aggregation. We can miss lots of important information about the characteristics of our latency. In <a data-type="xref" href="ch08.html#ch-obs-micro">“Microbenchmarks”</a>, we can do a lot with the mean for basic statistics (this is what the Go &#13;
<span class="keep-together">benchmarking tool</span> is using), but in measuring the efficiency of our software in the bigger system with more unknowns, we have to be mindful. For example, imagine we want to improve the latency of one operation that used to take around 10 seconds. We made a potential optimization using our TFBO flow. We want to assess the &#13;
<span class="keep-together">efficiency</span> on the macro level. During our tests, the system performed 500 operations within 5 seconds (faster!), but 50 operations were extremely slow, with a 40-second latency. Suppose we would stick to the average (8.1 seconds). In that case, we could make the wrong conclusion that our optimization was successful, missing the potential big problem that our optimization caused, leading to 9% of operations being extremely slow.</p>&#13;
&#13;
<p>This is why it’s helpful to measure specific metrics (like latency) in percentiles. This is what <a data-type="xref" href="#code-latency-metric">Example 6-7</a> instrumentation is for with the metric histogram type for our latency measurements.<a data-primary="Jones, C., on thinking of metrics as distributions" data-type="indexterm" id="idm45606830957712"/></p>&#13;
<blockquote>&#13;
<p> Most metrics are better thought of as distributions rather than averages. For example, for a latency SLI [service level indicator], some requests will be serviced quickly, while others will invariably take longer—sometimes much longer. A simple average can obscure these tail latencies, as well as changes in them. (...) Using percentiles for indicators allows you to consider the shape of the distribution and its differing attributes: a high-order percentile, such as the 99th or 99.9th, shows you a plausible worst-case value, while using the 50th percentile (also known as the median) emphasizes the typical case.</p>&#13;
<p data-type="attribution">C. Jones et al., <a href="https://oreil.ly/rMBW3"><i>Site Reliability Engineering</i>, “Service Level Objectives”</a> (O’Reilly, 2016)</p>&#13;
</blockquote>&#13;
&#13;
<p>The histogram metric I mentioned in <a data-type="xref" href="#code-latency-metric-om">Example 6-8</a> is great for latency measurements, as it counts how many operations fit into a certain latency range. In <a data-type="xref" href="#code-latency-metric">Example 6-7</a>, I have chosen<sup><a data-type="noteref" href="ch06.html#idm45606830952080" id="idm45606830952080-marker">21</a></sup> exponential buckets <code>0.001, 0.01, 0.1, 1, 10, 100</code>. The largest bucket should represent the longest operation duration you expect in your system (e.g., a timeout).<sup><a data-type="noteref" href="ch06.html#idm45606830949872" id="idm45606830949872-marker">22</a></sup></p>&#13;
&#13;
<p>In <a data-type="xref" href="#ch-obs-metrics">“Metrics”</a>, we discussed how we can use metrics using <code>PromQL</code>. For the histogram type of metrics and our latency semantics, the best way to understand this is to use the <code>histogram_quantile</code> function. See the example output in <a data-type="xref" href="#img-obs-metric-perc-5">Figure 6-8</a> for the median, and <a data-type="xref" href="#img-obs-metric-perc-9">Figure 6-9</a> for the 90th percentile.</p>&#13;
&#13;
<figure><div class="figure" id="img-obs-metric-perc-5">&#13;
<img alt="efgo 0608" src="assets/efgo_0608.png"/>&#13;
<h6><span class="label">Figure 6-8. </span>Fiftieth percentile (median) of latency across an operation per error type from our <a data-type="xref" href="#code-latency-metric">Example 6-7</a> instrumentation</h6>&#13;
</div></figure>&#13;
&#13;
<figure><div class="figure" id="img-obs-metric-perc-9">&#13;
<img alt="efgo 0609" src="assets/efgo_0609.png"/>&#13;
<h6><span class="label">Figure 6-9. </span>Ninetieth percentile of latency across the operation per error type from our <a data-type="xref" href="#code-latency-metric">Example 6-7</a> instrumentation</h6>&#13;
</div></figure>&#13;
&#13;
<p class="less_space pagebreak-before">Both results can lead to interesting conclusions for the program I measured. We can observe a few things:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Half of the operations were generally faster than 590 milliseconds, while 90% were faster than 1 second. So if our RAER (<a data-type="xref" href="ch03.html#ch-conq-req">“Resource-Aware Efficiency Requirements”</a>) states that 90% of operations should be less than 1 second, it could mean we don’t need to optimize further.</p>&#13;
</li>&#13;
<li>&#13;
<p>Operations that failed with <code>error_type=error1</code> were considerably slower (most likely some bottleneck exists in that code path).</p>&#13;
</li>&#13;
<li>&#13;
<p>Around 17:50 UTC, we can see a slight increase in latencies for all operations. This might mean some side effect or change in the environment that caused my laptop’s operating system to give less CPU to my test.<sup><a data-type="noteref" href="ch06.html#idm45606830933632" id="idm45606830933632-marker">23</a></sup></p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Such measured and defined latency can help us determine if our latency is good enough for our requirements and if any optimization we do helps or not. It can also help us to find parts that cause slowness using different benchmarking and bottleneck-finding strategies. We will explore those in <a data-type="xref" href="ch07.html#ch-observability2">Chapter 7</a>.<a data-startref="ix_ch06-asciidoc20" data-type="indexterm" id="idm45606830930368"/><a data-startref="ix_ch06-asciidoc19" data-type="indexterm" id="idm45606830929664"/><a data-startref="ix_ch06-asciidoc18" data-type="indexterm" id="idm45606830928992"/></p>&#13;
&#13;
<p>With the typical latency metric definition and example instrumentation, let’s move to the next resource we might want to measure in our efficiency journey: CPU usage.<a data-startref="ix_ch06-asciidoc16" data-type="indexterm" id="idm45606830927936"/><a data-startref="ix_ch06-asciidoc15" data-type="indexterm" id="idm45606830927232"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="CPU Usage" data-type="sect2"><div class="sect2" id="ch-obs-cpu-usage">&#13;
<h2>CPU Usage</h2>&#13;
&#13;
<p><a data-primary="CPU resource" data-secondary="efficiency metrics semantics" data-type="indexterm" id="ix_ch06-asciidoc21"/><a data-primary="efficiency metrics semantics" data-secondary="CPU usage" data-type="indexterm" id="ix_ch06-asciidoc22"/>In <a data-type="xref" href="ch04.html#ch-hardware">Chapter 4</a>, you learned how CPU is used when we execute our Go programs. I also explained that we look at CPU usage to reduce CPU-driven latency<sup><a data-type="noteref" href="ch06.html#idm45606830921280" id="idm45606830921280-marker">24</a></sup> and cost, and to enable running more processes on the same machine.</p>&#13;
&#13;
<p>A variety of metrics allow us to measure different parts of our program’s CPU usage. For example, with Linux tools like the <a href="https://oreil.ly/MJVHl"><code>proc</code> filesystem</a> and <a href="https://oreil.ly/QPMD9"><code>perf</code></a>, we can measure our <a href="https://oreil.ly/VdENl">Go program’s miss and hit rates, CPU branch prediction hit rates</a>, and other low-level statistics. However, for basic CPU efficiency, we usually focus on the CPU cycles, instructions, or time used:</p>&#13;
<dl>&#13;
<dt>CPU cycles</dt>&#13;
<dd>&#13;
<p>The total number of CPU clock cycles used to execute the program thread instructions on each CPU core.</p>&#13;
</dd>&#13;
<dt>CPU instructions</dt>&#13;
<dd>&#13;
<p>The total number of CPU instructions of our program’s threads executed in each CPU core. On some CPUs from the <a href="https://oreil.ly/ofvB7">RISC architecture</a> (e.g., ARM processors), this might be equal to the number of cycles, as one instruction always takes one cycle (amortized cost). However, on the CISC architecture (e.g., AMD and Intel x64 processors), different instructions might use additional cycles. Thus, counting how many instructions our CPU had to do to complete some program’s functionality might be more stable.</p>&#13;
&#13;
<p>Both cycles and instructions are great for comparing different algorithms with each other. It is because they are less noisy as:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>They don’t depend on the frequency the CPU core had during the program run</p>&#13;
</li>&#13;
<li>&#13;
<p>Latency of memory fetches, including different caches, misses, and RAM latency</p>&#13;
</li>&#13;
</ul>&#13;
</dd>&#13;
</dl>&#13;
<dl>&#13;
<dt>CPU time</dt>&#13;
<dd>&#13;
<p>The time (in seconds or nanoseconds) our program thread spends executing on each CPU core. As you will learn in <a data-type="xref" href="ch09.html#ch-obs-pprof-latency">“Off-CPU Time”</a>, this time is different (longer or shorter) from the latency of our program, as CPU time does not include I/O waiting time and OS scheduling time. Furthermore, our program’s OS threads might execute simultaneously on multiple CPU cores. Sometimes we also use CPU time divided by the CPU capacity, often referred to as CPU usage. For example, 1.5 CPU usage in seconds means our program requires (on average) one CPU core for 1 second and a second core for 0.5 seconds.</p>&#13;
&#13;
<p>On Linux, the CPU time is often split into User and System time:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>User time represents the time the program spends executing on the CPU in the user space.</p>&#13;
</li>&#13;
<li>&#13;
<p>System time is the CPU time spent executing certain functions in the kernel space on behalf of the user, e.g., syscalls like <a href="https://oreil.ly/xEQuM"><code>read</code></a>.</p>&#13;
</li>&#13;
</ul>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Usually, on higher levels such as containers, we don’t have the luxury of having all three metrics. We mostly have to rely on CPU time. Fortunately, the CPU time is typically a good enough metric to track down the work needed from our CPUs to execute our workload. On Linux, the simplest way to retrieve the current CPU time counted from the start of the process is to go to <em>/proc/<code>&lt;PID&gt;</code>/stat</em> (where <code>PID</code> means the process ID). We also have similar statistics on the thread level in <em>/proc/<code>&lt;PID&gt;</code>/tasks/<code>&lt;TID&gt;</code>/stat</em> (where <code>TID</code> means the thread ID). This is exactly what utilities like <code>ps</code> or <code>htop</code> use.<sup><a data-type="noteref" href="ch06.html#idm45606830897680" id="idm45606830897680-marker">25</a></sup></p>&#13;
&#13;
<p>The <code>ps</code> and <code>htop</code> tools might be indeed the simplest tools to measure the CPU time in the current moment. However, we usually need to assess the CPU time required for the full functionality we are optimizing. Unfortunately, <a data-type="xref" href="ch08.html#ch-obs-micro-go">“Go Benchmarks”</a> is not providing CPU time (only latency and allocations) per operation. You could perhaps obtain that number from the <code>stat</code> file, e.g., programmatically using the <a href="https://oreil.ly/ZcCDn"><code>procfs</code> Go library</a>, but there are two main ways I would suggest instead:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>CPU profiling, explained in <a data-type="xref" href="ch09.html#ch-obs-pprof-cpu">“CPU”</a>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Prometheus metric instrumentation. Let’s quickly look at that method next.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p><a data-primary="Prometheus" data-secondary="metric instrumentation" data-type="indexterm" id="ix_ch06-asciidoc23"/>In <a data-type="xref" href="#code-latency-metric">Example 6-7</a>, I showed a Prometheus instrumentation that registers custom latency metrics. It’s also very easy to add the CPU time metric, but the Prometheus <a href="https://oreil.ly/1r2zw">client library</a> has already built helpers for that. The recommended way is presented in <a data-type="xref" href="#code-cpu-metric">Example 6-11</a>.</p>&#13;
<div data-type="example" id="code-cpu-metric">&#13;
<h5><span class="label">Example 6-11. </span>Registering <code>proc</code> <code>stat</code> instrumentation about your process for Prometheus use</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kn">import</code><code class="w"> </code><code class="p">(</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"net/http"</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/prometheus/client_golang/prometheus"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/prometheus/client_golang/prometheus/collectors"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/prometheus/client_golang/prometheus/promhttp"</code><code class="w">&#13;
</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">ExampleCPUTimeMetric</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">reg</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">prometheus</code><code class="p">.</code><code class="nx">NewRegistry</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">reg</code><code class="p">.</code><code class="nx">MustRegister</code><code class="p">(</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">collectors</code><code class="p">.</code><code class="nx">NewProcessCollector</code><code class="p">(</code><code class="nx">collectors</code><code class="p">.</code><code class="nx">ProcessCollectorOpts</code><code class="p">{</code><code class="p">}</code><code class="p">)</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO11-1" id="co_efficiency_observability_CO11-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
&#13;
    </code><code class="k">go</code><code class="w"> </code><code class="kd">func</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">xTimes</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">             </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">doOperation</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">             </code><code class="c1">// ...</code><code class="w">&#13;
</code><code class="w">        </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">http</code><code class="p">.</code><code class="nx">ListenAndServe</code><code class="p">(</code><code class="w">&#13;
</code><code class="w">        </code><code class="s">":8080"</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">promhttp</code><code class="p">.</code><code class="nx">HandlerFor</code><code class="p">(</code><code class="nx">reg</code><code class="p">,</code><code class="w"> </code><code class="nx">promhttp</code><code class="p">.</code><code class="nx">HandlerOpts</code><code class="p">{</code><code class="p">}</code><code class="p">)</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="c1">// ...</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_efficiency_observability_CO11-1" id="callout_efficiency_observability_CO11-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The only thing you have to do to have the CPU time metric with Prometheus is to register the <code>collectors.NewProcessCollector</code> that uses the <code>/proc</code> <code>stat</code> file mentioned previously.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>The <code>collectors.ProcessCollector</code> provides multiple metrics, like <code>pro⁠cess_​open_fds</code>, <code>process_max_fds</code>, <code>process_start_time_seconds</code>, and so on. But the one we are interested in is <code>process_cpu_seconds_total</code>, which is a counter of CPU time used from the beginning of our program. What’s special about using Prometheus for this task is that it collects the values of this metric periodically from our Go program. This means we can query Prometheus for the process CPU time for a certain time window and map that to real time. We can do that with the <a href="https://oreil.ly/8BaUw"><code>rate</code></a> function duration that gives us the per second rate of that CPU time in a given time window. For example, <code>rate(process_cpu_sec⁠onds_​total{}[5m])</code> will give us the average CPU per second time that our program had during the last five minutes.</p>&#13;
&#13;
<p>You will find an example CPU time analysis based on this kind of metric in <a data-type="xref" href="ch08.html#ch-obs-macro-results">“Understanding Results and Observations”</a>. However, for now, I would love to show you one interesting and common case, where <code>process_cpu_seconds_total</code> helps narrow down a major efficiency problem. Imagine your machine has only two CPU cores (or we limit our program to use two CPU cores), you run the functionality you want to assess, and you see the CPU time rate of your Go program looking like <a data-type="xref" href="#img-obs-metric-cpu">Figure 6-10</a>.</p>&#13;
&#13;
<p>Thanks to this view, we can tell that the <code>labeler</code> process is experiencing a state of CPU saturation. This means that our Go process requires more CPU time than was available. Two signals tell us about the CPU saturation:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The typical “healthy” CPU usage is spikier (e.g., as presented in <a data-type="xref" href="ch08.html#img-macrobench-cpu">Figure 8-4</a> later in the book). This is because it’s unlikely that typical applications use the same amount of CPU all the time. However, in <a data-type="xref" href="#img-obs-metric-cpu">Figure 6-10</a>, we see the same CPU usage for five minutes.</p>&#13;
</li>&#13;
<li>&#13;
<p>Because of this, we never want our CPU time to be so close to the CPU limit (two in our case). In <a data-type="xref" href="#img-obs-metric-cpu">Figure 6-10</a>, we can clearly see a small choppiness around the CPU limit, which indicates full CPU saturation.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<figure><div class="figure" id="img-obs-metric-cpu">&#13;
<img alt="efgo 0610" src="assets/efgo_0610.png"/>&#13;
<h6><span class="label">Figure 6-10. </span>The Prometheus graph view of the CPU time for the <code>labeler</code> Go program (we will use it in an example in <a data-type="xref" href="ch08.html#ch-obs-macro">“Macrobenchmarks”</a>) after a test</h6>&#13;
</div></figure>&#13;
&#13;
<p>Knowing when we are at saturation of our CPU is critical. First of all, it might give the wrong impression that the current CPU time is the maximum that the process needs. Moreover, this situation also significantly slows down our program’s execution time (increases latency) or even stalls it completely. This is why the Prometheus-based CPU time metric, as you learned here, has proven to be critical for me in learning about such saturation cases. It is also one of the first things you must find out when analyzing your program’s efficiency. When saturation happens, we have to give more CPU cores to the process, optimize the CPU usage, or decrease the concurrency (e.g., limit the number of HTTP requests it can do concurrently).<a data-startref="ix_ch06-asciidoc23" data-type="indexterm" id="idm45606830673984"/></p>&#13;
&#13;
<p>On the other hand, CPU time allows us to find out about opposite cases where the process might be blocked. For example, if you expect CPU-bound functionality to run with 5 goroutines, and you see the CPU time of 0.5 (50% of one CPU core), &#13;
<span class="keep-together">it might</span> mean the goroutines are blocked (more on that in <a data-type="xref" href="ch09.html#ch-obs-pprof-latency">“Off-CPU Time”</a>) or whole machine and OS are busy.<a data-startref="ix_ch06-asciidoc22" data-type="indexterm" id="idm45606830671056"/><a data-startref="ix_ch06-asciidoc21" data-type="indexterm" id="idm45606830670384"/></p>&#13;
&#13;
<p>Let’s now look at memory usage metrics.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Memory Usage" data-type="sect2"><div class="sect2" id="ch-obs-mem-usage">&#13;
<h2>Memory Usage</h2>&#13;
&#13;
<p><a data-primary="efficiency metrics semantics" data-secondary="memory usage" data-type="indexterm" id="ix_ch06-asciidoc24"/><a data-primary="memory resource" data-secondary="efficiency metrics semantics" data-type="indexterm" id="ix_ch06-asciidoc25"/>As we learned in <a data-type="xref" href="ch05.html#ch-hardware2">Chapter 5</a>, there are complex layers of different mechanics on how our Go program uses memory. This is why the actual physical memory (RAM) usage is one of the most tricky to measure and attribute to our program. On most systems with an OS memory management mechanism like virtual memory, paging, and shared pages, every memory usage metric will be only an estimation. While imperfect, this is what we have to work with, so let’s take a short look at what works best for the Go program.</p>&#13;
&#13;
<p>There are two main sources of memory usage information for our Go process: the Go runtime heap memory statistics and the information that OS holds about memory pages. Let’s start with the in-process runtime stats.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="runtime heap statistics" data-type="sect3"><div class="sect3" id="idm45606830662720">&#13;
<h3>runtime heap statistics</h3>&#13;
&#13;
<p><a data-primary="heap" data-secondary="runtime heap statistics" data-type="indexterm" id="idm45606830661184"/><a data-primary="memory resource" data-secondary="runtime heap statistics" data-type="indexterm" id="idm45606830660208"/><a data-primary="runtime heap statistics" data-type="indexterm" id="idm45606830659264"/>As we learned in <a data-type="xref" href="ch05.html#ch-hw-go-mem">“Go Memory Management”</a>, the heap segment of the Go program virtual memory can be an adequate proxy for memory usage. This is because most bytes are allocated on the heap for typical Go applications. Moreover, such memory is also never evicted from the RAM (unless the swap is enabled). As a result, we can effectively assess our functionality’s memory usage by looking at the heap size.</p>&#13;
&#13;
<p>We are often most interested in assessing the memory space or the number of memory blocks needed to perform a certain operation. To try to estimate this, we usually use two semantics:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The total allocations of bytes or objects on the heap allow us to look at memory allocations without often nondeterministic GC impact.</p>&#13;
</li>&#13;
<li>&#13;
<p>The number of currently in-use bytes or objects on the heap.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The preceding statistics are very accurate and quick to access because Go runtime is responsible for heap management, so it tracks all the information we need. Before Go 1.16, the recommended way to access those statistics programmatically was using the <a href="https://oreil.ly/AwX75"><code>runtime.ReadMemStats</code> function</a>. It still works for compatibility reasons, but unfortunately, it requires STW (stop the world) events to gather all memory statistics. As a result of Go 1.16, we should all use the <a href="https://oreil.ly/WYiOd"><code>runtime/metrics</code></a> package that provides many cheap-to-collect insights about GC, memory allocations, and so on. The example usage of this package to get memory usage metrics is presented in <a data-type="xref" href="#code-rtm">Example 6-12</a>.</p>&#13;
<div class="less_space pagebreak-before" data-type="example" id="code-rtm">&#13;
<h5><span class="label">Example 6-12. </span>The simplest code prints total heap allocated bytes and currently used ones</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kn">import</code><code class="p">(</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"fmt"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"runtime"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"runtime/metrics"</code><code class="w">&#13;
</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">var</code><code class="w"> </code><code class="nx">memMetrics</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="p">[</code><code class="p">]</code><code class="nx">metrics</code><code class="p">.</code><code class="nx">Sample</code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">{</code><code class="nx">Name</code><code class="p">:</code><code class="w"> </code><code class="s">"/gc/heap/allocs:bytes"</code><code class="p">}</code><code class="p">,</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO12-1" id="co_efficiency_observability_CO12-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
    </code><code class="p">{</code><code class="nx">Name</code><code class="p">:</code><code class="w"> </code><code class="s">"/memory/classes/heap/objects:bytes"</code><code class="p">}</code><code class="p">,</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">printMemRuntimeMetric</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">runtime</code><code class="p">.</code><code class="nx">GC</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO12-2" id="co_efficiency_observability_CO12-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
    </code><code class="nx">metrics</code><code class="p">.</code><code class="nx">Read</code><code class="p">(</code><code class="nx">memMetrics</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO12-3" id="co_efficiency_observability_CO12-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
&#13;
    </code><code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="s">"Total bytes allocated:"</code><code class="p">,</code><code class="w"> </code><code class="nx">memMetrics</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">.</code><code class="nx">Value</code><code class="p">.</code><code class="nx">Uint64</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_efficiency_observability_CO12-4" id="co_efficiency_observability_CO12-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
    </code><code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="s">"In-use bytes:"</code><code class="p">,</code><code class="w"> </code><code class="nx">memMetrics</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="p">.</code><code class="nx">Value</code><code class="p">.</code><code class="nx">Uint64</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_efficiency_observability_CO12-1" id="callout_efficiency_observability_CO12-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>To read samples from <code>runtime/metrics</code>, we must first define them by referencing the desired metric name. The full list of metrics might be different (mostly added ones) across different Go versions, and you can see the list with descriptions at <a href="https://oreil.ly/HWGUJ"><em>pkg.go.dev</em></a>. For example, we can obtain the number of objects in a heap.</p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO12-2" id="callout_efficiency_observability_CO12-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Memory statistics are recorded right after a GC run, so we can trigger GC to have the latest information about the heap.</p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO12-3" id="callout_efficiency_observability_CO12-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p><code>metrics.Read</code> populates the value of our samples. You can reuse the same sample slice if you only care about the latest values.</p></dd>&#13;
<dt><a class="co" href="#co_efficiency_observability_CO12-4" id="callout_efficiency_observability_CO12-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Both metrics are of <code>uint64</code> type, so we use the <code>Uint64()</code> method to retrieve the value.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Programmatically accessing this information is useful for local debugging purposes, but it’s not sustainable on every optimization attempt. That’s why in the community, we typically see other ways to access that data:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Go benchmarking, explained in <a data-type="xref" href="ch08.html#ch-obs-micro-go">“Go Benchmarks”</a></p>&#13;
</li>&#13;
<li>&#13;
<p>Heap profiling, explained in <a data-type="xref" href="ch09.html#ch-obs-pprof-heap">“Heap”</a></p>&#13;
</li>&#13;
<li>&#13;
<p>Prometheus metric instrumentation</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p class="less_space pagebreak-before">To register <code>runtime/metric</code> as Prometheus metrics, we can add a single line to <a data-type="xref" href="#code-cpu-metric">Example 6-11</a>: <code>reg.MustRegister(collectors.NewGoCollector())</code>. The Go collector is a structure that, by default, exposes <a href="https://oreil.ly/Ib8D2">various memory statistics</a>. For historical reasons, those map to the <code>MemStats</code> Go structure, so the equivalents to the metrics defined in <a data-type="xref" href="#code-rtm">Example 6-12</a> would be <code>go_mem⁠stats_​heap_alloc_bytes_total</code> for a counter, and <code>go_memstats_heap_alloc_bytes</code> for a current usage gauge. We will show an analysis of Go heap metrics in <a data-type="xref" href="ch08.html#ch-obs-macro-example">“Go e2e Framework”</a>.</p>&#13;
&#13;
<p>Unfortunately, heap statistics are only an estimation. It is likely that the smaller the heap on our Go program, the better the memory efficiency. However, suppose you add some deliberate mechanisms like large off-heap memory allocations using explicit <code>mmap</code> syscall or thousands of goroutines with large stacks. In that case, that can cause an OOM on your machine, yet it’s not reflected in the heap statistics. Similarly, in <a data-type="xref" href="ch05.html#ch-hw-allocator">“Go Allocator”</a>, I explained rare cases where only part of the heap space is allocated on physical memory.</p>&#13;
&#13;
<p>Still, despite the downsides, heap allocations remain the most effective way to measure memory usage in modern Go programs.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="OS memory pages statistics" data-type="sect3"><div class="sect3" id="idm45606830662096">&#13;
<h3>OS memory pages statistics</h3>&#13;
&#13;
<p><a data-primary="memory resource" data-secondary="OS memory pages statistics" data-type="indexterm" id="idm45606830466800"/><a data-primary="operating system (OS) memory management" data-secondary="memory pages statistics" data-type="indexterm" id="idm45606830465584"/><a data-primary="paging" data-type="indexterm" id="idm45606830464624"/>We can check the numbers the Linux OS tracks per thread to learn more realistic yet more complex memory usage statistics. Similar to <a data-type="xref" href="#ch-obs-cpu-usage">“CPU Usage”</a>, <code>/proc/<em>&lt;PID&gt;</em>/statm</code> provides the memory usage statistics, measured in pages. Even more accurate numbers can be retrieved from per memory mapping statistics that we can see in <code>/proc/<em>&lt;PID&gt;</em>/smaps</code> (<a data-type="xref" href="ch05.html#ch-hw-memory-mmap-os">“OS Memory Mapping”</a>).</p>&#13;
&#13;
<p>Each page in this mapping can have a different state. A page might or might not be allocated on physical memory. Some pages might be shared across processes. Some pages might be allocated in physical memory and accounted for as memory used, yet marked by the program as “free” (see the <code>MADV_FREE</code> release method mentioned in <a data-type="xref" href="ch05.html#ch-hw-garbage">“Garbage Collection”</a>). Some pages might not even be accounted for in the <code>smaps</code> file, because for example, <a href="https://oreil.ly/uchws">it’s part of filesystem Linux cache buffers</a>. For these reasons, we should be very skeptical about the absolute values observed in the following metrics. In many cases, OS is lazy in releasing memory; e.g., part of the memory used by the program is cached in the best way that will be released immediately as long as somebody else is needing that.</p>&#13;
&#13;
<p>There are a few typical memory usage metrics we can obtain from the OS about our process:</p>&#13;
<dl>&#13;
<dt>VSS</dt>&#13;
<dd>&#13;
<p>Virtual set size represents the number of pages (or bytes, depending on instrumentation) allocated for the program. Not very useful metrics, as most virtual pages are never allocated on RAM.</p>&#13;
</dd>&#13;
<dt>RSS</dt>&#13;
<dd>&#13;
<p>Residential set size represents the number of pages (or bytes) resident in RAM. Note that different metrics might account for that differently; e.g., the <a href="https://oreil.ly/NL5Ab">cgroups RSS metric</a> does not include file-mapped memory, which is tracked separately.</p>&#13;
</dd>&#13;
<dt>PSS</dt>&#13;
<dd>&#13;
<p>Proportional set size represents memory with shared memory pages divided equally among all users.</p>&#13;
</dd>&#13;
<dt>WSS</dt>&#13;
<dd>&#13;
<p>Working set size estimates the number of pages (or bytes) currently used to perform work by our program. It was initially <a href="https://oreil.ly/rWy8D">introduced by Brendan Gregg</a> as the hot, frequently used memory—the minimum memory requirement by the &#13;
<span class="keep-together">program.</span></p>&#13;
&#13;
<p>The idea is that a program might have allocated 500 GB of memory, but within a couple of minutes, it might use only 50 MB for some localized computation. The rest of the memory could be, in theory, safely offloaded to disk.</p>&#13;
&#13;
<p>There are many implementations of WSS, but the most common I see is the <a href="https://oreil.ly/mXjA3">cadvisor interpretation</a> using the <a href="https://oreil.ly/ovSlH">cgroup memory controller</a>. It calculates the WSS as the RSS (including file mapping), plus some part of the cache pages (cache used for disk reads or writes), minus the <code>inactive_file</code> entry—so file mapping that were not touched for some time. It does not include inactive anonymous pages because the typical OS configuration can’t offload anonymous pages to disk (swap is disabled).</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>In practice, RSS or WSS is used to determine the memory usage of our Go program. Which one highly depends on the other workloads on the same machine and follows the flow of the RAM usage expanding to all available space, as mentioned in <a data-type="xref" href="ch05.html#ch-hw-memory">“Do We Have a Memory Problem?”</a>. The usefulness of each depends on the current Go version and instrumentation that gives you those metrics. In my experience, with the latest Go version and cgroup metrics, the RSS metric tends to give more reliable results.<sup><a data-type="noteref" href="ch06.html#idm45606830443712" id="idm45606830443712-marker">26</a></sup> Unfortunately, accurate or not, WSS is used in systems like <a href="https://oreil.ly/lnDkI">Kubernetes to trigger evictions (e.g., OOM)</a>, thus we should use it to assess memory efficiency that might lead to OOMs.</p>&#13;
&#13;
<p>Given my focus on infrastructure Go programs, I heavily lean on a metric exporter called <a href="https://oreil.ly/RJzKd">cadvisor</a> that converts cgroup metrics to Prometheus metrics. I will explain using it in detail in <a data-type="xref" href="ch08.html#ch-obs-macro-example">“Go e2e Framework”</a>. It allows analyzing metrics &#13;
<span class="keep-together">like <code>container_memory_rss + container_memory_mapped_file</code></span> and <code>container_memory_working_set_bytes</code>, which are commonly used in the<a data-startref="ix_ch06-asciidoc25" data-type="indexterm" id="idm45606830437584"/><a data-startref="ix_ch06-asciidoc24" data-type="indexterm" id="idm45606830436912"/><a data-startref="ix_ch06-asciidoc14" data-type="indexterm" id="idm45606830436240"/><a data-startref="ix_ch06-asciidoc13" data-type="indexterm" id="idm45606830435568"/><a data-startref="ix_ch06-asciidoc12" data-type="indexterm" id="idm45606830434896"/> &#13;
<span class="keep-together">community.</span></p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45606831183712">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Modern observability offers a set of techniques essential for our efficiency assessments and improvements. However, some argue that this kind of observability designed primarily for DevOps, SREs, and cloud-native solutions can’t work for developer use cases (in the past known as Application Performance Monitoring [APM]).</p>&#13;
&#13;
<p>I would argue that the same tools can be used for both developers (for those efficiency and debugging journeys) and system admins, operators, DevOps, and SREs to ensure the programs delivered by others are running effectively.</p>&#13;
&#13;
<p>In this chapter, we discussed the three first observability signals: metrics, logs, and tracing. Then, we went through example instrumentations for those in Go. Finally, I explained common semantics for the latency, CPU time, and memory usage measurements we will use in later chapters.<a data-startref="ix_ch06-asciidoc0" data-type="indexterm" id="idm45606830431120"/></p>&#13;
&#13;
<p>Now it’s time to learn how to use that efficiency observability to make data-driven decisions in practice. First, we will focus on how to simulate our program to assess the efficiency on different levels.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45606832729952"><sup><a href="ch06.html#idm45606832729952-marker">1</a></sup> <a data-primary="monitoring, observability versus" data-type="indexterm" id="idm45606832729552"/><a data-primary="observability" data-secondary="monitoring versus" data-type="indexterm" id="idm45606832728832"/>Some of you might ask why I am sticking to the word <em>observability</em> and don’t mention monitoring. In my eyes, I have to agree with my friend <a href="https://oreil.ly/9ado0">Björn Rabenstein</a> that the difference between monitoring and observability tends to be driven by marketing needs too much. One might say that observability has become meaningless these days. In theory, monitoring means answering known unknown problems (known questions), whereas observability allows learning about unknown unknowns (any question you might have in the future). In my eyes, monitoring is a subset of observability. In this book, we will stay pragmatic. Let’s focus on how we can leverage observability practically, not using theoretical concepts.</p><p data-type="footnote" id="idm45606832725840"><sup><a href="ch06.html#idm45606832725840-marker">2</a></sup> The fourth signal, profiling, just started to be considered by some as an observability signal. This is because only recently did the industry see a value and need for gathering profiling continuously.</p><p data-type="footnote" id="idm45606832699376"><sup><a href="ch06.html#idm45606832699376-marker">3</a></sup> As a recent example, we can give <a href="https://oreil.ly/sPlPe">this repository</a> that gathers information through eBPF probes and tries to search popular functions or libraries.</p><p data-type="footnote" id="idm45606832688544"><sup><a href="ch06.html#idm45606832688544-marker">4</a></sup> In some way, I am trying in this book to establish helpful processes around optimizations and efficiency, which by design yield standard questions we know up front. This aggregated information is usually enough for us here.</p><p data-type="footnote" id="idm45606832254464"><sup><a href="ch06.html#idm45606832254464-marker">5</a></sup> Given Go compatibility guarantees, even if the community agrees to improve it, we cannot change it until  <span class="keep-together">Go 2.0.</span></p><p data-type="footnote" id="idm45606832248016"><sup><a href="ch06.html#idm45606832248016-marker">6</a></sup> A nonexecutable module or package intended to be imported by others.</p><p data-type="footnote" id="idm45606832242864"><sup><a href="ch06.html#idm45606832242864-marker">7</a></sup> There are many Go libraries for logging. <code>go-kit</code> has a good enough API that allows us to do all kinds of logging we need in all the Go projects I have helped with so far. This does not mean <code>go-kit</code> is without flaws (e.g., it’s easy to forget you have to put an even number of arguments for the key-value–like logic). There is also a pending proposal from the Go community on <a href="https://oreil.ly/qnJ6y">structure logging in standard libraries (<code>slog</code> package)</a>. Feel free to use any other libraries, but make sure their API is simple, readable, and useful. Also make sure that the library of your choice is not introducing efficiency problems.</p><p data-type="footnote" id="idm45606832090240"><sup><a href="ch06.html#idm45606832090240-marker">8</a></sup> It’s a typical pattern allowing processes to print something useful to standard output and keep logs separate in the <code>stderr</code> Linux file.</p><p data-type="footnote" id="idm45606831687072"><sup><a href="ch06.html#idm45606831687072-marker">9</a></sup> Tail sampling is a logic that defers the decision if the trace should be excluded or sampled at the end of the transaction, for example, only after we know its status code. The problem with tail sampling is that your instrumentation might have already assumed that all spans will be sampled.</p><p data-type="footnote" id="idm45606831665920"><sup><a href="ch06.html#idm45606831665920-marker">10</a></sup> I maintain this library together with the Prometheus team. The <code>client_golang</code> is also the most used metric client SDK for Go when writing this book, <a href="https://oreil.ly/UW0fG">with over 53,000 open source projects</a> using it. It is free and open source.</p><p data-type="footnote" id="idm45606831400064"><sup><a href="ch06.html#idm45606831400064-marker">11</a></sup> It’s tempting to use global <code>prometheus.DefaultRegistry</code>. Don’t do this. We try to get away from this pattern that can cause many problems and side effects.</p><p data-type="footnote" id="idm45606831336912"><sup><a href="ch06.html#idm45606831336912-marker">12</a></sup> Always check errors and perform graceful termination on process teardown. See production-grade usage in the <a href="https://oreil.ly/yvvTM">Thanos project</a> that leverages the <a href="https://oreil.ly/sDIwW">run goroutine helper</a>.</p><p data-type="footnote" id="idm45606831225328"><sup><a href="ch06.html#idm45606831225328-marker">13</a></sup> Note that doing <code>rate</code> on the gauges type of metric will yield incorrect results.</p><p data-type="footnote" id="idm45606831205328"><sup><a href="ch06.html#idm45606831205328-marker">14</a></sup> On the contrary, for the push-based system, if you don’t see expected data, it’s hard to tell if it’s because the sender is down or the pipeline to send is down.</p><p data-type="footnote" id="idm45606831202864"><sup><a href="ch06.html#idm45606831202864-marker">15</a></sup> See our talk from <a href="https://oreil.ly/TtKwH">KubeCon EU 2022</a> about such cases.</p><p data-type="footnote" id="idm45606831155568"><sup><a href="ch06.html#idm45606831155568-marker">16</a></sup> This is why the <a href="https://oreil.ly/oJozb">Prometheus ecosystem suggests base units</a>.</p><p data-type="footnote" id="idm45606831016880"><sup><a href="ch06.html#idm45606831016880-marker">17</a></sup> For example, on my machine <code>time.Now</code> and <code>time.Since</code> take around 50–55 nanoseconds.</p><p data-type="footnote" id="idm45606831013664"><sup><a href="ch06.html#idm45606831013664-marker">18</a></sup> This is why it’s better to make thousands or even more of the same operation, measure the total latency, and get the average by dividing it by a number of operations. As a result, this is what Go benchmark is doing, as we will learn in <a data-type="xref" href="ch08.html#ch-obs-micro-go">“Go Benchmarks”</a>.</p><p data-type="footnote" id="idm45606831005584"><sup><a href="ch06.html#idm45606831005584-marker">19</a></sup> Did you know this date was picked simply because of <a href="https://oreil.ly/Oct6X"><em>Back to the Future Part II</em></a>?</p><p data-type="footnote" id="idm45606830976592"><sup><a href="ch06.html#idm45606830976592-marker">20</a></sup> The noteworthy example from my experience is measuring server-side latency of REST with a large response or HTTP/gRPC with a streamed response. The server-side latency does not depend only on the server but also on how fast the network and client side can consume those bytes (and write back acknowledge packets within <a href="https://oreil.ly/jcrSF">TCP control flow</a>).</p><p data-type="footnote" id="idm45606830952080"><sup><a href="ch06.html#idm45606830952080-marker">21</a></sup> Right now, the choice of buckets in a histogram if you want to use Prometheus is manual. However, the Prometheus community is working on <a href="https://oreil.ly/qFdC1">sparse histograms</a> with a dynamic number of buckets that adjust automatically.</p><p data-type="footnote" id="idm45606830949872"><sup><a href="ch06.html#idm45606830949872-marker">22</a></sup> More on using histograms can be read <a href="https://oreil.ly/VrWGe">here</a>.</p><p data-type="footnote" id="idm45606830933632"><sup><a href="ch06.html#idm45606830933632-marker">23</a></sup> It makes sense. I was utilizing my web browser heavily during the test, which confirms the knowledge we will discuss in <a data-type="xref" href="ch07.html#ch-obs-rel">“Reliability of Experiments”</a>.</p><p data-type="footnote" id="idm45606830921280"><sup><a href="ch06.html#idm45606830921280-marker">24</a></sup> As a reminder, we can improve the latency of our program’s functionality in many ways other than just by optimizing its CPU usage. We can improve that latency using concurrent execution that often increases total CPU time.</p><p data-type="footnote" id="idm45606830897680"><sup><a href="ch06.html#idm45606830897680-marker">25</a></sup> Also a useful <a href="https://oreil.ly/ZcCDn"><code>procfs</code> Go library</a> that allows retrieving <code>stats</code> file data number  <span class="keep-together">programmatically.</span></p><p data-type="footnote" id="idm45606830443712"><sup><a href="ch06.html#idm45606830443712-marker">26</a></sup> One reason is the <a href="https://oreil.ly/LKmSA">issue</a> in cadvisor that includes some still-reclaimable memory in the WSS.</p></div></div></section></body></html>