<html><head></head><body><section data-pdf-bookmark="Chapter 8. Benchmarking" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch-benchmarking">&#13;
<h1><span class="label">Chapter 8. </span>Benchmarking</h1>&#13;
&#13;
&#13;
<p><a data-primary="benchmarks/benchmarking" data-seealso="efficiency assessment" data-type="indexterm" id="ix_ch08-asciidoc0"/><a data-primary="benchmarks/benchmarking" data-secondary="implementation" data-type="indexterm" id="ix_ch08-asciidoc1"/>Hopefully, your Go IDE is ready and warmed up for some action! It’s time to stress our Go code to find its efficiency characteristics on the micro and macro levels mentioned in <a data-type="xref" href="ch07.html#ch-observability2">Chapter 7</a>.</p>&#13;
&#13;
<p>In this chapter, we will start with <a data-type="xref" data-xrefstyle="select:nopage" href="#ch-obs-micro">“Microbenchmarks”</a>, where we will go through the basics of microbenchmarking and introduce Go native benchmarking. Next, I will explain how to interpret the output with tools like <code>benchstat</code>. Then I will go through the microbenchmark aspects and tricks that I learned that are incredibly useful for the practical use of microbenchmarks.</p>&#13;
&#13;
<p>In the second half of this chapter, we’ll go through <a data-type="xref" href="#ch-obs-macro">“Macrobenchmarks”</a>, which is rarely in the scope of programming books due to its size and complexity. In my opinion, <span class="keep-together">macrobenchmarking</span> is as critical to Go development as microbenchmarking, so every developer caring about efficiency should be able to work with that level of testing. Next, in <a data-type="xref" href="#ch-obs-macro-example">“Go e2e Framework”</a> we will go through a complete example of a macro test written fully in Go using containers. We will discuss results and common observability in the process.</p>&#13;
&#13;
<p>Without further ado, let’s jump into the most agile way of assessing the efficiency of smaller parts of the code, namely microbenchmarking.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Microbenchmarks" data-type="sect1"><div class="sect1" id="ch-obs-micro">&#13;
<h1>Microbenchmarks</h1>&#13;
&#13;
<p><a data-primary="microbenchmarks/microbenchmarking" data-secondary="implementation" data-type="indexterm" id="ix_ch08-asciidoc2"/>A benchmark can be called a microbenchmark if it’s focused on a single, isolated functionality on a small piece of code running in a single process. You can think of microbenchmarks as a tool for efficiency assessment of optimizations made for a single component on the code or algorithm level (discussed in <a data-type="xref" href="ch03.html#ch-conq-opt-levels">“Optimization Design Levels”</a>). Anything more complex might be challenging to benchmark on the micro level. By more complex, I mean, for example, trying to benchmark:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Multiple functionalities at once.</p>&#13;
</li>&#13;
<li>&#13;
<p>Long-running functionalities (over 5–10 seconds long).</p>&#13;
</li>&#13;
<li>&#13;
<p>Bigger multistructure components.</p>&#13;
</li>&#13;
<li>&#13;
<p>Multiprocess functionalities. Multigoroutine functionalities are acceptable if they don’t spin too many goroutines (e.g., over one hundred) during our tests.</p>&#13;
</li>&#13;
<li>&#13;
<p>Functionalities that require more resources to run than a moderate development machine (e.g., allocating 40 GB of memory to compute an answer or prepare a test dataset).</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>If your code violates any of those elements, you might consider splitting it into smaller microbenchmarks or consider using macrobenchmarks on ones with different frameworks (see <a data-type="xref" href="#ch-obs-macro">“Macrobenchmarks”</a>).</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Keep Microbenchmarks Micro</h1>&#13;
<p>The more we are benchmarking at once on a micro level, the more time it takes to implement and perform such benchmarks. This results in cascading consequences—we try to make benchmarks more reusable and spend even more time building more abstractions over them. Ultimately, we try to make them stable and harder to change.</p>&#13;
&#13;
<p>This is a problem because microbenchmarks were designed for agility. We change code often, so we want benchmarks to be updated quickly and not get in our way. So you write them quickly, keep them simple, and change them.</p>&#13;
&#13;
<p>On top of that, Go benchmarks do not have (and should not have!) sophisticated observability, which is another reason to keep them small.</p>&#13;
</div>&#13;
&#13;
<p>The benchmark definition means that it’s very rare for the microbenchmark to validate if your program matches the high-level user RAER for certain functionality, e.g., “The p95 of this API should be under one minute.” In other words, it is usually not well suited to answer questions requiring absolute data. Therefore, while writing <span class="keep-together">microbenchmarks</span>, we should instead focus on answers that relate to a certain baseline or pattern, for example:</p>&#13;
<dl>&#13;
<dt>Learning about runtime complexity</dt>&#13;
<dd>&#13;
<p>Microbenchmarks are a fantastic way to learn more about the Go function or method efficiency behavior over certain dimensions. For example, how is latency impacted by different shares and sizes of the input and test data? Do allocations grow in an unbounded way with the size of input? What are the constant factors and the overhead of the algorithm you chose?</p>&#13;
&#13;
<p>Thanks to the quick feedback loop, it’s easy to manually play with test inputs and see what your function efficiency looks like for various test data and cases.</p>&#13;
</dd>&#13;
<dt>A/B testing</dt>&#13;
<dd>&#13;
<p><a data-primary="A/B testing" data-type="indexterm" id="idm45606829550672"/>A/B tests are defined by performing the same test on version A of your program and then on version B, which is different (ideally) only by one thing (e.g., you reused one slice). They can tell us the relative impact of our changes.</p>&#13;
&#13;
<p>Microbenchmarks are a great way to assess if a new change of the code, configuration, or hardware can potentially affect the efficiency. For example, suppose we know that the absolute latency of some requests is two minutes, and we know that 60% of that latency is caused by a certain Go function in a code we develop. In this case, we can try optimizing this function and perform a microbenchmark before and after. As long as our test data is reliable, if after optimization, our microbenchmark shows our optimization makes our code 20% faster, the full system will also be 18% faster.</p>&#13;
&#13;
<p>Sometimes the absolute numbers on microbenchmarking for latency might matter less. For example, it doesn’t tell us much if our microbenchmark shows 900 ms per operation on our machine. On a different laptop, it might show 500 ms. What matters is that on the same machine, with as few changes to the environment as possible and running one benchmark after another, the latency between version A and B is higher or lower. As we learned in <a data-type="xref" href="ch07.html#ch-obs-rel-repro">“Reproducing Production”</a>, there are high chances that this relation is then reproducible in any other environment where you will benchmark those versions.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The best way to implement and run microbenchmarks in Go is through its native benchmarking framework built into the <code>go test</code> tool. It is battle tested, integrated into testing flows, has native support for profiling, and you can see many benchmark examples in the Go community. I already mentioned the basics around the Go benchmark framework with <a data-type="xref" href="ch06.html#code-latency-go-bench">Example 6-3</a>, and we saw some preprocessed results in <a data-type="xref" href="ch07.html#code-sum-bench2">Example 7-2</a> outputs, but it’s now time to dive into details!</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Go Benchmarks" data-type="sect2"><div class="sect2" id="ch-obs-micro-go">&#13;
<h2>Go Benchmarks</h2>&#13;
&#13;
<p><a data-primary="Go benchmarks" data-secondary="microbenchmarks" data-type="indexterm" id="ix_ch08-asciidoc3"/>Creating <a href="https://oreil.ly/0h0y0">microbenchmarks in Go</a> starts by creating a particular function with a specific signature. Go tooling is not very picky—a function has to satisfy three elements to be considered a benchmark:</p>&#13;
&#13;
<ul class="less_space pagebreak-before">&#13;
<li>&#13;
<p>The file where the function is created must end with the <em>_test.go</em> suffix.<sup><a data-type="noteref" href="ch08.html#idm45606829538272" id="idm45606829538272-marker">1</a></sup></p>&#13;
</li>&#13;
<li>&#13;
<p>The function name must start with the case-sensitive <code>Benchmark</code> prefix, e.g., <code>BenchmarkSum</code>.</p>&#13;
</li>&#13;
<li>&#13;
<p>The function must have exactly one function argument of the type <code>*testing.B</code>.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In <a data-type="xref" href="ch07.html#ch-hw-complexity">“Complexity Analysis”</a>, we discussed the space complexity of the <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> code. In <a data-type="xref" href="ch10.html#ch-opt">Chapter 10</a>, I will show you how to optimize this code with a few different requirements. I wouldn’t be able to optimize those successfully without Go benchmarks. I used them to obtain estimated numbers for the number of allocations and latency. Let’s now see how that benchmarking process looks.</p>&#13;
<div data-type="tip"><h1>The Go Benchmark Naming Convention</h1>&#13;
<p><a data-primary="Go benchmarks" data-secondary="naming convention" data-type="indexterm" id="idm45606829528304"/>I try to follow the consistent naming pattern<sup><a data-type="noteref" href="ch08.html#idm45606829527136" id="idm45606829527136-marker">2</a></sup> for the <code>&lt;NAME&gt;</code> part on all types of functions in the Go testing framework, like benchmarks (<code>Benchmark&lt;NAME&gt;</code>), tests (<code>Test&lt;NAME&gt;</code>), fuzzing tests (<code>Fuzz&lt;NAME&gt;</code>), and examples (<code>Example&lt;NAME&gt;</code>). The idea is simple:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Calling a test <code>BenchmarkSum</code> means it tests the <code>Sum</code> function efficiency. <code>BenchmarkSum_withDuplicates</code> means the same, but the suffix (notice it starts with a lowercase letter) tells us a certain condition we test in.</p>&#13;
</li>&#13;
<li>&#13;
<p><code>BenchmarkCalculator_Sum</code> means it tests a method <code>Sum</code> from the <code>Calculator</code> struct. As above, we can add a suffix if we have more tests for the same method to distinguish between cases, e.g., <code>BenchmarkCalculator_Sum_withDuplicates</code>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Additionally, you can put an input size as yet another suffix e.g., <code>BenchmarkCalculator_Sum_10M</code>.</p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
&#13;
<p>Given that <code>Sum</code> in <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> is a single-purpose short function, one good &#13;
<span class="keep-together">microbenchmark</span> should suffice to tell its efficiency. So I created a new function in the <em>sum_test.go</em> file with the name <code>BenchmarkSum</code>. However, before I did anything else, I added the raw template of the small boilerplate required for most benchmarks, as presented in <a data-type="xref" href="#code-sum-go-bench-simple">Example 8-1</a>.</p>&#13;
<div data-type="example" id="code-sum-go-bench-simple">&#13;
<h5><span class="label">Example 8-1. </span>Core Go benchmark elements</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">func</code><code class="w"> </code><code class="nx">BenchmarkSum</code><code class="p">(</code><code class="nx">b</code><code class="w"> </code><code class="o">*</code><code class="nx">testing</code><code class="p">.</code><code class="nx">B</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">b</code><code class="p">.</code><code class="nx">ReportAllocs</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO1-1" id="co_benchmarking_CO1-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
&#13;
    </code><code class="c1">// TODO(bwplotka): Add any initialization that is needed.</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">b</code><code class="p">.</code><code class="nx">ResetTimer</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO1-2" id="co_benchmarking_CO1-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
    </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">b</code><code class="p">.</code><code class="nx">N</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO1-3" id="co_benchmarking_CO1-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
        </code><code class="c1">// TODO(bwplotka): Add tested functionality.</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO1-1" id="callout_benchmarking_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Optional <a href="https://oreil.ly/ootGE">method</a> that tells the Go benchmark to provide the number of allocations and the total amount of allocated memory. It’s equivalent to setting the <code>-benchmem</code> flag when running the test. While it might, in theory, add a tiny overhead to measured latency, it is only visible in very fast functions. I rarely need to remove allocation tracing in practice, so I always have it on. Often, it’s useful to see a number of allocations even if you expect the job to be only CPU sensitive. As mentioned in <a data-type="xref" href="ch05.html#ch-hw-mem">“Memory Relevance”</a>, some allocations can be surprising!</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO1-2" id="callout_benchmarking_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>In most cases, we don’t want to benchmark the resources required to initialize the test data, structure, or mocked dependencies. To do this “outside” of the latency clock and allocation tracking, <a href="https://oreil.ly/5et2N">reset the timer</a> right before the actual benchmark. If we don’t have any initialization, we can &#13;
<span class="keep-together">remove it.</span></p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO1-3" id="callout_benchmarking_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>This exact <code>for</code> loop sequence with <code>b.N</code> is a mandatory element of any Go benchmark. Never change it or remove it! Similarly, never use <code>i</code> from the loop for your function. It can be confusing at the start, but to run your benchmark, <code>go test</code> might run <code>BenchmarkSum</code> multiple times to find the right <code>b.N</code>, depending on how we run it. By default, <code>go test</code> will aim to run this benchmark for at least 1 second. This means it will execute our benchmark once with <code>b.N</code> that equals 1 m only to assess a single iteration duration. Based on that, it will try to find the smallest <code>b.N</code> that will make the whole <code>BenchmarkSum</code> execute at least 1 second.<sup><a data-type="noteref" href="ch08.html#idm45606829392480" id="idm45606829392480-marker">3</a></sup></p></dd>&#13;
</dl></div>&#13;
&#13;
<p>The <code>Sum</code> function I wanted to benchmark takes one argument—the filename containing a list of the integers to sum. As we discussed in <a data-type="xref" href="ch07.html#ch-hw-complexity">“Complexity Analysis”</a>, the algorithm used in <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> depends on the number of integers in the file. In this case, space and time complexity are <code>O(N)</code>, where <code>N</code> is a number of integers. This means that <code>Sum</code> with a single integer will be faster and allocate less memory than <code>Sum</code> with thousands of integers. As a result, the choice of input will significantly change the efficiency results. But how do we find the correct test input for our benchmark? Unfortunately, there is no single answer.</p>&#13;
<div data-type="note" epub:type="note"><h1>The Choice of Test Data and Conditions for Our Benchmarks</h1>&#13;
<p><a data-primary="benchmarks/benchmarking" data-secondary="choosing test data and conditions" data-type="indexterm" id="idm45606829383680"/>Generally, we want the smallest possible (thus quickest and cheapest to use!) dataset, which will give us enough knowledge and confidence in our program efficiency characteristic patterns. On the other hand, it should be big enough to trigger potential limits and bottlenecks that users might experience. As we mentioned in <a data-type="xref" href="ch07.html#ch-obs-rel-repro">“Reproducing Production”</a>, the test data should simulate the production workload as much as possible. We aim for &#13;
<span class="keep-together">“typicality.”</span></p>&#13;
&#13;
<p>However, if our functionality has a massive problem for specific input, we should also include that in our benchmarks!</p>&#13;
</div>&#13;
&#13;
<p>To make things more difficult, we are additionally constrained with the data size for microbenchmarks. Typically, we want to ensure those benchmarks can run at maximum within a matter of minutes and in our development environments for the best agility and shortest feedback loop possible. On the bright side, there are ways to find some efficiency pattern of your program, run benchmarks with a couple of times smaller dataset than the potential production dataset, and extrapolate the possible results.</p>&#13;
&#13;
<p>For example, on my machine it takes <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> about 78.4 ms to sum 2 million integers. If I benchmark with 1 million integers, it takes 30.5 ms. Given these two numbers, we could assume with some confidence<sup><a data-type="noteref" href="ch08.html#idm45606829378320" id="idm45606829378320-marker">4</a></sup> that our algorithm, on average, requires around 29 nanoseconds to sum a single integer.<sup><a data-type="noteref" href="ch08.html#idm45606829377392" id="idm45606829377392-marker">5</a></sup> If our RAER specifies, for example, that we have to sum 2 billion integers under 30 seconds, we can assume our implementation is too slow as 29 ns * 2 billion is around 58 seconds.</p>&#13;
&#13;
<p>For those reasons, I decided to stick with 2 million integers for the <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> benchmark. It is a big enough number to show some bottlenecks and efficiency patterns but small enough to keep our program relatively quick (on my machine, it can perform around 14 operations within 1 second.)<sup><a data-type="noteref" href="ch08.html#idm45606829374944" id="idm45606829374944-marker">6</a></sup> For now, I created a <em>testdata</em> directory (excluded from the compilation) and manually created a file called <em>test.2M.txt</em> with 2 million integers. With the test data and <a data-type="xref" href="#code-sum-go-bench-simple">Example 8-1</a>, I added the functionality I want to test, as presented in <a data-type="xref" href="#code-sum-go-bench">Example 8-2</a>.</p>&#13;
<div data-type="example" id="code-sum-go-bench">&#13;
<h5><span class="label">Example 8-2. </span>Simplest Go benchmark for assessing efficiency of the <code>Sum</code> function</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">func</code><code class="w"> </code><code class="nx">BenchmarkSum</code><code class="p">(</code><code class="nx">b</code><code class="w"> </code><code class="o">*</code><code class="nx">testing</code><code class="p">.</code><code class="nx">B</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w"/>&#13;
<code class="w">    </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">b</code><code class="p">.</code><code class="nx">N</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w"/>&#13;
<code class="w">        </code><code class="nx">_</code><code class="p">,</code><code class="w"> </code><code class="nx">_</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="nx">Sum</code><code class="p">(</code><code class="s">"testdata/test.2M.txt"</code><code class="p">)</code><code class="w"/>&#13;
<code class="w">    </code><code class="p">}</code><code class="w"/>&#13;
<code class="p">}</code><code class="w"/></pre></div>&#13;
&#13;
<p>To run this benchmark, we can use the <code>go test</code> command, which is available when we <a href="https://oreil.ly/dQ57t">install Go</a> on our machine. <code>go test</code> allows us to run all specified tests, fuzzing tests, or benchmarks. For benchmarks, <code>go test</code> has many options that allow us to control how it will execute our benchmark and what artifacts it will produce after a run. Let’s go through example options, presented in <a data-type="xref" href="#code-sum-go-bench-run">Example 8-3</a>.</p>&#13;
<div data-type="example" id="code-sum-go-bench-run">&#13;
<h5><span class="label">Example 8-3. </span>Example commands we can use to run <a data-type="xref" href="#code-sum-go-bench">Example 8-2</a></h5>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code>$</code><code> </code><code>go</code><code> </code><code class="nb">test</code><code> </code><code>-run</code><code> </code><code class="s1">'^$'</code><code> </code><code>-bench</code><code> </code><code class="s1">'^BenchmarkSum$'</code><code> </code><a class="co" href="#callout_benchmarking_CO2-1" id="co_benchmarking_CO2-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>$</code><code> </code><code>go</code><code> </code><code class="nb">test</code><code> </code><code>-run</code><code> </code><code class="s1">'^$'</code><code> </code><code>-bench</code><code> </code><code class="s1">'^BenchmarkSum$'</code><code> </code><code>-benchtime</code><code> </code><code>10s</code><code> </code><a class="co" href="#callout_benchmarking_CO2-2" id="co_benchmarking_CO2-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>$</code><code> </code><code>go</code><code> </code><code class="nb">test</code><code> </code><code>-run</code><code> </code><code class="s1">'^$'</code><code> </code><code>-bench</code><code> </code><code class="s1">'^BenchmarkSum$'</code><code> </code><code>-benchtime</code><code> </code><code>100x</code><code> </code><a class="co" href="#callout_benchmarking_CO2-3" id="co_benchmarking_CO2-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>$</code><code> </code><code>go</code><code> </code><code class="nb">test</code><code> </code><code>-run</code><code> </code><code class="s1">'^$'</code><code> </code><code>-bench</code><code> </code><code class="s1">'^BenchmarkSum$'</code><code> </code><code>-benchtime</code><code> </code><code>1s</code><code> </code><code>-count</code><code> </code><code class="m">5</code><code> </code><a class="co" href="#callout_benchmarking_CO2-4" id="co_benchmarking_CO2-4"><img alt="4" src="assets/4.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO2-1" id="callout_benchmarking_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>This command executes a single benchmark function with the explicit name <code>BenchmarkSum</code>. You can use the <a href="https://oreil.ly/KDIL9">RE2 regex language</a> to filter the tests you want to run. Notice the <code>-run</code> flag that strictly matches no functional test. This is to make sure no unit test will be run, allowing us to focus on the benchmark. Empty <code>-run</code> flags mean that all unit tests will be executed.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO2-2" id="callout_benchmarking_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>With <code>-benchtime</code>, we can control how long or how many iterations (functional operations) our benchmark should execute. In this example, we choose to have as many iterations as can fit in a 10-second interval.<sup><a data-type="noteref" href="ch08.html#idm45606829214608" id="idm45606829214608-marker">7</a></sup></p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO2-3" id="callout_benchmarking_CO2-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>We can choose to set <code>-benchtime</code> to the exact amount of iterations. This is used less often because, as a microbenchmark user, you want to focus on a quick feedback loop. When iterations are specified, we don’t know when the test will end and if we need to wait 10 seconds or 2 hours. This is why it’s often preferred to limit the benchmark time, and if we see too few iterations, increase the number in <code>-benchtime</code> a little, or change the benchmark implementation or test data.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO2-4" id="callout_benchmarking_CO2-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>We can also repeat the benchmark cycle with the <code>-count</code> flag. Doing so is very useful, as it allows us to calculate the variance between runs (with tools explained in <a data-type="xref" href="#ch-obs-micro-res">“Understanding the Results”</a>).</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>The full list of options is pretty long, and you can list them anytime using <a href="https://oreil.ly/F2wTM"><code>go help testflag</code></a>.</p>&#13;
<div data-type="note" epub:type="note"><h1>Running Go Benchmarks Through IDE</h1>&#13;
<p><a data-primary="Go benchmarks" data-secondary="running through IDE" data-type="indexterm" id="idm45606829173920"/><a data-primary="IDE (Integrated Development Environment), running Go benchmarks through" data-type="indexterm" id="idm45606829172944"/><a data-primary="Integrated Development Environment (IDE), running Go benchmarks through" data-type="indexterm" id="idm45606829172304"/>Almost all modern IDEs allow us to simply click on the Go benchmark function and execute it from the IDE. So feel free to do it. Just set up the correct options, or at least be aware of what options are there by default!</p>&#13;
&#13;
<p>I use the IDE to trigger initial, one-second benchmark runs, but I prefer good old CLI commands for more complex cases. They are easy to use and it’s easy to share the test run configuration with others. In the end, use what you feel the most comfortable with!</p>&#13;
</div>&#13;
&#13;
<p>For my <code>Sum</code> benchmark, I created a helpful one-liner with all the options I need, presented in <a data-type="xref" href="#code-sum-go-bench-all">Example 8-4</a>.</p>&#13;
<div data-type="example" id="code-sum-go-bench-all">&#13;
<h5><span class="label">Example 8-4. </span>One-line shell command to benchmark <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a></h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="err">$</code><code class="w"> </code><code class="nx">export</code><code class="w"> </code><code class="nx">ver</code><code class="p">=</code><code class="nx">v1</code><code class="w"> </code><code class="o">&amp;&amp;</code><code class="w"> </code><code class="err">\</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO3-1" id="co_benchmarking_CO3-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
    </code><code class="k">go</code><code class="w"> </code><code class="nx">test</code><code class="w"> </code><code class="o">-</code><code class="nx">run</code><code class="w"> </code><code class="err">'</code><code class="p">^</code><code class="err">$</code><code class="err">'</code><code class="w"> </code><code class="o">-</code><code class="nx">bench</code><code class="w"> </code><code class="err">'</code><code class="p">^</code><code class="nx">BenchmarkSum</code><code class="err">$</code><code class="err">'</code><code class="w"> </code><code class="o">-</code><code class="nx">benchtime</code><code class="w"> </code><code class="mi">10</code><code class="nx">s</code><code class="w"> </code><code class="o">-</code><code class="nx">count</code><code class="w"> </code><code class="mi">5</code><code class="w"> </code><code>\</code><code class="w">&#13;
</code><code class="w">        </code><code class="o">-</code><code class="nx">cpu</code><code class="w"> </code><code class="mi">4</code><code class="w"> </code><code class="err">\</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO3-2" id="co_benchmarking_CO3-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
        </code><code class="o">-</code><code class="nx">benchmem</code><code class="w"> </code><code class="err">\</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO3-3" id="co_benchmarking_CO3-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
        </code><code class="o">-</code><code class="nx">memprofile</code><code class="p">=</code><code class="err">$</code><code class="p">{</code><code class="nx">ver</code><code class="p">}</code><code class="p">.</code><code class="nx">mem</code><code class="p">.</code><code class="nx">pprof</code><code class="w"> </code><code class="o">-</code><code class="nx">cpuprofile</code><code class="p">=</code><code class="err">$</code><code class="p">{</code><code class="nx">ver</code><code class="p">}</code><code class="p">.</code><code class="nx">cpu</code><code class="p">.</code><code class="nx">pprof</code><code class="w"> </code><code class="err">\</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO3-4" id="co_benchmarking_CO3-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
    </code><code class="o">|</code><code class="w"> </code><code class="nx">tee</code><code class="w"> </code><code class="err">$</code><code class="p">{</code><code class="nx">ver</code><code class="p">}</code><code class="p">.</code><code class="nx">txt</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO3-5" id="co_benchmarking_CO3-5"><img alt="5" src="assets/5.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO3-1" id="callout_benchmarking_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>It is very tempting to write complex scripts or frameworks to save the result in the correct place, create automation that compares results for your use, etc. In many cases, that is a trap because Go benchmarks are typically ephemeral and easy to run. Still, I decided to add a tiny amount of bash scripting to ensure the artifacts my benchmark will produce have the same name I can refer to later. When I benchmark a new code version with optimizations, I can manually adjust the <code>ver</code> variable to different values like <code>v2</code>, <code>v3</code>, or <code>v2-with-streaming</code> for later comparisons.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO3-2" id="callout_benchmarking_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Sometimes if we aim to optimize latency via concurrent code, as in <a data-type="xref" href="ch10.html#ch-opt-latency-concurrency-example">“Optimizing Latency Using Concurrency”</a>, it is important to control the number &#13;
<span class="keep-together">of CPU</span> cores the benchmarks were allowed to use. This can be achieved with the <code>-cpu</code> flag. It sets the correct <code>GOMAXPROCS</code> setting. As we mentioned in <a data-type="xref" href="ch07.html#ch-obs-rel-unkn">“Performance Nondeterminism”</a>, the choice of the exact value highly depends on what the production environment looks like and how many CPUs your development machine has.<sup><a data-type="noteref" href="ch08.html#idm45606829011072" id="idm45606829011072-marker">8</a></sup></p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO3-3" id="callout_benchmarking_CO3-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>There is no point in optimizing latency if our optimization allocates an extreme amount of memory which, as we learned in <a data-type="xref" href="ch05.html#ch-hw-mem">“Memory Relevance”</a>, might be our first enemy. In my experience, the memory allocations cause more problems than CPU usage, so I always try to pay attention to allocations with &#13;
<span class="keep-together"><code>-benchmem</code>.</span></p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO3-4" id="callout_benchmarking_CO3-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>If you run your microbenchmark and see results you are not happy with, your first question is probably what caused that slowdown or high memory usage. This is why the Go benchmark has built-in support for profiling, explained in <a data-type="xref" href="ch09.html#ch-observability3">Chapter 9</a>. I am lazy, so I usually keep those options on by default, similar to &#13;
<span class="keep-together"><code>-benchtime</code>.</span> As a result, I can always dive into the profile to find the line of code that contributed to suspicious resource usage. Similar to <code>-benchtime</code> and &#13;
<span class="keep-together"><code>ReportAllocs</code>,</span> those are turned off by default because they add a slight overhead to latency measurements. However, it’s usually safe to leave them turned on unless you measure ultra-low latency operations (tens of nanoseconds). Especially the &#13;
<span class="keep-together"><code>-cpuprofile</code></span> option adds some allocations and latency in the &#13;
<span class="keep-together">background.</span></p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO3-5" id="callout_benchmarking_CO3-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>By default, <code>go test</code> prints results to standard output. However, to reliably compare and not get lost in what results correspond to what runs, I recommend saving them in temporary files. I recommend using <code>tee</code> to write both to file and standard output, so you can follow the progress of the benchmark.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p class="less_space pagebreak-before">With the benchmark implementation, input file, and execution command, it’s time to perform our benchmark. I executed <a data-type="xref" href="#code-sum-go-bench-all">Example 8-4</a> in the directory of the test file on my machine, and after 32 seconds, it finished. It created three files: <em>v1.cpu.pprof</em>, <em>v1.mem.pprof</em>, and <em>v1.txt</em>. In this chapter, we are most interested in the last file, so you can learn how to read and understand the Go benchmark output. Let’s do that in the next section.<a data-startref="ix_ch08-asciidoc3" data-type="indexterm" id="idm45606828991600"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Understanding the Results" data-type="sect2"><div class="sect2" id="ch-obs-micro-res">&#13;
<h2>Understanding the Results</h2>&#13;
&#13;
<p><a data-primary="Go benchmarks" data-secondary="understanding results" data-type="indexterm" id="ix_ch08-asciidoc5"/><a data-primary="microbenchmarks/microbenchmarking" data-secondary="tips and tricks for" data-type="indexterm" id="ix_ch08-asciidoc6"/><a data-primary="microbenchmarks/microbenchmarking" data-secondary="understanding results" data-type="indexterm" id="ix_ch08-asciidoc7"/>After each run, the <code>go test</code> benchmark prints the result in a consistent format.<sup><a data-type="noteref" href="ch08.html#idm45606828985008" id="idm45606828985008-marker">9</a></sup> <a data-type="xref" href="#code-sum-go-bench-out">Example 8-5</a> presents the output runs executed with <a data-type="xref" href="#code-sum-go-bench-all">Example 8-4</a> on the code presented in <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a>.</p>&#13;
<div data-type="example" id="code-sum-go-bench-out">&#13;
<h5><span class="label">Example 8-5. </span>The output of the <em>v1.txt</em> file produced by the <a data-type="xref" href="#code-sum-go-bench-all">Example 8-4</a> command</h5>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting"><code>goos: linux </code><a class="co" href="#callout_benchmarking_CO4-1" id="co_benchmarking_CO4-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
goarch: amd64&#13;
pkg: github.com/efficientgo/examples/pkg/sum&#13;
cpu: Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz&#13;
BenchmarkSum-4    67    79043706 ns/op    60807308 B/op    1600006 allocs/op </code><a class="co" href="#callout_benchmarking_CO4-2" id="co_benchmarking_CO4-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
BenchmarkSum-4    74    79312463 ns/op    60806508 B/op    1600006 allocs/op&#13;
BenchmarkSum-4    66    80477766 ns/op    60806472 B/op    1600006 allocs/op&#13;
BenchmarkSum-4    66    80010618 ns/op    60806224 B/op    1600006 allocs/op&#13;
BenchmarkSum-4    74    80793880 ns/op    60806445 B/op    1600006 allocs/op&#13;
PASS&#13;
ok     github.com/efficientgo/examples/pkg/sum    38.214s</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO4-1" id="callout_benchmarking_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Every benchmark run captures some basic information about the environment like architecture, operating system type, the package we run the benchmark in, and the CPU on the machine. Unfortunately, as we discussed in <a data-type="xref" href="ch07.html#ch-obs-rel">“Reliability of Experiments”</a>, there are many more elements that could be worth capturing<sup><a data-type="noteref" href="ch08.html#idm45606828962032" id="idm45606828962032-marker">10</a></sup> that can impact the benchmark.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO4-2" id="callout_benchmarking_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Every row represents a single run (i.e., if you ran the benchmark with <code>-count=1</code>, you would have just a single line). The line consists of three or more columns. The number depends on the benchmark configuration, but the order is consistent. From the left, we have:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Name of the benchmark with the suffix representing the number of CPUs available (in theory<sup><a data-type="noteref" href="ch08.html#idm45606828942480" id="idm45606828942480-marker">11</a></sup>) for this benchmark. This tells us what we can expect for concurrent implementations.</p>&#13;
</li>&#13;
<li>&#13;
<p>Number of iterations in this benchmark run. Pay attention to this number; if it’s too low, the numbers in the other columns might not reflect reality.</p>&#13;
</li>&#13;
<li>&#13;
<p>Nanoseconds per operation resulting from <code>-benchtime</code> divided by a number of runs.</p>&#13;
</li>&#13;
<li>&#13;
<p>Allocated bytes per operation on the heap. As you learned in <a data-type="xref" href="ch05.html#ch-hardware2">Chapter 5</a>, remember that this does not tell us how much memory is allocated in any other segments, like manual mappings, caches, and stack! This column is present only if the <code>-benchmem</code> flag was set (or <code>ReportAllocs</code>).</p>&#13;
</li>&#13;
<li>&#13;
<p>Number of allocations per operation on the heap (also only present with the <code>-benchmem</code> flag set).</p>&#13;
</li>&#13;
<li>&#13;
<p>Optionally, you can report your own metrics per operation using the <code>b.ReportMetric</code> method. See this <a href="https://oreil.ly/IuwYl">example</a>. This will appear as further columns and can be aggregated similarly with the tooling explained later.</p>&#13;
</li>&#13;
</ul></dd>&#13;
</dl></div>&#13;
<div data-type="tip">&#13;
<p>If you run <a data-type="xref" href="#code-sum-go-bench-all">Example 8-4</a> and you see no output for a long time, it might mean that the first run of your microbenchmark is taking that long. If your <code>-benchtime</code> is time based, the <code>go test</code> quickly checks how long it takes to run a single iteration to find the estimated number of iterations.</p>&#13;
&#13;
<p>If it takes too much time, unless you want to run 30+ minute tests, you might need to optimize the benchmark setup, reduce the data size, or split the microbenchmark into smaller functionality. Otherwise, you won’t achieve hundreds or dozens of required &#13;
<span class="keep-together">iterations.</span></p>&#13;
&#13;
<p>If you see the initial output (<code>goos</code>, <code>goarch</code>, <code>pkg</code>, and benchmark name), a single iteration run has completed, and a proper benchmark has started.</p>&#13;
</div>&#13;
&#13;
<p>The results presented in <a data-type="xref" href="#code-sum-go-bench-out">Example 8-5</a> can be read directly, but there are some challenges. First of all, the numbers are in the base unit—it’s not obvious at first glance to see if we allocate 600 MB, 60 MB, or 6 MB. It’s the same if we translate our latency to seconds. Secondly, we have five measurements, so which one do we choose? Finally, how do we compare a second microbenchmark result done for the code with the optimization?</p>&#13;
&#13;
<p><a data-primary="benchstat tool" data-type="indexterm" id="ix_ch08-asciidoc8"/>Fortunately, the Go community created another CLI tool, <a href="https://oreil.ly/PWSN4"><code>benchstat</code></a>, that performs further processing and statistical analysis of one or multiple benchmark results for easier assessment. As a result, it has become the most popular solution for presenting and interpreting Go microbenchmark results in recent years.</p>&#13;
&#13;
<p>You can install <code>benchstat</code> using the standard <code>go install</code> tooling, for example, <code>go install golang.org/x/perf/cmd/benchstat@latest</code>. Once completed, it will be present in your $GOBIN or <em>$GOPATH/bin</em> directory. You can then use it to present the results we got in <a data-type="xref" href="#code-sum-go-bench-out">Example 8-5</a>; see the example usage in <a data-type="xref" href="#code-sum-go-bench-benchstat">Example 8-6</a>.</p>&#13;
<div data-type="example" id="code-sum-go-bench-benchstat">&#13;
<h5><span class="label">Example 8-6. </span>Running <code>benchstat</code> on the results presented in <a data-type="xref" href="#code-sum-go-bench-out">Example 8-5</a></h5>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting"><code>$ benchstat v1.txt </code><a class="co" href="#callout_benchmarking_CO5-1" id="co_benchmarking_CO5-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
name   time/op&#13;
Sum-4  79.9ms ± 1% </code><a class="co" href="#callout_benchmarking_CO5-2" id="co_benchmarking_CO5-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
&#13;
name   alloc/op&#13;
Sum-4  60.8MB ± 0%&#13;
&#13;
name   allocs/op&#13;
Sum-4   1.60M ± 0%</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO5-1" id="callout_benchmarking_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>We can run <code>benchstat</code> with the <em>v1.txt</em> containing <a data-type="xref" href="#code-sum-go-bench-out">Example 8-5</a>. The <code>benchstat</code> can parse the format of the <code>go test</code> tooling from one or multiple benchmarks performed once or multiple times on the same code version.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO5-2" id="callout_benchmarking_CO5-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>For each benchmark, <code>benchstat</code> calculates the mean (average) of all runs and <code>±</code> the variance across runs (1% in this case). This is why it’s essential to run <code>go test</code> benchmarks multiple times (e.g., with the <code>-count</code> flag); otherwise, with just a single run, the variance will indicate a misleading 0%. Running more tests allows us to assess the repeatability of the result, as we discussed in <a data-type="xref" href="ch07.html#ch-obs-rel-unkn">“Performance Nondeterminism”</a>. Run <code>benchstat --help</code> to see more options.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Once we have confidence in our test run, we can call it baseline results. We typically want to assess the efficiency of our code with the new optimization by comparing it with our baseline. For example, in <a data-type="xref" href="ch10.html#ch-opt">Chapter 10</a> we will optimize the <code>Sum</code>, and one of the optimized versions will be twice as fast. I found this by changing the <code>Sum</code> function visible in <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> to <code>ConcurrentSum3</code> (the code is presented in <a data-type="xref" href="ch10.html#code-sum-concurrent3">Example 10-12</a>). Then I ran the benchmark implemented in <a data-type="xref" href="#code-sum-go-bench">Example 8-2</a> using exactly the same command shown in <a data-type="xref" href="#code-sum-go-bench-all">Example 8-4</a>, just changing <code>ver=v1</code> to <code>ver=v2</code> to produce <em>v2.txt</em> and <em>v2.cpu.pprof</em> and <em>v2.mem.pprof</em>.</p>&#13;
&#13;
<p>The <code>benchstat</code> helped us calculate variance and provided human-readable units. But there is another helpful feature: comparing results from different benchmark runs. For example, <a data-type="xref" href="#code-sum-go-bench-benchstat2">Example 8-7</a> shows how I checked the difference between the naive and improved concurrent implementation.</p>&#13;
<div data-type="example" id="code-sum-go-bench-benchstat2">&#13;
<h5><span class="label">Example 8-7. </span>Running <code>benchstat</code> to compare results from &#13;
<span class="plain">v1.txt</span> and &#13;
<span class="plain">v2.txt</span></h5>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting"><code>$ benchstat v1.txt v2.txt </code><a class="co" href="#callout_benchmarking_CO6-1" id="co_benchmarking_CO6-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
name   old time/op    new time/op    delta&#13;
Sum-4    79.9ms ± 1%    39.5ms ± 2%  -50.52%  (p=0.008 n=5+5) </code><a class="co" href="#callout_benchmarking_CO6-2" id="co_benchmarking_CO6-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
&#13;
name   old alloc/op   new alloc/op   delta&#13;
Sum-4    60.8MB ± 0%    60.8MB ± 0%     ~     (p=0.151 n=5+5)&#13;
&#13;
name   old allocs/op  new allocs/op  delta&#13;
Sum-4     1.60M ± 0%     1.60M ± 0%   +0.00%  (p=0.008 n=5+5)</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO6-1" id="callout_benchmarking_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Running <code>benchstat</code> with two files enables comparison mode.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO6-2" id="callout_benchmarking_CO6-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>In comparison mode, <code>benchstat</code> provides a delta column showing the delta between two means in a percentage or <code>~</code> if the significance test fails. The significance test is defaulted to the <a href="https://oreil.ly/ESCAz">Mann-Whitney U test</a> and can be disabled with <code>-delta-test=none</code>. The significance test is an extra statistical analysis that calculates the <a href="https://oreil.ly/6K0zl">p-value</a>, which by default should be smaller than <code>0.05</code> (configurable with <code>-alpha</code>). It gives us additional information on top of the variance (after <code>±</code>) if the results can be safely compared. The <code>n=5+5</code> represents the sample sizes in both results (both benchmark runs were done with <code>-count=5</code>).</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Thanks to <code>benchstat</code> and Go benchmarks, we can tell with some confidence that our concurrent implementation is around 50% faster and does not impact allocations.</p>&#13;
<div data-type="tip">&#13;
<p>Careful readers might notice that the allocation size failed the significance test of <code>benchstat</code> (<code>p</code> is higher than 0.05). I could improve that by running benchmarks with a higher <code>-count</code> (e.g., 8 or 10).</p>&#13;
&#13;
<p>I left this significance test failing on purpose to show you that there are cases when you can apply common reasoning. Both results indicate large 60.8 MB allocations with minimal variance. We can clearly say that both implementations use a similar amount of memory. Do we care whether one implementation uses a few KB more or less? Probably not, so we can skip the <code>benchstat</code> significance test that verifies if we can trust the delta. No need to spend more time here than needed!</p>&#13;
</div>&#13;
&#13;
<p>Analyzing microbenchmarks might be confusing initially, but hopefully, the presented flow using <code>benchstat</code> taught you how to assess efficiencies of different implementations without having a degree in data science! Generally, while using <code>benchstat</code>, remember to:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Run more tests than one (<code>-count</code>) to be able to spot the noise.</p>&#13;
</li>&#13;
<li>&#13;
<p>Check that the variance number after <code>±</code> is not higher than 3–5%. Be especially vigilant in variance for smaller numbers.</p>&#13;
</li>&#13;
<li>&#13;
<p>To rely on an accurate delta across results with higher variance, check the significance test (p-value)<a data-startref="ix_ch08-asciidoc8" data-type="indexterm" id="idm45606828810768"/>.<a data-startref="ix_ch08-asciidoc7" data-type="indexterm" id="idm45606828809936"/><a data-startref="ix_ch08-asciidoc6" data-type="indexterm" id="idm45606828809232"/><a data-startref="ix_ch08-asciidoc5" data-type="indexterm" id="idm45606828808560"/></p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>With this in mind, let’s go through a few common advanced tricks that you might find very useful in your day-to-day work with Go benchmarks!<a data-startref="ix_ch08-asciidoc2" data-type="indexterm" id="idm45606828807088"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tips and Tricks for Microbenchmarking" data-type="sect1"><div class="sect1" id="idm45606829569296">&#13;
<h1>Tips and Tricks for Microbenchmarking</h1>&#13;
&#13;
<p>The best practices for microbenchmarking are often learned from your own mistakes and rarely shared with others. Let’s break that up by mentioning some of the common aspects of Go microbenchmarks that are worth being aware of.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Too-High Variance" data-type="sect2"><div class="sect2" id="ch-obs-micro-high-var">&#13;
<h2>Too-High Variance</h2>&#13;
&#13;
<p><a data-primary="microbenchmarks/microbenchmarking" data-secondary="too-high variance" data-type="indexterm" id="idm45606828803344"/><a data-primary="variance, microbenchmarking and" data-type="indexterm" id="idm45606828802352"/>As we learned in <a data-type="xref" href="ch07.html#ch-obs-rel-unkn">“Performance Nondeterminism”</a>, knowing the variance of our tests is critical. If the difference between microbenchmarks is more than, let’s say, 5%, it indicates potential noise, and we might not be able to rely on those results entirely.</p>&#13;
&#13;
<p>I had this case when preparing <a data-type="xref" href="ch10.html#ch-opt-latency-concurrency-example">“Optimizing Latency Using Concurrency”</a>. When benchmarking, my results had way too large a variance as the <code>benchstat</code> result suggested. The results from that run are presented in <a data-type="xref" href="#code-sum-go-bench-benchstat-unr">Example 8-8</a>.</p>&#13;
<div data-type="example" id="code-sum-go-bench-benchstat-unr">&#13;
<h5><span class="label">Example 8-8. </span><code>benchstat</code> indicating large variance in latency results</h5>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting"><code>name   time/op&#13;
Sum-4  45.7ms ±19% </code><a class="co" href="#callout_benchmarking_CO7-1" id="co_benchmarking_CO7-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
&#13;
name   alloc/op&#13;
Sum-4  60.8MB ± 0%&#13;
&#13;
name   allocs/op&#13;
Sum-4   1.60M ± 0%</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO7-1" id="callout_benchmarking_CO7-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Nineteen percent variance is quite scary. We should ignore such results and stabilize the benchmark before making any conclusions.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>What can we do in this case? We already mentioned a few things in <a data-type="xref" href="ch07.html#ch-obs-rel-unkn">“Performance Nondeterminism”</a>. We should consider running the benchmark longer, redesigning our benchmark, or running it in different environmental conditions. In my case I had to close my browser and increase <code>-benchtime</code> from 5 s to 15 s to achieve the 2% variance run in <a data-type="xref" href="#code-sum-go-bench-benchstat2">Example 8-7</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Find Your Workflow" data-type="sect2"><div class="sect2" id="ch-obs-micro-workflow">&#13;
<h2>Find Your Workflow</h2>&#13;
&#13;
<p><a data-primary="microbenchmarks/microbenchmarking" data-secondary="finding your workflow" data-type="indexterm" id="ix_ch08-asciidoc9"/><a data-primary="workflow, microbenchmarking" data-type="indexterm" id="ix_ch08-asciidoc10"/>In <a data-type="xref" href="#ch-obs-micro-go">“Go Benchmarks”</a>, you followed me through my efficiency assessment cycle on a micro level. Of course, this can vary, but it is generally based on <code>git</code> branches, and can be summarized as follows:</p>&#13;
<ol>&#13;
<li>&#13;
<p>I check for any existing microbenchmark implementation for what I want to test. If none exists, I will create one.</p>&#13;
</li>&#13;
<li>&#13;
<p>In my terminal, I execute a command similar to <a data-type="xref" href="#code-sum-go-bench-all">Example 8-4</a> to run the benchmark several times (5–10). I save results to something like <em>v1.txt</em>, save profiles, and assume that as my baseline.</p>&#13;
</li>&#13;
<li>&#13;
<p>I assess the <em>v1.txt</em> results to check if the resource consumption is roughly what I expect from my understanding of the implementation and the input size. To confirm or reject, I perform the bottleneck analysis explained in <a data-type="xref" href="ch09.html#ch-observability3">Chapter 9</a>. I might perform more benchmarks for different inputs at this stage to learn more. This tells me roughly if there is room for some easy optimizations, should I invest in more dangerous and deliberate optimization, or should I move to optimizations on a different level.</p>&#13;
</li>&#13;
<li>&#13;
<p>Assuming room for some optimizations, I create a new <a href="https://oreil.ly/AcM1D"><code>git</code> branch</a> and implement it.</p>&#13;
</li>&#13;
<li>&#13;
<p>Following the TFBO flow, I test my implementation first.</p>&#13;
</li>&#13;
<li>&#13;
<p>I commit the changes, run the benchmarking function with the same command, and save it to, e.g., <em>v2.txt</em>.</p>&#13;
</li>&#13;
<li>&#13;
<p>I compare the results with <code>benchstat</code> and adjust the benchmark or optimizations to achieve the best results.</p>&#13;
</li>&#13;
<li>&#13;
<p>If I want to try a different optimization, I create yet another <code>git</code> branch or build new commits on the same branch and repeat the process (e.g., produce <em>v3.txt</em>, <em>v4.txt</em>, and so on). This allows me to get back to previous optimizations if an attempt makes me pessimistic.</p>&#13;
</li>&#13;
<li>&#13;
<p>I jot findings in my notes, commit message, or repository change set (e.g., pull requests), and discard my <em>.txt</em> results (expiration date!).</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>This flow works for me, but you might want to try a different one! As long as it’s not confusing for you, is reliable, and follows the TFBO pattern we discussed in <a data-type="xref" href="ch03.html#ch-conq-eff-flow">“Efficiency-Aware Development Flow”</a>, use it. There are many other options, for example:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>You can use your terminal history to track benchmarking results.</p>&#13;
</li>&#13;
<li>&#13;
<p>You can create different functions for the same functionality with different optimizations. Then you can swap what function you use in your benchmark functions if you don’t want to use <code>git</code> here.</p>&#13;
</li>&#13;
<li>&#13;
<p>Use <code>git stash</code> instead of commits.</p>&#13;
</li>&#13;
<li>&#13;
<p>Finally, you can follow the <a href="https://oreil.ly/1MJNT">Dave Cheney flow</a> that uses the <code>go test -c</code> command to build the testing framework and your code into a separate binary. You can then save this binary and perform benchmarks without rebuilding source code or saving your test results.<sup><a data-type="noteref" href="ch08.html#idm45606828737680" id="idm45606828737680-marker">12</a></sup></p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>I would propose trying different flows and learning what helps you the most!</p>&#13;
<div data-type="note" epub:type="note">&#13;
<p>I would suggest avoiding writing too complex automation for our local microbenchmarking workflow (e.g., complex bash script to automate some steps). Microbenchmarks are meant to be more interactive, where you can manually dig information you care for. Writing complex automation might mean more overhead and a longer feedback loop than needed. Still, if this is working for you, do it!<a data-startref="ix_ch08-asciidoc10" data-type="indexterm" id="idm45606828734240"/><a data-startref="ix_ch08-asciidoc9" data-type="indexterm" id="idm45606828733536"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Test Your Benchmark for Correctness!" data-type="sect2"><div class="sect2" id="ch-obs-micro-corr">&#13;
<h2>Test Your Benchmark for Correctness!</h2>&#13;
&#13;
<p><a data-primary="microbenchmarks/microbenchmarking" data-secondary="testing for correctness" data-type="indexterm" id="ix_ch08-asciidoc11"/><a data-primary="testing" data-secondary="microbenchmarks" data-type="indexterm" id="ix_ch08-asciidoc12"/>One of the most common mistakes we make in benchmarking is assessing the efficiency of the function that does not provide correct results. Due to the nature of deliberate optimizations, it is easy to introduce a bug that breaks the functionality of our code. Sometimes, optimizing failed executions is important,<sup><a data-type="noteref" href="ch08.html#idm45606828728032" id="idm45606828728032-marker">13</a></sup> but it should be an explicit decision.</p>&#13;
&#13;
<p>The “Testing” part in TFBO, explained in <a data-type="xref" href="ch03.html#ch-conq-eff-flow">“Efficiency-Aware Development Flow”</a>, is not there by mistake. Our priority should be to write a unit test for the same functionality we will benchmark. An example unit test for our <code>Sum</code> function can look like <a data-type="xref" href="#code-sum-go-test">Example 8-9</a>.</p>&#13;
<div data-type="example" id="code-sum-go-test">&#13;
<h5><span class="label">Example 8-9. </span>Example unit test to assess the correctness of the <code>Sum</code> function</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="c1">// import "github.com/efficientgo/core/testutil"</code><code class="w"/>&#13;
&#13;
<code class="kd">func</code><code class="w"> </code><code class="nx">TestSum</code><code class="p">(</code><code class="nx">t</code><code class="w"> </code><code class="o">*</code><code class="nx">testing</code><code class="p">.</code><code class="nx">T</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w"/>&#13;
<code class="w">    </code><code class="nx">ret</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">Sum</code><code class="p">(</code><code class="s">"testdata/input.txt"</code><code class="p">)</code><code class="w"/>&#13;
<code class="w">    </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">t</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="p">)</code><code class="w"/>&#13;
<code class="w">    </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Equals</code><code class="p">(</code><code class="nx">t</code><code class="p">,</code><code class="w"> </code><code class="mi">3110800</code><code class="p">,</code><code class="w"> </code><code class="nx">ret</code><code class="p">)</code><code class="w"/>&#13;
<code class="p">}</code><code class="w"/></pre></div>&#13;
&#13;
<p>Having the unit test ensures that with the right CI configured, when we propose our change to the main repository (perhaps via a <a href="https://oreil.ly/r24MR">pull request</a> [PR]), we will notice if our code is correct or not. So this already improves the reliability of our optimization job.</p>&#13;
&#13;
<p>However, there are still things we could do to improve this process. If you only test as the last development step, you might have already performed all the effort of benchmarking and optimizing without realizing that the code is broken. This can be mitigated by manually running the unit test in <a data-type="xref" href="#code-sum-go-bench-test">Example 8-10</a> before each benchmarking run, e.g., the <a data-type="xref" href="#code-sum-go-bench">Example 8-2</a> code. This helps, but there are still some slight &#13;
<span class="keep-together">problems:</span></p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>It is tedious to run yet another thing after our changes. So it’s too tempting to skip that manual process of running functional tests after the change to save time and achieve an even quicker feedback loop.</p>&#13;
</li>&#13;
<li>&#13;
<p>The function might be well tested in the unit test, but there are differences between how you invoke your function in the unit test and the benchmark.</p>&#13;
</li>&#13;
<li>&#13;
<p>Additionally, as you learned in <a data-type="xref" href="ch07.html#ch-obs-bench-intro-fun">“Comparison to Functional Testing”</a>, for benchmarks we need different inputs. A new thing means a new place for making an error! For example, when preparing the benchmark for this book in <a data-type="xref" href="#code-sum-go-bench">Example 8-2</a>, I accidentally made a typo in the filename (<em>testdata/test2M.txt</em> instead of <em>testdata/test.2M.txt</em>). When I ran my benchmark, it passed with very low latency results. Turns out the <code>Sum</code> did not work other than failing with the file does not exist error. Because in <a data-type="xref" href="#code-sum-go-bench">Example 8-2</a> I ignored all errors for simplicity, I missed that information. Only intuition told me that my benchmark ran a bit too quickly to be true, so I double-checked what <code>Sum</code> actually returned.</p>&#13;
</li>&#13;
<li>&#13;
<p>During benchmarking at higher load, new errors might appear. For example, perhaps we could not open another file due to the limit of file descriptors on the machine, or our code does not clean files on disk, so we can’t write changes to the file due to a lack of disk space.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Fortunately, an easy solution to that problem is adding a quick error check to the benchmark iteration. It could look like <a data-type="xref" href="#code-sum-go-bench-test">Example 8-10</a>.</p>&#13;
<div data-type="example" id="code-sum-go-bench-test">&#13;
<h5><span class="label">Example 8-10. </span>Go benchmark for assessing the efficiency of the <code>Sum</code> function with error check</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">func</code><code class="w"> </code><code class="nx">BenchmarkSum</code><code class="p">(</code><code class="nx">b</code><code class="w"> </code><code class="o">*</code><code class="nx">testing</code><code class="p">.</code><code class="nx">B</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">b</code><code class="p">.</code><code class="nx">N</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">       </code><code class="nx">_</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">Sum</code><code class="p">(</code><code class="s">"testdata/test.2M.txt"</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">b</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO8-1" id="co_benchmarking_CO8-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
    </code><code class="p">}</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO8-1" id="callout_benchmarking_CO8-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Asserting <code>Sum</code> does not return an error on every iteration loop.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>It’s important to notice that the efficiency metrics we get after the benchmark will include the latency contributed by the <code>testutil.Ok(b, err)</code> invocation,<sup><a data-type="noteref" href="ch08.html#idm45606828555728" id="idm45606828555728-marker">14</a></sup> even if there is no error. This is because we invoke this function in our <code>b.N</code> loop, so it adds a certain overhead.</p>&#13;
&#13;
<p>Should we accept this overhead? This is the same question we have about including &#13;
<span class="keep-together"><code>-benchmem</code></span> and profile generation for tests, which also can add small noise. Such overhead is unacceptable if we try to benchmark very fast operations (let’s say under milliseconds fast). For the majority of benchmarks, however, such an assertion will not change your benchmarking results. One would even argue that such error assertion will exist in production, so it should be included in the efficiency assessment.<sup><a data-type="noteref" href="ch08.html#idm45606828553248" id="idm45606828553248-marker">15</a></sup> Similar to <code>-benchmem</code> and profiles, I add that assertion to almost all microbenchmarks I work with.</p>&#13;
&#13;
<p>In some ways, we are still prone to mistakes. Perhaps with the large input, the <code>Sum</code> function does not provide a correct answer without returning an error. As with all testing, we will never stop all mistakes—there has to be a balance between the effort of writing, executing, and maintaining extra tests and confidence. It’s up to you to decide how much you trust your workflow.</p>&#13;
&#13;
<p>If you want to choose the preceding case for more confidence, you can add a check that compares the returned sum with the expected result. In our case, it will not be a big overhead to add <code>testutil.Equals(t, &lt;expected number&gt;, ret)</code>, but usually it is more expensive and thus inappropriate to add for microbenchmarks. For those purposes, I created a small <a href="https://oreil.ly/wMX6O"><code>testutil.TB</code> object</a> that allows you to run a single iteration of your microbenchmark for unit test purposes. This allows it to be always up-to-date in terms of correctness, which is especially &#13;
<span class="keep-together">challenging</span> in bigger shared code repositories. For example, continuous testing of our <code>Sum</code> benchmark could look like <a data-type="xref" href="#code-sum-go-bench-test2">Example 8-11</a>.<sup><a data-type="noteref" href="ch08.html#idm45606828507296" id="idm45606828507296-marker">16</a></sup></p>&#13;
<div data-type="example" id="code-sum-go-bench-test2">&#13;
<h5><span class="label">Example 8-11. </span>Testable Go benchmark for assessing the efficiency of the <code>Sum</code> function</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">func</code><code class="w"> </code><code class="nx">TestBenchSum</code><code class="p">(</code><code class="nx">t</code><code class="w"> </code><code class="o">*</code><code class="nx">testing</code><code class="p">.</code><code class="nx">T</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">benchmarkSum</code><code class="p">(</code><code class="nx">testutil</code><code class="p">.</code><code class="nx">NewTB</code><code class="p">(</code><code class="nx">t</code><code class="p">)</code><code class="p">)</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">BenchmarkSum</code><code class="p">(</code><code class="nx">b</code><code class="w"> </code><code class="o">*</code><code class="nx">testing</code><code class="p">.</code><code class="nx">B</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">benchmarkSum</code><code class="p">(</code><code class="nx">testutil</code><code class="p">.</code><code class="nx">NewTB</code><code class="p">(</code><code class="nx">b</code><code class="p">)</code><code class="p">)</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">benchmarkSum</code><code class="p">(</code><code class="nx">tb</code><code class="w"> </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">TB</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO9-1" id="co_benchmarking_CO9-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
    </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">tb</code><code class="p">.</code><code class="nx">N</code><code class="p">(</code><code class="p">)</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO9-2" id="co_benchmarking_CO9-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
        </code><code class="nx">ret</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">Sum</code><code class="p">(</code><code class="s">"testdata/test.2M.txt"</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">tb</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">        </code><code class="k">if</code><code class="w"> </code><code class="p">!</code><code class="nx">tb</code><code class="p">.</code><code class="nx">IsBenchmark</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">            </code><code class="c1">// More expensive result checks can be here.</code><code class="w">&#13;
</code><code class="w">            </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Equals</code><code class="p">(</code><code class="nx">tb</code><code class="p">,</code><code class="w"> </code><code class="nb">int64</code><code class="p">(</code><code class="mi">6221600000</code><code class="p">)</code><code class="p">,</code><code class="w"> </code><code class="nx">ret</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO9-3" id="co_benchmarking_CO9-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
        </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO9-1" id="callout_benchmarking_CO9-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p><code>testutil.TB</code> is an interface that allows running a function as both benchmarks and a unit test. Furthermore, it allows us to design our code, so the same benchmark is executed by other functions, e.g., with extra profiling, as shown in <a data-type="xref" href="ch10.html#code-sum-go-bench-fgprof">Example 10-2</a>.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO9-2" id="callout_benchmarking_CO9-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The <code>tb.N()</code> method returns <code>b.N</code> for the benchmark, allowing normal &#13;
<span class="keep-together">microbenchmark</span> execution. It returns <code>1</code> to perform one test run for unit tests.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO9-3" id="callout_benchmarking_CO9-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>We can now put the extra code that might be more expensive (e.g., more complex test assertions) in the space unreachable for benchmarks, thanks to the <code>tb.IsBenchmark()</code> method.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>To sum up, please test your microbenchmark code. It will save you and your team time in the long run. On top of that, it can provide a natural countermeasure against unwanted compiler optimizations, explained in <a data-type="xref" href="#ch-obs-micro-comp">“Compiler Optimizations Versus Benchmark”</a>.<a data-startref="ix_ch08-asciidoc12" data-type="indexterm" id="idm45606828291664"/><a data-startref="ix_ch08-asciidoc11" data-type="indexterm" id="idm45606828291056"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Sharing Benchmarks with the Team (and Your Future Self)" data-type="sect2"><div class="sect2" id="ch-obs-micro-share">&#13;
<h2>Sharing Benchmarks with the Team (and Your Future Self)</h2>&#13;
&#13;
<p><a data-primary="microbenchmarks/microbenchmarking" data-secondary="sharing with team" data-type="indexterm" id="ix_ch08-asciidoc13"/><a data-primary="team, sharing microbenchmarks with" data-type="indexterm" id="ix_ch08-asciidoc14"/>Once you finish your TFBO cycle and are happy with your next optimization iteration, it’s time to commit to new code. Share what you found or achieved with your team for more than your small one-person project. When someone proposes an optimization change, it’s not uncommon to see the optimization in the production code and only a small description: “I benchmarked it, and it was 30% faster.” This is not ideal for multiple reasons:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>It’s hard for the reviewer to validate the benchmark without seeing the actual microbenchmark code you use. It’s not that reviewers should not trust that you tell the truth, but rather it’s easy to make a mistake, forget a side effect, or benchmark wrongly.<sup><a data-type="noteref" href="ch08.html#idm45606828284848" id="idm45606828284848-marker">17</a></sup> For example, the input has to be of a certain size to trigger the problem, or the input does not reflect the expected use cases. This can only be validated by another person looking at your benchmarking code. It’s especially important when we work remotely with the team and in  open source projects, where strong communication is essential.</p>&#13;
</li>&#13;
<li>&#13;
<p>Once merged, it’s likely any other change that touches this code might accidentally introduce efficiency regression.</p>&#13;
</li>&#13;
<li>&#13;
<p>If you or anyone else wants to try to improve the same part of code, they have no other option than to re-create the benchmark and go through the same effort you did in your pull request because the previous benchmark implementation is gone (or stored on your machine).</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The solution here is to provide as much context as possible on your experiment details, input, and implementation of the benchmark. Of course, we can provide that in some form of documentation (e.g., in the description of the pull report), but there is nothing better than committing the actual microbenchmark next to your production code! In practice, however, it isn’t so simple. Some extra pieces are worth adding before sharing the microbenchmark with others.</p>&#13;
&#13;
<p>I optimized our <code>Sum</code> function and explained my benchmarking process. However, you don’t want to write an entire chapter to explain the optimization you made to your team (and your future self)! Instead, you could provide all that is needed in a single piece of code as presented in <a data-type="xref" href="#code-sum-go-bench2">Example 8-12</a>.</p>&#13;
<div data-type="example" id="code-sum-go-bench2">&#13;
<h5><span class="label">Example 8-12. </span>Well-documented, reusable Go benchmark for assessing concurrent implementations of the <code>Sum</code> function</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="c1">// BenchmarkSum assesses `Sum` function. </code><a class="co" href="#callout_benchmarking_CO10-1" id="co_benchmarking_CO10-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="c1">// NOTE(bwplotka): Test it with a maximum of 4 CPU cores, given we don't allocate</code><code class="w">&#13;
</code><code class="c1">// more in our production containers.</code><code class="w">&#13;
</code><code class="c1">//</code><code class="w">&#13;
</code><code class="c1">// Recommended run options:</code><code class="w">&#13;
</code><code class="cm">/*&#13;
export ver=v1 &amp;&amp; go test \&#13;
    -run '^$' -bench '^BenchmarkSum$' \&#13;
    -benchtime 10s -count 5 -cpu 4 -benchmem \&#13;
    -memprofile=${ver}.mem.pprof -cpuprofile=${ver}.cpu.pprof \&#13;
  | tee ${ver}.txt </code><a class="co" href="#callout_benchmarking_CO10-2" id="co_benchmarking_CO10-2"><img alt="2" src="assets/2.png"/></a><code class="cm">&#13;
*/</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">BenchmarkSum</code><code class="p">(</code><code class="nx">b</code><code class="w"> </code><code class="o">*</code><code class="nx">testing</code><code class="p">.</code><code class="nx">B</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">   </code><code class="c1">// Create 7.55 MB file with 2 million lines.</code><code class="w">&#13;
</code><code class="w">   </code><code class="nx">fn</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">filepath</code><code class="p">.</code><code class="nx">Join</code><code class="p">(</code><code class="nx">b</code><code class="p">.</code><code class="nx">TempDir</code><code class="p">(</code><code class="p">)</code><code class="p">,</code><code class="w"> </code><code class="s">"/test.2M.txt"</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">   </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">b</code><code class="p">,</code><code class="w"> </code><code class="nx">createTestInput</code><code class="p">(</code><code class="nx">fn</code><code class="p">,</code><code class="w"> </code><code class="mf">2e6</code><code class="p">)</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO10-3" id="co_benchmarking_CO10-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
&#13;
   </code><code class="nx">b</code><code class="p">.</code><code class="nx">ResetTimer</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">   </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">b</code><code class="p">.</code><code class="nx">N</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">      </code><code class="nx">_</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">Sum</code><code class="p">(</code><code class="nx">fn</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">      </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">b</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO10-4" id="co_benchmarking_CO10-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
   </code><code class="p">}</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO10-1" id="callout_benchmarking_CO10-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>It might feel excessive for a simple benchmark, but good documentation significantly increases the reliability of your and your team’s benchmarking. Mention any surprising facts around this benchmark, dataset choice, conditions, or prerequisites in the commentary.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO10-2" id="callout_benchmarking_CO10-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>I recommend commenting on the benchmark with the suggested way to invoke it. It’s not to force anything but rather to describe how you envisioned running this benchmark (e.g., for how long). Future you or your team members will thank you!</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO10-3" id="callout_benchmarking_CO10-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Provide the exact input you intend to run your benchmark with. You could create a static file for unit tests and commit it to your repository. Unfortunately, the benchmarking inputs are often too big to be committed to your source code (e.g., <code>git</code>). For this purpose, I created a small <code>createTestInput</code> function that can generate a dynamic number of lines. Notice the use of <a href="https://oreil.ly/elBJa"><code>b.TempDir()</code></a>, which creates a temporary directory and cares about cleaning it manually afterward.<sup><a data-type="noteref" href="ch08.html#idm45606828161072" id="idm45606828161072-marker">18</a></sup></p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO10-4" id="callout_benchmarking_CO10-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Because you want to reuse this benchmark in the future, and it will also be used by other team members, it makes sense to ensure others do not measure the wrong thing, thus testing for basic error modes even in the benchmark.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Thanks to <code>b.ResetTimer()</code>, even if the input file creation is relatively slow, latency and resource usage won’t be visible in the benchmarking results. However, it might not be very pleasant for you while repeatedly running that benchmark. Even more, you will experience that slowness more than once after. As we learned in <a data-type="xref" href="#ch-obs-micro-go">“Go Benchmarks”</a>, Go can run the benchmark multiple times to find the correct <code>N</code> value. If the initialization takes too much time and impacts your feedback loop, you can add the code that will cache test the input on the filesystem. See <a data-type="xref" href="#code-sum-go-bench3">Example 8-13</a> for how you can add a simple <code>os.Stat</code> to achieve this.</p>&#13;
<div data-type="example" id="code-sum-go-bench3">&#13;
<h5><span class="label">Example 8-13. </span>Example of the benchmark with input creation executed only once and cached on disk</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">func</code><code class="w"> </code><code class="nx">lazyCreateTestInput</code><code class="p">(</code><code class="nx">tb</code><code class="w"> </code><code class="nx">testing</code><code class="p">.</code><code class="nx">TB</code><code class="p">,</code><code class="w"> </code><code class="nx">numLines</code><code class="w"> </code><code class="kt">int</code><code class="p">)</code><code class="w"> </code><code class="kt">string</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">tb</code><code class="p">.</code><code class="nx">Helper</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO11-1" id="co_benchmarking_CO11-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
&#13;
    </code><code class="nx">fn</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">fmt</code><code class="p">.</code><code class="nx">Sprintf</code><code class="p">(</code><code class="s">"testdata/test.%v.txt"</code><code class="p">,</code><code class="w"> </code><code class="nx">numLines</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">if</code><code class="w"> </code><code class="nx">_</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">os</code><code class="p">.</code><code class="nx">Stat</code><code class="p">(</code><code class="nx">fn</code><code class="p">)</code><code class="p">;</code><code class="w"> </code><code class="nx">errors</code><code class="p">.</code><code class="nx">Is</code><code class="p">(</code><code class="nx">err</code><code class="p">,</code><code class="w"> </code><code class="nx">os</code><code class="p">.</code><code class="nx">ErrNotExist</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO11-2" id="co_benchmarking_CO11-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
        </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">tb</code><code class="p">,</code><code class="w"> </code><code class="nx">createTestInput</code><code class="p">(</code><code class="nx">fn</code><code class="p">,</code><code class="w"> </code><code class="nx">numLines</code><code class="p">)</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w"> </code><code class="k">else</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">tb</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">return</code><code class="w"> </code><code class="nx">fn</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">BenchmarkSum</code><code class="p">(</code><code class="nx">b</code><code class="w"> </code><code class="o">*</code><code class="nx">testing</code><code class="p">.</code><code class="nx">B</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="c1">// Create a 7.55 MB file with 2 million lines if it does not exist.</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">fn</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">lazyCreateTestInput</code><code class="p">(</code><code class="nx">tb</code><code class="p">,</code><code class="w"> </code><code class="mf">2e6</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">b</code><code class="p">.</code><code class="nx">ResetTimer</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">b</code><code class="p">.</code><code class="nx">N</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">_</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">Sum</code><code class="p">(</code><code class="nx">fn</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">b</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">   </code><code class="p">}</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO11-1" id="callout_benchmarking_CO11-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p><code>t.Helper</code> tells the testing framework to point out the line that invokes &#13;
<span class="keep-together"><code>lazyCreateTestInput</code></span> when a potential error happens.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO11-2" id="callout_benchmarking_CO11-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p><code>os.Stat</code> stops executing <code>createTestInput</code> if the file exists. Be careful when changing&#13;
the characteristics or size of the input file. If you don’t change the filename, the risk is that people who ran those tests will have a cached old version of the input. However, that small risk is worth it if the&#13;
creation of the input is slower than a few seconds or so.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Such a benchmark provides elegant and concise information about the benchmark implementation, purpose, input, run command, and prerequisites. Moreover, it allows you and your team to replicate or reuse the same benchmark with little effort.<a data-startref="ix_ch08-asciidoc14" data-type="indexterm" id="idm45606827900208"/><a data-startref="ix_ch08-asciidoc13" data-type="indexterm" id="idm45606827899536"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Running Benchmarks for Different Inputs" data-type="sect2"><div class="sect2" id="idm45606827898608">&#13;
<h2>Running Benchmarks for Different Inputs</h2>&#13;
&#13;
<p><a data-primary="microbenchmarks/microbenchmarking" data-secondary="running for different inputs" data-type="indexterm" id="ix_ch08-asciidoc15"/>It’s often helpful to learn how the efficiency of our implementation changes for different sizes and types of input. Sometimes it’s fine to manually change the input in our code and rerun our benchmark, but sometimes we would like to program benchmarks for the same piece of code against different inputs in our source code (e.g., for our team to use later). Table tests are perfect for such use cases. Typically, we see this pattern in functional tests, but we can use it in microbenchmarks, as presented in <a data-type="xref" href="#code-sum-go-bench-cases">Example 8-14</a>.</p>&#13;
<div data-type="example" id="code-sum-go-bench-cases">&#13;
<h5><span class="label">Example 8-14. </span>Table benchmark using a common pattern with <code>b.Run</code></h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">func</code><code class="w"> </code><code class="nx">BenchmarkSum</code><code class="p">(</code><code class="nx">b</code><code class="w"> </code><code class="o">*</code><code class="nx">testing</code><code class="p">.</code><code class="nx">B</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">for</code><code class="w"> </code><code class="nx">_</code><code class="p">,</code><code class="w"> </code><code class="nx">tcase</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="k">range</code><code class="w"> </code><code class="p">[</code><code class="p">]</code><code class="kd">struct</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO12-1" id="co_benchmarking_CO12-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
       </code><code class="nx">numLines</code><code class="w"> </code><code class="kt">int</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="p">{</code><code class="nx">numLines</code><code class="p">:</code><code class="w"> </code><code class="mi">0</code><code class="p">}</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">        </code><code class="p">{</code><code class="nx">numLines</code><code class="p">:</code><code class="w"> </code><code class="mf">1e2</code><code class="p">}</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">        </code><code class="p">{</code><code class="nx">numLines</code><code class="p">:</code><code class="w"> </code><code class="mf">1e4</code><code class="p">}</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">        </code><code class="p">{</code><code class="nx">numLines</code><code class="p">:</code><code class="w"> </code><code class="mf">1e6</code><code class="p">}</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">        </code><code class="p">{</code><code class="nx">numLines</code><code class="p">:</code><code class="w"> </code><code class="mf">2e6</code><code class="p">}</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">b</code><code class="p">.</code><code class="nx">Run</code><code class="p">(</code><code class="nx">fmt</code><code class="p">.</code><code class="nx">Sprintf</code><code class="p">(</code><code class="s">"lines-%d"</code><code class="p">,</code><code class="w"> </code><code class="nx">tcase</code><code class="p">.</code><code class="nx">numLines</code><code class="p">)</code><code class="p">,</code><code class="w"> </code><code class="kd">func</code><code class="p">(</code><code class="nx">b</code><code class="w"> </code><code class="o">*</code><code class="nx">testing</code><code class="p">.</code><code class="nx">B</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO12-2" id="co_benchmarking_CO12-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
            </code><code class="nx">b</code><code class="p">.</code><code class="nx">ReportAllocs</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO12-3" id="co_benchmarking_CO12-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
&#13;
            </code><code class="nx">fn</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">lazyCreateTestInput</code><code class="p">(</code><code class="nx">tb</code><code class="p">,</code><code class="w"> </code><code class="nx">tcase</code><code class="p">.</code><code class="nx">numLines</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">            </code><code class="nx">b</code><code class="p">.</code><code class="nx">ResetTimer</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">            </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">b</code><code class="p">.</code><code class="nx">N</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO12-4" id="co_benchmarking_CO12-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
                </code><code class="nx">_</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">Sum</code><code class="p">(</code><code class="nx">fn</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">                </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">b</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">            </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">        </code><code class="p">}</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO12-1" id="callout_benchmarking_CO12-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>An inlined slice of anonymous structures works well here because you don’t need to reference this type anywhere. Feel free to add any fields here to map test cases as you need.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO12-2" id="callout_benchmarking_CO12-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>In the test case loop, we can run <code>b.Run</code> that tells <code>go test</code> about a subbenchmark. If you put the <code>""</code> empty string as the name, <code>go test</code> will use numbers as your test case identification. I decided to present a number of lines as a unique description of each test case. The test case identification will be added as a suffix, so <code>BenchmarkSum/&lt;test-case&gt;</code>.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO12-3" id="callout_benchmarking_CO12-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>For these tests, <code>go test</code> ignores any <code>b.ReportAllocs</code> and other benchmark methods outside the <code>b.Run</code>, so make sure to repeat them here.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO12-4" id="callout_benchmarking_CO12-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>A common pitfall here is to accidentally use <code>b</code> from the main function, not from the closure created for the inner function. This is common if you try to &#13;
<span class="keep-together">avoid shadowing</span> the <code>b</code> variable and use a different variable name for the inner &#13;
<span class="keep-together"><code>*testing.B,</code></span> e.g., <code>b.Run("", func(b2 *testing.B)</code>. These problems are hard to debug, so I recommend always using the same name, e.g., <code>b</code>.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Amazingly, we can use the same recommended <code>run</code> command presented in <a data-type="xref" href="#code-sum-go-bench-all">Example 8-4</a> for a nontable test. The example run output processes by <code>benchstat</code> will then look like <a data-type="xref" href="#code-sum-go-bench-test-out">Example 8-15</a>.</p>&#13;
<div data-type="example" id="code-sum-go-bench-test-out">&#13;
<h5><span class="label">Example 8-15. </span><code>benchstat</code> output on results from the <a data-type="xref" href="#code-sum-go-bench-cases">Example 8-14</a> test</h5>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">name                 time/op&#13;
Sum/lines-0-4        2.79µs ± 1%&#13;
Sum/lines-100-4      8.10µs ± 5%&#13;
Sum/lines-10000-4     407µs ± 6%&#13;
Sum/lines-1000000-4  40.5ms ± 1%&#13;
Sum/lines-2000000-4  78.4ms ± 3%&#13;
&#13;
name                 alloc/op&#13;
Sum/lines-0-4          872B ± 0%&#13;
Sum/lines-100-4      3.82kB ± 0%&#13;
Sum/lines-10000-4     315kB ± 0%&#13;
Sum/lines-1000000-4  30.4MB ± 0%&#13;
Sum/lines-2000000-4  60.8MB ± 0%&#13;
&#13;
name                 allocs/op&#13;
Sum/lines-0-4          6.00 ± 0%&#13;
Sum/lines-100-4        86.0 ± 0%&#13;
Sum/lines-10000-4     8.01k ± 0%&#13;
Sum/lines-1000000-4    800k ± 0%&#13;
Sum/lines-2000000-4   1.60M ± 0%</pre></div>&#13;
&#13;
<p>I find the table tests great for quickly learning about the estimated complexity (discussed in <a data-type="xref" href="ch07.html#ch-hw-complexity">“Complexity Analysis”</a>) of our application. Then, after I learn more, I can trim the number of cases to those that can truly trigger bottlenecks we saw in the past. In addition, committing such a benchmark to our team’s source code will increase the chances that other team members (and yourself!) will reuse it and run a microbenchmark with all cases that matter for the project.<a data-startref="ix_ch08-asciidoc15" data-type="indexterm" id="idm45606827543360"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Microbenchmarks Versus Memory Management" data-type="sect2"><div class="sect2" id="ch-obs-micro-mem">&#13;
<h2>Microbenchmarks Versus Memory Management</h2>&#13;
&#13;
<p><a data-primary="memory management" data-secondary="microbenchmarks versus" data-type="indexterm" id="ix_ch08-asciidoc16"/><a data-primary="microbenchmarks/microbenchmarking" data-secondary="memory management versus" data-type="indexterm" id="ix_ch08-asciidoc17"/>The simplicity of microbenchmarks has many benefits but also downsides. One of the most surprising problems is that the memory statistics reported in the <code>go test</code> benchmarks don’t tell a lot. Unfortunately, given how memory management is implemented in Go (<a data-type="xref" href="ch05.html#ch-hw-go-mem">“Go Memory Management”</a>), we can’t reproduce all the aspects of memory efficiency of our Go programs with microbenchmarks.</p>&#13;
&#13;
<p>As we saw in <a data-type="xref" href="#code-sum-go-bench-benchstat">Example 8-6</a>, the naive implementation of <code>Sum</code> in <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> allocates around 60 MB of memory on the heap with the 1.6 million objects to calculate a sum for 2 million integers. This tells us less about memory efficiency than we might think. It only tells us three things:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Some of the latency we experience in microbenchmark results inevitably come from the sole fact of making so many allocations (and we can confirm with profiles how much it matters).</p>&#13;
</li>&#13;
<li>&#13;
<p>We can compare that number and size of allocations with other &#13;
<span class="keep-together">implementations.</span></p>&#13;
</li>&#13;
<li>&#13;
<p>We can compare the number and size of the allocation with expected space complexity (<a data-type="xref" href="ch07.html#ch-hw-complexity">“Complexity Analysis”</a>).</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Unfortunately, any other conclusion based on those numbers is in the realm of &#13;
<span class="keep-together">estimations,</span> which only can be verified when we run <a data-type="xref" href="ch07.html#ch-obs-benchmarking-macro">“Macrobenchmarks”</a> or <a data-type="xref" href="ch07.html#ch-obs-benchmarking-prod">“Benchmarking in Production”</a>. The reason is very simple—there is no special GC schedule for benchmarks because we want to ensure as close to production simulation as possible. They run on a normal schedule like in production code, which means that during our 100 iterations of our benchmark, the GC might run 1,000 times, 10 times, or for fast benchmarks it might not run at all! Therefore, any attempts to manually trigger <code>runtime.GC()</code> are also poor options, given that it’s not how it will be running in production and might clash with normal GC schedules.</p>&#13;
&#13;
<p>As a result, the microbenchmark will not give us a clear idea and the following memory efficiency questions:</p>&#13;
<dl>&#13;
<dt>GC latency</dt>&#13;
<dd>&#13;
<p>As we learned in <a data-type="xref" href="ch05.html#ch-hw-go-mem">“Go Memory Management”</a>, a bigger heap (more objects in a heap) will mean more work for the GC, which always translates to increased CPU usage or, more often, GC cycles (even with fair 25% CPU usage mechanisms). Because of nondeterministic GC and quick benchmarking operations, we most likely won’t see GC impact on a microbenchmark level.<sup><a data-type="noteref" href="ch08.html#idm45606827504960" id="idm45606827504960-marker">19</a></sup></p>&#13;
</dd>&#13;
<dt>Maximum memory usage</dt>&#13;
<dd>&#13;
<p>If a single operation allocates 60 MB, does it mean that the program performing one such operation at the time will need no more and no less than ~60 MB of memory in our system? Unfortunately, for the same reason mentioned previously, we can’t tell with microbenchmarks.</p>&#13;
&#13;
<p>It might be that our single operation doesn’t need all objects for the full duration. This might mean that the maximum usage of memory will be, for example, only 10 MB, despite the 60 MB allocation number, as the GC can do clean-up runs multiple times in practice.</p>&#13;
&#13;
<p>You might even have the opposite situation too! Especially for <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a>, most of the memory is kept during the whole operation (it is kept in the file buffer—we can tell that from profiling, explained in <a data-type="xref" href="ch09.html#ch-obs-profiling">“Profiling in Go”</a>). On &#13;
<span class="keep-together">top of</span> that, the GC might not clean the memory fast enough, resulting in the next operation allocating 60 MB on top of  the original 60 MB, requiring 120 MB in total from the OS. This situation can be even worse if we do a larger concurrency of our operations.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>This is unfortunate, as the preceding problems are often seen in our Go code. If we could verify those problems on microbenchmarks, it would be easier to tell if we can reuse memory better (e.g., through <a data-type="xref" href="ch11.html#ch-basic-pool">“Memory Reuse and Pooling”</a>) or if &#13;
<span class="keep-together">we should</span> straight reduce allocation and to what level. Unfortunately, to tell for sure, we need to move to <a data-type="xref" href="#ch-obs-macro">“Macrobenchmarks”</a>.</p>&#13;
&#13;
<p>Still, the microbenchmark allocation information is incredibly useful if we assume that, generally, more allocations can cause more problems. This is why simply focusing on reducing the number of allocations or allocated space in our micro-optimization cycle is still very effective. What we need to acknowledge, however, is that those numbers from just microbenchmarking might not give us complete confidence about whether the end GC overhead or maximum memory usage will be acceptable or problematic. We can try to estimate this, but we won’t know for sure until we move to the macro level to assess that.<a data-startref="ix_ch08-asciidoc17" data-type="indexterm" id="idm45606827493968"/><a data-startref="ix_ch08-asciidoc16" data-type="indexterm" id="idm45606827493264"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Compiler Optimizations Versus Benchmark" data-type="sect2"><div class="sect2" id="ch-obs-micro-comp">&#13;
<h2>Compiler Optimizations Versus Benchmark</h2>&#13;
&#13;
<p><a data-primary="compiler optimizations, microbenchmarks versus" data-type="indexterm" id="ix_ch08-asciidoc18"/><a data-primary="microbenchmarks/microbenchmarking" data-secondary="compiler optimizations versus" data-type="indexterm" id="ix_ch08-asciidoc19"/>There is a very interesting “meta” dynamic between microbenchmarking and compiler optimizations, which is sometimes controversial. It is worth knowing about this problem, the potential consequences, and how to mitigate them.</p>&#13;
&#13;
<p>Our goal when microbenchmarking is to assess the efficiency of the small part of our production code with as high confidence as possible (given the amount of time available and problem constraints). For this reason, the Go compiler treats our <a data-type="xref" href="#ch-obs-micro-go">“Go Benchmarks”</a> benchmarking function like any other production code. The same AST conversions, type safety, memory safety, dead code elimination, and optimizations rules discussed in <a data-type="xref" href="ch04.html#ch-hw-compilation">“Understanding Go Compiler”</a> are performed by the compiler on all parts of the code—no special exceptions for benchmarks. Therefore, we are reproducing all production conditions, including the compilation stage.</p>&#13;
&#13;
<p>This premise is great, but what gets in the way of this philosophy is that &#13;
<span class="keep-together">microbenchmarks</span> are a little special. From the runtime process perspective, there are three main differences between how this code is executed on production and when we want to learn about production code efficiency:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>No other user code is running at the same time in the same process.<sup><a data-type="noteref" href="ch08.html#idm45606827483632" id="idm45606827483632-marker">20</a></sup></p>&#13;
</li>&#13;
<li>&#13;
<p>We are invoking the same code in a loop.</p>&#13;
</li>&#13;
<li>&#13;
<p>We typically don’t use the output or return arguments.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Those three elements might not seem like a big difference, but as we learned in <a data-type="xref" href="ch04.html#ch-hw-mem-wall">“CPU and Memory Wall Problem”</a>, modern CPUs can already run differently in those cases due to, e.g., different branch prediction and L-cache locality. On top of that, you can imagine a smart enough compiler that will adjust the machine code differently based on those cases too!</p>&#13;
&#13;
<p><a data-primary="Java, benchmarking in" data-type="indexterm" id="idm45606827478512"/>This problem is especially visible when programming in Java because some compilation phases are done in runtime, thanks to the mature just-in-time (JIT) compiler. As a result, Java engineers must be <a href="https://oreil.ly/OJKNS">very careful when benchmarking</a> and use special <a href="https://oreil.ly/Cil2Z">frameworks</a> for Java to ensure simulating production conditions with warm-up phases and other tricks to increase the reliability of benchmarks.</p>&#13;
&#13;
<p class="less_space pagebreak-before">In Go, things are simpler. The compiler is less mature than Java’s, and no JIT compilation exists. While JIT is not even planned, some form of <a href="https://oreil.ly/yFYut">runtime profile-guided compiler optimization (PGO)</a> is being <a href="https://oreil.ly/jDYqF">considered for Go</a>, which might make our microbenchmark more complex in future. Time will tell.</p>&#13;
&#13;
<p>However, even if we focus on the current compiler, it sometimes can apply unwanted optimizations to our benchmarking code. <a data-primary="dead code elimination" data-type="indexterm" id="idm45606827473376"/>One of the known problems is called <a href="https://oreil.ly/QG1y1">dead code elimination</a>. Let’s consider a low-level function &#13;
<span class="keep-together">representing</span> <a href="https://oreil.ly/lnuMl"><code>population count</code> instruction</a> and the naive &#13;
<span class="keep-together">microbenchmark</span> in <a data-type="xref" href="#code-bench-popcnt">Example 8-16</a>.<sup><a data-type="noteref" href="ch08.html#idm45606827468608" id="idm45606827468608-marker">21</a></sup></p>&#13;
<div data-type="example" id="code-bench-popcnt">&#13;
<h5><span class="label">Example 8-16. </span><code>popcnt</code> function with the naive implementation of microbenchmark impacted by compiler optimizations</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">const</code><code class="w"> </code><code class="nx">m1</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="mh">0x5555555555555555</code><code class="w">&#13;
</code><code class="kd">const</code><code class="w"> </code><code class="nx">m2</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="mh">0x3333333333333333</code><code class="w">&#13;
</code><code class="kd">const</code><code class="w"> </code><code class="nx">m4</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="mh">0x0f0f0f0f0f0f0f0f</code><code class="w">&#13;
</code><code class="kd">const</code><code class="w"> </code><code class="nx">h01</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="mh">0x0101010101010101</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">popcnt</code><code class="p">(</code><code class="nx">x</code><code class="w"> </code><code class="kt">uint64</code><code class="p">)</code><code class="w"> </code><code class="kt">uint64</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">   </code><code class="nx">x</code><code class="w"> </code><code class="o">-=</code><code class="w"> </code><code class="p">(</code><code class="nx">x</code><code class="w"> </code><code class="o">&gt;&gt;</code><code class="w"> </code><code class="mi">1</code><code class="p">)</code><code class="w"> </code><code class="o">&amp;</code><code class="w"> </code><code class="nx">m1</code><code class="w">&#13;
</code><code class="w">   </code><code class="nx">x</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="p">(</code><code class="nx">x</code><code class="w"> </code><code class="o">&amp;</code><code class="w"> </code><code class="nx">m2</code><code class="p">)</code><code class="w"> </code><code class="o">+</code><code class="w"> </code><code class="p">(</code><code class="p">(</code><code class="nx">x</code><code class="w"> </code><code class="o">&gt;&gt;</code><code class="w"> </code><code class="mi">2</code><code class="p">)</code><code class="w"> </code><code class="o">&amp;</code><code class="w"> </code><code class="nx">m2</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">   </code><code class="nx">x</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="p">(</code><code class="nx">x</code><code class="w"> </code><code class="o">+</code><code class="w"> </code><code class="p">(</code><code class="nx">x</code><code class="w"> </code><code class="o">&gt;&gt;</code><code class="w"> </code><code class="mi">4</code><code class="p">)</code><code class="p">)</code><code class="w"> </code><code class="o">&amp;</code><code class="w"> </code><code class="nx">m4</code><code class="w">&#13;
</code><code class="w">   </code><code class="k">return</code><code class="w"> </code><code class="p">(</code><code class="nx">x</code><code class="w"> </code><code class="o">*</code><code class="w"> </code><code class="nx">h01</code><code class="p">)</code><code class="w"> </code><code class="o">&gt;&gt;</code><code class="w"> </code><code class="mi">56</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">BenchmarkPopcnt</code><code class="p">(</code><code class="nx">b</code><code class="w"> </code><code class="o">*</code><code class="nx">testing</code><code class="p">.</code><code class="nx">B</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">   </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">b</code><code class="p">.</code><code class="nx">N</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">      </code><code class="nx">popcnt</code><code class="p">(</code><code class="nx">math</code><code class="p">.</code><code class="nx">MaxUint64</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO13-1" id="co_benchmarking_CO13-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
   </code><code class="p">}</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO13-1" id="callout_benchmarking_CO13-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>In the original issue #14813, the input for the function was taken from <code>uint64(i)</code>, which is a huge anti-pattern. You should never use <code>i'</code> from the <code>b.N</code> loop! I want to focus on the surprising compiler optimization risk in this example, so let’s imagine we want to assess the efficiency of <code>popcnt</code> working on the largest unsigned integer possible (using <code>math.MaxInt64</code> to obtain it). This also will expose us to an unexpected behavior mentioned below.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>If we execute this benchmark for a second, we will get slightly concerning output, as presented in <a data-type="xref" href="#code-bench-popcnt-out">Example 8-17</a>.</p>&#13;
<div data-type="example" id="code-bench-popcnt-out">&#13;
<h5><span class="label">Example 8-17. </span>The output of the <code>BenchmarkPopcnt</code> benchmark from <a data-type="xref" href="#code-bench-popcnt">Example 8-16</a></h5>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting"><code>goos: linux&#13;
goarch: amd64&#13;
pkg: github.com/efficientgo/examples/pkg/comp-opt-away&#13;
cpu: Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz&#13;
BenchmarkPopcnt&#13;
BenchmarkPopcnt-12     1000000000          0.2344 ns/op </code><a class="co" href="#callout_benchmarking_CO14-1" id="co_benchmarking_CO14-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
PASS</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO14-1" id="callout_benchmarking_CO14-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Every time you see your benchmark making a billion iterations (maximum number of iterations <code>go test</code> will do), you know your benchmark is wrong. It means we will see a loop overhead rather than the latency we are measuring. This can be caused by the compiler optimizing away your code or by measuring something too fast to be measured with a Go benchmark (e.g., single instruction).</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>What is happening? The first problem is that the Go compiler inlines the <code>popcnt</code> code, and further optimization phases detected that no other code is using the result of the inlined calculation. The compiler detects that no change in observable behavior would occur if we remove this code, so it elides that inlined code part. If we would list assembly code using <code>-gcflags=-S</code> on <code>go build</code> or <code>go test</code>, you would notice there is no code responsible for performing statements behind <code>popcnt</code> (we run an empty loop!). This can also be confirmed by running <code>GOSSAFUNC=BenchmarkPopcnt go build</code> and opening <em>ssa.html</em> in your browser, which also lists the generated assembly more interactively. We can verify this problem by running a test with <code>-gcflags=-N</code>, which turns off all compiler optimizations. Executing or looking at the assembly will show you the large difference.</p>&#13;
&#13;
<p>The second problem is that all the iterations of our benchmark run <code>popcnt</code> with the same constant number—the largest unsigned integer. Even if code elimination did not happen, with inlining, the Go compiler is smart enough to precompute some logic (sometimes referred to as <a href="https://oreil.ly/NEOyQ"><code>intrinsic</code></a>). The result of <code>popcnt(math.MaxUint64)</code> is always 64, no matter how many times and where we run it; thus, the machine code will simply use <code>64</code> instead of calculating <code>popcnt</code> in every iteration.</p>&#13;
&#13;
<p><a data-primary="benchmarks/benchmarking" data-secondary="compiler optimization countermeasures" data-type="indexterm" id="idm45606827236912"/>Generally, there are three practical countermeasures against compiler optimization in benchmarks:</p>&#13;
<dl>&#13;
<dt>Move to the macro level.</dt>&#13;
<dd>&#13;
<p>On a macro level, there is no special code within the same binary, so we can use the same machine code for both benchmarks and production code.</p>&#13;
</dd>&#13;
</dl>&#13;
<dl class="less_space pagebreak-before">&#13;
<dt>Microbenchmark more complex functionality.</dt>&#13;
<dd>&#13;
<p>If compiler optimizations impact, you might be optimizing Go on a too low level.</p>&#13;
&#13;
<p>I personally haven’t been impacted by compiler optimization, because I tend to microbenchmark on higher-level functionalities. If you benchmark really small functions like <a data-type="xref" href="#code-bench-popcnt">Example 8-16</a>, typically inlined and a few nanoseconds fast, expect the CPU and compiler effect to impact you more. For more complex code, the compiler typically is not as clever to inline or adjust the machine code for benchmarking purposes. The number of instructions and data on bigger macrobenchmarks will also more likely break the CPU branch predictor and cache locality like it would at production.<sup><a data-type="noteref" href="ch08.html#idm45606827194896" id="idm45606827194896-marker">22</a></sup></p>&#13;
</dd>&#13;
<dt>Outsmart compiler in microbenchmark.</dt>&#13;
<dd>&#13;
<p>If you want to microbenchmark such a tiny function like <a data-type="xref" href="#code-bench-popcnt">Example 8-16</a>, there is no other way to obfuscate the compiler code analysis. What typically works is using exported global variables. They are hard to predict given the current per-package Go compilation logic<sup><a data-type="noteref" href="ch08.html#idm45606827192512" id="idm45606827192512-marker">23</a></sup> or using <code>runtime.KeepAlive</code>, which is a newer way to tell compile that “this variable is used” (which is a side effect of telling the GC to keep this variable on the heap). The <code>//go:noinline</code> directive that stops the compiler from inlining function might also work, but it’s not recommended as on production, your code might be inlined and optimized, which we want to benchmark too.</p>&#13;
&#13;
<p><a data-primary="Sink pattern" data-type="indexterm" id="ix_ch08-asciidoc20"/>If we would like to improve the Go benchmark shown in <a data-type="xref" href="#code-bench-popcnt">Example 8-16</a>, we could add the <code>Sink</code> pattern<sup><a data-type="noteref" href="ch08.html#idm45606827188416" id="idm45606827188416-marker">24</a></sup> and global variable for input, as presented in &#13;
<span class="keep-together"><a data-type="xref" href="#code-bench-popcnt2">Example 8-18</a>.</span> This works in Go 1.18 with the <code>gc</code> compiler, but it’s not prone to future improvements in the Go compiler.</p>&#13;
</dd>&#13;
</dl>&#13;
<div data-type="example" id="code-bench-popcnt2">&#13;
<h5><span class="label">Example 8-18. </span><code>Sink</code> pattern and variable input countermeasure unwanted compiler optimization on microbenchmarks</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">var</code><code class="w"> </code><code class="nx">Input</code><code class="w"> </code><code class="kt">uint64</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="nx">math</code><code class="p">.</code><code class="nx">MaxUint64</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO15-1" id="co_benchmarking_CO15-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="kd">var</code><code class="w"> </code><code class="nx">Sink</code><code class="w"> </code><code class="kt">uint64</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO15-2" id="co_benchmarking_CO15-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">BenchmarkPopcnt</code><code class="p">(</code><code class="nx">b</code><code class="w"> </code><code class="o">*</code><code class="nx">testing</code><code class="p">.</code><code class="nx">B</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="kd">var</code><code class="w"> </code><code class="nx">s</code><code class="w"> </code><code class="kt">uint64</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">b</code><code class="p">.</code><code class="nx">ResetTimer</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="p">&lt;</code><code class="w"> </code><code class="nx">b</code><code class="p">.</code><code class="nx">N</code><code class="p">;</code><code class="w"> </code><code class="nx">i</code><code class="o">++</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">       </code><code class="nx">s</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="nx">popcnt</code><code class="p">(</code><code class="nx">Input</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO15-3" id="co_benchmarking_CO15-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
    </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">Sink</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="nx">s</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO15-1" id="callout_benchmarking_CO15-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The global <code>Input</code> variable masks the fact that <code>math.MaxUint64</code> is constant. This forces the compiler to not be lazy and do the work in our benchmark iteration. This works because the compiler can’t tell if anyone else will change this variable in runtime before or during experiments.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO15-2" id="callout_benchmarking_CO15-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p><code>Sink</code> is a similar global variable to <code>Input</code>, but it hides from the compiler that the value of our function is never used, so the compiler won’t assume it’s a dead code.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO15-3" id="callout_benchmarking_CO15-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Notice that we don’t assign a value directly to the global variable as it’s <a href="https://oreil.ly/yvNAi">more expensive</a>, thus potentially adding even more overhead to our benchmark.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Thanks to the techniques presented in <a data-type="xref" href="#code-bench-popcnt2">Example 8-18</a>, I can assess that such an operation on my machine takes around 1.6 nanoseconds. Unfortunately, although I got a stable result that (one would hope) is realistic, assessing efficiency for such low-level code is fragile and complicated. Outsmarting the compiler or disabling optimizations are quite controversial techniques—they go against the philosophy that benchmarked code should be as close to production code as possible.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Don’t Put Sinks Everywhere!</h1>&#13;
<p>This section might feel scary and complicated. Initially, when I learned about these complex compilation impacts, I was putting a sink to all my microbenchmarks or assert errors only to avoid potential elision problems.</p>&#13;
&#13;
<p>That is unnecessary. Be pragmatic, be vigilant of benchmarking results you can’t explain (as mentioned in <a data-type="xref" href="ch07.html#ch-obs-rel-err">“Human Errors”</a>), and add those special <a data-primary="Cox, Russ" data-secondary="on sinks" data-type="indexterm" id="idm45606827039120"/>countermeasures.</p>&#13;
</div>&#13;
<blockquote><p>Personally, I’d rather not see sinks appear everywhere until they are needed. In many cases they won’t be, and the code is clearer without them. My advice is to wait until the benchmark is clearly optimized away and only then put them in. The details of the sink can depend on the context. If you have a function returning an int, it’s fine to sum them up and then assign the result to a global, for example.</p>&#13;
<p data-type="attribution">Russ Cox (rsc), “Benchmarks vs Dead Code Elimination,” <a href="https://oreil.ly/xGDYr">email thread</a></p></blockquote>&#13;
&#13;
<p>In<a data-startref="ix_ch08-asciidoc20" data-type="indexterm" id="idm45606827035280"/> summary, be mindful of how the compiler can impact your microbenchmark. It does not happen too often, especially if you are benchmarking on a reasonable level, but when it happens, you should now know how to mitigate those problems. My recommendation is to avoid relying on a microbenchmark at such a low level. Instead, unless you are an experienced engineer interested in the ultra-high performance of your Go code for a specific use case, move to a higher level by testing more complex functionality. Fortunately, most of the code you will work with will likely be too complex to trigger such a “battle” with the Go compiler<a data-startref="ix_ch08-asciidoc19" data-type="indexterm" id="idm45606827034416"/><a data-startref="ix_ch08-asciidoc18" data-type="indexterm" id="idm45606827033744"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Macrobenchmarks" data-type="sect1"><div class="sect1" id="ch-obs-macro">&#13;
<h1>Macrobenchmarks</h1>&#13;
&#13;
<p><a data-primary="macrobenchmarks/macrobenchmarking" data-secondary="implementation" data-type="indexterm" id="ix_ch08-asciidoc21"/>Programming books that cover performance and optimization topics don’t usually describe benchmarking on a larger level than micro. This is because testing on a macro level is a gray area for developers. Typically, it is the responsibility of dedicated tester teams or QA engineers. However, for backend applications and services, such macrobenchmarking involves experience, skills, and tools to work with many dependencies, orchestration systems, and generally bigger infrastructure. As a result, such activity used to be the domain of operation teams, system administrators, and DevOps engineers.</p>&#13;
&#13;
<p>However, things are changing a bit, especially for the infrastructure software, which is my area of expertise. The cloud-native ecosystem makes infrastructure tools more accessible for developers, with standards and technologies like <a href="https://kubernetes.io">Kubernetes</a>, containers, and paradigms like <a href="https://sre.google">Site Reliability Engineering (SRE)</a>. On top of that, the popular microservice architecture allows breaking functional pieces into smaller programs with clear APIs. This allows developers to take more responsibility for their areas of expertise. Therefore, in the last decades, we are seeing the move toward making testing (and running) software on all levels easier for developers.</p>&#13;
<div data-type="tip"><h1>Participate in Macrobenchmarks That Touch Your Software!</h1>&#13;
<p>As a developer, it is extremely insightful to participate in testing your software, even on a macro level. Seeing your software’s bugs and slowdowns gives crystal clarity to the priority. Additionally, if you catch those problems on the setup you control or are familiar with, it is easier to debug the problem or find the bottleneck, ensuring a quick fix or optimization.</p>&#13;
</div>&#13;
&#13;
<p>I would like to break the mentioned convention and introduce you to some basic concepts required for effective macrobenchmarking. Especially for backend applications, developers these days have much more to say when it comes to accurate efficiency assessment and bottleneck analysis at higher levels. So let’s use this fact and discuss some basic principles and provide a practical example of running a macrobenchmark via <code>go test</code>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Basics" data-type="sect2"><div class="sect2" id="ch-obs-macro-basics">&#13;
<h2>Basics</h2>&#13;
&#13;
<p><a data-primary="macrobenchmarks/macrobenchmarking" data-secondary="basics" data-type="indexterm" id="ix_ch08-asciidoc22"/>As we learned in <a data-type="xref" href="ch07.html#ch-obs-benchmarking">“Benchmarking Levels”</a>, macrobenchmarks focus on testing your code at the product level (application, service, or system) close to your functional and efficiency requirements (as described in <a data-type="xref" href="ch03.html#ch-conq-req-formal">“Efficiency Requirements Should Be Formalized”</a>). As a result, we could compare macrobenchmarking to integration or end-to-end (e2e) functional testing.</p>&#13;
&#13;
<p>In this section, I will mostly focus on benchmarking server-side, multicomponent Go backend applications. There are three reasons why:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>That’s my speciality.</p>&#13;
</li>&#13;
<li>&#13;
<p>It’s the typical target environment of applications written in the Go language.</p>&#13;
</li>&#13;
<li>&#13;
<p>This application typically involves working with nontrivial infrastructure and many complex dependencies.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Especially the last two items make it beneficial for me to focus on backend applications, as other types of programs (CLI, frontend, mobile) might require less-complex architecture. Still, all types will reuse some patterns and learnings from this section.</p>&#13;
&#13;
<p>For instance, in <a data-type="xref" href="#ch-obs-micro">“Microbenchmarks”</a>, we assessed the efficiency of the <code>Sum</code> function (<a data-type="xref" href="ch04.html#code-sum">Example 4-1</a>) in our Go code, but that function might have been a bottleneck for a much bigger product or service. Imagine that our team’s task is to develop and maintain a bigger microservice called <code>labeler</code> that uses the <code>Sum</code>.</p>&#13;
&#13;
<p>The <code>labeler</code> will run in a container and connect to an object storage<sup><a data-type="noteref" href="ch08.html#idm45606827011232" id="idm45606827011232-marker">25</a></sup> with various files. Each file has potentially millions of integers in each new line (the same input as in our <code>Sum</code> problem). The <code>labeler</code> job is to return a label—the metadata and &#13;
<span class="keep-together">some statistics</span> of the specified object when the user calls the HTTP <code>GET</code> method &#13;
<span class="keep-together"><code>/label_object</code>.</span> The returned label contains attributes like the object name, object size, checksum, and more. One of the key label fields is the sum of all numbers in the object.<sup><a data-type="noteref" href="ch08.html#idm45606827007440" id="idm45606827007440-marker">26</a></sup></p>&#13;
&#13;
<p>You learned first how to assess the efficiency of the smaller <code>Sum</code> function on a micro level because it’s simpler. On the product level the situation is much more complex. That’s why to perform reliable benchmarking (or bottleneck analysis) on a macro level, there are a few differences to notice and extra components to have. Let’s go through them, as presented in <a data-type="xref" href="#img-opt-macro-bench">Figure 8-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-opt-macro-bench">&#13;
<img alt="efgo 0801" src="assets/efgo_0801.png"/>&#13;
<h6><span class="label">Figure 8-1. </span>Common elements required for the macrobenchmark, for example, to benchmark the <code>labeler</code> service</h6>&#13;
</div></figure>&#13;
&#13;
<p>The specific differences from our <code>Sum</code> microbenchmark can be outlined as follows:</p>&#13;
<dl>&#13;
<dt>Our Go program as a separate process</dt>&#13;
<dd>&#13;
<p>Thanks to <a data-type="xref" href="#ch-obs-micro-go">“Go Benchmarks”</a>, we understand the efficiency of the <code>Sum</code> function and can optimize it. But what if another part of the code is now a bigger bottleneck in our flow? This is why we typically want to benchmark our Go program with its full user flow on a macro level. This means running the process in a similar fashion and configuration as in production. But unfortunately, this also means we can’t run the <code>go test</code> benchmarking framework anymore as we benchmark on the process level.</p>&#13;
</dd>&#13;
<dt>Dependencies, e.g., object storage</dt>&#13;
<dd>&#13;
<p>One of the key elements of macrobenchmarks is that we typically want to analyze the efficiency of the full system, including all key dependencies. This is especially important when our code might rely on certain efficiency characteristics of the dependency. In our <code>labeler</code> example, we use object storage, which usually means transferring bytes over the network. There might be little point in optimizing <code>Sum</code> if the object storage communication is the main bottleneck in latency or resource consumption. There are generally three ways of handling dependencies on a macro level:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>We can try to use realistic dependency (e.g., in our example, the exact object storage provider that will be used on production, with a similar dataset size). This is typically the best idea if we want to test the end-to-end efficiency of the whole system.</p>&#13;
</li>&#13;
</ul>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<ul class="less_space pagebreak-before">&#13;
<li>&#13;
<p>We can try to implement or use a <a href="https://oreil.ly/06UmC">fake</a> or adapter that will simulate production problems. However, this often takes too much effort and it’s hard to simulate the exact behavior of, for example, a slow TCP connection or server.</p>&#13;
</li>&#13;
<li>&#13;
<p>We could implement the simplest fake for our dependency and assess the isolated efficiency of our program. In our example, this might mean running local, open source object storage like <a href="https://min.io">Minio</a>. It will not reflect all the problems we might have with production dependencies, but it will give us some estimates on the problems and overhead for our program. We will use this in <a data-type="xref" href="#ch-obs-macro-example">“Go e2e Framework”</a> for simplicity.</p>&#13;
<dl>&#13;
<dt>Observability</dt>&#13;
<dd>&#13;
<p>We can’t use <a data-type="xref" href="#ch-obs-micro-go">“Go Benchmarks”</a> on a macro level, so we don’t have built-in support for latency, allocations, and custom metrics. So we have to provide our observability and monitoring solution. Fortunately, we already discussed instrumentation and observability for Go programs in <a data-type="xref" href="ch06.html#ch-observability">Chapter 6</a>, which we can use on a macro level. In <a data-type="xref" href="#ch-obs-macro-example">“Go e2e Framework”</a>, I will show you &#13;
<span class="keep-together">a framework</span> that has built-in support for the open source <a href="https://prometheus.io">Prometheus</a> project, which allows gathering latency, usage, and custom benchmarking metrics. You can enrich this setup with other tools like tracing, logging, and continuous profiling to debug the functional and efficiency problems even easier.</p>&#13;
</dd>&#13;
<dt>Load tester</dt>&#13;
<dd>&#13;
<p>Another consequence of getting out of the Go benchmark framework is the missing logic of triggering the experiment cases. Go benchmark was executing our code the desired amount of times with desired arguments. On the macro level, we might want to use this service as the user would use the HTTP REST API for web services like <code>labeler</code>. This is why we need some load-tester code that understands our APIs and will call them the desired amount of times and arguments.</p>&#13;
&#13;
<p>You can implement your own to simulate the user traffic, which unfortunately is prone to errors.<sup><a data-type="noteref" href="ch08.html#idm45606826978976" id="idm45606826978976-marker">27</a></sup> There are ways to “fork” or replay production traffic to the testing product using more advanced solutions like Kafka. Perhaps the easiest solution is to pick an off-the-shelf framework like an open source <a href="https://k6.io">k6</a> project, which is designed and battle-tested for &#13;
<span class="keep-together">load-testing</span> purposes. I will present an example of using k6 in <a data-type="xref" data-xrefstyle="select:nopage" href="#ch-obs-macro-example">“Go e2e Framework”</a>.</p>&#13;
</dd>&#13;
<dt>Continuous Integration (CI) and Continuous Deployment (CD)</dt>&#13;
<dd>&#13;
<p>Finally, we rarely run macrobenchmarks on local development machines for more complex systems. This means we might want to invest in automation that schedules the load test and deploys required components with the desired &#13;
<span class="keep-together">version.</span></p>&#13;
</dd>&#13;
</dl>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>With such architecture, we can perform the efficiency analysis on a macro level. Our goals are similar to what we have for <a data-type="xref" href="#ch-obs-micro">“Microbenchmarks”</a>, just on a more complex system, such as A/B testing and learning the space and runtime complexity of your system functionality. However, given that we are closer to how users use our system, we can also treat it as an acceptance test that will validate efficiency with our RAER.</p>&#13;
&#13;
<p>The theory is important, but how does it look in practice? Unfortunately, there is no consistent way of performing macrobenchmarks with Go, as it highly depends on your use case, environment, and goals. However, I would like to provide an example of a pragmatic and fast macrobenchmark of <code>labeler</code> that we can perform on our local development machine using Go code! So let’s dive into the next section.<a data-startref="ix_ch08-asciidoc22" data-type="indexterm" id="idm45606826969888"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Go e2e Framework" data-type="sect2"><div class="sect2" id="ch-obs-macro-example">&#13;
<h2>Go e2e Framework</h2>&#13;
&#13;
<p><a data-primary="containers" data-secondary="Go e2e framework and" data-type="indexterm" id="ix_ch08-asciidoc23"/><a data-primary="Docker containers, Go e2e framework and" data-type="indexterm" id="ix_ch08-asciidoc24"/><a data-primary="e2e framework" data-type="indexterm" id="ix_ch08-asciidoc25"/><a data-primary="Go e2e framework" data-type="indexterm" id="ix_ch08-asciidoc26"/><a data-primary="macrobenchmarks/macrobenchmarking" data-secondary="Go e2e framework" data-type="indexterm" id="ix_ch08-asciidoc27"/>Backend macrobenchmarking does not necessarily always mean using the same deployment mechanism we have in production (e.g., Kubernetes). However, to reduce the feedback loop, we can try macrobenchmarking with all the required dependencies, dedicated load tester, and observability on our developer machine or small virtual machine (VM). In many cases, it might give you reliable enough results on a macro level.</p>&#13;
&#13;
<p>For experiments, you can manually deploy all the elements mentioned in <a data-type="xref" href="#ch-obs-macro-basics">“Basics”</a> on your machine. For example, you can write a bash script or <a href="https://oreil.ly/x9LTf">Ansible</a> runbook. However, since we are Go developers looking to improve the efficiency of our code, what about implementing such a benchmark in Go code and saving it next to your benchmarked code?</p>&#13;
&#13;
<p>For this purpose, I would like to introduce you to the <a href="https://oreil.ly/f0IJo"><code>e2e</code></a> Go framework that allows running interactive or automated experiments on a single machine using Go code and Docker containers. <a href="https://oreil.ly/aMXxz">The container</a> is a concept that allows running processes in an isolated, secure sandbox environment while reusing the host’s kernel. In this concept, we execute software inside predefined container images. This means we must build (or download) a required image of the software we want to run beforehand. Alternatively, we can build our container image and add required software like pre-build binary of our Go program, e.g., <code>labeler</code>.</p>&#13;
&#13;
<p class="less_space pagebreak-before">A container is not a first-class citizen on any OS. Instead, it can be constructed &#13;
<span class="keep-together">with existing</span> Linux mechanisms like <code>cgroups</code>, <code>namespaces</code>, and Linux Security Modules (<a href="https://oreil.ly/C4h3z">LSMs</a>). Docker provides one implementation of the container engine, among others.<sup><a data-type="noteref" href="ch08.html#idm45606826953008" id="idm45606826953008-marker">28</a></sup> Containers are also heavily used for large cloud-native infrastructure thanks to orchestration systems like Kubernetes.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45606826951488">&#13;
<h5>Benefits of Benchmarking in Containers</h5>&#13;
<p><a data-primary="containers" data-secondary="macrobenchmarking in" data-type="indexterm" id="idm45606826950320"/><a data-primary="macrobenchmarks/macrobenchmarking" data-secondary="containers for" data-type="indexterm" id="idm45606826949344"/>There are many reasons why on a macro level, I prefer using containers, even for single-node local tests:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>They allow isolating our processes, enabling more reliable observability and limitation facilities. This allows us to constraint certain resources to simulate different production aspects and account for resource usage to a given process (e.g., network usage or CPU usage).</p>&#13;
</li>&#13;
<li>&#13;
<p>If you use containers on production, you can use the same container images in your macrobenchmarks. This ensures higher reliability—no unknowns are introduced by building, packaging, or installing phases.</p>&#13;
</li>&#13;
<li>&#13;
<p>Similarly, for analyzing the benchmarking situation, we can use the same instrumentation and observability as we use for production.<sup><a data-type="noteref" href="ch08.html#idm45606826944448" id="idm45606826944448-marker">29</a></sup></p>&#13;
</li>&#13;
<li>&#13;
<p>The isolation of containers has little overhead compared to heavier virtualization like <a href="https://oreil.ly/HEtBk">virtual machines (VMs)</a> that have to fully virtualize hardware resources like memory and CPU.</p>&#13;
</li>&#13;
<li>&#13;
<p>Easier installation and use of dependencies (portability!).</p>&#13;
</li>&#13;
</ul>&#13;
</div></aside>&#13;
<div data-type="warning" epub:type="warning">&#13;
<p>To leverage all benefits of containers, run only one process per container! Putting more processes (e.g., local database) into one container is tempting. But that defies the point of observing and isolating containers. Tools like Kubernetes or Docker are designed for singular processes per container, so put auxiliary processes in sidecar containers.</p>&#13;
</div>&#13;
&#13;
<p>Let’s go through a complete macrobenchmark implementation divided into &#13;
<span class="keep-together">two parts,</span> Examples <a data-type="xref" data-xrefstyle="select:labelnumber" href="#code-macrobench">8-19</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#code-macrobench2">8-20</a>, that assess latency and memory usage of our <code>labeler</code> service introduced in <a data-type="xref" href="#ch-obs-macro-basics">“Basics”</a>. For convenience, our implementation can be scripted and executed as a normal <code>go test</code> guarded by <code>t.Skip</code> or <a href="https://oreil.ly/tyue6">build tag</a> to execute it manually or in a different cadence than functional tests.<sup><a data-type="noteref" href="ch08.html#idm45606826932640" id="idm45606826932640-marker">30</a></sup></p>&#13;
<div data-type="example" id="code-macrobench">&#13;
<h5><span class="label">Example 8-19. </span>Go test running the macrobenchmark in interactive mode (part 1)</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kn">import</code><code class="w"> </code><code class="p">(</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"testing"</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/efficientgo/e2e"</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">e2edb</code><code class="w"> </code><code class="s">"github.com/efficientgo/e2e/db"</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">e2einteractive</code><code class="w"> </code><code class="s">"github.com/efficientgo/e2e/interactive"</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">e2emonitoring</code><code class="w"> </code><code class="s">"github.com/efficientgo/e2e/monitoring"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/efficientgo/core/testutil"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/thanos-io/objstore/providers/s3"</code><code class="w">&#13;
</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">TestLabeler_LabelObject</code><code class="p">(</code><code class="nx">t</code><code class="w"> </code><code class="o">*</code><code class="nx">testing</code><code class="p">.</code><code class="nx">T</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">e</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">e2e</code><code class="p">.</code><code class="nx">NewDockerEnvironment</code><code class="p">(</code><code class="s">"labeler"</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO16-1" id="co_benchmarking_CO16-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
    </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">t</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">t</code><code class="p">.</code><code class="nx">Cleanup</code><code class="p">(</code><code class="nx">e</code><code class="p">.</code><code class="nx">Close</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">mon</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">e2emonitoring</code><code class="p">.</code><code class="nx">Start</code><code class="p">(</code><code class="nx">e</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO16-2" id="co_benchmarking_CO16-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
    </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">t</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">t</code><code class="p">,</code><code class="w"> </code><code class="nx">mon</code><code class="p">.</code><code class="nx">OpenUserInterfaceInBrowser</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO16-3" id="co_benchmarking_CO16-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
&#13;
    </code><code class="nx">minio</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">e2edb</code><code class="p">.</code><code class="nx">NewMinio</code><code class="p">(</code><code class="nx">e</code><code class="p">,</code><code class="w"> </code><code class="s">"object-storage"</code><code class="p">,</code><code class="w"> </code><code class="s">"test"</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO16-4" id="co_benchmarking_CO16-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
    </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">t</code><code class="p">,</code><code class="w"> </code><code class="nx">e2e</code><code class="p">.</code><code class="nx">StartAndWaitReady</code><code class="p">(</code><code class="nx">minio</code><code class="p">)</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">labeler</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">e2e</code><code class="p">.</code><code class="nx">NewInstrumentedRunnable</code><code class="p">(</code><code class="nx">e</code><code class="p">,</code><code class="w"> </code><code class="s">"labeler"</code><code class="p">)</code><code class="p">.</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO16-5" id="co_benchmarking_CO16-5"><img alt="5" src="assets/5.png"/></a><code class="w">&#13;
        </code><code class="nx">WithPorts</code><code class="p">(</code><code class="kd">map</code><code class="p">[</code><code class="kt">string</code><code class="p">]</code><code class="kt">int</code><code class="p">{</code><code class="s">"http"</code><code class="p">:</code><code class="w"> </code><code class="mi">8080</code><code class="p">}</code><code class="p">,</code><code class="w"> </code><code class="s">"http"</code><code class="p">)</code><code class="p">.</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">Init</code><code class="p">(</code><code class="nx">e2e</code><code class="p">.</code><code class="nx">StartOptions</code><code class="p">{</code><code class="w">&#13;
</code><code class="w">            </code><code class="nx">Image</code><code class="p">:</code><code class="w"> </code><code class="s">"labeler:test"</code><code class="p">,</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO16-6" id="co_benchmarking_CO16-6"><img alt="6" src="assets/6.png"/></a><code class="w">&#13;
            </code><code class="nx">LimitCPUs</code><code class="p">:</code><code class="w"> </code><code class="mf">4.0</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">            </code><code class="nx">Command</code><code class="p">:</code><code class="w"> </code><code class="nx">e2e</code><code class="p">.</code><code class="nx">NewCommand</code><code class="p">(</code><code class="w">&#13;
</code><code class="w">                </code><code class="s">"/labeler"</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">                </code><code class="s">"-listen-address=:8080"</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">                </code><code class="s">"-objstore.config="</code><code class="o">+</code><code class="nx">marshal</code><code class="p">(</code><code class="nx">t</code><code class="p">,</code><code class="w"> </code><code class="nx">client</code><code class="p">.</code><code class="nx">BucketConfig</code><code class="p">{</code><code class="w">&#13;
</code><code class="w">                    </code><code class="nx">Type</code><code class="p">:</code><code class="w"> </code><code class="nx">client</code><code class="p">.</code><code class="nx">S3</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">                    </code><code class="nx">Config</code><code class="p">:</code><code class="w"> </code><code class="nx">s3</code><code class="p">.</code><code class="nx">Config</code><code class="p">{</code><code class="w">&#13;
</code><code class="w">                        </code><code class="nx">Bucket</code><code class="p">:</code><code class="w">    </code><code class="s">"test"</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">                        </code><code class="nx">AccessKey</code><code class="p">:</code><code class="w"> </code><code class="nx">e2edb</code><code class="p">.</code><code class="nx">MinioAccessKey</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">                        </code><code class="nx">SecretKey</code><code class="p">:</code><code class="w"> </code><code class="nx">e2edb</code><code class="p">.</code><code class="nx">MinioSecretKey</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">                        </code><code class="nx">Endpoint</code><code class="p">:</code><code class="w">  </code><code class="nx">minio</code><code class="p">.</code><code class="nx">InternalEndpoint</code><code class="p">(</code><code class="nx">e2edb</code><code class="p">.</code><code class="nx">AccessPortName</code><code class="p">)</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">                        </code><code class="nx">Insecure</code><code class="p">:</code><code class="w">  </code><code class="kc">true</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">                    </code><code class="p">}</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">                </code><code class="p">}</code><code class="p">)</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">            </code><code class="p">)</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">        </code><code class="p">}</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">t</code><code class="p">,</code><code class="w"> </code><code class="nx">e2e</code><code class="p">.</code><code class="nx">StartAndWaitReady</code><code class="p">(</code><code class="nx">labeler</code><code class="p">)</code><code class="p">)</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO16-1" id="callout_benchmarking_CO16-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The e2e project is a Go module that allows the creation of end-to-end testing environments. It currently supports running the components (in any language) in <a href="https://oreil.ly/iXrgX">Docker containers</a>, which allows clean isolation for both filesystems, network, and observability. Containers can talk to each other but can’t connect with the host. Instead, the host can connect to the container via mapped <code>localhost</code> ports printed at the container start.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO16-2" id="callout_benchmarking_CO16-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The <code>e2emonitoring.Start</code> method starts Prometheus and <a href="https://oreil.ly/v9gEL">cadvisor</a>. The latter translates cgroups related to our containers &#13;
<span class="keep-together">to Prometheus</span> metric format so it can collect them. Prometheus will &#13;
<span class="keep-together">also automatically</span> collect metrics from all containers started using <code>e2e.New​InstrumentedRunnable</code>.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO16-3" id="callout_benchmarking_CO16-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>For an interactive exploration of resource usage and application metrics, we can invoke <code>mon.OpenUserInterfaceInBrowser()</code> that will open the Prometheus UI in our browser (if running on a desktop).</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO16-4" id="callout_benchmarking_CO16-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p><code>Labeler</code> uses object storage dependency. As mentioned in <a data-type="xref" href="#ch-obs-macro-basics">“Basics”</a>, I simplified this benchmark by focusing on <code>labeler</code> Go program efficiency without the impact of remote object storage. For that purpose, local <code>Minio</code> container is suitable.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO16-5" id="callout_benchmarking_CO16-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Finally, it’s time to start our <code>labeler</code> Go program in the container. It is worth noticing that I set the container CPU limit to <code>4</code> (enforced by Linux <code>cgroups</code>) to ensure our local benchmark is not saturating all the CPUs my machines have. Finally, we inject object storage configuration to connect with the local <code>minio</code> instance.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO16-6" id="callout_benchmarking_CO16-6"><img alt="6" src="assets/6.png"/></a></dt>&#13;
<dd><p>I used the <code>labeler:test</code> image that is built locally. I often add a script in <code>Makefile</code> to produce such an image, e.g., <code>make docker</code>. You risk forgetting to build the image with the desired Go program version you want to benchmark, so be mindful of what you are testing!</p></dd>&#13;
</dl></div>&#13;
<div data-type="example" id="code-macrobench2">&#13;
<h5><span class="label">Example 8-20. </span>Go test running the macrobenchmark in interactive mode (part 2)</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="w">    </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">t</code><code class="p">,</code><code class="w"> </code><code class="nx">uploadTestInput</code><code class="p">(</code><code class="nx">minio</code><code class="p">,</code><code class="w"> </code><code class="s">"object1.txt"</code><code class="p">,</code><code class="w"> </code><code class="mf">2e6</code><code class="p">)</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO17-1" id="co_benchmarking_CO17-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
&#13;
    </code><code class="nx">k6</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">e</code><code class="p">.</code><code class="nx">Runnable</code><code class="p">(</code><code class="s">"k6"</code><code class="p">)</code><code class="p">.</code><code class="nx">Init</code><code class="p">(</code><code class="nx">e2e</code><code class="p">.</code><code class="nx">StartOptions</code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">Command</code><code class="p">:</code><code class="w"> </code><code class="nx">e2e</code><code class="p">.</code><code class="nx">NewCommandRunUntilStop</code><code class="p">(</code><code class="p">)</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">Image</code><code class="p">:</code><code class="w"> </code><code class="s">"grafana/k6:0.39.0"</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">t</code><code class="p">,</code><code class="w"> </code><code class="nx">e2e</code><code class="p">.</code><code class="nx">StartAndWaitReady</code><code class="p">(</code><code class="nx">k6</code><code class="p">)</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">url</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">fmt</code><code class="p">.</code><code class="nx">Sprintf</code><code class="p">(</code><code class="w">&#13;
</code><code class="w">        </code><code class="s">"http://%s/label_object?object_id=object1.txt"</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">        </code><code class="nx">labeler</code><code class="p">.</code><code class="nx">InternalEndpoint</code><code class="p">(</code><code class="s">"http"</code><code class="p">)</code><code class="p">,</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">t</code><code class="p">,</code><code class="w"> </code><code class="nx">k6</code><code class="p">.</code><code class="nx">Exec</code><code class="p">(</code><code class="nx">e2e</code><code class="p">.</code><code class="nx">NewCommand</code><code class="p">(</code><code class="w">&#13;
</code><code class="w">        </code><code class="s">"/bin/sh"</code><code class="p">,</code><code class="w"> </code><code class="s">"-c"</code><code class="p">,</code><code class="w"> </code><code class="s">`cat &lt;&lt; EOF | k6 run -u 1 -d 5m - </code><a class="co" href="#callout_benchmarking_CO17-2" id="co_benchmarking_CO17-2"><img alt="2" src="assets/2.png"/></a><code class="s">&#13;
import http from 'k6/http'; </code><a class="co" href="#callout_benchmarking_CO17-3" id="co_benchmarking_CO17-3"><img alt="3" src="assets/3.png"/></a><code class="s">&#13;
import { check, sleep } from 'k6';&#13;
&#13;
export default function () {&#13;
    const res = http.get('`</code><code class="o">+</code><code class="nx">url</code><code class="s">`');&#13;
    check(res, { </code><a class="co" href="#callout_benchmarking_CO17-4" id="co_benchmarking_CO17-4"><img alt="4" src="assets/4.png"/></a><code class="s">&#13;
        'is status 200': (r) =&gt; r.status === 200,&#13;
        'response': (r) =&gt;&#13;
            r.body.includes(&#13;
    '{"object_id":"object1.txt","sum":6221600000,"checksum":"SUUr'&#13;
            ),&#13;
    });&#13;
    sleep(0.5)&#13;
}&#13;
EOF`</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">testutil</code><code class="p">.</code><code class="nx">Ok</code><code class="p">(</code><code class="nx">t</code><code class="p">,</code><code class="w"> </code><code class="s">`e2einteractive.RunUntilEndpointHit()`</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_benchmarking_CO17-5" id="co_benchmarking_CO17-5"><img alt="5" src="assets/5.png"/></a><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO17-1" id="callout_benchmarking_CO17-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>We have to upload some test data. In our simple test, we upload a single file with two million lines, using a similar pattern we used in <a data-type="xref" href="#ch-obs-micro-go">“Go Benchmarks”</a>.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO17-2" id="callout_benchmarking_CO17-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>I choose <code>k6</code> as my load tester. <code>k6</code> works as a batch job, so I first have to create a long-running empty container. I can then execute new processes in the <code>k6</code> environment to put the desired load on my <code>labeler</code> service. As a shell command, I pass the load-testing script as an input to the <code>k6</code> CLI. I also specify the number of virtual users (<code>-u</code> or <code>--vus</code>) I want. VUS represents the workers or threads running load-test functions specified in the script. To keep our tests and results simple, let’s stick to one user for now to avoid simultaneous HTTP calls. The <code>-d</code> (short flag for <code>--duration</code>) is similar to the <code>-benchtime</code> flag in our <a data-type="xref" href="#ch-obs-micro-go">“Go Benchmarks”</a>. See more tips about using <a href="https://oreil.ly/AbLOD"><code>k6</code> here</a>.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO17-3" id="callout_benchmarking_CO17-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p><code>k6</code> accepts load-testing logic programmed in simple JavaScript code. My load test is simple. Make an HTTP <code>GET</code> call to the <code>labeler</code> path I want to benchmark. I choose to sleep 500 ms after each HTTP call to give the <code>labeler</code> server time to clean resources after each call.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO17-4" id="callout_benchmarking_CO17-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Similar to <a data-type="xref" href="#ch-obs-micro-corr">“Test Your Benchmark for Correctness!”</a>, we have to test the output. If we trigger a bug in the <code>labeler</code> code or macrobenchmark implementation, we might be measuring the wrong thing! Using the <code>check</code> JavaScript functions allows us to assert the expected HTTP code and output.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO17-5" id="callout_benchmarking_CO17-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>We might want to add here the automatic assertion rules that pass these tests when latency or memory usage is within a certain threshold. However, as we learned in <a data-type="xref" href="ch07.html#ch-obs-bench-intro-fun">“Comparison to Functional Testing”</a>, finding reliable assertion for efficiency is difficult. Instead, I recommend learning about our <code>labeler</code> efficiency in a more interactive way. The <code>e2einteractive.RunUntilEndpointHit()</code> stops the <code>go test</code> benchmark until you hit the printed HTTP URL. It allows us to explore all outputs and our observability signals, e.g., collected metrics about <code>labeler</code> and the test in Prometheus.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>The code snippet might be long, but it’s relatively small and readable compared to how many things it orchestrates. On the other hand, it has to describe quite a complex macrobenchmark to configure and schedule five processes in one reliable benchmark with rich instrumentation for containers and internal Go metrics.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Keep Your Container Images Versioned!</h1>&#13;
<p><a data-primary="containers" data-secondary="macrobenchmarking in" data-type="indexterm" id="idm45606826255056"/><a data-primary="containers" data-secondary="versioning of images" data-type="indexterm" id="idm45606826254176"/>It is important to ensure you benchmark against a deterministic version of dependencies. This is why you should avoid using <code>:latest</code> tags, as it is very common to update them without noticing them transparently. Furthermore, it’s quite upsetting to realize after the second benchmark that you cannot compare it to the result of the first one because the dependency version changed, which might (or might not!) potentially impact the results.</p>&#13;
</div>&#13;
&#13;
<p>You can start the benchmark in <a data-type="xref" href="#code-macrobench">Example 8-19</a> either via your IDE or a simple <code>go test . -v -run TestLabeler_LabelObject</code> command. Once the <code>e2e</code> framework creates a new Docker network, start Prometheus, cadvisor, <code>labeler</code>, and <code>k6</code> containers, and stream their output to your terminal. Finally, the <code>k6</code> load test will be executed. After the specified five minutes, we should have results printed with summarized statistics around correctness and latency for our tested functionality. The test will stop when we hit the printed URL. If we do that, the test will remove all containers and the Docker network.</p>&#13;
<div data-type="note" epub:type="note"><h1>Duration of Macrobenchmarks</h1>&#13;
<p>In <a data-type="xref" href="#ch-obs-micro-go">“Go Benchmarks”</a>, it was often enough to run a benchmark for 5–15 seconds. Why do I choose to run the macro load test for five minutes? Two main reasons:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Generally, the more complex functionality we benchmark, the more time and iterations we want to repeat to stabilize all the system components. For example, as we learned in <a data-type="xref" href="#ch-obs-micro-mem">“Microbenchmarks Versus Memory Management”</a>, microbenchmarks do not give us an accurate impact that GC might have on our code. With macrobenchmarks, we run a full <code>labeler</code> process, so we want to see how the Go GC will cope with the <code>labeler</code> work. However, to see the frequency, the impact of GC, and maximum memory usage, we need to run our program longer under stress.</p>&#13;
</li>&#13;
<li>&#13;
<p>For sustainable and cheaper observability and monitoring in production, we avoid measuring the state of our application too often. This is how the recommended Prometheus collection (scrape) interval is around 15 to 30 s. As a result, we might want to run our test through a couple of collection periods to obtain accurate measurements while also sharing the same observability as production.<a data-startref="ix_ch08-asciidoc27" data-type="indexterm" id="idm45606826242528"/><a data-startref="ix_ch08-asciidoc26" data-type="indexterm" id="idm45606826241824"/><a data-startref="ix_ch08-asciidoc25" data-type="indexterm" id="idm45606826241152"/><a data-startref="ix_ch08-asciidoc24" data-type="indexterm" id="idm45606826240480"/><a data-startref="ix_ch08-asciidoc23" data-type="indexterm" id="idm45606826239808"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
&#13;
<p>In the next section, I will go through the outputs this experiment gives us and potential observations we can make.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Understanding Results and Observations" data-type="sect2"><div class="sect2" id="ch-obs-macro-results">&#13;
<h2>Understanding Results and Observations</h2>&#13;
&#13;
<p><a data-primary="macrobenchmarks/macrobenchmarking" data-secondary="understanding results/observations" data-type="indexterm" id="idm45606826236208"/>As we saw in <a data-type="xref" href="#ch-obs-micro-res">“Understanding the Results”</a>, experimenting is only half of the success. The second half is to correctly interpret the results. After running <a data-type="xref" href="#code-macrobench">Example 8-19</a> for around seven minutes, we should see <code>k6</code> output<sup><a data-type="noteref" href="ch08.html#idm45606826232800" id="idm45606826232800-marker">31</a></sup> that might look like <a data-type="xref" href="#code-macrobench-k6-out">Example 8-21</a>.</p>&#13;
<div data-type="example" id="code-macrobench-k6-out">&#13;
<h5><span class="label">Example 8-21. </span>Last 24 lines of the macrobenchmark output from a 7-minute test with one virtual user (VUS) using <code>k6</code></h5>&#13;
&#13;
<pre data-type="programlisting">running (5m00.0s), 1/1 VUs, 476 complete and 0 interrupted iterations&#13;
default   [ 100% ] 1 VUs  5m00.0s/5m0s&#13;
running (5m00.4s), 0/1 VUs, 477 complete and 0 interrupted iterations&#13;
default ✓ [ 100% ] 1 VUs  5m0s&#13;
✓ is status 200&#13;
✓ response&#13;
checks....................: 100.00% ✓ 954      ✗ 0 <a class="co" href="#callout_benchmarking_CO18-1" id="co_benchmarking_CO18-1"><img alt="1" src="assets/1.png"/></a>&#13;
data_received.............: 108 kB  359 B/s&#13;
data_sent.................: 57 kB   191 B/s&#13;
http_req_blocked..........: avg=9.05µs  min=2.48µs  med=8.5µs    max=553.13µs&#13;
    p(90)=11.69µs p(95)=14.68µs&#13;
http_req_connecting.......: avg=393ns   min=0s      med=0s       max=187.71µs&#13;
http_req_duration.........: avg=128.9ms min=92.53ms med=126.05ms max=229.35ms <a class="co" href="#callout_benchmarking_CO18-2" id="co_benchmarking_CO18-2"><img alt="2" src="assets/2.png"/></a>&#13;
    p(90)=160.43ms p(95)=186.77ms <a class="co" href="#callout_benchmarking_CO18-2" id="co_benchmarking_CO18-3"><img alt="2" src="assets/2.png"/></a>&#13;
{ expected_response:true }: avg=128.9ms min=92.53ms med=126.05ms max=229.35ms&#13;
    p(90)=160.43ms p(95)=186.77ms&#13;
http_req_failed...........: 0.00%   ✓ 0        ✗ 477&#13;
http_req_receiving........: avg=60.17µs min=30.98µs med=46.48µs  max=348.96µs&#13;
    p(90)=95.05µs  p(95)=124.73µs&#13;
http_req_sending..........: avg=35.12µs min=11.34µs med=36.72µs  max=139.1µs&#13;
    p(90)=59.99µs  p(95)=67.34µs&#13;
http_req_waiting..........: avg=128.81ms min=92.45ms med=125.97ms max=229.22ms&#13;
    p(90)=160.24ms p(95)=186.7ms&#13;
http_reqs.................: 477     1.587802/s <a class="co" href="#callout_benchmarking_CO18-3" id="co_benchmarking_CO18-4"><img alt="3" src="assets/3.png"/></a>&#13;
iteration_duration........: avg=629.75ms min=593.8ms med=626.51ms max=730.08ms&#13;
    p(90)=661.23ms p(95)=687.81ms&#13;
iterations................: 477     1.587802/s <a class="co" href="#callout_benchmarking_CO18-3" id="co_benchmarking_CO18-5"><img alt="3" src="assets/3.png"/></a>&#13;
vus.......................: 1       min=1      max=1&#13;
vus_max...................: 1       min=1      max=1</pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_benchmarking_CO18-1" id="callout_benchmarking_CO18-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Check this line to ensure you measure successful calls!</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO18-2" id="callout_benchmarking_CO18-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p><code>http_req_duration</code> is the most important measurement if we want to track the latency of the total HTTP request latency.</p></dd>&#13;
<dt><a class="co" href="#co_benchmarking_CO18-4" id="callout_benchmarking_CO18-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>It’s also important to note the total number of calls we made (the more iterations we have, the more reliable it will be).</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>From the client’s perspective, the <code>k6</code> results can tell us much about the achieved throughput and latencies of different HTTP stages. It seems that with just one “worker” calling our method and waiting 500 ms, we reached around 1.6 calls per second (<code>http_reqs</code>) and the average client latency of 128.9 ms (<code>http_req_duration</code>). As we learned in <a data-type="xref" href="ch06.html#ch-obs-latency">“Latency”</a>, tail latency might be more relevant for latency measurements. For that, <code>k6</code> calculates the percentiles as well, which indicates that 90% of requests (<code>p90</code>) were faster than 160 ms. In <a data-type="xref" href="#ch-obs-micro-go">“Go Benchmarks”</a>, we learned that the <code>Sum</code> function involved in the process is taking 79 ms on average, which means it accounts for most of the average latency or even total <code>p90</code> latency. If we care about optimizing latency in this case, we should try to optimize <code>Sum</code>. We will learn how to verify that percentage and identify other bottlenecks in <a data-type="xref" href="ch09.html#ch-observability3">Chapter 9</a> with tools like profiling.</p>&#13;
&#13;
<p class="less_space pagebreak-before">Another important result we should check is the variance of our runs. I wish <code>k6</code> provided out-of-the-box variance calculation because it’s hard to tell how repeatable our iterations were without it. For example, we see that the fastest request took 92 ms, while the slowest took 229 ms. This looks concerning, but it’s normal to have first requests take longer. To tell for sure, we would need to perform the same test twice and measure the average and percentile values variance. For example, on my machine, the next run of the same 5-minute test gave me an average of 129 ms and a <code>p90</code> of 163 ms, which suggests the variance is small. Still, it’s best to gather those numbers in some spreadsheet and calculate the standard deviation to find the variance percentage. There might be room for a quick CLI tool like <code>benchstat</code> that would give us a similar analysis. This is important, as the same <a data-type="xref" href="ch07.html#ch-obs-rel">“Reliability of Experiments”</a> aspects apply to macrobenchmarks. If our results are not repeatable, we might want to improve our testing environment, reduce the number of unknowns, or test longer.</p>&#13;
&#13;
<p>The <code>k6</code> output is not everything we have! The beauty of macrobenchmarks with good usage monitoring and observability, like Prometheus, is that we can assess and debug many efficiency problems and questions. In the <a data-type="xref" href="#code-macrobench">Example 8-19</a> setup, we have instrumentation that gives us <code>cgroup</code> metrics about containers and processes thanks to <code>cadvisor</code>, built-in process and heap metrics from the <code>labeler</code> Go runtime, and application-level HTTP metrics I manually instrumented in <code>labeler</code> code. As a result, we can check the usage metrics we care for based on our goals and the RAER (see <a data-type="xref" href="ch03.html#ch-conq-eff-flow">“Efficiency-Aware Development Flow”</a>), for example, the metrics we discussed in <a data-type="xref" href="ch06.html#ch-obs-semantics">“Efficiency Metrics Semantics”</a> and more.</p>&#13;
&#13;
<p>Let’s go through some metric visualizations I could see in Prometheus after my run.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Server-side latency" data-type="sect3"><div class="sect3" id="idm45606826194512">&#13;
<h3>Server-side latency</h3>&#13;
&#13;
<p><a data-primary="latency" data-secondary="macrobenchmarking and" data-type="indexterm" id="ix_ch08-asciidoc28"/><a data-primary="macrobenchmarks/macrobenchmarking" data-secondary="server-side latency" data-type="indexterm" id="ix_ch08-asciidoc29"/><a data-primary="server-side latency, macrobenchmarking and" data-type="indexterm" id="ix_ch08-asciidoc30"/>In our local tests, we use a local network, so there should be almost no difference between server and client latency (we talked about this difference in <a data-type="xref" href="ch06.html#ch-obs-latency">“Latency”</a>). However, more complex macro tests that may load test systems from different servers or remote devices in another geolocation might introduce network overhead that we may want or don’t want to account for in our results. If we don’t, we can query Prometheus for the average request duration server handled for our &#13;
<span class="keep-together"><code>/label_object</code></span> path, as presented in <a data-type="xref" href="#img-macrobench-avglat">Figure 8-2</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-macrobench-avglat">&#13;
<img alt="efgo 0803" src="assets/efgo_0803.png"/>&#13;
<h6><span class="label">Figure 8-2. </span>Dividing <code>http_request_duration_seconds</code> histogram sum by count rates to obtain server-side latency</h6>&#13;
</div></figure>&#13;
&#13;
<p>The results confirm what we saw in <a data-type="xref" href="#code-macrobench-k6-out">Example 8-21</a>. The observed average latency is around 0.12–0.15 seconds, depending on the moment. The metric comes from manually created HTTP middleware I added in Go using the <a href="https://oreil.ly/j1k4E"><code>prometheus/client_golang</code> library</a>.<sup><a data-type="noteref" href="ch08.html#idm45606826182432" id="idm45606826182432-marker">32</a></sup></p>&#13;
<div data-type="note" epub:type="note"><h1>Prometheus Rate Duration</h1>&#13;
<p><a data-primary="Prometheus" data-secondary="rate duration" data-type="indexterm" id="idm45606826179520"/>Notice I am using <code>[1m]</code> range vectors for Prometheus counters in queries for this macrobenchmark. This is because we only run our tests for 5 minutes. With a 15-second scrape, 1 minute should have enough samples for <code>rate</code> to make sense, but also I can see more details in my metric value with one-time minute window &#13;
<span class="keep-together">granularity.</span></p>&#13;
</div>&#13;
&#13;
<p>When it comes to the server-side percentile, we rely on a bucketed histogram. This means that the accuracy of the result is up to the nearest bucket. In <a data-type="xref" href="#code-macrobench-k6-out">Example 8-21</a>, we saw that results are 92 ms to 229 ms, with <code>p90</code> equal to 136 ms. At the moment of benchmark, the buckets were defined in <code>labeler</code> as follows: <code>0.001, 0.01, 0.1, 0.3, 0.6, 1, 3, 6, 9, 20, 30, 60, 90, 120, 240, 360, 720</code>. As a result, we can only tell that 90% of requests were faster than 300 ms, as presented in <a data-type="xref" href="#img-macrobench-p90">Figure 8-3</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-macrobench-p90">&#13;
<img alt="efgo 0804" src="assets/efgo_0804.png"/>&#13;
<h6><span class="label">Figure 8-3. </span>Using the <code>http_request_duration_seconds</code> histogram to calculate the <code>p90</code> quantile of the <code>/label_object</code> request</h6>&#13;
</div></figure>&#13;
&#13;
<p>To find more accurate results, we might need to adjust buckets manually or use a new sparse histogram feature in the upcoming Prometheus 2.40 version. The default buckets work well in cases when we don’t care if the request was handled in 100 ms or 300 ms, but we care if it was suddenly 1 second.<a data-startref="ix_ch08-asciidoc30" data-type="indexterm" id="idm45606826168656"/><a data-startref="ix_ch08-asciidoc29" data-type="indexterm" id="idm45606826167952"/><a data-startref="ix_ch08-asciidoc28" data-type="indexterm" id="idm45606826167280"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="CPU time" data-type="sect3"><div class="sect3" id="idm45606826166352">&#13;
<h3>CPU time</h3>&#13;
&#13;
<p><a data-primary="CPU resource" data-secondary="macrobenchmarking and" data-type="indexterm" id="ix_ch08-asciidoc31"/><a data-primary="macrobenchmarks/macrobenchmarking" data-secondary="CPU time and" data-type="indexterm" id="ix_ch08-asciidoc32"/>Latency is one thing, but CPU time can tell us how much time the CPU needs to fulfill its job, how much concurrency can help, and if our process is CPU or I/O bound. We can also tell if we gave enough CPU for the current process load. As we learned in <a data-type="xref" href="ch04.html#ch-hardware">Chapter 4</a>, higher latency of our iterations might be a result of the CPU saturation—our program using all available CPU cores (or close to the limit), in effect slowing the execution of all goroutines.</p>&#13;
&#13;
<p>In our benchmark we can use either the Go runtime <code>process_cpu_seconds_total</code> counter or the <code>cadvisor</code> <code>container_cpu_usage_seconds_total</code> counter to find that number. This is because <code>labeler</code> is the only process in its container. Both metrics look similar, with the latter presented in <a data-type="xref" href="#img-macrobench-cpu">Figure 8-4</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-macrobench-cpu">&#13;
<img alt="efgo 0805" src="assets/efgo_0805.png"/>&#13;
<h6><span class="label">Figure 8-4. </span>Using the <code>container_cpu_usage_seconds_total</code> counter to assess <code>labeler</code> CPU usage</h6>&#13;
</div></figure>&#13;
&#13;
<p class="less_space pagebreak-before">The value oscillates between 0.25–0.27 CPU seconds, which represents the amount of CPU time the <code>labeler</code> needed for this load. I limited <code>labeler</code> to 4 CPU cores, but it used a maximum of 27% of a single CPU. This means that, most likely, the CPUs are not saturated (unless there are a lot of noisy neighbors running at the same moment, which we would see in the latency numbers). The 270 ms of CPU time per second seems like a sane value given that our requests take, on average, 128.9 ms, and after that, <code>k6</code> was waiting for 500 ms. This gives us 20%<sup><a data-type="noteref" href="ch08.html#idm45606826151952" id="idm45606826151952-marker">33</a></sup> of load-testing time, so the <code>k6</code> was actually demanding some work from <code>labeler</code>, which might not all be used on CPU, but also on I/O time. The <code>labeler</code> <code>/label_object</code> execution in our current version is sequential, but there are some background tasks, like listening to signal, metric collection, GC, and HTTP background goroutines. Again, see <a data-type="xref" href="ch09.html#ch-obs-profiling">“Profiling in Go”</a> as the best way to tell exactly what’s taking the CPU here.<a data-startref="ix_ch08-asciidoc32" data-type="indexterm" id="idm45606826148000"/><a data-startref="ix_ch08-asciidoc31" data-type="indexterm" id="idm45606826147248"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Memory" data-type="sect3"><div class="sect3" id="idm45606826154432">&#13;
<h3>Memory</h3>&#13;
&#13;
<p><a data-primary="macrobenchmarks/macrobenchmarking" data-secondary="memory and" data-type="indexterm" id="ix_ch08-asciidoc33"/><a data-primary="memory resource" data-secondary="macrobenchmarking and" data-type="indexterm" id="ix_ch08-asciidoc34"/>In <a data-type="xref" href="#ch-obs-micro">“Microbenchmarks”</a>, we learned how much memory <code>Sum</code> allocates, but <code>Sum</code> is not the only logic <code>labeler</code> has to perform. Therefore, if we want to assess the memory efficiency of <code>labeler</code>, we need to look at the process or container level memory metrics we gathered during our benchmark. On top of that, we mentioned in <a data-type="xref" href="#ch-obs-micro-mem">“Microbenchmarks Versus Memory Management”</a> that only on the macro level do we have a chance to learn more about GC impact and maximum memory usage of our <code>labeler</code> process.</p>&#13;
&#13;
<p>Looking at the heap metric presented in <a data-type="xref" href="#img-macrobench-heap">Figure 8-5</a>, we can observe that a single &#13;
<span class="keep-together"><code>/label_object</code></span> is using the nontrivial amount of memory. This is not unexpected after seeing the <code>Sum</code> function microbenchmarks results in <a data-type="xref" href="#code-sum-go-bench-benchstat2">Example 8-7</a> showing 60.8 MB per iteration.</p>&#13;
&#13;
<p>This observation shows us the eventuality of GC that might cause problems. Given a single “worker” (VUS) in <code>k6</code>, the <code>labeler</code> should never need more than ~61 MB of live memory if the <code>Sum</code> is the main bottleneck. However, we can see that for durations of 2 scrapes (30 seconds) and then 1 scrape, the memory got bumped to 118 MB. Most likely, GC had not released memory from the previous HTTP <code>/label_object</code> call before the second call started. If we account for spikes, the overall maximum heap size is stable at around 120 MB, which should tell us there are no immediate memory leaks.<sup><a data-type="noteref" href="ch08.html#idm45606826132608" id="idm45606826132608-marker">34</a></sup></p>&#13;
&#13;
<figure><div class="figure" id="img-macrobench-heap">&#13;
<img alt="efgo 0806" src="assets/efgo_0806.png"/>&#13;
<h6><span class="label">Figure 8-5. </span>Using the <code>go_memstats_heap_alloc_bytes</code> gauge to assess <code>labeler</code> heap usage</h6>&#13;
</div></figure>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45606826128480">&#13;
<h5>go_memstats_heap_alloc_bytes Gauge and Temporary Changes</h5>&#13;
<p><a data-primary="Prometheus" data-secondary="gauge metric problems" data-type="indexterm" id="idm45606826127200"/>Be careful with any Prometheus gauges that monitor changes that occur more often than the scrape interval. For example, our Go program might have more spikes like the two we see in <a data-type="xref" href="#img-macrobench-heap">Figure 8-5</a>, but they were too short to be observed by Prometheus in the <code>go_memstats_heap_alloc_bytes</code> metric.<sup><a data-type="noteref" href="ch08.html#idm45606826124784" id="idm45606826124784-marker">35</a></sup></p>&#13;
&#13;
<p>Something similar can happen when querying a gauge metric over a long period, like a dozen hours or days. The UI resolution (so-called <code>step</code>) is adjusted for longer periods and can potentially hide interesting moments. Ensure lower resolution or use <code>max_over_time</code> to know for sure what were the observed maximums (or <code>min_over_time</code> for minimums).</p>&#13;
&#13;
<p>This is rarely the problem in terms of memory as the GC and OS react very slowly with lazy memory release mechanisms, explained in <a data-type="xref" href="ch05.html#ch-hw-memory-os">“OS Memory Management”</a>.</p>&#13;
</div></aside>&#13;
&#13;
<p>Unfortunately, as we learned in <a data-type="xref" href="ch05.html#ch-hw-memory-os">“OS Memory Management”</a> and <a data-type="xref" href="ch06.html#ch-obs-mem-usage">“Memory Usage”</a>, the memory used by the heap is only a portion of the RAM space that is used by the Go program. The space allocated for goroutine stacks, manually created memory maps, and kernel cache (e.g., for file access) requires the OS to reserve more pages on the physical memory. We can see that when we look at our container-level RSS metric presented in <a data-type="xref" href="#img-macrobench-rss">Figure 8-6</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-macrobench-rss">&#13;
<img alt="efgo 0807" src="assets/efgo_0807.png"/>&#13;
<h6><span class="label">Figure 8-6. </span>Using the <code>container_memory_rss</code> gauge to assess <code>labeler</code> physical RAM usage</h6>&#13;
</div></figure>&#13;
&#13;
<p>Fortunately, nothing unexpected on the RSS side as well. The active memory pages were more or less the size of the heap and returned to a smaller level as soon as the test finished. So we can assess that <code>labeler</code> requires around 130 MB of memory for this load.<a data-startref="ix_ch08-asciidoc34" data-type="indexterm" id="idm45606826111856"/><a data-startref="ix_ch08-asciidoc33" data-type="indexterm" id="idm45606826111152"/></p>&#13;
&#13;
<p>To sum up, we assessed the efficiency of latency and resources like CPU and memory on a macro level. In practice, we can assess much more, depending on our efficiency goals like disk, network, I/O devices, DB usage, and more. The <code>k6</code> configuration was straightforward in our test—single worker and sequential calls with a pause. Let’s explore other variations and possibilities in the next section.<a data-startref="ix_ch08-asciidoc21" data-type="indexterm" id="idm45606826109680"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Common Macrobenchmarking Workflows" data-type="sect1"><div class="sect1" id="ch-obs-macro-workflows">&#13;
<h1>Common Macrobenchmarking Workflows</h1>&#13;
&#13;
<p><a data-primary="macrobenchmarks/macrobenchmarking" data-secondary="common workflows" data-type="indexterm" id="ix_ch08-asciidoc35"/>The example test in <a data-type="xref" href="#ch-obs-macro-example">“Go e2e Framework”</a> should give you some awareness of how to configure the example load-testing tool, hook in dependencies, and set up and use pragmatic observability for efficiency analysis. On top of that, you can expand such local <code>e2e</code> tests in the direction you and your project need based on the efficiency goals. For example:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Load test your system with more than one worker to assess how many resources it takes to sustain a given request per second (RPS) rate while sustaining a desired <code>p90</code> latency.<sup><a data-type="noteref" href="ch08.html#idm45606826102992" id="idm45606826102992-marker">36</a></sup></p>&#13;
</li>&#13;
<li>&#13;
<p>Run <code>k6</code> or other load-testing tools to simulate realistic client traffic in a different location.</p>&#13;
</li>&#13;
<li>&#13;
<p>Deploy the macrobenchmark on remote servers, perhaps with the same hardware as your production.</p>&#13;
</li>&#13;
<li>&#13;
<p>Deploy dependencies in a remote location; e.g., in our <code>labeler</code> example, use the <a href="https://oreil.ly/pzeua">AWS S3 service</a> instead of the local object storage instance.</p>&#13;
</li>&#13;
<li>&#13;
<p>Scale out your macro test and services to multiple replicas to check if the traffic can be load balanced properly, so the system’s efficiency stays predictable.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Similar to <a data-type="xref" href="#ch-obs-micro-workflow">“Find Your Workflow”</a>, you should find the workflow for performing such experiments and analysis that suits you the most. For example, for myself and the teams I worked with, the process of designing and using the macrobenchmark like in <a data-type="xref" href="#ch-obs-macro-example">“Go e2e Framework”</a> might look as follows:</p>&#13;
<ol>&#13;
<li>&#13;
<p>As a team, we plan the macrobenchmark elements, dependencies, what aspects we want to benchmark, and what load we want to put on it.</p>&#13;
</li>&#13;
<li>&#13;
<p>I ensure a clean code state for <code>labeler</code> and macrobenchmark code. I commit all the changes to know what I am testing and with what benchmark. Let’s say we end up with a benchmark as in <a data-type="xref" href="#ch-obs-macro-example">“Go e2e Framework”</a>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Before starting the benchmark, I create a shared Google Document<sup><a data-type="noteref" href="ch08.html#idm45606826088720" id="idm45606826088720-marker">37</a></sup> and note all the experiment details like environmental conditions and software version.</p>&#13;
</li>&#13;
<li>&#13;
<p>I perform the benchmark to assess the efficiency of a given program version:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>I run my macrobenchmarks, e.g., by starting the <code>go test</code> with the Go e2e framework (see <a data-type="xref" href="#ch-obs-macro-example">“Go e2e Framework”</a>) in Goland IDE and waiting until the load test finishes.</p>&#13;
</li>&#13;
<li>&#13;
<p>I confirm no functional errors are present.</p>&#13;
</li>&#13;
<li>&#13;
<p>I save the <code>k6</code> results to Google Documents.</p>&#13;
</li>&#13;
<li>&#13;
<p>I gather interesting observations of the resources I want to focus on, for example, heap and RSS to assess memory efficiency. I capture screenshots and paste them to my Google document.<sup><a data-type="noteref" href="ch08.html#idm45606826080928" id="idm45606826080928-marker">38</a></sup> Finally, I note all conclusions I made.</p>&#13;
</li>&#13;
<li>&#13;
<p>Optionally, I gather profiles for the <a data-type="xref" href="ch09.html#ch-obs-profiling">“Profiling in Go”</a> process.</p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
<li>&#13;
<p>If the findings allowed me to find the optimization in my code, I implement it and save it as a new <code>git</code> commit. Then I benchmark again (see step 5) and save the new results to the same Google Doc under a different version, so I can compare my A/B test later on.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>The preceding workflow allows us to analyze the results and conclude an efficiency assessment given the assumptions that can be formulated thanks to the document I create. Linking the exact benchmark, which ideally is committed to the source code, allows others to reproduce the same test to verify results or perform further benchmarks and tests. Again, feel free to use any practice you need as long as you care for the elements mentioned in <a data-type="xref" href="ch07.html#ch-obs-rel">“Reliability of Experiments”</a>. There is no single consistent procedure and framework for macrobenchmarking, and it all highly depends on the type of software, production conditions, and price you want to invest in to ensure your product’s efficiency.</p>&#13;
&#13;
<p>It’s also worth mentioning that macrobenchmarking is not so far from <a data-type="xref" href="ch07.html#ch-obs-benchmarking-prod">“Benchmarking in Production”</a>. You can reuse many elements for macrobenchmarks like load tester and observability tooling in benchmarking against production (and vice versa). Such interoperability allows us to save time on building and learning new tools. The main difference in performing benchmarks in a production environment is to assure the quality of the production users—either by ensuring basic qualities of a new software version on different testing and benchmarking levels, or by leveraging beta testers or canary deployments.<a data-startref="ix_ch08-asciidoc35" data-type="indexterm" id="idm45606826073520"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45606826072720">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Congratulations! With this chapter, you should now understand how to practically perform micro- and macrobenchmarks, which are core ways to understand if we have to optimize our software further, what to optimize if we have to, and how much. Moreover, both micro- and macrobenchmarks are also invaluable in other aspects of software development connected to efficiency like capacity planning and scalability.<sup><a data-type="noteref" href="ch08.html#idm45606826071280" id="idm45606826071280-marker">39</a></sup></p>&#13;
&#13;
<p>In my daily career in software development, I lean heavily on micro- and macrobenchmarks. Thanks to the micro-level fast feedback loop, I often do them for smaller functions in the critical path to decide how the implementation should go. They are easy to write and easy to delete.</p>&#13;
&#13;
<p>Macrobenchmarks require more investment, so I especially recommend creating and doing such benchmarks:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>As an acceptance test against the RAER assessment of the entire system after a bigger feature or release.</p>&#13;
</li>&#13;
<li>&#13;
<p>When debugging and optimizing regressions or incidents that trigger efficiency problems.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The experimentation involved in both micro- and macrobenchmarks is useful for efficiency assessment and in <a data-type="xref" href="ch03.html#ch-conq-eff-flow-6">“6. Find the main bottleneck”</a>. However, during that benchmark, we can also perform profiling of our Go program to deduce the main efficiency bottlenecks. Let’s see how to do that in action in the next chapter!<a data-startref="ix_ch08-asciidoc1" data-type="indexterm" id="idm45606826064912"/><a data-startref="ix_ch08-asciidoc0" data-type="indexterm" id="idm45606826064240"/></p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45606829538272"><sup><a href="ch08.html#idm45606829538272-marker">1</a></sup> For bigger projects, I would suggest adding the <em>_bench_test.go</em> suffix for an easier way of discovering  <span class="keep-together">benchmarks.</span></p><p data-type="footnote" id="idm45606829527136"><sup><a href="ch08.html#idm45606829527136-marker">2</a></sup> It is well explained in the <a href="https://oreil.ly/PRrlW">testing package’s Example documentation</a>.</p><p data-type="footnote" id="idm45606829392480"><sup><a href="ch08.html#idm45606829392480-marker">3</a></sup> If we would remove <code>b.N</code> completely, the Go benchmark will try to increase a number of <code>N</code> until the whole <code>BenchmarkSum</code> will take at least 1 second. Without the <code>b.N</code> loop, our benchmark will never exceed 1 second as it does not depend on <code>b.N</code>. Such a benchmark will stop at <code>b.N</code> being equal to 1 billion iterations, but with just a single iteration being executed, the benchmark results will be wrong.</p><p data-type="footnote" id="idm45606829378320"><sup><a href="ch08.html#idm45606829378320-marker">4</a></sup> As mentioned earlier, microbenchmarks are always based on some amount of assumptions; we cannot simulate everything in such a small test.</p><p data-type="footnote" id="idm45606829377392"><sup><a href="ch08.html#idm45606829377392-marker">5</a></sup> Note that it definitely will not take 29 nanoseconds for a benchmark with a single integer. This number is a latency we see for a larger number of integers.</p><p data-type="footnote" id="idm45606829374944"><sup><a href="ch08.html#idm45606829374944-marker">6</a></sup> Note that it is acceptable to change test data in future versions of our program and benchmark. Usually, our optimizations over time make our test dataset “too small,” so we can increase it over time to spot different problems if we need to optimize further.</p><p data-type="footnote" id="idm45606829214608"><sup><a href="ch08.html#idm45606829214608-marker">7</a></sup> As explained previously, note that the full benchmarking process can take longer than 10 seconds because the Go framework will try to find a correct number of iterations. The more variance in the test results—potentially the longer the test will last.</p><p data-type="footnote" id="idm45606829011072"><sup><a href="ch08.html#idm45606829011072-marker">8</a></sup> You can also provide multiple numbers after a comma. For example, <code>-cpu=1,2,3</code> will run a test with <code>GOMAXPROCS</code> set to 1, then to 2, and the third run with 3 CPUs.</p><p data-type="footnote" id="idm45606828985008"><sup><a href="ch08.html#idm45606828985008-marker">9</a></sup> The internal representation of that format can be explored by looking at <a href="https://oreil.ly/90wO2"><code>BenchmarkResult</code> type</a>.</p><p data-type="footnote" id="idm45606828962032"><sup><a href="ch08.html#idm45606828962032-marker">10</a></sup> Things like the Go version, Linux kernel version, other processes running at the same time, CPU mode, etc. Unfortunately, the full list is almost impossible to capture.</p><p data-type="footnote" id="idm45606828942480"><sup><a href="ch08.html#idm45606828942480-marker">11</a></sup> The Go testing framework does not check how many CPUs are free to be used for this benchmark. As you learned in <a data-type="xref" href="ch04.html#ch-hardware">Chapter 4</a>, CPUs are shared fairly across other processes, so with more processes in the system, the four CPUs, in my case, are not fully reserved for the benchmark. On top of that, programmatic changes to <code>runtime.GOMAXPROCS</code> are not reflected here.</p><p data-type="footnote" id="idm45606828737680"><sup><a href="ch08.html#idm45606828737680-marker">12</a></sup> Make sure to strictly control the Go version you use to build those binaries. Testing binaries built using a different Go version might create misleading results. For example, you can build a binary and add a suffix to its name with the <code>git</code> hash of the version of your source code.</p><p data-type="footnote" id="idm45606828728032"><sup><a href="ch08.html#idm45606828728032-marker">13</a></sup> This is especially important for distributed systems and user-facing applications that handle errors very often, and it’s part of the normal program life cycle. For example, I often worked with code that was fast for database writes, but was allocating an extreme amount of memory on failed runs, causing cascading failures.</p><p data-type="footnote" id="idm45606828555728"><sup><a href="ch08.html#idm45606828555728-marker">14</a></sup> In my benchmarks, on my machine, this instruction alone takes 244 ns and allocates zero bytes.</p><p data-type="footnote" id="idm45606828553248"><sup><a href="ch08.html#idm45606828553248-marker">15</a></sup> Profiling, explained in <a data-type="xref" href="ch09.html#ch-obs-profiling">“Profiling in Go”</a>, can also help determine how much your benchmark affects those overheads.</p><p data-type="footnote" id="idm45606828507296"><sup><a href="ch08.html#idm45606828507296-marker">16</a></sup> Note that <code>TB</code> is my own invention and it’s not common or recommended by the Go community, so use with care!</p><p data-type="footnote" id="idm45606828284848"><sup><a href="ch08.html#idm45606828284848-marker">17</a></sup> In fact, we should not even trust ourselves there! A second careful reviewer is always a good idea.</p><p data-type="footnote" id="idm45606828161072"><sup><a href="ch08.html#idm45606828161072-marker">18</a></sup> Note that the <code>t.TempDir</code> and <code>b.TempDir</code> methods create a new, unique directory every time they are invoked!</p><p data-type="footnote" id="idm45606827504960"><sup><a href="ch08.html#idm45606827504960-marker">19</a></sup> For longer microbenchmarks, you might see the GC latency. Some tutorials also recommend running <a href="https://oreil.ly/7v3oE">microbenchmarks without GC</a> (using <code>GOGC=off</code>), but I found this not useful in practice. Ideally, move to the macro level to understand the full impact.</p><p data-type="footnote" id="idm45606827483632"><sup><a href="ch08.html#idm45606827483632-marker">20</a></sup> Unless you run with the parallel option I discouraged in <a data-type="xref" href="ch07.html#ch-obs-rel-unkn">“Performance Nondeterminism”</a>.</p><p data-type="footnote" id="idm45606827468608"><sup><a href="ch08.html#idm45606827468608-marker">21</a></sup> The idea behind this function comes from amazing Dave’s <a href="https://oreil.ly/BKZfr">tutorial</a> and <a href="https://oreil.ly/m3Yiy">issue 14813</a>, with some modifications.</p><p data-type="footnote" id="idm45606827194896"><sup><a href="ch08.html#idm45606827194896-marker">22</a></sup> I am not discouraging microbenchmarks on super low-level functions. You can still compare things, but be mindful that production numbers might surprise you.</p><p data-type="footnote" id="idm45606827192512"><sup><a href="ch08.html#idm45606827192512-marker">23</a></sup> This does not mean that the future Go compiler won’t be able to be smarter and consider optimization with global variables.</p><p data-type="footnote" id="idm45606827188416"><sup><a href="ch08.html#idm45606827188416-marker">24</a></sup> The <code>sink</code> pattern is also popular in C++ for <a href="https://oreil.ly/UpGFo">the same reasons</a>.</p><p data-type="footnote" id="idm45606827011232"><sup><a href="ch08.html#idm45606827011232-marker">25</a></sup> Object storage is cheap cloud storage with simple APIs for uploading objects and reading them or their byte ranges. It treats all data in the form of objects with a certain ID that typically looks similar to the file path.</p><p data-type="footnote" id="idm45606827007440"><sup><a href="ch08.html#idm45606827007440-marker">26</a></sup> You can find simplified microservice code in the <a href="https://oreil.ly/myFWw"><code>labeler</code> package</a>.</p><p data-type="footnote" id="idm45606826978976"><sup><a href="ch08.html#idm45606826978976-marker">27</a></sup> One common pitfall is to implement inefficient load-testing code. There is a risk that your application does not allow the throughput you want only because the client is not sending the traffic fast enough!</p><p data-type="footnote" id="idm45606826953008"><sup><a href="ch08.html#idm45606826953008-marker">28</a></sup> This space expanded quite quickly with two separate specifications (CRI and OCI) and various implementations of various parts of the container ecosystem. Read more about it <a href="https://oreil.ly/yKSL8">here</a>.</p><p data-type="footnote" id="idm45606826944448"><sup><a href="ch08.html#idm45606826944448-marker">29</a></sup> This is often underestimated. Creating reusable dashboards, learning about your instrumentation, and what metrics mean takes a nontrivial amount of work. If our local testing and production environment share the same metrics and other signals, it saves us a lot of time and increases the chances our observability is high quality.</p><p data-type="footnote" id="idm45606826932640"><sup><a href="ch08.html#idm45606826932640-marker">30</a></sup> You can run this code yourself or explore the <code>e2e</code> framework to see how it configures all components <a href="https://oreil.ly/ftAY1">here</a>.</p><p data-type="footnote" id="idm45606826232800"><sup><a href="ch08.html#idm45606826232800-marker">31</a></sup> There is also a way to push those results directly to <a href="https://oreil.ly/1UdNR">Prometheus</a>.</p><p data-type="footnote" id="idm45606826182432"><sup><a href="ch08.html#idm45606826182432-marker">32</a></sup> See the <a href="https://oreil.ly/22YQp">example code</a> that <code>labeler</code> uses.</p><p data-type="footnote" id="idm45606826151952"><sup><a href="ch08.html#idm45606826151952-marker">33</a></sup> 128.9 ms divided by 128.9+500 milliseconds to tell what portion of time the load tester was actively  <span class="keep-together">load-testing.</span></p><p data-type="footnote" id="idm45606826132608"><sup><a href="ch08.html#idm45606826132608-marker">34</a></sup> Looking on <code>go_goroutines</code> also helps. If we see a visible trend, we might forget to close some resources.</p><p data-type="footnote" id="idm45606826124784"><sup><a href="ch08.html#idm45606826124784-marker">35</a></sup> The solution is to use counters. For memory, it would mean using the existing <code>rate(go_memstats_alloc_bytes_total[1m])</code> and dividing it by the rate of bytes released by the GC. Unfortunately, the Prometheus Go collector does not expose such metrics. Go <a href="https://oreil.ly/Noqnp">allows us to get this information</a>, so it is possible to get it added in the future.</p><p data-type="footnote" id="idm45606826102992"><sup><a href="ch08.html#idm45606826102992-marker">36</a></sup> For bigger tests, consider making sure your load tester has enough resources. For <code>k6</code>, see <a href="https://oreil.ly/v4DGs">this guide</a>.</p><p data-type="footnote" id="idm45606826088720"><sup><a href="ch08.html#idm45606826088720-marker">37</a></sup> Any other medium like Jira ticket comments or GitHub issue works too. Just ensure you can easily paste screenshots so it’s less fuss and there are fewer occasions to make mistakes on what screenshot was for what experiment!</p><p data-type="footnote" id="idm45606826080928"><sup><a href="ch08.html#idm45606826080928-marker">38</a></sup> Don’t just make it all screenshots first and delay describing them until later. Try to iterate on each observation in Google Documents, as it’s easy to forget later what situation you were capturing. Additionally, I saw many incidents of thinking screenshots were saved in my laptop’s local directory, then losing all benchmarking results.</p><p data-type="footnote" id="idm45606826071280"><sup><a href="ch08.html#idm45606826071280-marker">39</a></sup> Explained well in <a href="https://oreil.ly/M9RYQ">Martin Kleppmann’s book <em>Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems</em></a> (O’Reilly).</p></div></div></section></body></html>