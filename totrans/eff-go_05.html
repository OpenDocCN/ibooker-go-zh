<html><head></head><body><section data-pdf-bookmark="Chapter 5. How Go Uses Memory Resource" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch-hardware2">&#13;
<h1><span class="label">Chapter 5. </span>How Go Uses Memory Resource</h1>&#13;
&#13;
&#13;
<p><a data-primary="memory resource" data-type="indexterm" id="ix_ch05-asciidoc0"/><a data-primary="memory resource" data-secondary="Go’s use of" data-type="indexterm" id="ix_ch05-asciidoc1"/>In <a data-type="xref" href="ch04.html#ch-hardware">Chapter 4</a>, we started looking under the hood of the modern computer. We discussed the efficiency aspects of using the CPU resource. Efficient execution of instructions in the CPU is important, but the sole purpose of performing those instructions is to modify the data. Unfortunately, the path of changing data is not always trivial. For example, in <a data-type="xref" href="ch04.html#ch-hardware">Chapter 4</a> we learned that in the von Neumann architecture (presented in <a data-type="xref" href="ch04.html#img-uma">Figure 4-1</a>), we experience the CPU and memory wall problem when accessing data from the main memory (RAM).</p>&#13;
&#13;
<p>The industry invented numerous technologies and optimization layers to overcome challenges like that, including memory safety and ensuring large memory capacities. As a result of those inventions, accessing eight bytes from RAM to the CPU register might be represented as a simple <code>MOVQ &lt;destination register&gt; &lt;address XYZ&gt;</code> instruction. However, the actual process done by the CPU to get that information from the physical chip storing those bytes is very complex. We discussed mechanisms like the hierarchical cache system, but there is much more.</p>&#13;
&#13;
<p>In some ways, those mechanisms are abstracted from programmers as much as possible. So, for example, when we define a variable in Go code, we don’t need to think about how much memory has to be reserved, where, and in how many L-caches it has to fit. This is great for development speed, but sometimes it might surprise us when we need to process a lot of data. In those cases, we need to revive our <a href="https://oreil.ly/Co2IM">mechanical sympathy</a> toward memory resource, optimizing TFBO flow (<a data-type="xref" href="ch03.html#ch-conq-eff-flow">“Efficiency-Aware Development Flow”</a>), and good tooling.</p>&#13;
&#13;
<p>This chapter will focus on understanding the RAM resource. We will start by exploring overall memory relevance. Then we will set the context in <a data-type="xref" href="#ch-hw-memory">“Do We Have a Memory Problem?”</a>. Next, we will explain the patterns and consequences of &#13;
<span class="keep-together">each element</span> involved in the memory access from bottom to top. The data journey for memory starts in <a data-type="xref" href="#ch-hw-memory-ph">“Physical Memory”</a>, the hardware memory chips. Then we will move to operating system (OS) memory management techniques that allow managing limited physical memory space in multiprocess systems: <a data-type="xref" href="#ch-hw-memory-vt">“Virtual Memory”</a> and <a data-type="xref" href="#ch-hw-memory-mmap-os">“OS Memory Mapping”</a>, with a more detailed &#13;
<span class="keep-together">explanation of the</span> <a data-type="xref" href="#ch-hw-memory-mmap">“mmap Syscall”</a>.</p>&#13;
&#13;
<p>With the lower layers of memory access explained, we can move to the key knowledge for Go programmers looking to optimize memory efficiency—the explanation of <a data-type="xref" href="#ch-hw-go-mem">“Go Memory Management”</a>. This includes the necessary elements like memory layout, what <a data-type="xref" href="#ch-hw-allocations">“Values, Pointers, and Memory Blocks”</a> mean, and the basics of the <a data-type="xref" href="#ch-hw-allocator">“Go Allocator”</a> with its measurable consequences. &#13;
<span class="keep-together">Finally, we</span> will explore <a data-type="xref" href="#ch-hw-garbage">“Garbage Collection”</a>.</p>&#13;
&#13;
<p>We will go into many details about memory in this chapter, but the key aim is to build an instinct toward the patterns and behavior of Go programs when it comes to memory usage. For example, what problems can occur while accessing memory? How do we measure memory usage? What does it mean to allocate memory? How can we release it? We will explore answers to those questions in this chapter. But let’s start this chapter by clarifying why RAM is relevant to our program execution. What makes it so important?</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Memory Relevance" data-type="sect1"><div class="sect1" id="ch-hw-mem">&#13;
<h1>Memory Relevance</h1>&#13;
&#13;
<p><a data-primary="memory resource" data-secondary="relevance of" data-type="indexterm" id="ix_ch05-asciidoc2"/>All Linux programs require more resources than just the CPU to perform their programmed functionalities. For example, let’s take a web server like <a href="https://oreil.ly/7F0cZ">NGINX</a> (written in C) or <a href="https://oreil.ly/MpHMZ">Caddy</a> (written in Go). Those programs allow serving static content from disk or proxy HTTP requests, among other functionalities. They use the CPU to execute written code. However, a web server like this also interacts with other resources, for example:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>With RAM to cache basic HTTP responses</p>&#13;
</li>&#13;
<li>&#13;
<p>With a disk to load configuration, static content, or write log lines for observability needs</p>&#13;
</li>&#13;
<li>&#13;
<p>With a network to serve HTTP requests from remote clients</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>As a result, the CPU resource is only one part of the equation. This is the same for most programs—they are created to save, read, manage, operate, and transform data from different mediums.</p>&#13;
&#13;
<p>One would argue that the “memory” resource, often called RAM,<sup><a data-type="noteref" href="ch05.html#idm45606834619920" id="idm45606834619920-marker">1</a></sup> sits at the core of those interactions. The RAM is the backbone of the computer because every external piece of data (bytes from disk, network, or another device) has to be buffered in memory to be accessible to the CPU. So, for example, the first thing the OS does to start a new process is load part of the program’s machine code and initial data to memory for the CPU to execute it.</p>&#13;
&#13;
<p>Unfortunately, we must be aware of three main caveats when using memory in our programs:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>RAM access is significantly slower than CPU operational speed.</p>&#13;
</li>&#13;
<li>&#13;
<p>There is always a finite amount of RAM in our machines (typically from &#13;
<span class="keep-together">a few GB</span> to hundreds of GB per machine), which forces us to care about space efficiency.<sup><a data-type="noteref" href="ch05.html#idm45606834615344" id="idm45606834615344-marker">2</a></sup></p>&#13;
</li>&#13;
<li>&#13;
<p>Unless <a href="https://oreil.ly/uaPiN">the persistent type of memory</a> will be commoditized with RAM-like speeds, pricing, and robustness, our main memory is &#13;
<span class="keep-together">strictly volatile.</span> When the computer power goes down, all information is completely lost.<sup><a data-type="noteref" href="ch05.html#idm45606834611488" id="idm45606834611488-marker">3</a></sup></p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The ephemeral characteristics of memory and its finite size are why we are forced to add an auxiliary, persistent I/O resource to our computer,  i.e., a disk. These days we have relatively fast solid state drive (SSD) disks (yet still around 10x slower than RAM) with a limited lifetime (~five years). On the other hand, we have a slower and cheaper hard disk drive (HDD). While cheaper than RAM, the disk resource is also a scarce resource.</p>&#13;
&#13;
<p>Last but not least, for scalability and reliability reasons, our computers rely on data from remote locations. Industry invented different networks and protocols that allow us to communicate with remote software (e.g., databases) or even remote hardware (via iSCSI or NFS protocols). We typically abstract this type of I/O as a network resource usage. Unfortunately, the network is one of the most challenging resources to work with because of its unpredictable nature, limited bandwidth, and bigger latencies.</p>&#13;
&#13;
<p>While using any of those resources, we use it through the memory resource. As a result, it is essential to understand its mechanics. There are many things a programmer can do to impact the application’s memory usage. But unfortunately, without proper education, our implementations tend to be prone to inefficiencies and unnecessary waste of computer resources or execution time. This problem is amplified by the vast amount of data our programs have to process these days. This is why we often say that efficient programming is all about the data.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Memory Inefficiency Is Usually the Most Common Problem in &#13;
<span class="keep-together">Go Programs</span></h1>&#13;
<p><a data-primary="garbage collection (GC)" data-secondary="memory inefficiency and" data-type="indexterm" id="idm45606834606352"/>Go is a garbage collected language, which allows Go to be an extremely productive language. However, the garbage collector (GC) sacrifices some visibility and control over memory management (more on that in <a data-type="xref" href="#ch-hw-garbage">“Garbage Collection”</a>).</p>&#13;
&#13;
<p>But even when we forget about GC overhead, for cases where we need to process a significant amount of data or are under some resource constraints, we have to take more care with how our program uses memory. Therefore, I recommend reading this chapter with extra care since most first-level optimizations are usually around memory resources.<a data-startref="ix_ch05-asciidoc2" data-type="indexterm" id="idm45606834603456"/></p>&#13;
</div>&#13;
&#13;
<p>When should we start the memory optimization process? A few common symptoms might reveal that we might have a memory efficiency issue.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Do We Have a Memory Problem?" data-type="sect1"><div class="sect1" id="ch-hw-memory">&#13;
<h1>Do We Have a Memory Problem?</h1>&#13;
&#13;
<p><a data-primary="memory resource" data-secondary="identifying problems with" data-type="indexterm" id="idm45606834600496"/><a data-primary="memory resource" data-secondary="when to debug/optimize" data-type="indexterm" id="idm45606834599456"/>It’s useful to understand how Go uses the computer’s main memory and its efficiency consequences, but we must also follow the pragmatic approach. As with any optimizations, we should refrain from optimizing memory until we know there is a problem. We can define a set of situations that should trigger our interest in Go memory usage and potential optimizations in this area:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Our physical computer, virtual machine, container, or process crashed because of an out-of-memory (OOM) signal, or our process is about to hit that memory limit.<sup><a data-type="noteref" href="ch05.html#idm45606834596960" id="idm45606834596960-marker">4</a></sup></p>&#13;
</li>&#13;
<li>&#13;
<p>Our Go program is executing slower than usual, while the memory usage is higher than average. Spoiler: our system might be under memory pressure causing trashing or swapping, as explained in <a data-type="xref" href="#ch-hw-memory-mmap-os">“OS Memory Mapping”</a>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Our Go program is executing slower than usual, with high spikes of CPU utilization. Spoiler: allocation or releasing memory slows our programs if an excessive number of short-lived objects is created.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>If you encounter any of those situations, it might be time to debug and optimize the memory usage of your Go program. As I will teach you in <a data-type="xref" href="ch07.html#ch-hw-complexity">“Complexity Analysis”</a>, if you know what you are looking for, a set of early warning signals can indicate huge memory problems that could be avoided easily. Moreover, building such a proactive instinct can make you a valuable team asset!</p>&#13;
&#13;
<p>But we can’t build anything without good foundations. As with the CPU resource, you won’t be able to apply optimizations without actually understanding them! We have to understand the reasons behind those optimizations. For example, <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> allocates 30.5 MB of memory for 1 million integers in the input. But what does it mean? Where was that space reserved? Does it mean we used exactly 30.5 MB of physical memory, or more? Was this memory released at some point? This chapter aims to give you awareness, allowing you to answer all of these questions. We will learn why memory is often the issue and what we can do about it.</p>&#13;
&#13;
<p>Let’s start with the basics of memory management from the point of view of hardware (HW), operating system (OS), and the Go runtime. Let’s start with essential details about physical memory directly impacting our program execution. On top of that, this knowledge might help you better understand the specifications and documentation of modern physical memory!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Physical Memory" data-type="sect1"><div class="sect1" id="ch-hw-memory-ph">&#13;
<h1>Physical Memory</h1>&#13;
&#13;
<p><a data-primary="memory resource" data-secondary="physical" data-type="indexterm" id="ix_ch05-asciidoc3"/><a data-primary="physical memory" data-type="indexterm" id="ix_ch05-asciidoc4"/><a data-primary="random-access memory (RAM)" data-type="indexterm" id="ix_ch05-asciidoc5"/>We store information digitally in the form of bits, the basic computer storage unit. A bit can have one of two values, 0 or 1. With enough bits, we can represent any information: integer, floating value, letters, messages, sounds, images, videos, programs, <a href="https://oreil.ly/il8Tz">metaverses</a>, etc.</p>&#13;
&#13;
<p><a data-primary="dynamic random-access memory (DRAM)" data-type="indexterm" id="idm45606834581744"/>The main physical memory that we use when we execute our programs (RAM) is based on dynamic random-access memory (<a href="https://oreil.ly/hbo59">DRAM</a>). These chips are soldered into modules, often referred to as RAM “sticks.” When connected to the motherboard, these chips allow us to store and read data bits as long as the DRAM is continuously powered.</p>&#13;
&#13;
<p><a data-primary="memory cells" data-type="indexterm" id="idm45606834579600"/>DRAM contains billions of memory cells (as many cells as the number of bits DRAM can store). Each memory cell comprises one access transistor acting as a switch and one storage capacitor. The transistor guards the access to the capacitor, which is charged to the store 1 or drained to keep the 0 value. This allows each memory cell to store a single bit of information. This architecture is much simpler and cheaper to produce and use than Static RAM (SRAM), which is generally faster and used for smaller types of memory like registers and hierarchical caches in the CPU.</p>&#13;
&#13;
<p>At the time of this writing, the most popular memory used for RAM is the simpler, synchronous (clock) version in the DRAM family—<a href="https://oreil.ly/07efG">SDRAM</a>. Particularly, the fifth generation of SDRAM called DDR4.</p>&#13;
&#13;
<p><a data-primary="byte (definition)" data-type="indexterm" id="idm45606834577184"/>Eight bits form a “byte.” That number came from the fact that in the past, the smallest number of bits that could hold a text character was eight.<sup><a data-type="noteref" href="ch05.html#idm45606834576192" id="idm45606834576192-marker">5</a></sup> The industry standardized a “byte” as the smallest meaningful unit of information.</p>&#13;
&#13;
<p>As a result, most hardware is byte addressable. This means that, from a software programmer’s point of view, there are instructions to access data through individual bytes. If you want to access a single bit, you need to access the whole byte and use <a href="https://oreil.ly/pFoxI">bitmasks</a> to get or write the bit you want.</p>&#13;
&#13;
<p>The byte addressability makes developer life easier when working with data from different mediums like memory, disk, network, etc. Unfortunately, that creates a certain illusion that the data is always accessible with byte granularity. Don’t let that mislead you. More often than not, the underlying hardware has to transfer a much larger chunk of data to give you the desired byte.</p>&#13;
&#13;
<p>For example, in <a data-type="xref" href="ch04.html#ch-hw-lcache">“Hierachical Cache System”</a>, we learned that CPU registers are typically 64 bits (8 bytes), and the cache line is even bigger (64 bytes). Yet we have CPU instructions that can copy a single byte from memory to the CPU register. However, an experienced developer will notice that to copy that single byte, in many cases, the CPU will fetch not 1 byte but at least a complete cache line (64 bytes) from physical memory.</p>&#13;
&#13;
<p>From a high-level point of view, physical memory (RAM) can also be seen as byte addressable, as presented in <a data-type="xref" href="#img-physical-addr">Figure 5-1</a>.</p>&#13;
&#13;
<p>Memory space can be seen as a contiguous set of one-byte slots with a unique address. Each address is a number from zero to the total memory capacity in the system in bytes. For this reason, 32-bit systems that use only 32-bit integers for memory addresses typically could not handle RAM with more capacity than 4 GB—the largest number we can represent with 32 bits is <math alttext="2 Superscript 32">&#13;
  <msup><mn>2</mn> <mn>32</mn> </msup>&#13;
</math>. This limitation was removed with the introduction of the 64-bit operating systems that use 64-bit (8-byte)<sup><a data-type="noteref" href="ch05.html#idm45606834567216" id="idm45606834567216-marker">6</a></sup> integers for memory addressing.</p>&#13;
&#13;
<figure><div class="figure" id="img-physical-addr">&#13;
<img alt="efgo 0501" src="assets/efgo_0501.png"/>&#13;
<h6><span class="label">Figure 5-1. </span>Physical memory addresses space</h6>&#13;
</div></figure>&#13;
&#13;
<p>We discussed in <a data-type="xref" href="ch04.html#ch-hw-mem-wall">“CPU and Memory Wall Problem”</a> that memory access is not that fast compared to, for example, CPU speed. But there is more. Addressability, in theory, should allow fast, random access to bytes from the main memory. After all, this is why that main memory is called “random-access memory.” Unfortunately, if we look at our napkin math in <a data-type="xref" href="app01.html#appendix-napkin-math">Appendix A</a>, sequential memory access can be 10 times (or more) faster than random access!</p>&#13;
&#13;
<p>But there is more—we don’t expect any improvements in this area in the future. Within the last few decades, we only improved the speed (bandwidth) of the sequential read. We did not improve random access latency at all! The lack of improvement on the latency side is not a mistake. It is a strategic choice—the internal designs of the modern RAM modules have to work against various requirements and limitations, for example:</p>&#13;
<dl>&#13;
<dt>Capacity</dt>&#13;
<dd>&#13;
<p>There is a strong demand for bigger capacities of RAM, e.g., to compute more data or run more realistic games.</p>&#13;
</dd>&#13;
<dt>Bandwidth and latency</dt>&#13;
<dd>&#13;
<p>We want to wait less time to access memory while writing or reading large chunks of data since memory access is the major slowdown for CPU operations.</p>&#13;
</dd>&#13;
<dt>Voltage</dt>&#13;
<dd>&#13;
<p>There is a demand for a lower voltage requirement for each memory chip, which would allow for running more of them while maintaining low power consumption and manageable thermal characteristics (more time on battery for our laptops and smartphones!).</p>&#13;
</dd>&#13;
<dt>Cost</dt>&#13;
<dd>&#13;
<p>RAM is a fundamental piece of the computer required in large quantities; thus, production and usage costs must be kept low.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Slower random access has many implications for the layers of many managers we will learn about in this chapter. For example, this is why the CPU with L-caches fetches and caches bigger chunks of memory up front, even if only one byte is needed for computation.</p>&#13;
&#13;
<p>Let’s summarize a few things worth remembering about modern generations of hardware for RAM like DDR4 SDRAM:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Random access of the memory is relatively slow, and generally, there aren’t many good ideas to improve that soon. If anything, lower power consumption, larger capacity, and bandwidth only increase that delay.</p>&#13;
</li>&#13;
<li>&#13;
<p>Industry is improving overall memory bandwidth by allowing us to transfer bigger chunks of adjacent (sequential) memory. This means that efforts to align Go data structures and knowing how they are stored in memory matter—ensuring we can access them faster.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Whether sequentially or randomly, our programs never directly access physical memory—the OS manages the RAM space. This is great for developers, as we don’t need to understand low-level memory access details. But there are more important reasons why there has to be an OS between our programs and hardware. So let’s discuss why and what it means for our Go programs.<a data-startref="ix_ch05-asciidoc5" data-type="indexterm" id="idm45606834549184"/><a data-startref="ix_ch05-asciidoc4" data-type="indexterm" id="idm45606834548480"/><a data-startref="ix_ch05-asciidoc3" data-type="indexterm" id="idm45606834547808"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="OS Memory Management" data-type="sect1"><div class="sect1" id="ch-hw-memory-os">&#13;
<h1>OS Memory Management</h1>&#13;
&#13;
<p><a data-primary="memory management" data-secondary="OS memory management" data-type="indexterm" id="ix_ch05-asciidoc6"/><a data-primary="memory resource" data-secondary="OS memory management" data-type="indexterm" id="ix_ch05-asciidoc7"/><a data-primary="operating system (OS) memory management" data-type="indexterm" id="ix_ch05-asciidoc8"/>What <a data-primary="operating system (OS) memory management" data-secondary="goals for" data-type="indexterm" id="ix_ch05-asciidoc9"/>are the operating system’s goals for memory management? Hiding complexities of physical memory access is only one thing. The other, more important, goal is to allow using the same physical memory simultaneously and securely across thousands of processes and their OS threads.<sup><a data-type="noteref" href="ch05.html#idm45606834540672" id="idm45606834540672-marker">7</a></sup> The problem of multiprocess execution on common memory space is nontrivial for multiple reasons:</p>&#13;
<dl>&#13;
<dt>Dedicated memory space for each process</dt>&#13;
<dd>&#13;
<p>Programs are compiled assuming nearly full and continuous access to the RAM. As a result, the OS must track which slots from the physical memory from our address space (shown in <a data-type="xref" href="#img-physical-addr">Figure 5-1</a>) belong to which process. Then we need to find a way to coordinate those “reservations” to the processes so only allocated addresses are accessed.</p>&#13;
</dd>&#13;
<dt>Avoiding external fragmentation</dt>&#13;
<dd>&#13;
<p>Having thousands of processes with dynamic memory usage poses a great risk of waste in memory due to inefficient packing. We call this problem <a href="https://oreil.ly/lBfRq">the external fragmentation of memory</a>.</p>&#13;
</dd>&#13;
<dt>Memory isolation</dt>&#13;
<dd>&#13;
<p>We have to ensure that no process touches the physical memory address reserved for other processes running on the same machine (e.g., operating system processes!). This is because any accidental write or read from outside of process memory (out-of-bounds memory access) can crash other processes,  malform data on persistent mediums (e.g., disk), or crash the whole machine (e.g., if you corrupt the memory used by the OS).</p>&#13;
</dd>&#13;
<dt>Memory safety</dt>&#13;
<dd>&#13;
<p>Operating systems are usually multiuser systems, which means processes can have different permissions to different resources (e.g., files on disk or other process memory space). This is why the mentioned out-of-bounds memory accesses have serious security risks.<sup><a data-type="noteref" href="ch05.html#idm45606834529664" id="idm45606834529664-marker">8</a></sup> Imagine a malicious process with no permissions reading credentials from other process memory, or causing a Denial-of-Service (DoS) attack.<sup><a data-type="noteref" href="ch05.html#idm45606834527872" id="idm45606834527872-marker">9</a></sup> This is especially important for virtualized environments, where a single memory unit can be shared across different operating systems and even more users.</p>&#13;
</dd>&#13;
<dt>Efficient memory usage</dt>&#13;
<dd>&#13;
<p>Programs never use all the memory they asked for at the same time. For example, instruction code and statically allocated data (e.g., constant variables) can be as large as dozens of megabytes. But for single-threaded applications, a maximum of a few kilobytes of data is used in a given second. Instructions for error handling are rarely used. Arrays are often oversized for worst-case scenarios.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>To solve all those challenges, modern OS manages memory using three fundamental mechanisms we will learn about in this section: paged virtual memory, memory mapping, and hardware address translation. Let’s start by explaining virtual memory.<a data-startref="ix_ch05-asciidoc9" data-type="indexterm" id="idm45606834524592"/></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Virtual Memory" data-type="sect2"><div class="sect2" id="ch-hw-memory-vt">&#13;
<h2>Virtual Memory</h2>&#13;
&#13;
<p><a data-primary="memory resource" data-secondary="virtual memory" data-type="indexterm" id="ix_ch05-asciidoc10"/><a data-primary="operating system (OS) memory management" data-secondary="virtual memory" data-type="indexterm" id="ix_ch05-asciidoc11"/><a data-primary="virtual memory" data-type="indexterm" id="ix_ch05-asciidoc12"/>The key idea behind <a href="https://oreil.ly/RBiCV">virtual memory</a> is that every process is given its own logical, simplified view of the RAM. As a result, programming language designers and developers can effectively manage process memory space as if they had an entire memory space for themselves. Even more, with virtual memory, the process can use a full range of addresses from 0 to <math alttext="2 Superscript 64 Baseline minus 1">&#13;
  <mrow>&#13;
    <msup><mn>2</mn> <mn>64</mn> </msup>&#13;
    <mo>-</mo>&#13;
    <mn>1</mn>&#13;
  </mrow>&#13;
</math> for its data, even if the physical memory has, for example, the capacity to accommodate only <math alttext="2 Superscript 35">&#13;
  <msup><mn>2</mn> <mn>35</mn> </msup>&#13;
</math> addresses (32 GB of memory). This frees the process from coordinating the memory among other processes, bin packing challenges, and other important tasks (e.g., physical memory defragmentation, security, limits, and swap). Instead, all of these complex and error-prone memory management tasks can be delegated to the kernel (a core part of the Linux operating system).</p>&#13;
&#13;
<p><a data-primary="paging" data-type="indexterm" id="idm45606834510912"/>There are a few ways of implementing virtual memory, but the most popular technique is called <em>paging</em>.<sup><a data-type="noteref" href="ch05.html#idm45606834509584" id="idm45606834509584-marker">10</a></sup> The OS divides physical and virtual memory into fixed-size chunks of memory. <a data-primary="pages" data-type="indexterm" id="idm45606834507184"/>The virtual memory chunks are called <a href="https://oreil.ly/JTWoU"><em>pages</em></a>, whereas <a data-primary="frames" data-type="indexterm" id="idm45606834505520"/>physical memory chunks are called <em>frames</em>. Both pages and frames can be then individually managed. The default page size is usually 4 KB,<sup><a data-type="noteref" href="ch05.html#idm45606834504144" id="idm45606834504144-marker">11</a></sup> but it can be changed to larger page sizes with respect to specific CPU capabilities.<sup><a data-type="noteref" href="ch05.html#idm45606834502960" id="idm45606834502960-marker">12</a></sup> It is also possible to use 4 KB pages for normal workloads and dedicated (sometimes transparent to processes!) <a href="https://oreil.ly/7KuGx">huge pages</a> from 2 MB to 1 GB.</p>&#13;
<div class="note1" data-type="note" epub:type="note"><h1>The Importance of Page Size</h1>&#13;
<p><a data-primary="page size, importance of" data-type="indexterm" id="idm45606834499344"/>The 4 KB number was chosen in the 1980s, and many say that it’s time to bump this number up, given modern hardware and cheaper RAM (in terms of dollars per byte).</p>&#13;
&#13;
<p>Yet the choice of page size is a game of trade-offs. Larger pages inevitably waste more memory space,<sup><a data-type="noteref" href="ch05.html#idm45606834497952" id="idm45606834497952-marker">13</a></sup> which is often referred to as <a href="https://oreil.ly/PnOuT">the internal memory fragmentation</a>. On the other hand, keeping a 4 KB page size or making it smaller makes memory access slower and memory management more expensive, eventually blocking the ability to use larger RAM modules in our computers.</p>&#13;
</div>&#13;
&#13;
<p>The OS can dynamically map pages in virtual memory to specific physical memory frames (or other mediums like chunks of disk space), mostly transparently to the processes. The mapping, state, permissions, and additional metadata of the page are stored in the page entry in the many hierarchical page tables maintained by the OS.<sup><a data-type="noteref" href="ch05.html#idm45606834494864" id="idm45606834494864-marker">14</a></sup></p>&#13;
&#13;
<p>To achieve an easy-to-use and dynamic virtual memory, we need to have a versatile address translation mechanism. The problem is that only the OS knows about the current memory space mapping between virtual and physical space (or lack of it). Our running program’s process only knows about virtual memory addresses, so all CPU instructions in machine code use virtual addresses. Our programs will be even slower if we try to consult the OS for every memory access to translate each address, so the industry figured out dedicated hardware support for translating memory pages.</p>&#13;
&#13;
<p><a data-primary="Memory Management Unit (MMU)" data-type="indexterm" id="idm45606834492528"/><a data-primary="MMU (Memory Management Unit)" data-type="indexterm" id="idm45606834491808"/>From the 1980s, almost every CPU architecture started to include the Memory Management Unit (MMU) used for every memory access. MMU translates each memory address referenced by CPU instructions to a physical address based on the OS page table entries. <a data-primary="TLB (Translation Lookaside Buffer)" data-type="indexterm" id="idm45606834490720"/><a data-primary="Translation Lookaside Buffer (TLB)" data-type="indexterm" id="idm45606834490032"/>To avoid accessing RAM to search for the relevant page tables, engineers added the Translation Lookaside Buffer (TLB). TLB is a small cache that can cache a few thousand page table entries (typically 4 KB of entries). The overall flow looks like <a data-type="xref" href="#img-mem-vm-mmu">Figure 5-2</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-mem-vm-mmu">&#13;
<img alt="efgo 0502" src="assets/efgo_0502.png"/>&#13;
<h6><span class="label">Figure 5-2. </span>Address translation mechanism done by MMU and TLB in CPU. OS has to inject the relevant page tables so MMU knows what virtual addresses correspond to physical addresses.</h6>&#13;
</div></figure>&#13;
&#13;
<p>TLB is very fast, but it has limited capacity. If MMU cannot find the accessed virtual address in the TLB, we have a TLB miss. This means that either the CPU (hardware TLB management) or OS (software-managed TLB) has to walk through page tables in RAM, which causes significant latency (around one hundred CPU clock cycles)!</p>&#13;
&#13;
<p>It is essential to mention that not every “allocated” virtual memory page will have a reserved physical memory page behind it. In fact, most of the virtual memory is not backed up by RAM at all. As a result, we can almost always see large amounts of virtual memory used by the process (called <code>VSS</code> or <code>VSZ</code> in various Linux tools like <code>ps</code>). Still, the actual physical memory (often called <code>RSS</code> or <code>RES</code> from “resident memory”) reserved for this process might be tiny. There are often cases where a single process allocates more virtual memory than is available to the whole machine! See an example situation like this on my machine in <a data-type="xref" href="#img-mem-vss">Figure 5-3</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-mem-vss">&#13;
<img alt="efgo 0503" src="assets/efgo_0503.png"/>&#13;
<h6><span class="label">Figure 5-3. </span>First few lines of <code>htop</code> output, showing the current usage of a few Chrome browser processes, sorted by virtual memory size</h6>&#13;
</div></figure>&#13;
&#13;
<p>As we can see in <a data-type="xref" href="#img-mem-vss">Figure 5-3</a>, my machine has 32 GB of physical memory, with 16.2 GB currently used. Yet we see Chrome processes using 45.7 GB of virtual memory each! However, if you look at the <code>RES</code> column, it has only 507 MB resident, with 126 MB of it shared with other processes. So how this is possible? How can the process think that it has 45.7 GB of RAM available, given the machine has only 32 GB and the system actually allocated just a few hundred MBs in RAM?</p>&#13;
&#13;
<p><a data-primary="memory overcommitment" data-type="indexterm" id="idm45606834476560"/><a data-primary="overcommitment" data-type="indexterm" id="idm45606834475856"/>We can call such a situation a <a href="https://oreil.ly/wbZGf">memory overcommitment</a>, and it exists because of the very same reasons <a href="https://oreil.ly/El9iy">airlines often overbook seats for their flights</a>. On average, many travelers cancel their trips at the last minute or do not show up for their flight. As a result, to maximize the plane’s used capacity, it is more profitable for airlines to sell more tickets than seats in the airplane and handle the rare “out of seats” situations “gracefully” (e.g., by moving the unlucky customer to another flight). This means that the true “allocation” of seats happens when travelers actually “access” them during the flight onboarding process.</p>&#13;
&#13;
<p>The OS performs the same overcommitment strategy by default<sup><a data-type="noteref" href="ch05.html#idm45606834472688" id="idm45606834472688-marker">15</a></sup> for processes trying to allocate physical memory. The physical memory is only allocated when our program accesses it, not when it “creates” a big object, for example, <code>make([]byte, 1024)</code> (you will see a practical example of this in <a data-type="xref" href="#ch-hw-allocator">“Go Allocator”</a>).</p>&#13;
&#13;
<p><a data-primary="mmap syscall" data-type="indexterm" id="ix_ch05-asciidoc13"/><a data-primary="operating system (OS) memory management" data-secondary="mmap syscall" data-type="indexterm" id="ix_ch05-asciidoc14"/>Overcommitment is implemented with the pages and memory mapping techniques. Typically, memory mapping refers to a low-level memory management capability offered with the <a href="https://oreil.ly/m5n7A"><code>mmap</code></a> system call on Linux (and the similar <code>MapViewOfFile</code> function in Windows).</p>&#13;
<div data-type="note" epub:type="note"><h1>Developers Can Utilize mmap Explicitly in Programs for &#13;
<span class="keep-together">Specific Use Cases</span></h1>&#13;
<p>The <code>mmap</code> call is used extensively in almost every database software, e.g., in <a href="https://oreil.ly/o8a5o">MySQL</a> and <a href="https://oreil.ly/scByc">PostgreSQL</a> as well as those written in Go, like <a href="https://oreil.ly/2Sa3P">Prometheus</a>, <a href="https://oreil.ly/tFBUf">Thanos</a>, and <a href="https://oreil.ly/Jg3wb">M3db</a> projects. The <code>mmap</code> (among other memory allocation techniques) is also what Go runtime and other programming languages use under the hood to allocate memory from OS, e.g., for the heap (discussed in <a data-type="xref" href="#ch-hw-go-mem">“Go Memory Management”</a>).</p>&#13;
</div>&#13;
&#13;
<p class="less_space pagebreak-before">Using explicit <code>mmap</code> for most Go applications is not recommended. Instead, we should stick to the Go runtime’s standard allocation mechanisms, which we will learn in <a data-type="xref" href="#ch-hw-go-mem">“Go Memory Management”</a>. As our <a data-type="xref" href="ch03.html#ch-conq-eff-flow">“Efficiency-Aware Development Flow”</a> said, only if we see indications through benchmarking that this is not enough, might we consider moving to more advanced methods like <code>mmap</code>. This is why <code>mmap</code> is not even on my <a data-type="xref" href="ch11.html#ch-opt2">Chapter 11</a> list!</p>&#13;
&#13;
<p>However, there is a reason why I explain <code>mmap</code> at the start of our journey with the memory resource. Even if we don’t use it explicitly, the OS uses the same memory mapping mechanism to manage all allocated pages in our system. The data structures we use in our Go programs are indirectly saved to certain virtual memory pages, which are then <code>mmap</code>-like managed by the OS or Go runtime. As a result, understanding the explicit <code>mmap</code> syscall will conveniently explain the on-demand paging and mapping techniques Linux OS uses to manage virtual memory.<a data-startref="ix_ch05-asciidoc14" data-type="indexterm" id="idm45606834450704"/><a data-startref="ix_ch05-asciidoc13" data-type="indexterm" id="idm45606834450000"/><a data-startref="ix_ch05-asciidoc12" data-type="indexterm" id="idm45606834449328"/></p>&#13;
&#13;
<p>Let’s focus on the Linux <code>mmap</code> syscall next.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="mmap Syscall" data-type="sect2"><div class="sect2" id="ch-hw-memory-mmap">&#13;
<h2>mmap Syscall</h2>&#13;
&#13;
<p>To learn about OS memory mapping patterns, let’s discuss the <a href="https://oreil.ly/m5n7A"><code>mmap</code></a> syscall. <a data-type="xref" href="#code-mmap">Example 5-1</a> shows a simplified abstraction, using <code>mmap</code> OS syscall, that allows allocating a byte slice in our process virtual memory without Go memory management coordination.</p>&#13;
<div data-type="example" id="code-mmap">&#13;
<h5><span class="label">Example 5-1. </span>The adapted snippet of Linux-specific <a href="https://oreil.ly/KJ4dD">Prometheus <code>mmap</code> abstraction</a> that allows creating and maintaining read-only memory-mapped byte arrays</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kn">import</code><code class="w"> </code><code class="p">(</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"os"</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/efficientgo/core/errors"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"github.com/efficientgo/core/merrors"</code><code class="w">&#13;
</code><code class="w">    </code><code class="s">"golang.org/x/sys/unix"</code><code class="w">&#13;
</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">type</code><code class="w"> </code><code class="nx">MemoryMap</code><code class="w"> </code><code class="kd">struct</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">f</code><code class="w"> </code><code class="o">*</code><code class="nx">os</code><code class="p">.</code><code class="nx">File</code><code class="w"> </code><code class="c1">// nil if anonymous.</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">b</code><code class="w"> </code><code class="p">[</code><code class="p">]</code><code class="kt">byte</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="nx">OpenFileBacked</code><code class="p">(</code><code class="nx">path</code><code class="w"> </code><code class="kt">string</code><code class="p">,</code><code class="w"> </code><code class="nx">size</code><code class="w"> </code><code class="kt">int</code><code class="p">)</code><code class="w"> </code><code class="p">(</code><code class="nx">mf</code><code class="w"> </code><code class="o">*</code><code class="nx">MemoryMap</code><code class="p">,</code><code class="w"> </code><code class="nx">_</code><code class="w"> </code><code class="kt">error</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO1-1" id="co_how_go_uses_memory_resource_CO1-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
    </code><code class="nx">f</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">os</code><code class="p">.</code><code class="nx">Open</code><code class="p">(</code><code class="nx">path</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">if</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">!=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="k">return</code><code class="w"> </code><code class="kc">nil</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w">&#13;
</code><code class="w">    </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">b</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">unix</code><code class="p">.</code><code class="nx">Mmap</code><code class="p">(</code><code class="nb">int</code><code class="p">(</code><code class="nx">f</code><code class="p">.</code><code class="nx">Fd</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="p">,</code><code class="w"> </code><code class="mi">0</code><code class="p">,</code><code class="w"> </code><code class="nx">size</code><code class="p">,</code><code class="w"> </code><code class="nx">unix</code><code class="p">.</code><code class="nx">PROT_READ</code><code class="p">,</code><code class="w"> </code><code class="nx">unix</code><code class="p">.</code><code class="nx">MAP_SHARED</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO1-2" id="co_how_go_uses_memory_resource_CO1-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
    </code><code class="k">if</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">!=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">        </code><code class="k">return</code><code class="w"> </code><code class="kc">nil</code><code class="p">,</code><code class="w"> </code><code class="nx">merrors</code><code class="p">.</code><code class="nx">New</code><code class="p">(</code><code class="nx">f</code><code class="p">.</code><code class="nx">Close</code><code class="p">(</code><code class="p">)</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="p">)</code><code class="p">.</code><code class="nx">Err</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO1-3" id="co_how_go_uses_memory_resource_CO1-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
    </code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">return</code><code class="w"> </code><code class="o">&amp;</code><code class="nx">MemoryMap</code><code class="p">{</code><code class="nx">f</code><code class="p">:</code><code class="w"> </code><code class="nx">f</code><code class="p">,</code><code class="w"> </code><code class="nx">b</code><code class="p">:</code><code class="w"> </code><code class="nx">b</code><code class="p">}</code><code class="p">,</code><code class="w"> </code><code class="kc">nil</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="p">(</code><code class="nx">f</code><code class="w"> </code><code class="o">*</code><code class="nx">MemoryMap</code><code class="p">)</code><code class="w"> </code><code class="nx">Close</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="kt">error</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">errs</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">merrors</code><code class="p">.</code><code class="nx">New</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">errs</code><code class="p">.</code><code class="nx">Add</code><code class="p">(</code><code class="nx">unix</code><code class="p">.</code><code class="nx">Munmap</code><code class="p">(</code><code class="nx">f</code><code class="p">.</code><code class="nx">b</code><code class="p">)</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO1-4" id="co_how_go_uses_memory_resource_CO1-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
    </code><code class="nx">errs</code><code class="p">.</code><code class="nx">Add</code><code class="p">(</code><code class="nx">f</code><code class="p">.</code><code class="nx">f</code><code class="p">.</code><code class="nx">Close</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="w">&#13;
</code><code class="w">    </code><code class="k">return</code><code class="w"> </code><code class="nx">errs</code><code class="p">.</code><code class="nx">Err</code><code class="p">(</code><code class="p">)</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">func</code><code class="w"> </code><code class="p">(</code><code class="nx">f</code><code class="w"> </code><code class="o">*</code><code class="nx">MemoryMappedFile</code><code class="p">)</code><code class="w"> </code><code class="nx">Bytes</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><code class="p">[</code><code class="p">]</code><code class="kt">byte</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><code class="k">return</code><code class="w"> </code><code class="nx">f</code><code class="p">.</code><code class="nx">b</code><code class="w"> </code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO1-1" id="callout_how_go_uses_memory_resource_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p><code>OpenFileBacked</code> creates explicit memory mapped backed up by the file from the provided path.</p></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO1-2" id="callout_how_go_uses_memory_resource_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p><code>unix.Mmap</code> is a Unix-specific Go helper that uses the <code>mmap</code> syscall to create a direct mapping between bytes from the file on disk (between 0 and the <code>size</code> address) and virtual memory allocated by the returned <code>[]byte</code> array in the <code>b</code> variable. We also pass the read-only flag (<code>PROT_READ</code>) and shared flag (<code>MAP_SHARED</code>).<sup><a data-type="noteref" href="ch05.html#idm45606834159392" id="idm45606834159392-marker">16</a></sup> We can also skip the passing file descriptor, and pass 0 as the first argument and <code>MAP_ANON</code> as the last argument to create anonymous mapping (more on that later).<sup><a data-type="noteref" href="ch05.html#idm45606834158000" id="idm45606834158000-marker">17</a></sup></p></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO1-3" id="callout_how_go_uses_memory_resource_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>We use the <a href="https://oreil.ly/lnrJM"><code>merrors</code></a> package to ensure the we capture both errors if <code>Close</code> also returns an error.</p></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO1-4" id="callout_how_go_uses_memory_resource_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p><code>unix.Munmap</code> is one of the few ways to remove mapping and de-allocate <code>mmap</code>-ed bytes from virtual memory.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>The returned byte slice from the open-ed <code>MemoryMap.Bytes</code> structure can be read as a regular byte slice acquired in typical ways, e.g., <code>make([]byte, size)</code>. However, since we marked this memory-mapped location as read-only (<code>unix.PROT_READ</code>), writing to such a slice will cause the OS to terminate the Go process with the <code>SIGSEGV</code> reason.<sup><a data-type="noteref" href="ch05.html#idm45606834147584" id="idm45606834147584-marker">18</a></sup> Furthermore, a segmentation fault will also happen if we read from this slice after doing <code>Close</code> (<code>Unmap</code>) on it.</p>&#13;
&#13;
<p>At first glance, the <code>mmap</code>-ed byte array looks like a regular byte slice with extra steps and constraints. So what’s unique about it? It’s best to explain that using an example! Imagine that we want to buffer a 600 MB file in the <code>[]byte</code> slice so we can quickly access a couple of bytes on demand from random offsets of that file. The 600 MB might sound excessive, but such a requirement is commonly seen in databases or caches where reading from a disk on demand might be too slow.</p>&#13;
&#13;
<p>The naive solution without an explicit <code>mmap</code> could look like <a data-type="xref" href="#code-naive-read-usage">Example 5-2</a>. Every few instructions, we will look at what the OS memory statistics told us about the allocated pages on physical RAM.</p>&#13;
<div data-type="example" id="code-naive-read-usage">&#13;
<h5><span class="label">Example 5-2. </span>Buffering 600 MB from a file to access three bytes from three different locations</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="nx">f</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">os</code><code class="p">.</code><code class="nx">Open</code><code class="p">(</code><code class="s">"test686mbfile.out"</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO2-1" id="co_how_go_uses_memory_resource_CO2-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="k">if</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">!=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">   </code><code class="k">return</code><code class="w"> </code><code class="nx">err</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="nx">b</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nb">make</code><code class="p">(</code><code class="p">[</code><code class="p">]</code><code class="kt">byte</code><code class="p">,</code><code class="w"> </code><code class="mi">600</code><code class="o">*</code><code class="mi">1024</code><code class="o">*</code><code class="mi">1024</code><code class="p">)</code><code class="w">&#13;
</code><code class="k">if</code><code class="w"> </code><code class="nx">_</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">f</code><code class="p">.</code><code class="nx">Read</code><code class="p">(</code><code class="nx">b</code><code class="p">)</code><code class="p">;</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">!=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO2-2" id="co_how_go_uses_memory_resource_CO2-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
   </code><code class="k">return</code><code class="w"> </code><code class="nx">err</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="s">"Reading the 5000th byte"</code><code class="p">,</code><code class="w"> </code><code class="nx">b</code><code class="p">[</code><code class="mi">5000</code><code class="p">]</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO2-3" id="co_how_go_uses_memory_resource_CO2-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
</code><code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="s">"Reading the 100 000th byte"</code><code class="p">,</code><code class="w"> </code><code class="nx">b</code><code class="p">[</code><code class="mi">100000</code><code class="p">]</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO2-3" id="co_how_go_uses_memory_resource_CO2-4"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
</code><code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="s">"Reading the 104 000th byte"</code><code class="p">,</code><code class="w"> </code><code class="nx">b</code><code class="p">[</code><code class="mi">104000</code><code class="p">]</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO2-3" id="co_how_go_uses_memory_resource_CO2-5"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
&#13;
</code><code class="k">if</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">f</code><code class="p">.</code><code class="nx">Close</code><code class="p">(</code><code class="p">)</code><code class="p">;</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">!=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">   </code><code class="k">return</code><code class="w"> </code><code class="nx">err</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO2-1" id="callout_how_go_uses_memory_resource_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>We open the 600+ MB file. At this point, if you ran the <code>ls -l /proc/$PID/fd</code> (where <code>$PID</code> is the process ID of this executed program) command on a Linux machine, you would see file descriptors telling you that this process has used these files. One of the descriptors is a symbolic link to our <code>test686mbfile.out</code> file we just opened. The process will hold that file descriptor until the file is closed.</p></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO2-2" id="callout_how_go_uses_memory_resource_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>We read 600 MB into a pre-allocated <code>[]byte</code> slice. After the <code>f.Read</code> method execution, the RSS of the process shows 621 MB.<sup><a data-type="noteref" href="ch05.html#idm45606833899392" id="idm45606833899392-marker">19</a></sup> This means that we need over 600 MB of free physical RAM to run this program. The virtual memory size (VSZ) increased too, hitting 1.3 GB.</p></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO2-3" id="callout_how_go_uses_memory_resource_CO2-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>No matter what bytes we access from our buffer, our program will not allocate any more bytes on RSS for our buffer (however, it might need extra bytes for the <code>Println</code> logic).</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Generally, <a data-type="xref" href="#code-naive-read-usage">Example 5-2</a> proves that without an explicit <code>mmap</code>, we would need to reserve at least 600 MB of memory (~150,000 pages) on physical RAM from the very beginning. We also keep all of them reserved for our process until it is collected by the garbage collection process.</p>&#13;
&#13;
<p>What would the same functionality look like with the explicit <code>mmap</code>? Let’s do something similar in <a data-type="xref" href="#code-mmap-usage">Example 5-3</a> using the <a data-type="xref" href="#code-mmap">Example 5-1</a> abstraction.</p>&#13;
<div data-type="example" id="code-mmap-usage">&#13;
<h5><span class="label">Example 5-3. </span>Memory mapping 600 MB from file to access three bytes from three different locations, using <a data-type="xref" href="#code-mmap">Example 5-1</a></h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="nx">f</code><code class="p">,</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">mmap</code><code class="p">.</code><code class="nx">OpenFileBacked</code><code class="p">(</code><code class="s">"test686mbfile.out,"</code><code class="w"> </code><code class="mi">600</code><code class="o">*</code><code class="mi">1024</code><code class="o">*</code><code class="mi">1024</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO3-1" id="co_how_go_uses_memory_resource_CO3-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="k">if</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">!=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">   </code><code class="k">return</code><code class="w"> </code><code class="nx">err</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="nx">b</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">f</code><code class="p">.</code><code class="nx">Bytes</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO3-2" id="co_how_go_uses_memory_resource_CO3-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
&#13;
</code><code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="s">"Reading the 5000th byte"</code><code class="p">,</code><code class="w"> </code><code class="nx">b</code><code class="p">[</code><code class="mi">5000</code><code class="p">]</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO3-3" id="co_how_go_uses_memory_resource_CO3-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
</code><code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="s">"Reading the 100 000th byte"</code><code class="p">,</code><code class="w"> </code><code class="nx">b</code><code class="p">[</code><code class="mi">100000</code><code class="p">]</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO3-4" id="co_how_go_uses_memory_resource_CO3-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
</code><code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="s">"Reading the 104 000th byte"</code><code class="p">,</code><code class="w"> </code><code class="nx">b</code><code class="p">[</code><code class="mi">104000</code><code class="p">]</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO3-5" id="co_how_go_uses_memory_resource_CO3-5"><img alt="5" src="assets/5.png"/></a><code class="w">&#13;
&#13;
</code><code class="k">if</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nx">f</code><code class="p">.</code><code class="nx">Close</code><code class="p">(</code><code class="p">)</code><code class="p">;</code><code class="w"> </code><code class="nx">err</code><code class="w"> </code><code class="o">!=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO3-6" id="co_how_go_uses_memory_resource_CO3-6"><img alt="6" src="assets/6.png"/></a><code class="w">&#13;
   </code><code class="k">return</code><code class="w"> </code><code class="nx">err</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO3-1" id="callout_how_go_uses_memory_resource_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>We open our test file and memory map 600 MB of its content into the <code>[]byte</code> slice. At this point, similar to <a data-type="xref" href="#code-naive-read-usage">Example 5-2</a>, we see a related file descriptor for our <code>test686mbfile.out</code> file in the <em>fd</em> directory. More importantly, however, if you executed the <code>ls -l /proc/$PID&gt;/map_files</code> (again, <code>$PID</code> is the process ID) command, you would also have another symbolic link to the <code>test686mbfile.out</code> file we just referenced. This represents a file-backed memory map.</p></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO3-2" id="callout_how_go_uses_memory_resource_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>After this statement, we have the byte buffer <code>b</code> with the file content. However, if we check the memory statistics for this process, the OS did not allocate any page in physical memory for our slice elements.<sup><a data-type="noteref" href="ch05.html#idm45606833705184" id="idm45606833705184-marker">20</a></sup> So the total RSS is as small as 1.6 MB, despite having 600 MB of content accessible in <code>b</code>! The VSZ, on the other hand, is around 1.3 GB, which indicates the OS is telling the Go program that it can access this space.</p></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO3-3" id="callout_how_go_uses_memory_resource_CO3-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>After accessing a single byte from our slice, we see an increase in RSS, around 48–70 KB worth of RAM pages for this mapping. This means that the OS only allocated a few (10 or so) pages on RAM when our code wanted to access a single, concrete byte from <code>b</code>.</p></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO3-4" id="callout_how_go_uses_memory_resource_CO3-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Accessing a different byte far away from already allocated pages triggers the allocation of extra pages. RSS reading would show 100–128 KB.</p></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO3-5" id="callout_how_go_uses_memory_resource_CO3-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>If we access a single byte 4,000 bytes away from the previous read, OS does not allocate any additional pages. This might be for a few reasons.<sup><a data-type="noteref" href="ch05.html#idm45606833635152" id="idm45606833635152-marker">21</a></sup> For instance, when our program read the file’s contents at offset 100,000, the OS already allocated a 4 KB page with the byte we accessed here. Thus RSS reading would still show 100–128 KB.</p></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO3-6" id="callout_how_go_uses_memory_resource_CO3-6"><img alt="6" src="assets/6.png"/></a></dt>&#13;
<dd><p>If we remove the memory mapping, all our related pages will eventually be unmapped from RAM. This means our process total RSS number should be smaller.<sup><a data-type="noteref" href="ch05.html#idm45606833630880" id="idm45606833630880-marker">22</a></sup></p></dd>&#13;
</dl></div>&#13;
<div data-type="tip"><h1>An Underrated Way to Learn More About Your Process and &#13;
<span class="keep-together">OS Resource Behavior</span></h1>&#13;
<p>Linux provides amazing statistics and debugging information for the current process or thread state. Everything is accessible as special files inside <em>/proc/<code>&lt;PID&gt;</code></em>. The ability to debug each detailed statistic (e.g., every little memory mapping status) and configuration was eye-opening for me. Learn more about what you can do by reading the <a href="https://oreil.ly/jxBig">proc</a> (process pseudofilesystem) documentation.</p>&#13;
&#13;
<p>I recommend getting familiar with the Linux pseudofilesystem or the tools using it if you plan to work more on low-level Linux &#13;
<span class="keep-together">software.</span></p>&#13;
</div>&#13;
&#13;
<p><a data-primary="on demand paging" data-type="indexterm" id="idm45606833624176"/>One of the main behaviors highlighted when we used explicit <code>mmap</code> in <a data-type="xref" href="#code-mmap-usage">Example 5-3</a> is called on-demand paging. When the process asks the OS for any virtual memory using <code>mmap</code>, the OS will not allocate any page on RAM, no matter how large. Instead, the OS will only give the process the virtual &#13;
<span class="keep-together">address range.</span> Further along, when &#13;
<span class="keep-together">the CPU</span> performs the first instruction that accesses memory from that virtual address range (e.g., our <code>fmt.Println("Reading the 5000th byte," b[5000])</code> in <a data-type="xref" href="#code-mmap-usage">Example 5-3</a>), <a data-primary="page fault" data-type="indexterm" id="idm45606833618800"/>the MMU will generate a page fault. Page fault is a hardware interrupt that is handled by the OS kernel. The OS can then respond in various ways:</p>&#13;
<dl>&#13;
<dt>Allocate more RAM frames</dt>&#13;
<dd>&#13;
<p>If we have free frames (physical memory pages) in RAM, the OS can mark some of them as used and map them to the process that triggered the page fault. This is the only moment when the OS actually “allocates” RAM (and increases the <code>RSS</code> metric).</p>&#13;
</dd>&#13;
<dt>De-allocate unused RAM frames and reuse them</dt>&#13;
<dd>&#13;
<p>If no free frame exists (high memory usage on the machine), the OS can remove a couple of frames that belong to file-backed mappings for any process as long as the frames are not currently accessed. As a result, many pages can be unmapped from physical frames before OS has to resort to more brutal methods. Still, this will potentially cause other processes to generate another page fault. If this situation happens very often, our whole OS with all processes will be seriously slowed down (memory trashing situation).</p>&#13;
</dd>&#13;
<dt>Triggering out-of-memory (OOM) situation</dt>&#13;
<dd>&#13;
<p><a data-primary="OOM (out-of-memory)" data-type="indexterm" id="idm45606833613008"/><a data-primary="out-of-memory (OOM)" data-type="indexterm" id="idm45606833612304"/>If the situation worsens and all unused file-backed memory-mapped pages are freed, and we still have no free pages, the OS is essentially out of memory. Handling that situation can be configured in the OS, but generally, there are three options:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The OS can start unmapping pages from physical memory for memory mappings backed by anonymous files. To avoid data loss, a swap disk partition can be configured (the <code>swapon --show</code> command will show you the existence and usage of swap partitions in your Linux system). This disk space is then used to back up virtual memory pages from the anonymous file memory map. As you can imagine, this can cause a similar (if not worse) memory trashing situation and overall system slowdown.<sup><a data-type="noteref" href="ch05.html#idm45606833609392" id="idm45606833609392-marker">23</a></sup></p>&#13;
</li>&#13;
<li>&#13;
<p>A second option for the OS is to simply reboot the system, generally known as <a href="https://oreil.ly/BboW0">the system-level OOM crash</a>.</p>&#13;
</li>&#13;
<li>&#13;
<p>The last option is to recover from the OOM situation by immediately terminating a few lower-priority processes (e.g., from the user space). This is typically done by the OS sending the <a href="https://oreil.ly/SLWOv"><code>SIGKILL</code> signal</a>. The detection of what processes to kill varies,<sup><a data-type="noteref" href="ch05.html#idm45606833605136" id="idm45606833605136-marker">24</a></sup> but if we want more determinisms, the system administrator can configure specific memory limits per process or group of processes using, for example, <a href="https://oreil.ly/E72wh"><code>cgroups</code></a><sup><a data-type="noteref" href="ch05.html#idm45606833602304" id="idm45606833602304-marker">25</a></sup> or <a href="https://oreil.ly/fF12F"><code>ulimit</code></a>.</p>&#13;
</li>&#13;
</ul>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>On top of the on-demand paging strategy, it’s worth mentioning that the OS never releases any frame pages from RAM at the moment of process termination or when it explicitly releases some virtual memory. Only virtual mapping is updated at that point. Instead, physical memory is mainly reclaimed lazily (on demand) with the help of <a href="https://oreil.ly/ruKUM">a page frame reclaiming algorithm (PFRA)</a> that we won’t discuss in this book.</p>&#13;
&#13;
<p>Generally, the <code>mmap</code> syscall might seem complex to use and understand. Yet, it explains what it means when our program allocates some RAM by asking the OS. Let’s now compose what we learned into the big picture of how the OS manages the RAM and talk about the consequences we developers might observe when dealing with a memory resource.<a data-startref="ix_ch05-asciidoc11" data-type="indexterm" id="idm45606833597104"/><a data-startref="ix_ch05-asciidoc10" data-type="indexterm" id="idm45606833596400"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="OS Memory Mapping" data-type="sect2"><div class="sect2" id="ch-hw-memory-mmap-os">&#13;
<h2>OS Memory Mapping</h2>&#13;
&#13;
<p class="fix_tracking"><a data-primary="mapping, OS memory" data-type="indexterm" id="ix_ch05-asciidoc15"/><a data-primary="memory mapping" data-type="indexterm" id="ix_ch05-asciidoc16"/><a data-primary="memory resource" data-secondary="OS memory mapping" data-type="indexterm" id="ix_ch05-asciidoc17"/><a data-primary="operating system (OS) memory management" data-secondary="OS memory mapping" data-type="indexterm" id="ix_ch05-asciidoc18"/>The explicit memory mapping presented in <a data-type="xref" href="#code-mmap-usage">Example 5-3</a> is just one example of the possible OS memory mapping techniques. Besides, rare file-backed mapping and advanced off-heap solutions, there is almost no need to explicitly use such <code>mmap</code> syscalls in our Go programs. However, to manage virtual memory efficiently, the OS is transparently using the same technique of page memory mapping for nearly all the RAM! The example memory mappings situation is presented in <a data-type="xref" href="#img-mem-vm">Figure 5-4</a>, which pulls into one graphic a few common page mapping situations we could have in our machine.</p>&#13;
&#13;
<figure><div class="figure" id="img-mem-vm">&#13;
<img alt="efgo 0504" src="assets/efgo_0504.png"/>&#13;
<h6><span class="label">Figure 5-4. </span>Example MMU translation of a few memory pages from the virtual memory of two processes</h6>&#13;
</div></figure>&#13;
&#13;
<p>The situation in <a data-type="xref" href="#img-mem-vm">Figure 5-4</a> might look complicated, but we have already discussed some of those cases. Let’s enumerate them from the perspective of Process 1 or 2:</p>&#13;
<dl>&#13;
<dt>Page <code>A</code></dt>&#13;
<dd>&#13;
<p><a data-primary="anonymous file mapping" data-type="indexterm" id="idm45606833581104"/>Represents the simplest case of <em>anonymous file mapping</em> that has already mapped the frame on RAM. So, for example, if Process 1 writes or reads a byte from an address between <code>0x2000</code> and <code>0x2FFF</code> in its virtual space, the MMU will translate the address to RAM physical address <code>0x9000</code>, plus the required offset. As a result, the CPU will be able to fetch or write it as a cache line to its L-caches and &#13;
<span class="keep-together">desired register.</span></p>&#13;
</dd>&#13;
<dt>Page <code>B</code></dt>&#13;
<dd>&#13;
<p><a data-primary="file-based memory page" data-type="indexterm" id="idm45606833576480"/>Represents a <em>file-based memory page</em> mapped to a physical frame like we created in <a data-type="xref" href="#code-mmap-usage">Example 5-3</a>. This frame is also shared with another process since there is no need to keep two copies of the same data as both mappings map to the same file on a disk. This is only allowed if the mapping is not set as <code>MAP_PRIVATE</code>.</p>&#13;
</dd>&#13;
<dt>Page <code>C</code></dt>&#13;
<dd>&#13;
<p>This is an anonymous file mapping that wasn’t yet accessed. For example, if Process 1 writes a byte to an address between <code>0x0</code> and <code>0xFFF</code>, a page fault hardware interrupt is generated by the CPU, and the OS will need to find a free frame.</p>&#13;
</dd>&#13;
<dt>Page <code>D</code></dt>&#13;
<dd>&#13;
<p>This is an anonymous page like <code>C</code>, but some data was already written on it. Yet the OS seems to have <code>swap</code> enabled and unmaps it from RAM because this page was not used for a long time by Process 2, or the system is under memory pressure. The OS backed the data to swap files in the swap partition to avoid data loss. Process 2 accessing any byte from a virtual address between <code>0x1000</code> and <code>0x1FFF</code> would result in a page fault, which will tell the OS to find a free frame on RAM and read page <code>D</code> content from the swap file. Only then can data be available to Process 2. Note that such swap logic for anonymous pages is disabled by default on most operating systems.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>You should now have a clearer view of OS memory management basics and virtual memory patterns. So let’s now go through a list of important consequences those pose on Go (and any other programming language):</p>&#13;
<dl>&#13;
<dt>Practically speaking, observing the size of virtual memory is never useful.</dt>&#13;
<dd>&#13;
<p>On-demand paging is why we always see larger virtual memory usage (represented by virtual set size, or VSS) than resident memory usage (RSS) for a process (e.g., the browser memory usage in <a data-type="xref" href="#img-mem-vss">Figure 5-3</a>). While the process thinks that all pages it sees on the virtual address space are in RAM, most of them might be currently unmapped and stored on disk (mapped file or swap partition). In most cases, you <a href="https://oreil.ly/u9l5k">can ignore</a> the VSS metric when assessing the amount of memory your Go program uses.</p>&#13;
</dd>&#13;
</dl>&#13;
<dl>&#13;
<dt>It is impossible to tell precisely how much memory a process (or system) has used in a given time.</dt>&#13;
<dd>&#13;
<p>What metric can we use if the VSS metric does not help assess process memory usage? For Go developers interested in the memory efficiency of their programs, knowing the current and past memory usage is essential information. It tells how efficient our code is and if our optimizations work as expected.</p>&#13;
&#13;
<p>Unfortunately, because of the on-demand paging and memory mapping behavior we learned in this section, this is currently very hard—we can only roughly estimate. We will discuss the best available metrics in <a data-type="xref" href="ch06.html#ch-obs-mem-usage">“Memory Usage”</a>, but don’t be surprised if the RSS metric shows a few kilobytes or even megabytes more or less than you expected.</p>&#13;
</dd>&#13;
</dl>&#13;
<dl class="fix_tracking">&#13;
<dt>OS memory usage expands to all available RAM.</dt>&#13;
<dd>&#13;
<p>Due to lazy release and page caches, even if our Go process released all memory, sometimes the RSS will still look very high if there’s generally low memory pressure on the system. This means that there’s enough physical RAM to satisfy the rest of the processes, so the OS doesn’t bother to release our pages. This is often why the RSS metric is not very reliable, as discussed in <a data-type="xref" href="ch06.html#ch-obs-mem-usage">“Memory Usage”</a>.</p>&#13;
</dd>&#13;
<dt>Tail latency of our Go program memory access is much slower than just physical DRAM access latency.</dt>&#13;
<dd>&#13;
<p>There is a high price to pay for using OS with virtual memory. In the worst cases, already slow memory access caused by DRAM design (mentioned in <a data-type="xref" href="#ch-hw-memory-ph">“Physical Memory”</a>) is even slower. If we stack up things that can happen, like TLB miss, page fault, looking for a free page, or on-demand memory loading from disk, we have extreme latency, which can waste thousands of CPU cycles. The OS does as much as possible to ensure those bad cases rarely happen, so the amortized (average) access latency is as low as possible.</p>&#13;
&#13;
<p>As Go developers, we have some control to reduce the risk of those extra latencies happening more often. For example, we can use less memory in our programs or prefer sequential memory access (more on that later).</p>&#13;
</dd>&#13;
<dt>High usage of RAM might cause slow program execution.</dt>&#13;
<dd>&#13;
<p>When our system executes many processes that want to access large quantities of pages close to RAM capacity, memory access latencies and OS cleanup routines can take most of the CPU cycles. Furthermore, as we discussed, things like memory trashing, constant memory swaps, and page reclaim mechanisms will slow the whole system. As a result, if your program latency is high, it is not necessarily doing too much work on the CPU or executing slow operations (e.g., I/O), it might just use a lot of the memory!</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Hopefully, you understand the impact of OS memory management on how we should think about the memory resource. As in <a data-type="xref" href="#ch-hw-memory-ph">“Physical Memory”</a>, I only explained the basics of memory management. This is because the kernel algorithms evolve, and different OSes manage memory differently. The information I provided should give you a rough understanding of the standard techniques and their consequences. Such a foundation should also give you a kick-start toward learning more from materials like <a href="https://oreil.ly/Wr1nY"><em>Understanding the Linux Kernel</em></a> by Daniel P. Bovet and Marco Cesati (O’Reilly) or <a href="https://lwn.net">LWN.net</a>.<a data-startref="ix_ch05-asciidoc18" data-type="indexterm" id="idm45606833547936"/><a data-startref="ix_ch05-asciidoc17" data-type="indexterm" id="idm45606833547200"/><a data-startref="ix_ch05-asciidoc16" data-type="indexterm" id="idm45606833546528"/><a data-startref="ix_ch05-asciidoc15" data-type="indexterm" id="idm45606833545856"/></p>&#13;
&#13;
<p class="fix_tracking">With that knowledge, let’s discuss how Go has chosen to leverage the memory functionalities the OS and hardware offer. It should help us find the right optimizations to try in our TFBO flow if we have to focus on the memory efficiency of our Go program.<a data-startref="ix_ch05-asciidoc8" data-type="indexterm" id="idm45606833544688"/><a data-startref="ix_ch05-asciidoc7" data-type="indexterm" id="idm45606833543984"/><a data-startref="ix_ch05-asciidoc6" data-type="indexterm" id="idm45606833543312"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Go Memory Management" data-type="sect1"><div class="sect1" id="ch-hw-go-mem">&#13;
<h1>Go Memory Management</h1>&#13;
&#13;
<p><a data-primary="Go (generally)" data-secondary="memory management" data-type="indexterm" id="ix_ch05-asciidoc19"/><a data-primary="Go memory management" data-type="indexterm" id="ix_ch05-asciidoc20"/><a data-primary="memory management" data-secondary="Go memory management" data-type="indexterm" id="ix_ch05-asciidoc21"/><a data-primary="memory resource" data-secondary="Go memory management" data-type="indexterm" id="ix_ch05-asciidoc22"/>The programming language task here is to ensure that developers who write programs can create variables, abstractions, and operations that use memory safely, efficiently, and (ideally) without fuss! So let’s dig into how the Go language enables that.</p>&#13;
&#13;
<p>Go uses a relatively standard internal process memory management pattern that other languages (e.g., C/C++) share, with some unique elements. As we learned in <a data-type="xref" href="ch04.html#ch-hw-os-scheduler">“Operating System Scheduler”</a>, when a new process starts, the operating system creates various metadata about the process, including a new dedicated virtual address space. The OS also creates initial memory mappings for a few starting segments based on information stored in the program binary. Once the process starts, it uses <code>mmap</code> or <a href="https://oreil.ly/31emh"><code>brk/sbrk</code></a><sup><a data-type="noteref" href="ch05.html#idm45606833533232" id="idm45606833533232-marker">26</a></sup> to dynamically allocate more pages on virtual memory when needed. An example organization of the virtual memory in Go is presented in <a data-type="xref" href="#img-mem-layout">Figure 5-5</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-mem-layout">&#13;
<img alt="efgo 0505" src="assets/efgo_0505.png"/>&#13;
<h6><span class="label">Figure 5-5. </span>Memory layout of an executed Go program in virtual address space</h6>&#13;
</div></figure>&#13;
&#13;
<p class="less_space pagebreak-before">We can enumerate a couple of common sections:</p>&#13;
<dl>&#13;
<dt><code>.text</code>, <code>.data</code>, and shared libraries</dt>&#13;
<dd>&#13;
<p>Program code and all global data like global variables are automatically memory mapped by the OS when the process starts (whether it takes 1 MB or 100 GB of virtual memory). This data is read-only, backed up by the binary file. Additionally, only a small contiguous part of the program is executed at a time by the CPU so that the OS can keep a minimal amount of pages with code and data in the physical memory. Those pages are also heavily shared (more processes are started using the same binary, plus some dynamically linked shared libraries).</p>&#13;
</dd>&#13;
<dt>Block starting symbol (<code>.bss</code>)</dt>&#13;
<dd>&#13;
<p><a data-primary="block starting symbol (.bss)" data-type="indexterm" id="idm45606833523488"/>When OS starts a process, it also allocates anonymous pages for uninitialized data (<code>.bss)</code>. The amount of space used by <code>.bss</code> is known in advance—for example, the <code>http</code> package defines the <a href="https://oreil.ly/7m0Wv"><code>DefaultTransport</code></a> global variable. While we don’t know the value of this variable, we know it will be a pointer, so we need to prepare eight bytes of memory for it. This type of memory allocation is called static allocation. This space is allocated once, backed by anonymous pages, and is never freed (from virtual memory at least; if swapping is enabled, it can be unmapped from RAM).</p>&#13;
</dd>&#13;
<dt>Heap</dt>&#13;
<dd>&#13;
<p><a data-primary="heap" data-secondary="Go memory management and" data-type="indexterm" id="idm45606833518800"/>The first (and probably the most important) dynamic segment in <a data-type="xref" href="#img-mem-layout">Figure 5-5</a> is the memory reserved for dynamic allocations, typically called the <em>heap</em> (do not confuse it with the <a href="https://oreil.ly/740nv">data structure</a> with the same name). Dynamic allocations are required for program data (e.g., variables) that have to be available outside a single function scope. As a result, such allocations are unknown in advance and must be stored in memory for an unpredictable time. When the process starts, the OS prepares the initial number of anonymous pages for the heap. After that, the OS gives the process some control over that space. It can then increase or decrease its size using the <code>sbrk</code> syscall or by preparing or removing extra virtual memory using the <code>mmap</code> and <code>unmmap</code> syscalls. It’s up to the process to organize and manage the heap in the best possible way, and different languages do that differently:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>C forces the programmer to manually allocate and free memory for variables (using <code>malloc</code> and <code>free</code> functions).</p>&#13;
</li>&#13;
<li>&#13;
<p>C++ adds smart pointers like <a href="https://oreil.ly/QS9zj"><code>std::unique_ptr</code></a> and <a href="https://oreil.ly/QbQqQ"><code>std::shared_ptr</code></a>, which offer simple counting mechanisms to track the object lifecycle (reference counting).<sup><a data-type="noteref" href="ch05.html#idm45606833508688" id="idm45606833508688-marker">27</a></sup></p>&#13;
</li>&#13;
<li>&#13;
<p>Rust has a powerful <a href="https://oreil.ly/MajFo">memory ownership mechanism</a>, but it makes programming much more difficult for nonmemory critical code areas.<sup><a data-type="noteref" href="ch05.html#idm45606833506480" id="idm45606833506480-marker">28</a></sup></p>&#13;
</li>&#13;
<li>&#13;
<p>Finally, languages like Python, C#, Java, and others implement advanced heap allocators and garbage collector mechanisms. Garbage collectors periodically check if any memory is unused and can be released.</p>&#13;
&#13;
<p>In this sense, Go is closer to Java with memory management than C. Go implicitly (transparently to the programmer) allocates memory that requires dynamic allocation on the heap. For that purpose, Go has its unique components (implemented in Go and Assembly); see <a data-type="xref" href="#ch-hw-allocator">“Go Allocator”</a> &#13;
<span class="keep-together">and <a data-type="xref" href="#ch-hw-garbage">“Garbage Collection”</a>.</span></p>&#13;
</li>&#13;
</ul>&#13;
</dd>&#13;
</dl>&#13;
<div data-type="tip"><h1>Most of the Time, It’s Enough to Optimize the Heap Usage</h1>&#13;
<p><a data-primary="heap" data-secondary="optimizing usage" data-type="indexterm" id="idm45606833500416"/>Heap is the memory that usually stores the largest amounts of data in physical memory pages. It is so significant that it’s enough to look at the heap size to assess the Go process memory usage in most cases. On top of that, the overhead of heap management with runtime garbage collection is significant too. Both make the heap our first choice to analyze when optimizing memory use.</p>&#13;
</div>&#13;
<dl>&#13;
<dt>Manual process mappings</dt>&#13;
<dd>&#13;
<p>Both Go runtime and the developer writing Go code can manually allocate additional memory-mapped regions (e.g., using our <a data-type="xref" href="#code-mmap">Example 5-1</a> abstraction). Of course, it’s up to the process what kind of memory mapping to use (private or shared, read or write, anonymous or file backed), but all of them have a dedicated space in the process’s virtual memory, presented in <a data-type="xref" href="#img-mem-layout">Figure 5-5</a>.</p>&#13;
</dd>&#13;
<dt>Stack</dt>&#13;
<dd>&#13;
<p><a data-primary="function stack" data-type="indexterm" id="idm45606833494096"/><a data-primary="stack (function stack)" data-type="indexterm" id="idm45606833493392"/>The last section of the Go memory layout is reserved for function stacks. The stack is a simple yet fast structure allowing accessing values in last in, first out (LIFO) order. Programming languages use them to store all the elements (e.g., variables) that can use automatic allocation. As opposed to dynamic allocations fulfilled by the heap, automatic allocations work well for local data like local variables, function input, or return arguments. Allocations of those elements can be “automatic” because the compiler can deduce their lifespan before the program starts.</p>&#13;
&#13;
<p>Some programming languages might have a single stack or a stack per thread. Go is a bit unique here. As we learned in <a data-type="xref" href="ch04.html#ch-hw-concurrency">“Go Runtime Scheduler”</a>, the Go execution flow is designed around goroutines. Thus Go maintains a single dynamically sized stack per Go routine. This might even mean <a href="https://oreil.ly/zrqhj">hundreds of thousands of stacks</a>. Whenever the goroutine invokes another function, we can push its local variables and arguments to stack in a stack frame. We can pop those elements (de-allocate the stack frame) from the stack when we leave the function. If stack structures require more space than what’s reserved in virtual memory, Go will ask the OS for more memory attributed to the stack segment, e.g., via the <code>mmap</code> syscall.</p>&#13;
&#13;
<p>Stacks are incredibly fast as there is no extra overhead to figure out when memory used by certain elements must be removed (no usage tracking). Thus ideally, we write our algorithms so that they allocate primarily on the stack instead of the heap. Unfortunately, this is impossible in many cases due to stack limitations (we can’t allocate too-large objects) or when the variable has to live longer than the function’s scope. Therefore, the compiler decides which data can be allocated automatically (on the stack) and which must be allocated dynamically (on the heap). This process is called escape analysis, which you saw in <a data-type="xref" href="ch04.html#code-comp-sum">Example 4-3</a>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>All the mechanisms discussed (except manual mappings) are helping Go developers. We don’t need to care where and how we should allocate memory for our variables. That is a huge win—for example, when we want to make some HTTP calls, we &#13;
<span class="keep-together">simply create</span> an HTTP client using a standard library, e.g., with the <code>client := http.Cli⁠ent{}</code> code statement. As a result of Go’s memory design, we can immediately start using <code>client</code>, focusing on our code’s functionality, readability, and reliability. In &#13;
<span class="keep-together">particular:</span></p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>We don’t need to ensure that the OS has a free virtual memory page to hold the <code>client</code> variable. Likewise, we don’t need to find a valid segment and virtual address for it. Both will be done automatically by the compiler (if the variable can be stored on the stack) or runtime allocator (dynamic allocation on the heap).</p>&#13;
</li>&#13;
<li>&#13;
<p>We don’t need to remember to release memory kept by the <code>client</code> variable when we stop using it. Instead, suppose the <code>client</code> would go beyond code reach (nothing references it). In that case, the data in Go will be released—immediately when stored on the stack or in the next garbage collection execution cycle if stored on the heap (more on that in <a data-type="xref" href="#ch-hw-garbage">“Garbage Collection”</a>).</p>&#13;
&#13;
<p>Such automation is much less error-prone to potential memory leaks (“I forgot to release memory for <code>client</code>”) or dangling pointers (“I released memory for &#13;
<span class="keep-together"><code>client</code>,</span> but actually some code still uses it”).</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Generally, we don’t need to care what segment is used for our objects for everyday use of the Go language.</p>&#13;
<blockquote><p>How do I know whether a variable is allocated on the heap or the stack? From a correctness standpoint, you don’t need to know. Each variable in Go exists as long as there are references to it. The storage location chosen by the implementation is irrelevant to the semantics of the language.</p>&#13;
<p>The storage location does have an effect on writing efficient programs.</p>&#13;
<p data-type="attribution">The Go Team, <a href="https://oreil.ly/UUGgI">“Go: Frequently Asked Questions (FAQ)”</a></p></blockquote>&#13;
&#13;
<p>However, since allocations are so effortless, there is a risk of not noticing the memory waste.<a data-primary="heap" data-secondary="stack versus" data-type="indexterm" id="idm45606833474560"/><a data-primary="variables" data-secondary="heap versus stack allocation" data-type="indexterm" id="idm45606833473616"/></p>&#13;
<div data-type="note" epub:type="note"><h1>Transparent Allocations Mean There Is a Risk of Overdoing Them</h1>&#13;
<p>Allocations are implicit in Go, making coding much easier, but there are trade-offs. One is around memory efficiency: if we don’t see explicit memory allocations and releases, it’s easier to miss apparent high memory usage in our code.</p>&#13;
&#13;
<p>It’s similar to going shopping with cash versus a credit card. You will likely overspend with a credit card than with cash since you don’t see that money flowing. With a credit card, money spent is almost transparent to us—it is the same with allocations in Go.</p>&#13;
</div>&#13;
&#13;
<p>To sum up, Go is a very productive language because, when programming, we don’t need to worry about where and how the data held by our variables and abstractions is stored. Yet sometimes when our measurements indicate efficiency problems, it’s useful to have a basic awareness of the parts of our program that might allocate some memory, how this occurs, and how the memory is released. So let’s uncover that.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Values, Pointers, and Memory Blocks" data-type="sect2"><div class="sect2" id="ch-hw-allocations">&#13;
<h2>Values, Pointers, and Memory Blocks</h2>&#13;
&#13;
<p><a data-primary="memory management" data-secondary="values, pointers, and memory blocks" data-type="indexterm" id="ix_ch05-asciidoc23"/>Let’s get this straight before we start—you don’t need to know what type of statements trigger memory allocation, where (on a stack or heap), and how much memory was allocated. But, as you will learn in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch07.html#ch-observability2">7</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch09.html#ch-observability3">9</a>, many robust tools can tell us all that accurately and quickly. In most cases, we can find what code line and roughly how much was allocated within seconds. Thus, there is generally a common theme: we should not guess that information (since humans tend to guess wrong) because there are tools for that.</p>&#13;
&#13;
<p>This is generally true, but there is no harm in building some basic allocation awareness. On the contrary, it might make us more effective while using those tools to analyze memory usage. The aim is to build a healthy instinct for what pieces of code can potentially allocate the suspicious amount of memory and where we need to be &#13;
<span class="keep-together">careful.</span></p>&#13;
&#13;
<p class="less_space pagebreak-before">Many books try to teach this by listing examples of common statements that allocate. This is great, but it’s a bit like giving someone <a href="https://oreil.ly/utQIG">a fish instead of a fishing rod</a>. So again, it’s helpful, but only for “common” statements. Ideally, I want you to understand the underlying rules for why something allocates.</p>&#13;
&#13;
<p>Let’s dive into how we reference objects in Go to start noticing that allocation more quickly. Our code can perform certain operations on objects stored in some memory. Therefore, we must link those objects to operations, and we typically do that via variables. We describe those variables using Go’s type system to make it even easier for the compiler and developers.</p>&#13;
&#13;
<p><a data-primary="values" data-type="indexterm" id="ix_ch05-asciidoc24"/>However, Go is <a href="https://oreil.ly/lgy2S">value oriented</a> rather than reference oriented (like many <a href="https://oreil.ly/ben85">managed runtime</a> languages). This means that Go variables never reference objects. Instead, the variables always store the whole <em>value</em> of the object. There is no exception to this rule!</p>&#13;
&#13;
<p>To understand this better, the memory representation of three variables is shown in <a data-type="xref" href="#img-mem-blocks">Figure 5-6</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-mem-blocks">&#13;
<img alt="efgo 0506" src="assets/efgo_0506.png"/>&#13;
<h6><span class="label">Figure 5-6. </span>Representation of three variables allocated on the process’s virtual memory</h6>&#13;
</div></figure>&#13;
<div data-type="tip"><h1>Think About Variables as Boxes Holding Values</h1>&#13;
<p><a data-primary="memory blocks" data-type="indexterm" id="ix_ch05-asciidoc25"/>Whenever the compiler sees a definition of the <code>var</code> variable or function arguments (including parameters) in the invocation scope, it allocates a contiguous “memory block” for a box. The box is big enough to contain the whole value of the given type. For example, <code>var var1 int</code> and <code>var var2 int</code> will need a box for eight bytes.<sup><a data-type="noteref" href="ch05.html#idm45606833447648" id="idm45606833447648-marker">29</a></sup></p>&#13;
</div>&#13;
&#13;
<p>Thanks to our available space in “boxes,” we can copy some values. In <a data-type="xref" href="#img-mem-blocks">Figure 5-6</a>, we can copy an integer <code>1</code> to <code>var1</code>. Now, Go does not have reference variables, so even if we assign the <code>var1</code> value to another box named <code>var2</code>, this is yet another box with unique space. We can confirm that by printing <code>&amp;var1</code> and <code>&amp;var2</code>. It should print <code>0xA040</code> and <code>0xA038</code>, respectively. As a result, a simple assignment is always a copy, which adds latency proportional to the value’s size.<a data-primary="Cheney, Dave" data-secondary="on variables" data-type="indexterm" id="idm45606833440736"/></p>&#13;
<blockquote><p>Unlike C++, each variable defined in a Go program occupies a unique memory location. It is not possible to create a Go program where two variables share the same storage location in memory. It is possible to create two variables whose contents point to the same storage location, but that is not the same thing.</p><p data-type="attribution">Dave Cheney, <a href="https://oreil.ly/iPu5w">“There Is No Pass-By-Reference in Go”</a></p></blockquote>&#13;
&#13;
<p><a data-primary="pointers" data-type="indexterm" id="ix_ch05-asciidoc26"/>The <code>var3</code> box is a pointer to the integer type. A “pointer” variable is a box that stores the value representing the memory address. The type of memory address is just <code>uintptr</code> or <code>unsafe.Pointer</code>, so simply a 64-bit unsigned integer that allows pointing to another value in memory. As a result, any pointer variable needs a box for eight bytes.</p>&#13;
&#13;
<p>The pointer can also be <code>nil</code> (Go’s NULL value), a special value indicating that the pointer does not point to anything. In <a data-type="xref" href="#img-mem-blocks">Figure 5-6</a>, we can see that the <code>var3</code> box contains a value too—a memory address of the <code>var1</code> box.</p>&#13;
&#13;
<p>This is also consistent with more complex types. For example, both <code>var var4</code> and <code>var var5</code> require boxes for only 24 bytes. This is because the <code>slice</code> struct value has three integers.</p>&#13;
<div data-type="note" epub:type="note"><h1>Memory Structure for Go Slice</h1>&#13;
<p><a data-primary="Go slice, memory structure for" data-type="indexterm" id="idm45606833429296"/><a data-primary="slice, memory structure for" data-type="indexterm" id="idm45606833428528"/>Slice allows easy dynamic behavior of the underlying array of a given&#13;
type. A slice data structure requires a memory block that can hold <code>length</code>, <code>capacity</code>, and <code>pointer</code> to the desired array.<sup><a data-type="noteref" href="ch05.html#idm45606833426272" id="idm45606833426272-marker">30</a></sup></p>&#13;
</div>&#13;
&#13;
<p>Generally, the slice is just a more complex struct. You can think about a struct as a cabinet—it is full of drawers (struct fields) that are simply boxes that share a memory block with other drawers in the same cabinet. So, for example, the <code>slice</code> type has three drawers. One of them is of pointer type.</p>&#13;
&#13;
<p>There are two special behaviors of <code>slice</code> and a few other special types:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>You can use the <a href="https://oreil.ly/Mlx6Q"><code>make</code></a> built-in function that only works for <code>map</code>, <code>chan</code>, and <code>slice</code> types. It returns the type’s value<sup><a data-type="noteref" href="ch05.html#idm45606833418784" id="idm45606833418784-marker">31</a></sup> and allocates underlying structures, like an array for slices, a buffer for channels, and a hashmap for maps.</p>&#13;
</li>&#13;
<li>&#13;
<p>We can put <code>nil</code> into boxes of types, like <code>func</code>, <code>map</code>, <code>chan</code>, or <code>slice</code>, although they are not strictly pointers, e.g., <code>[]byte(nil)</code>.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>One drawer of the <code>var4</code> and <code>var5</code> cabinets is a type of pointer that holds the memory address. Thanks to <code>make([]byte, 5000)</code> in <code>var5</code>, it points to another memory block containing a 5,000-element byte array.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Structure Padding</h1>&#13;
<p><a data-primary="padding" data-type="indexterm" id="idm45606833409120"/><a data-primary="structure padding" data-type="indexterm" id="idm45606833408416"/>The slice structure with three 64-bit fields requires a 24-byte long memory block. But the memory block size for a structure type is not always the sum of the size of its fields!</p>&#13;
&#13;
<p>Smart compilers like in Go might attempt to align type sizes to the typical cache lines or the OS or internal Go allocator page sizes. For this reason, Go compilers sometimes add padding between fields.<sup><a data-type="noteref" href="ch05.html#idm45606833407008" id="idm45606833407008-marker">32</a></sup><a data-startref="ix_ch05-asciidoc26" data-type="indexterm" id="idm45606833404848"/></p>&#13;
</div>&#13;
&#13;
<p>To reinforce that knowledge, let’s ask a common question when designing a new function or method: should my arguments be pointers of values? Of course, the first thing we should answer is obviously, if we want the caller to see the modifications of that value. But there is an efficiency aspect as well. Let’s discuss the difference in <a data-type="xref" href="#code-copy-ptr">Example 5-4</a>, assuming we don’t need to see modifications of those arguments from outside.</p>&#13;
<div data-type="example" id="code-copy-ptr">&#13;
<h5><span class="label">Example 5-4. </span>Different arguments highlight the differences using values, pointers, and special types like <code>slice</code></h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">func</code><code class="w"> </code><code class="nx">myFunction</code><code class="p">(</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">arg1</code><code class="w"> </code><code class="kt">int</code><code class="p">,</code><code class="w"> </code><code class="nx">arg2</code><code class="w"> </code><code class="o">*</code><code class="kt">int</code><code class="p">,</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO4-1" id="co_how_go_uses_memory_resource_CO4-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
    </code><code class="nx">arg3</code><code class="w"> </code><code class="nx">biggie</code><code class="p">,</code><code class="w"> </code><code class="nx">arg4</code><code class="w"> </code><code class="o">*</code><code class="nx">biggie</code><code class="p">,</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO4-2" id="co_how_go_uses_memory_resource_CO4-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
    </code><code class="nx">arg5</code><code class="w"> </code><code class="p">[</code><code class="p">]</code><code class="kt">byte</code><code class="p">,</code><code class="w"> </code><code class="nx">arg6</code><code class="w"> </code><code class="o">*</code><code class="p">[</code><code class="p">]</code><code class="kt">byte</code><code class="p">,</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO4-3" id="co_how_go_uses_memory_resource_CO4-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
    </code><code class="nx">arg7</code><code class="w"> </code><code class="kd">chan</code><code class="w"> </code><code class="kt">byte</code><code class="p">,</code><code class="w"> </code><code class="nx">arg8</code><code class="w"> </code><code class="kd">map</code><code class="p">[</code><code class="kt">string</code><code class="p">]</code><code class="kt">int</code><code class="p">,</code><code class="w"> </code><code class="nx">arg9</code><code class="w"> </code><code class="kd">func</code><code class="p">(</code><code class="p">)</code><code class="p">,</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO4-4" id="co_how_go_uses_memory_resource_CO4-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w">&#13;
</code><code class="w">   </code><code class="c1">// ...</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="kd">type</code><code class="w"> </code><code class="nx">biggie</code><code class="w"> </code><code class="kd">struct</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO4-2" id="co_how_go_uses_memory_resource_CO4-5"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
    </code><code class="nx">huge</code><code class="w"> </code><code class="p">[</code><code class="mf">1e8</code><code class="p">]</code><code class="kt">byte</code><code class="w">&#13;
</code><code class="w">    </code><code class="nx">other</code><code class="w"> </code><code class="o">*</code><code class="nx">biggie</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO4-1" id="callout_how_go_uses_memory_resource_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Function arguments are like any newly declared variable: boxes. So for <code>arg1</code>, it will create an eight-byte box (most likely allocate it on the stack) and copy the passed integer during the <code>myFunction</code> invocation. For <code>arg2</code>, it will create a similar eight-byte box that will copy the pointer instead.</p>&#13;
&#13;
<p>For such simple types, avoiding the pointer makes more sense if you don’t need to modify the value. You use the same amount of memory and the same copying overhead. The only difference is that the value pointed to by <code>arg2</code> has to live on the heap, which is more expensive and, in many cases, can be avoided.</p></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO4-2" id="callout_how_go_uses_memory_resource_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The rule is the same for custom <code>struct</code> arguments, but the size and copying overhead might matter more. For example, <code>arg3</code> is of <code>biggie</code> <code>struct</code>, which is of extraordinary size. Because of the static array with 100 million elements, the type requires a ~100 MB memory block.</p>&#13;
&#13;
<p>For bigger types like this, we should consider using a pointer when passing through functions. This is because every <code>myFunction</code> invocation will allocate 100 MB on the heap for the <code>arg3</code> box (it’s too large to be on the stack)! On top of that, it will spend CPU time copying large objects between boxes. So, <code>arg4</code> will allocate eight bytes on the stack (and copy only that) and point to memory on the heap with the <code>biggie</code> object, which can be reused across function calls.</p>&#13;
&#13;
<p>Note that despite <code>biggie</code> being copied in <code>arg3</code>, the copy is <em>shallow</em>, i.e., <code>arg3.other</code> will share a memory with the previous box!</p></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO4-3" id="callout_how_go_uses_memory_resource_CO4-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>The <code>slice</code> type behaves like the <code>biggie</code> type. We must remember the <a href="https://oreil.ly/Tla4w">underlying <code>struct</code> type of the slice</a>.</p>&#13;
&#13;
<p>As a result, <code>arg5</code> will allocate a 24-byte box and copy three integers. In contrast, <code>arg6</code> will allocate an eight-byte box and copy only one integer (pointer). From the efficiency point of view, it does not matter. It only matters if we want to expose modifications of the underlying array (both <code>arg5</code> and <code>arg6</code> allow that) or if we want to also expose changes to the <code>pointer</code>, <code>len</code>, and <code>cap</code> fields as <code>arg6</code> allows.</p></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO4-4" id="callout_how_go_uses_memory_resource_CO4-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Special types like <code>chan</code>, <code>map</code>, and <code>func()</code> can be treated similarly to pointers. They share memory through the heap, and the only cost is to allocate and copy the pointer value into <code>arg7</code>, <code>arg8</code>, or <code>arg9</code> boxes.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>The same decision flow can be applied to decide about pointer versus value types for:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Return arguments</p>&#13;
</li>&#13;
<li>&#13;
<p>The <code>struct</code> fields</p>&#13;
</li>&#13;
<li>&#13;
<p>Elements of map, slice, or channels</p>&#13;
</li>&#13;
<li>&#13;
<p>The method receiver (e.g., <code>func (receiver) Method()</code>)</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Hopefully, the preceding information will give you an understanding of which Go code statements allocate memory and roughly how much. Generally:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Every variable declaration (including function arguments, return arguments, and method receiver) allocates the whole type or just a pointer to it.</p>&#13;
</li>&#13;
<li>&#13;
<p><code>make</code> allocates special types and their underlying (pointed) structures.</p>&#13;
</li>&#13;
<li>&#13;
<p><code>new(&lt;type&gt;)</code> is the same as <code>&amp;&lt;type&gt;</code>, so it allocates a pointer box and the type on the heap in the separate memory block<a data-startref="ix_ch05-asciidoc25" data-type="indexterm" id="idm45606833213776"/>.<a data-startref="ix_ch05-asciidoc24" data-type="indexterm" id="idm45606833212944"/></p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Most program memory allocations are only known in runtime; thus, dynamic allocation (in a heap) is needed. Therefore, when we optimize memory in Go programs, 99% of the time we just focus on the heap. Go comes with two important runtime components: Allocator and GC, responsible for heap management. Those components are nontrivial pieces of software that often introduce certain waste in terms of extra CPU cycles by the program runtime and some memory waste. Given its nondeterministic and nonimmediate memory release nature, it’s worth discussing this in detail. Let’s do that in the next two sections.<a data-startref="ix_ch05-asciidoc23" data-type="indexterm" id="idm45606833211600"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Go Allocator" data-type="sect2"><div class="sect2" id="ch-hw-allocator">&#13;
<h2>Go Allocator</h2>&#13;
&#13;
<p><a data-primary="Go Allocator" data-type="indexterm" id="ix_ch05-asciidoc27"/><a data-primary="Go memory management" data-secondary="Go Allocator" data-type="indexterm" id="ix_ch05-asciidoc28"/><a data-primary="heap" data-secondary="Go Allocator and" data-type="indexterm" id="ix_ch05-asciidoc29"/><a data-primary="memory management" data-secondary="Go Allocator" data-type="indexterm" id="ix_ch05-asciidoc30"/>It’s far from easy to manage the heap, as it poses similar challenges as the OS has toward physical memory. For example, the Go program runs multiple goroutines, and each wants a few (dynamically sized!) segments of the heap memory for a different amount of time.</p>&#13;
&#13;
<p>The Go Allocator is a piece of internal runtime Go code maintained by the Go team. As the name suggests, it can dynamically (in runtime) allocate the memory blocks required to operate on objects. In addition, it is optimized to avoid locking and fragmentation, and to mitigate slow syscalls to the OS.</p>&#13;
&#13;
<p>During compilation, the Go compiler performs a complex stack escape analysis to detect if the memory for objects can be automatically allocated (mentioned in <a data-type="xref" href="ch04.html#code-comp-sum">Example 4-3</a>). If yes,  it adds appropriate CPU instructions that store related memory blocks in the stack segment of the memory layout. However, in most cases the compiler can’t avoid putting most of our memory on the heap. In these cases, it generates different CPU instructions invoking the Go Allocator code.</p>&#13;
&#13;
<p>The Go Allocator is responsible for <a href="https://oreil.ly/l27Jv">bin packing</a> the memory blocks in the virtual memory space. It also asks for more space from the OS if needed using <code>mmap</code> with private, anonymous pages, which are initialized by zero.<sup><a data-type="noteref" href="ch05.html#idm45606833200480" id="idm45606833200480-marker">33</a></sup> As we learned in <a data-type="xref" href="#ch-hw-memory-mmap-os">“OS Memory Mapping”</a>, those pages are also allocated on the physical RAM only when accessed.</p>&#13;
&#13;
<p>Generally, the Go developer can live without learning details about Go Allocator internals. However, it’s enough to remember that:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>It is based on a custom Google <code>C++</code> <code>malloc</code> implementation called <a href="https://oreil.ly/AZ5S7">TCMalloc</a>.</p>&#13;
</li>&#13;
<li>&#13;
<p>It is OS virtual memory page aware, but it operates with 8 KB pages.</p>&#13;
</li>&#13;
<li>&#13;
<p>It mitigates fragmentation by allocating memory blocks to certain spans that hold one or multiple 8 KB pages. Each span is created for class memory block sizes. For example, in Go 1.18, there are 67 different <a href="https://oreil.ly/tMlnv">size classes</a> (size buckets), the largest being 32 KB.</p>&#13;
</li>&#13;
<li>&#13;
<p>Memory blocks for objects that do not contain a pointer are marked with the <code>noscan</code> type, making it easier to track nested objects in the garbage collection phase.</p>&#13;
</li>&#13;
<li>&#13;
<p>Objects with over 32 KB memory block (e.g., 600 MB byte array) are treated specially (allocated directly without span).</p>&#13;
</li>&#13;
<li>&#13;
<p>If runtime needs more virtual space from OS for the heap, it allocates a bigger chunk of memory at once (at least 1 MB), which amortizes the latency of the syscall.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>All of the preceding points are constantly changing, with the open source community and Go team adding various small optimizations and features.</p>&#13;
&#13;
<p>They say one code snippet is worth a thousand words, so let’s visualize and explain some of these allocation characteristics caused by a mix of Go, OS, and hardware using an example. <a data-type="xref" href="#code-mem-alloc-slice">Example 5-5</a> shows the same functionality as <a data-type="xref" href="#code-mmap-usage">Example 5-3</a>, but instead of explicit <code>mmap</code>, we will rely on Go memory management and no underlying file.</p>&#13;
<div data-type="example" id="code-mem-alloc-slice">&#13;
<h5><span class="label">Example 5-5. </span>Allocation of a large <code>[]byte</code> slice followed by different access patterns</h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="nx">b</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nb">make</code><code class="p">(</code><code class="p">[</code><code class="p">]</code><code class="kt">byte</code><code class="p">,</code><code class="w"> </code><code class="mi">600</code><code class="o">*</code><code class="mi">1024</code><code class="o">*</code><code class="mi">1024</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO5-1" id="co_how_go_uses_memory_resource_CO5-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="nx">b</code><code class="p">[</code><code class="mi">5000</code><code class="p">]</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="mi">1</code><code class="w">&#13;
</code><code class="nx">b</code><code class="p">[</code><code class="mi">100000</code><code class="p">]</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="mi">1</code><code class="w">&#13;
</code><code class="nx">b</code><code class="p">[</code><code class="mi">104000</code><code class="p">]</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="mi">1</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO5-2" id="co_how_go_uses_memory_resource_CO5-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="k">range</code><code class="w"> </code><code class="nx">b</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO5-3" id="co_how_go_uses_memory_resource_CO5-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
   </code><code class="nx">b</code><code class="p">[</code><code class="nx">i</code><code class="p">]</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="mi">1</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO5-1" id="callout_how_go_uses_memory_resource_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The <code>b</code> variable is declared as a <code>[]byte</code> slice. The following <code>make</code> statement is tasked to create a byte array with 600 MB of data (~600 million elements in the array). This memory block is allocated on the heap.<sup><a data-type="noteref" href="ch05.html#idm45606833118576" id="idm45606833118576-marker">34</a></sup></p>&#13;
&#13;
<p>If we would analyze this situation closely, the Go Allocator seemed to create three contiguous anonymous mappings for that slice with different (virtual) memory sizes: 2 MB, 598 MB, and 4 MB. (The total size is usually bigger than the requested 600 MB because of the Go Allocator internal bucketed algorithm.) Let’s summarize the interesting statistics:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The RSS for three memory mappings used by our slice: 548 KB, 0 KB, and 120 KB (much lower than VSS numbers).</p>&#13;
</li>&#13;
<li>&#13;
<p>Total RSS of the whole process shows 21 MB. Profiling shows that most of this comes from outside the heap.</p>&#13;
</li>&#13;
<li>&#13;
<p>Go reports 600.15 MB of the heap size (despite RSS being significantly lower).</p>&#13;
</li>&#13;
</ul></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO5-2" id="callout_how_go_uses_memory_resource_CO5-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Only after we start accessing the slice elements (either by writing or reading) will the OS start reserving actual physical memory surrounding those elements. Our statistics:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The RSS for three memory mappings: 556 KB, (still) 0 KB, and 180 KB (only a few KB more than before accessing).</p>&#13;
</li>&#13;
<li>&#13;
<p>Total RSS still shows 21 MB.</p>&#13;
</li>&#13;
<li>&#13;
<p>Go reports 600.16 MB of the heap size (actually a few KB more, probably due to background goroutines).</p>&#13;
</li>&#13;
</ul></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO5-3" id="callout_how_go_uses_memory_resource_CO5-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>After we loop over all elements to access it, we will see that the OS mapped on demand all pages for our <code>b</code> slice in physical memory. Our statistics prove this:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The RSS for three memory mappings: 1.5 MB, (fully mapped) 598 MB, and 1.2 MB.</p>&#13;
</li>&#13;
<li>&#13;
<p>Total RSS of the whole process shows 621.7 MB (finally, same as heap size).</p>&#13;
</li>&#13;
<li>&#13;
<p>Go reports the same 600.16 MB of the heap size.</p>&#13;
</li>&#13;
</ul></dd>&#13;
</dl></div>&#13;
&#13;
<p>This example might feel similar to Examples <a data-type="xref" data-xrefstyle="select:labelnumber" href="#code-naive-read-usage">5-2</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#code-mmap-usage">5-3</a>, but it’s a bit different. Notice that in <a data-type="xref" href="#code-mem-alloc-slice">Example 5-5</a>,  there is no (explicit) file involved that could store some data if the page is not mapped. We also utilize the Go Allocator to organize &#13;
<span class="keep-together">and manage</span> different anonymous page mappings most efficiently, whereas in <a data-type="xref" href="#code-mmap-usage">Example 5-3</a>, the Go Allocator is unaware of that memory usage.</p>&#13;
<div data-type="tip"><h1>Internal Go Runtime Knowledge Versus OS Knowledge</h1>&#13;
<p><a data-primary="Go Allocator" data-secondary="internal Go runtime knowledge versus OS knowledge" data-type="indexterm" id="idm45606833051568"/><a data-primary="observability" data-secondary="internal Go runtime knowledge versus OS knowledge" data-type="indexterm" id="idm45606833050496"/>The Go Allocator tracks certain information we can collect through different observability mechanisms discussed in &#13;
<span class="keep-together"><a data-type="xref" href="ch06.html#ch-observability">Chapter 6</a>.</span></p>&#13;
&#13;
<p>Be mindful when using those. In the preceding example, we saw that the heap size tracked by the Go Allocator was significantly larger than the actual amount of memory used on physical RAM (RSS)!<sup><a data-type="noteref" href="ch05.html#idm45606833047648" id="idm45606833047648-marker">35</a></sup> Similarly, the memory used by explicit <code>mmap</code>, as in <a data-type="xref" href="#code-mmap-usage">Example 5-3</a>, is not reflected in any Go runtime metrics. This is why it’s good to rely on more than one metric on our TFBO journey, as discussed in <a data-type="xref" href="ch06.html#ch-obs-mem-usage">“Memory Usage”</a>.</p>&#13;
</div>&#13;
&#13;
<p>The behavior of Go heap management backed up by on-demand paging tends to be indeterministic and fuzzy. We cannot control it directly either. For instance, if you tried to reproduce <a data-type="xref" href="#code-mem-alloc-slice">Example 5-5</a> on your machine, you would most likely observe slightly different mappings, more or less different RSS numbers (with a tolerance of few MBs), and different heap sizes. It all depends on the Go version you build a program with, the kernel version, the RAM capacity and model, and the load on your system. This poses important challenges to the assessment step of our TFBO process, which we will discuss in <a data-type="xref" href="ch07.html#ch-obs-rel">“Reliability of Experiments”</a>.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Don’t Be Bothered by a Small Memory Increase</h1>&#13;
<p>Don’t try to understand where every hundred bytes or kilobytes of your process RSS memory came from. In most cases, it is impossible to tell or control at that low level. Heap management overhead, speculative page allocations by both the OS and the Go Allocator, dynamic OS mapping behavior, and eventual memory collection (we will learn about that in the next section) make things indeterministic on such a “micro” kilobyte level.</p>&#13;
&#13;
<p>Even if you spot some pattern in one environment, it will be different in others unless we talk about bigger numbers like hundreds of megabytes or more!</p>&#13;
</div>&#13;
&#13;
<p>The lesson here is that we have to adjust our mindsets. There will always be a few unknowns. What matters is to understand bigger unknowns that contribute the most to the potentially too-high memory usage situation. Together with this allocator awareness, you will learn how to do that in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.html#ch-observability">6</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch09.html#ch-observability3">9</a>.</p>&#13;
&#13;
<p>So far, we have discussed how to efficiently reserve memory for our memory blocks through the Go Allocator and how to access it. However, we can’t just reserve more memory indefinitely if there is no logic for removing the memory blocks our code doesn’t need anymore. That’s why it’s critical to understand the second part of heap management responsible for releasing unused objects from the heap—garbage collection.<a data-startref="ix_ch05-asciidoc30" data-type="indexterm" id="idm45606833035776"/><a data-startref="ix_ch05-asciidoc29" data-type="indexterm" id="idm45606833035072"/><a data-startref="ix_ch05-asciidoc28" data-type="indexterm" id="idm45606833034400"/><a data-startref="ix_ch05-asciidoc27" data-type="indexterm" id="idm45606833033728"/> Let’s explore that in the next section.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Garbage Collection" data-type="sect2"><div class="sect2" id="ch-hw-garbage">&#13;
<h2>Garbage Collection</h2>&#13;
<blockquote>&#13;
<p>You pay for memory allocation more than once. The first is obviously when you allocate it. But you also pay every time the garbage collection runs.</p>&#13;
<p data-type="attribution">Damian Gryski, <a href="https://oreil.ly/yg1LK">“go-perfbook”</a></p>&#13;
</blockquote>&#13;
&#13;
<p><a data-primary="garbage collection (GC)" data-type="indexterm" id="ix_ch05-asciidoc31"/><a data-primary="garbage collection (GC)" data-secondary="heap management and" data-type="indexterm" id="ix_ch05-asciidoc32"/><a data-primary="Go memory management" data-secondary="garbage collection" data-type="indexterm" id="ix_ch05-asciidoc33"/>The <a data-primary="Gryski, Damian, on GC and cost of memory allocation" data-type="indexterm" id="idm45606833024960"/>second part of heap management is similar to vacuuming your house. It is related to a process that removes the proverbial garbage—unused objects from the program’s heap. Generally speaking, the garbage collector (GC) is an additional background routine that executes “collection” at certain moments. The cadence of collections is critical:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>If the GC runs less often, we risk allocating a significant amount of new RAM space without the ability to reuse the memory pages currently allocated by garbage (unused objects).</p>&#13;
</li>&#13;
<li>&#13;
<p>If the GC runs too often, we risk spending most of the program time and CPU on GC work instead of moving our functionality forward. As we will learn later, the GC is relatively fast but can directly or indirectly impact the execution of other goroutines in the system, especially if we have many objects in a heap (if we allocate a lot).</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The interval of the GC runs is not based on time. Instead, two configuration variables (working independently) define the pace: <code>GOGC</code> and, from Go 1.19, <code>GOMEMLIMIT</code>. To learn more about them, read <a href="https://oreil.ly/f2F6H">an official detailed guide about GC tuning</a>. For this book, let’s explain both very briefly:</p>&#13;
<dl>&#13;
<dt>The <code>GOGC</code> option represents the “GC percentage.”</dt>&#13;
<dd>&#13;
<p><a data-primary="GOGC option" data-type="indexterm" id="idm45606833016960"/><code>GOGC</code> is enabled by default with a 100 value. It means that the next GC collection will be done when the heap size expands to 100% of the size it has at the end of the last GC cycle. GC’s pacing algorithm estimates when that goal will be reached based on current heap growth. It can also be set programmatically with the <a href="https://oreil.ly/7khRe"><code>debug.SetGCPercent</code> function</a>.</p>&#13;
</dd>&#13;
<dt>The <code>GOMEMLIMIT</code> option controls the soft memory limit.</dt>&#13;
<dd>&#13;
<p><a data-primary="GOMEMLIMIT option" data-type="indexterm" id="idm45606833012912"/>The <code>GOMEMLIMIT</code> option was introduced in Go 1.19. It is disabled by default (set to <code>math.MaxInt64</code>), and offers running GC more often when we are close (or above) the set memory limit. It can be used with <code>GOGC=off</code> (disabled) or together with <code>GOGC</code>. This option can also be set programmatically with the <a href="https://oreil.ly/etDUv"><code>debug.Set​Me⁠moryLimit</code> function</a>.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>GOMEMLIMIT Does Not Prevent Your Program from Allocating More than the Set Value!</h1>&#13;
<p>The GC’s soft memory limit configuration is called “soft” for a reason. It tells the GC how much memory overhead space there is for the GC “laziness” to save the CPU.</p>&#13;
&#13;
<p>However, when your program allocates and uses more memory than the desired limit, with the <code>GOMEMLIMIT</code> option set, it will only make things worse. This is because the GC will run nearly continuously, taking up 25% of the precious CPU time from other functionalities.</p>&#13;
&#13;
<p>We still have to optimize the memory efficiency of our &#13;
<span class="keep-together">programs!</span></p>&#13;
</div>&#13;
</dd>&#13;
<dt>Manual trigger.</dt>&#13;
<dd>&#13;
<p>Programmers can also trigger another GC collection on demand by invoking <a href="https://oreil.ly/znoCL"><code>runtime.GC()</code></a>. It is mostly used in testing or benchmarking code, as it can block the entire program. Other pacing configurations like <code>GOGC</code> and <code>GOMEMLIMIT</code> might run in between.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The Go GC implementation can be described as <a href="https://oreil.ly/vvOgl">the concurrent, nongenerational, tricolor mark and&#13;
sweep collector</a> implementation. Whether invoked by the programmer or by the runtime-based <code>GOGC</code> or <code>GOMEMLIMIT</code> option, the <code>runtime.GC()</code> implementation comprises a few phases. The first one is a mark phase that has to:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Perform a “stop the world” (STW) event to inject an essential <a href="https://oreil.ly/Sl9PI">write barrier</a> (a lock on writing data) into all goroutines. Even though STW is relatively fast (10–30 microseconds on average), it is pretty impactful—it suspends the execution of all goroutines in our process for that time.</p>&#13;
</li>&#13;
<li>&#13;
<p>Try to use 25% of the CPU capacity given to the process to concurrently mark all objects in the heap that are still in use.</p>&#13;
</li>&#13;
<li>&#13;
<p>Terminate marking by removing the write barrier from the goroutines. This requires another STW event.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>After the mark phase, the GC function is generally complete. As interesting as it sounds, the GC doesn’t release any memory! Instead, the sweeping phase releases objects that were not marked as in use. It is done lazily: every time a goroutine wants to allocate memory through the Go Allocator, it must perform a sweeping work first, then allocate. This is counted as an <code>allocation</code> latency, even though it is technically a garbage collection functionality—worth noting!</p>&#13;
&#13;
<p>Generally speaking, the Go Allocator and GC compose a sophisticated implementation of bucketed <a href="https://oreil.ly/r1K18">object pooling</a>, where each pool of slots of different sizes are prepared for incoming allocations. When an allocation is not needed anymore, it is eventually released. The memory space for this allocation is not immediately released to the OS since it can be assigned to another incoming allocation soon (this is similar to the pooling pattern using <code>sync.Pool</code> we will discuss in <a data-type="xref" href="ch11.html#ch-basic-pool">“Memory Reuse and Pooling”</a>). When the number of free buckets is big enough, Go releases memory to the OS. But even then, it does not necessarily mean that runtime deletes mapped regions straight away. For example, on Linux, Go runtime typically “releases” memory through the <a href="https://oreil.ly/pxXum"><code>madvise</code> syscall</a> with the <code>MADV_DONTNEED</code> argument by default.<sup><a data-type="noteref" href="ch05.html#idm45606832989440" id="idm45606832989440-marker">36</a></sup> This is because our mapped region might be needed again pretty soon, so it’s faster to keep them just in case and ask the OS to take them back only if other processes require this physical memory.</p>&#13;
<blockquote><p>Note that, when applied to shared mappings, <code>MADV_DONTNEED</code> might not lead to immediate freeing of the pages in the range. The kernel is free to delay freeing the pages until an appropriate moment. The resident set size (RSS) of the calling process will be immediately reduced, however.</p><p data-type="attribution"> Linux Community, <a href="https://oreil.ly/JDuS7">"<code>madvise(2)</code>, Linux Manual Page”</a></p></blockquote>&#13;
&#13;
<p>With the theory behind the GC algorithm, it will be easier for us to understand in <a data-type="xref" href="#code-mem-dealloc-slice">Example 5-6</a> what happens if we try to clean the memory used for the large, 600 MB byte slice we created in <a data-type="xref" href="#code-mem-alloc-slice">Example 5-5</a>.</p>&#13;
<div data-type="example" id="code-mem-dealloc-slice">&#13;
<h5><span class="label">Example 5-6. </span>Memory release (de-allocation) of large slice created in <a data-type="xref" href="#code-mem-alloc-slice">Example 5-5</a></h5>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="nx">b</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="nb">make</code><code class="p">(</code><code class="p">[</code><code class="p">]</code><code class="kt">byte</code><code class="p">,</code><code class="w"> </code><code class="mi">600</code><code class="o">*</code><code class="mi">1024</code><code class="o">*</code><code class="mi">1024</code><code class="p">)</code><code class="w">&#13;
</code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="k">range</code><code class="w"> </code><code class="nx">b</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO6-1" id="co_how_go_uses_memory_resource_CO6-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
   </code><code class="nx">b</code><code class="p">[</code><code class="nx">i</code><code class="p">]</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="mi">1</code><code class="w">&#13;
</code><code class="p">}</code><code class="w">&#13;
</code><code class="w">&#13;
</code><code class="nx">b</code><code class="p">[</code><code class="mi">5000</code><code class="p">]</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="mi">1</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO6-2" id="co_how_go_uses_memory_resource_CO6-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="nx">b</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="kc">nil</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO6-3" id="co_how_go_uses_memory_resource_CO6-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
</code><code class="nx">runtime</code><code class="p">.</code><code class="nx">GC</code><code class="p">(</code><code class="p">)</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO6-4" id="co_how_go_uses_memory_resource_CO6-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
&#13;
</code><code class="c1">// Let's allocate another one, this time 300 MB!</code><code class="w">&#13;
</code><code class="nx">b</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="nb">make</code><code class="p">(</code><code class="p">[</code><code class="p">]</code><code class="kt">byte</code><code class="p">,</code><code class="w"> </code><code class="mi">300</code><code class="o">*</code><code class="mi">1024</code><code class="o">*</code><code class="mi">1024</code><code class="p">)</code><code class="w">&#13;
</code><code class="k">for</code><code class="w"> </code><code class="nx">i</code><code class="w"> </code><code class="o">:=</code><code class="w"> </code><code class="k">range</code><code class="w"> </code><code class="nx">b</code><code class="w"> </code><code class="p">{</code><code class="w"> </code><a class="co" href="#callout_how_go_uses_memory_resource_CO6-5" id="co_how_go_uses_memory_resource_CO6-5"><img alt="5" src="assets/5.png"/></a><code class="w">&#13;
   </code><code class="nx">b</code><code class="p">[</code><code class="nx">i</code><code class="p">]</code><code class="w"> </code><code class="p">=</code><code class="w"> </code><code class="mi">2</code><code class="w">&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO6-1" id="callout_how_go_uses_memory_resource_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>As we discussed in <a data-type="xref" href="#code-mem-alloc-slice">Example 5-5</a>, the statistics after allocating a large slice and accessing all elements might look as follows:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Slice is allocated in three memory mappings with the corresponding virtual memory size (VSS) numbers: 2 MB, 598 MB, and 4 MB.</p>&#13;
</li>&#13;
<li>&#13;
<p>The RSS for three memory mappings: 1.5 MB, 598 MB, and 1.2 MB.</p>&#13;
</li>&#13;
<li>&#13;
<p>Total RSS of the whole process shows 621.7 MB.</p>&#13;
</li>&#13;
<li>&#13;
<p>Go reports 600.16 MB of the heap size.</p>&#13;
</li>&#13;
</ul></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO6-2" id="callout_how_go_uses_memory_resource_CO6-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>After the last statement where data from <code>b</code> is accessed, even before <code>b = nil</code>, the <code>Mark</code> phase of GC would consider <code>b</code> as a “garbage” to clean. Yet, the GC has its own pace; thus, immediately after this statement, no memory will be released—memory statistics will be the same.</p></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO6-3" id="callout_how_go_uses_memory_resource_CO6-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>In typical cases when you no longer use the <code>b</code> value and the function scope ends, or you will replace <code>b</code> content with a pointer to a different object, there is no need for an explicit <code>b = nil</code> statement. The GC will know that the array pointed to by <code>b</code> is garbage. Yet sometimes, especially on long-living functions (e.g., a goroutine that performs background job items delivered by the Go channel), it is useful to set the variable to <code>nil</code> to make sure the next GC run will mark it for cleaning earlier.</p></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO6-4" id="callout_how_go_uses_memory_resource_CO6-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>In our tests, let’s invoke the GC manually to see what happens. After this statement, the statistics will look as follows:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>All three memory mappings still exist, with the same VSS values. This proves what we mentioned about the Go Allocator only advising on memory mappings, not removing those straightaway!</p>&#13;
</li>&#13;
<li>&#13;
<p>The RSS for three memory mappings: 1.5 MB, 0 (RSS released), and 60 KB.</p>&#13;
</li>&#13;
<li>&#13;
<p>Total RSS of the whole process shows 21 MB (back to the initial number).</p>&#13;
</li>&#13;
<li>&#13;
<p>Go reports 159 KB of the heap size.</p>&#13;
</li>&#13;
</ul></dd>&#13;
<dt><a class="co" href="#co_how_go_uses_memory_resource_CO6-5" id="callout_how_go_uses_memory_resource_CO6-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Let’s allocate another twice smaller slice. The following memory statistics prove the theory that Go will try to reuse previous memory mappings!</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Same three memory mappings still exist, with the same VSS values.</p>&#13;
</li>&#13;
<li>&#13;
<p>The RSS for three memory mappings: 1.5 MB, 300 MB, and 60 KB.</p>&#13;
</li>&#13;
<li>&#13;
<p>Total RSS of the whole process shows 321 MB.</p>&#13;
</li>&#13;
<li>&#13;
<p>Go reports 300.1 KB of the heap size.</p>&#13;
</li>&#13;
</ul></dd>&#13;
</dl></div>&#13;
&#13;
<p>As we mentioned earlier, the beauty of GC is that it simplifies programmer life thanks to carefree allocations, memory safety, and solid efficiency for most applications. Unfortunately, it also makes our life a bit harder when our program violates our efficiency expectations, and the reason is not what you might think. The main problem with the Go Allocator and GC pair is that they hide the root cause of our memory efficiency problems—in almost all cases, our code allocates too much<a data-primary="Flake, Halvar, on GC" data-type="indexterm" id="idm45606832789056"/> &#13;
<span class="keep-together">memory!</span></p>&#13;
<blockquote><p>Think of a garbage collector like a Roomba: Just because you have one does not mean you tell your children not to drop arbitrary pieces of garbage onto the floor.</p><p data-type="attribution">Halvar Flake, <a href="https://oreil.ly/ukXDV">Twitter</a></p></blockquote>&#13;
&#13;
<p>Let’s explore the potential symptoms we might notice in Go when we are not careful with the number and type of the allocations:</p>&#13;
<dl><dt>CPU overhead</dt><dd><p>First and foremost, the GC must go through all the objects stored on the heap to tell which ones are in use. This can use a significant portion of the CPU resource, especially if there are many objects in heap.<sup><a data-type="noteref" href="ch05.html#idm45606832784288" id="idm45606832784288-marker">37</a></sup></p><p>This is especially visible if the objects stored on the heap are rich in pointer types, which forces the GC to traverse them to check if they don’t point to an object that was not yet marked as “in use.” Given the limited CPU resources in our computers, the more work we have to do for the GC, the less work we can perform toward the core program functionality, which translates to higher program latency.</p>&#13;
<blockquote><p>In platforms with garbage collection, memory pressure naturally translates into increased CPU consumption.</p><p data-type="attribution">Google Teams, <a href="https://oreil.ly/PhZaD"><i>Site Reliability Engineering</i></a></p></blockquote></dd><dt>Additional increase in program latency</dt><dd><p>CPU time spent on GC is one thing, but there is more. First, the STW event performed twice slows down all goroutines. This is because the GC must stop all goroutines and inject (and then remove) a write barrier. It also prevents some goroutines that have to store some data in memory from doing any further work for the moment of GC marking.</p><p>There is also a second, often missed effect. The GC collection runs are destructive to the hierarchical cache system efficiency.</p><blockquote><p>For your program to be fast, you want everything you’re doing to be in the cache. ... There are technical and physical reasons in the silicon why allocating memory, throwing it away and GC cleaning that for you, is going to not only slow your program down, because GC is doing its work, but it slows the rest of your program down, because it kicked everything out of [the CPU] cache.</p><p data-type="attribution">Bryan Boreham, <a href="https://oreil.ly/cDw6c">“Make Your Go Go Faster!”</a></p></blockquote></dd><dt>Memory overhead</dt>&#13;
<dd><p>Since Go 1.19, there has been a way to set a soft memory limit for the GC. This still means that we have to often implement on our side checks against unbounded allocations (e.g., rejecting reading too-large HTTP body requests), but at least the GC is more prompt if you need to avoid that overhead.</p><p>Still, the collection phase is eventual. This means we might be unable to release some memory blocks before new allocations come in. Changing the <code>GOGC</code> option to run GC less often only amplifies the problem but might be a good trade-off if you optimize for the CPU resource and have spare RAM on your machines.</p><p>Additionally, in extreme cases, our program might even leak memory if <a href="https://oreil.ly/4giW6">the GC is not fast enough to deal with all new allocations</a>!</p></dd></dl>&#13;
&#13;
<p>The GC can sometimes have surprising effects on our program efficiency. Hopefully, after this section, you will be able to notice when you are affected. You will also be able to notice the GC bottlenecks with the observability tools explained in <a data-type="xref" href="ch09.html#ch-observability3">Chapter 9</a>.</p>&#13;
<div data-type="tip"><h1>The Solution to Most Memory Efficiency Issues</h1>&#13;
<p>Produce less garbage!</p>&#13;
&#13;
<p>It’s easy to overallocate memory in Go. This is why the best way to solve GC bottleneck or other memory efficiency issues is to allocate less. I will introduce <a data-type="xref" href="ch11.html#ch-hw-rrr">“The Three Rs Optimization Method”</a>, which goes through different optimizations that help with those efficiency problems<a data-startref="ix_ch05-asciidoc33" data-type="indexterm" id="idm45606832769232"/><a data-startref="ix_ch05-asciidoc32" data-type="indexterm" id="idm45606832768560"/><a data-startref="ix_ch05-asciidoc31" data-type="indexterm" id="idm45606832767888"/>.<a data-startref="ix_ch05-asciidoc22" data-type="indexterm" id="idm45606832767088"/><a data-startref="ix_ch05-asciidoc21" data-type="indexterm" id="idm45606832766384"/><a data-startref="ix_ch05-asciidoc20" data-type="indexterm" id="idm45606832765712"/><a data-startref="ix_ch05-asciidoc19" data-type="indexterm" id="idm45606832765040"/></p>&#13;
</div>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45606833542048">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>It was a long chapter, but you made it! Unfortunately, memory resource is one of the hardest to explain and master. Probably that’s why there are so many opportunities to reduce the size or number of our Go program’s allocations.</p>&#13;
&#13;
<p>You learned the long, multilayer path between our code that needs to allocate bits on memory and bits landing on the DRAM chip. You learned about many memory trade-offs, behaviors, and consequences on the OS level. Finally, you now know how Go uses those mechanisms and why memory allocations in Go are so transparent.</p>&#13;
&#13;
<p>Perhaps you can already figure out the root causes of why <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> was using 30.5 MB of the heap for every single operation when the input file was 3 MB large. In <a data-type="xref" href="ch10.html#ch-opt-mem-example">“Optimizing Memory Usage”</a>, I will propose the algorithm and code improvements to <a data-type="xref" href="ch04.html#code-sum">Example 4-1</a> that allow it to use memory in numbers that are a fraction of the input file size, while also improving the latency.</p>&#13;
&#13;
<p>It is important to note that this space is evolving. Go compiler, Go garbage collector, and Go Allocator are constantly being improved, changed, and scaled for the needs of Go users. Yet most of the incoming changes will likely be only iterations of what we have now in Go.<a data-startref="ix_ch05-asciidoc1" data-type="indexterm" id="idm45606832758560"/><a data-startref="ix_ch05-asciidoc0" data-type="indexterm" id="idm45606832757856"/></p>&#13;
&#13;
<p>Ahead of us are Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.html#ch-observability">6</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch07.html#ch-observability2">7</a>, which I consider two of the most crucial chapters in the book. I have already mentioned many tools I used to explain the main concepts in past chapters: metrics, benchmarking, and profiling. It’s time to learn them in detail!</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45606834619920"><sup><a href="ch05.html#idm45606834619920-marker">1</a></sup> In this book when I say “memory,” I mean RAM and vice versa. Other mediums offer “memorizing” data in computer architecture (e.g., L-caches), but we tend to treat RAM as the “main” memory resource.</p><p data-type="footnote" id="idm45606834615344"><sup><a href="ch05.html#idm45606834615344-marker">2</a></sup> Not only because of physical limitations like not enough chip pins, space, and energy for transistors, but also because managing large memory poses huge overhead as we will learn in <a data-type="xref" href="#ch-hw-memory-os">“OS Memory Management”</a>.</p><p data-type="footnote" id="idm45606834611488"><sup><a href="ch05.html#idm45606834611488-marker">3</a></sup> In some way, RAM volatility can sometimes be treated as a feature, not a bug! Have you ever wondered why restarting a computer or process often fixes your problem? The memory volatility forces programmers to implement robust initialization techniques that rebuild the state from backup mediums, enhancing reliability and mitigating potential program bugs. In extreme cases, <a href="https://oreil.ly/DAbDs">crash-only software</a> with the restart is the primary way of failure handling.</p><p data-type="footnote" id="idm45606834596960"><sup><a href="ch05.html#idm45606834596960-marker">4</a></sup> We can resolve that problem by simply adding more memory to the system or switching to the server (or virtual machine) with more memory resource. That might be a solid solution if we are willing to pay additionally if it’s not a memory leak and if such a resource can be increased (e.g., the cloud has virtual machines with more memory). Yet I suggest investigating your program memory usage, especially if you continuously have to expand the system memory. Then there might be easy wins, thanks to trivially wasted space we could optimize.</p><p data-type="footnote" id="idm45606834576192"><sup><a href="ch05.html#idm45606834576192-marker">5</a></sup> Nowadays, popular encodings like UTF-8 can dynamically use from one up to four bytes of memory per single character.</p><p data-type="footnote" id="idm45606834567216"><sup><a href="ch05.html#idm45606834567216-marker">6</a></sup> By just doubling the “pointer” size, we moved the limit to how many elements we can address to extreme sizes. We could even estimate that 64-bit is enough to <a href="https://oreil.ly/By1J3">address all grains of sand from all beaches on Earth</a>!</p><p data-type="footnote" id="idm45606834540672"><sup><a href="ch05.html#idm45606834540672-marker">7</a></sup> I introduced the <em>process</em> and <em>thread</em> terms in <a data-type="xref" href="ch04.html#ch-hw-os-scheduler">“Operating System Scheduler”</a>.</p><p data-type="footnote" id="idm45606834529664"><sup><a href="ch05.html#idm45606834529664-marker">8</a></sup> Many Common Vulnerabilities and Exposures (CVE) issues exist due to various bugs that allow <a href="https://oreil.ly/iSbqk">out-of-bounds memory access</a>.</p><p data-type="footnote" id="idm45606834527872"><sup><a href="ch05.html#idm45606834527872-marker">9</a></sup> It might be less intuitive, but the malicious process can perform a DoS if access to another process memory is not restricted. For example, by setting counters to incorrect values or breaking loop invariants, the victim program might error out or exhaust machine resources.</p><p data-type="footnote" id="idm45606834509584"><sup><a href="ch05.html#idm45606834509584-marker">10</a></sup> In the past, <a href="https://oreil.ly/8BFmb">segmentation</a> was used to implement virtual memory. This has proven to have less versatility, especially the inability to move this space around for defragmentation (better packing of memory). Still, even with paging, segmentation is applied to virtual memory by the process itself (with underlying paging). Plus, the kernel sometimes still uses nonpaged segmentation for its part of critical kernel  <span class="keep-together">memory.</span></p><p data-type="footnote" id="idm45606834504144"><sup><a href="ch05.html#idm45606834504144-marker">11</a></sup> You can check the current page size on the Linux system using the <code>getconf PAGESIZE</code> command.</p><p data-type="footnote" id="idm45606834502960"><sup><a href="ch05.html#idm45606834502960-marker">12</a></sup> For example, typically, Intel CPUs are capable of hardware-supported <a href="https://oreil.ly/mxlry">4 KB, 2 MB, or 1 GB pages</a>.</p><p data-type="footnote" id="idm45606834497952"><sup><a href="ch05.html#idm45606834497952-marker">13</a></sup> Even naive and conservative calculations indicate around <a href="https://oreil.ly/iklRd">24% of total memory is wasted for 2 MB pages</a>.</p><p data-type="footnote" id="idm45606834494864"><sup><a href="ch05.html#idm45606834494864-marker">14</a></sup> We won’t discuss the implementation of page tables since it’s pretty complex and not something Go developers have to worry about. Yet this topic is quite interesting as the trivial implementation of paging would have a massive overhead in memory usage (what’s the point of memory management that would take the majority of memory space it manages?). You can learn more <a href="https://oreil.ly/jU9Is">here</a>.</p><p data-type="footnote" id="idm45606834472688"><sup><a href="ch05.html#idm45606834472688-marker">15</a></sup> There is also an option to <a href="https://oreil.ly/h82uS">disable an overcommitment mechanism</a> on Linux. When disabled, the virtual memory size (VSS) is not allowed to be bigger than the physical memory used by the process (RSS). You might want to do this so the process will have generally faster memory accesses, but the waste of memory is enormous. As a result, I have never seen such an option used in practice.</p><p data-type="footnote" id="idm45606834159392"><sup><a href="ch05.html#idm45606834159392-marker">16</a></sup> <code>MAP_SHARED</code> means that any other process can reuse the same physical memory page if it accesses the same file. This is harmless if the mapped file does not change over time, but it has more complex nuances for mapping modifiable content.</p><p data-type="footnote" id="idm45606834158000"><sup><a href="ch05.html#idm45606834158000-marker">17</a></sup> A full list of options can be found in the <a href="https://oreil.ly/m5n7A"><code>mmap</code> documentation</a>.</p><p data-type="footnote" id="idm45606834147584"><sup><a href="ch05.html#idm45606834147584-marker">18</a></sup> <code>SIGSEV</code> means a segmentation fault. This tells us that the process wants to access an invalid memory address.</p><p data-type="footnote" id="idm45606833899392"><sup><a href="ch05.html#idm45606833899392-marker">19</a></sup> On Linux, you can find this information by doing <code>ps -ax --format=pid,rss,vsz | grep $PID</code>, where <code>$PID</code> is process ID.</p><p data-type="footnote" id="idm45606833705184"><sup><a href="ch05.html#idm45606833705184-marker">20</a></sup> How do I know? We can have exact statistics for each memory mapping process we use on Linux thanks to the <code>/proc/<em>&lt;PID&gt;</em>/smaps</code> file.</p><p data-type="footnote" id="idm45606833635152"><sup><a href="ch05.html#idm45606833635152-marker">21</a></sup> There are many reasons why accessing nearby bytes might not need allocating more pages on RAM  in the memory-mapped situation. For example, the cache hierarchy (discussed in <a data-type="xref" href="ch04.html#ch-hw-lcache">“Hierachical Cache System”</a>), the OS, and compiler deciding to pull more at once, or such a page being already a shared or private page because of previous accesses.</p><p data-type="footnote" id="idm45606833630880"><sup><a href="ch05.html#idm45606833630880-marker">22</a></sup> Note that physical frames for this file can still be allocated on physical memory by the OS (just not accounted for our process). This is called <code>page cache</code> and can be useful if any process tries to memorize the same file. Page cache is stored as best effort in the memory that would otherwise not be used. It can be released when the system is under high memory pressure or manually by the administrator, e.g., with <code>sysctl -w vm.drop_caches=1</code>.</p><p data-type="footnote" id="idm45606833609392"><sup><a href="ch05.html#idm45606833609392-marker">23</a></sup> Swapping is usually turned off by default on most machines.</p><p data-type="footnote" id="idm45606833605136"><sup><a href="ch05.html#idm45606833605136-marker">24</a></sup> <a href="https://oreil.ly/AFDh0">“Teaching the OOM killer”</a> explains some problems in choosing what process to kill first. The lesson here is that the global OOM killer is often hard to <a href="https://oreil.ly/4rPzk">predict</a>.</p><p data-type="footnote" id="idm45606833602304"><sup><a href="ch05.html#idm45606833602304-marker">25</a></sup> Exact implementation of memory controller can be found <a href="https://oreil.ly/Ken3G">here</a>.</p><p data-type="footnote" id="idm45606833533232"><sup><a href="ch05.html#idm45606833533232-marker">26</a></sup> Remember, whatever type or amount of virtual memory the OS is giving to the process, it uses the memory mapping technique. <code>sbrk</code> allows simpler resizing of the virtual memory section typically covered by the heap. However, it behaves like any other <code>mmap</code> using anonymous pages.</p><p data-type="footnote" id="idm45606833508688"><sup><a href="ch05.html#idm45606833508688-marker">27</a></sup> Of course no one blocks anyone from implementing external garbage collection on top of those mechanisms in C and C++.</p><p data-type="footnote" id="idm45606833506480"><sup><a href="ch05.html#idm45606833506480-marker">28</a></sup> It’s hard that the ownership model in Rust requires the programmer to be hyperaware of every memory allocation and what part owns it. Despite that, I am a huge fan of the Rust ownership model if we could scope this memory management only to a certain part of our code. I believe it would be beneficial to bring some ownership pattern to Go, where a small amount of code could use that, whereas the rest would use GC. Wish list for someday? :)</p><p data-type="footnote" id="idm45606833447648"><sup><a href="ch05.html#idm45606833447648-marker">29</a></sup> You can reveal the box size with the <a href="https://oreil.ly/QtpSf"><code>unsafe.Sizeof</code></a> function.</p><p data-type="footnote" id="idm45606833426272"><sup><a href="ch05.html#idm45606833426272-marker">30</a></sup> See the handy <a href="https://oreil.ly/9unR4"><code>reflect.SliceHeader</code></a> struct that represents a slice.</p><p data-type="footnote" id="idm45606833418784"><sup><a href="ch05.html#idm45606833418784-marker">31</a></sup> Technically speaking, the type <code>map</code> variable is a pointer to the hashmap. However, to avoid always typing <code>*map</code>, the Go team decided to <a href="https://oreil.ly/mfwDa">hide that detail</a>.</p><p data-type="footnote" id="idm45606833407008"><sup><a href="ch05.html#idm45606833407008-marker">32</a></sup> We won’t cover <a href="https://oreil.ly/1gx5O">struct padding</a> in this edition. There is also an amazing utility that helps you to notice the waste <a href="https://oreil.ly/WtYFZ">introduced by struct misalignment</a>.</p><p data-type="footnote" id="idm45606833200480"><sup><a href="ch05.html#idm45606833200480-marker">33</a></sup> This is one of the reasons why in Go, every new structure has defined zero value or nil at the start, instead of random value.</p><p data-type="footnote" id="idm45606833118576"><sup><a href="ch05.html#idm45606833118576-marker">34</a></sup> We know that because <code>go build -gcflags="-m=1" slice.go</code> outputs the <code>./slice.go:11:11: make([]byte, size) escapes to heap</code> line.</p><p data-type="footnote" id="idm45606833047648"><sup><a href="ch05.html#idm45606833047648-marker">35</a></sup> This behavior was often leveraged by more advanced memory ballasting, which generally is less needed after Go 1.19 introduced the memory soft limit discussed in <a data-type="xref" href="#ch-hw-garbage">“Garbage Collection”</a>.</p><p data-type="footnote" id="idm45606832989440"><sup><a href="ch05.html#idm45606832989440-marker">36</a></sup> It’s also possible to change Go memory release strategy by changing the <code>GODEBUG</code> <a href="https://oreil.ly/ynNXr">environment variable</a>. For example, we can set <code>GODEBUG=madvdontneed=0</code>, so <code>MADV_FREE</code> will be used instead to notify the OS about unneeded memory space. The difference between <code>MADV_DONTNEED</code> and <code>MADV_FREE</code> is precisely around the point mentioned in the Linux Community quote. For <code>MADV_FREE</code>, memory release is even faster for Go programs, but the resident set size (RSS) metric of the calling process might not be immediately reduced until the OS reclaims that space. This has proven to cause a massive problem on some systems (e.g., lightly virtualized systems like Kubernetes) that rely on RSS to manage the processes. This happened in 2019 when Go defaulted to <code>MADV_FREE</code> for a couple of versions. More on that is explained in my <a href="https://oreil.ly/UYXJy">blog post</a>.</p><p data-type="footnote" id="idm45606832784288"><sup><a href="ch05.html#idm45606832784288-marker">37</a></sup> To be strict, Go <a href="https://oreil.ly/9rtOs">ensures that a maximum of 25% of the total CPU assigned for the process is used for the GC</a>. This is, however, not a silver-bullet solution. By reducing the maximum CPU time used, we simply use the same amount, just over longer periods.</p></div></div></section></body></html>