<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 7. Scalability"><div class="chapter" id="chapter_7">&#13;
<h1><span class="label">Chapter 7. </span>Scalability</h1>&#13;
&#13;
<blockquote>&#13;
<p>Some of the best programming is done on paper, really. Putting it into the computer is just a minor detail.<sup><a data-type="noteref" id="idm45983630129928-marker" href="ch07.xhtml#idm45983630129928">1</a></sup></p>&#13;
<p data-type="attribution">Max Kanat-Alexander, <cite>Code Simplicity: The Fundamentals of Software</cite></p>&#13;
</blockquote>&#13;
&#13;
<p>In the summer of 2016, I joined a small company that digitized the kind of forms and miscellaneous paperwork that state and local governments are known and loved for. The state of their core application was pretty typical of early-stage startups, so we got to work and, by that fall, had managed to containerize it, describe its infrastructure in code, and fully automate its deployment.</p>&#13;
&#13;
<p>One of our clients was a small coastal city in southeastern Virginia, so when Hurricane Matthew—the first Category 5 Atlantic hurricane in nearly a decade—was forecast to make landfall not far from there, the local officials dutifully declared a state of emergency and used our system to create the necessary paperwork for citizens to fill out. Then they posted it to social media, and half a million people all logged in at the same time.</p>&#13;
&#13;
<p>When the pager went off, the on-call checked the metrics and found that aggregated CPU for the servers was pegged at 100%, and that hundreds of thousands of requests were timing out.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="autoscaling" id="idm45983630125224"/>So, we added a zero to the desired server count, created a “to-do” task to implement autoscaling, and went back to our day. Within 24 hours, the rush had passed, so we scaled the servers in.</p>&#13;
&#13;
<p>What did we learn from this, other than the benefits of autoscaling?<sup><a data-type="noteref" id="idm45983630123800-marker" href="ch07.xhtml#idm45983630123800">2</a></sup></p>&#13;
&#13;
<p>First of all, it underscored the fact that without the ability to scale, our system would have certainly suffered extended downtime. But being able to add resources on demand meant that we could serve our users even under load far beyond what we had ever anticipated. As an added benefit, if any one server failed, its work could have been divided among the survivors.</p>&#13;
&#13;
<p>Second, having far more resources than necessary isn’t just wasteful, it’s expensive. The ability to scale our instances back in when demand ebbed meant that we were only paying for the resources that we needed. A major plus for a startup on a budget.</p>&#13;
&#13;
<p>Unfortunately, because unscalable services can seem to function perfectly well under initial conditions, scalability isn’t always a consideration during service design. While this might be perfectly adequate in the short term, services that aren’t capable of growing much beyond their original expectations also have a limited lifetime value. What’s more, it’s often fiendishly difficult to refactor a service for scalability, so building with it in mind can save both time and money in the long run.</p>&#13;
&#13;
<p>First and foremost, this is meant to be a Go book, or at least more of a Go book than an infrastructure or architecture book. While we will discuss things like scalable architecture and messaging patterns, much of this chapter will focus on demonstrating how Go can be used to produce services that lean on the other (non-infrastructure) part of the scalability equation: efficiency.<sup><a data-type="noteref" id="idm45983630119912-marker" href="ch07.xhtml#idm45983630119912">3</a></sup></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect1" data-pdf-bookmark="What Is Scalability?"><div class="sect1" id="idm45983630118088">&#13;
<h1>What Is Scalability?</h1>&#13;
&#13;
<p>You may recall that concept of scalability was first introduced way back in <a data-type="xref" href="ch01.xhtml#chapter_1">Chapter 1</a>, where it was defined as the ability of a system to continue to provide correct service in the face of significant changes in demand. By this definition, a system can be considered to be scalable if it doesn’t need to be redesigned to perform its intended function during steep increases in load.</p>&#13;
&#13;
<p>Note that this definition<sup><a data-type="noteref" id="idm45983630115000-marker" href="ch07.xhtml#idm45983630115000">4</a></sup> doesn’t actually say anything at all about adding physical resources. Rather, it calls out a system’s ability to handle large swings in demand. The thing being “scaled” here is the magnitude of the demand. While adding resources is one perfectly acceptable means of achieving scalability, it isn’t exactly the same as being scalable. To make things just a little more confusing, the word “scaling” can also be applied to a system, in which case it <em>does</em> mean a change in the amount of dedicated resources.</p>&#13;
&#13;
<p>So how do we handle high demand without adding resources? As we’ll discuss in <a data-type="xref" href="#section_ch07_efficiency">“Scaling Postponed: Efficiency”</a>, systems built with <em>efficiency</em> in mind are inherently more scalable by virtue of their ability to gracefully absorb high levels of demand, without immediately having to resort to adding hardware in response to every dramatic swing in demand, and without having to massively over-provision “just in case.”<a data-type="indexterm" data-primary="scalability" data-secondary="definition of" data-startref="ch07_term1" id="idm45983630111240"/></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="Different Forms of Scaling"><div class="sect2" id="section_ch07_scalability">&#13;
<h2>Different Forms of Scaling</h2>&#13;
&#13;
<p><a data-type="indexterm" data-primary="scalability" data-secondary="forms of" id="ch07_term2"/>Unfortunately, even the most efficient of efficiency strategies has its limit, and eventually you’ll find yourself needing to scale your service to provide additional resources. There are two different ways that this can be done (see <a data-type="xref" href="#img_ch07_scaling">Figure 7-1</a>), each with its own associated pros and cons:</p>&#13;
<dl>&#13;
<dt><a data-type="indexterm" data-primary="vertical scaling" id="idm45983630105192"/>Vertical scaling</dt>&#13;
<dd>&#13;
<p>A system can be <em>vertically scaled</em> (or <em>scaled up</em>) by increasing its resource allocations. In a public cloud, an existing server can be vertically scaled fairly easily just by changing its instance size, but only until you run out of larger instance types (or money).</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="horizontally scaling" id="idm45983630101880"/>Horizontal scaling</dt>&#13;
<dd>&#13;
<p>A system can be <em>horizontally scaled</em> (or <em>scaled out</em>) by duplicating the system or service to limit the burden on any individual server. Systems using this strategy can typically scale to handle greater amounts of load, but as you’ll see in <a data-type="xref" href="#section_ch07_state_and_statelessness">“State and Statelessness”</a>, the presence of state can make this strategy difficult or impossible for some systems.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<figure><div id="img_ch07_scaling" class="figure">&#13;
<img src="Images/cngo_0701.png" alt="cngo 0701" width="867" height="387"/>&#13;
<h6><span class="label">Figure 7-1. </span>Vertical scaling can be an effective short-term solution; horizontal scaling is more technically challenging but may be a better long-term strategy</h6>&#13;
</div></figure>&#13;
&#13;
<p>These two terms are used to describe the most common way of thinking about scaling: taking an entire system, and just making <em>more</em> of it. There are a variety of other scaling strategies in use, however.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="functional partitioning" id="ch07_term3"/>Perhaps the most common of these is <em>functional partitioning</em>, which you’re no doubt already familiar with, if not by name. Functional partitioning involves decomposing complex systems into smaller functional units that can be independently optimized, managed, and scaled. You might recognize this as a generalization of a number of best practices ranging from basic program design to advanced distributed systems design.<a data-type="indexterm" data-primary="functional partitioning" data-startref="ch07_term3" id="idm45983630092712"/></p>&#13;
&#13;
<p>Another approach common in systems with large amounts of data—particularly <a data-type="indexterm" data-primary="patterns" data-secondary="sharding" id="idm45983630091128"/><a data-type="indexterm" data-primary="sharding" id="idm45983630090152"/><a data-type="indexterm" data-primary="shards" id="idm45983630089480"/>databases—is <em>sharding</em>. Systems that use this strategy distribute load by dividing their data into partitions called <em>shards</em>, each of which holds a specific subset of the larger dataset. A basic example of this is presented in <a data-type="xref" href="#section_ch07_sharding">“Minimizing locking with sharding”</a>.<a data-type="indexterm" data-primary="scalability" data-secondary="forms of" data-startref="ch07_term2" id="idm45983630086792"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect1" data-pdf-bookmark="The Four Common Bottlenecks"><div class="sect1" id="idm45983630085544">&#13;
<h1>The Four Common Bottlenecks</h1>&#13;
&#13;
<p><a data-type="indexterm" data-primary="bottlenecks" id="idm45983630084168"/><a data-type="indexterm" data-primary="scalability" data-secondary="bottlenecks in" id="ch07_term4"/>As the demands on a system increase, there will inevitably come a point at which one resource just isn’t able to keep pace, effectively stalling any further efforts to scale. That resource has become a <em>bottleneck</em>.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="vertical scaling" id="idm45983630081192"/>Returning the system to operable performance levels requires identifying and addressing the bottleneck. This can be done in the short-term by increasing the bottlenecked component—vertically scaling—by adding more memory or up-sizing the CPU, for instance. As you might recall from the discussion in <a data-type="xref" href="#section_ch07_scalability">“Different Forms of Scaling”</a>, this approach isn’t always possible (or cost-effective), and it can never be relied upon forever.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="horizontally scaling" id="idm45983630078856"/>However, it’s often possible to address a bottleneck by enhancing or reducing the burden on the affected component by utilizing another resource that the system still has in abundance. A database might avoid disk I/O bottlenecking by caching data in RAM; conversely a memory-hungry service could page data to disk. Horizontally scaling doesn’t make a system immune: adding more instances can mean more communication overhead, which puts additional strain on the network. Even highly-concurrent systems can become victims of their own inner workings as the demand on them increases, and phenomena like lock contention come into play. Using resources effectively often means making tradeoffs.</p>&#13;
&#13;
<p>Of course, fixing a bottleneck requires that you first identify the constrained component, and while there are many different resources that can emerge as targets for scaling efforts—whether by actually scaling the resource or by using it more efficiently—such efforts tend to focus on just four resources:</p>&#13;
<dl>&#13;
<dt><a data-type="indexterm" data-primary="CPU" id="idm45983630075784"/>CPU</dt>&#13;
<dd>&#13;
<p>The number of operations per unit of time that can be performed by a system’s central processor and a common bottleneck for many systems. Scaling strategies for CPU include caching the results of expensive deterministic operations (at the expense of memory), or simply increasing the size or number of processors (at the expense of network I/O if scaling out).</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="memory" id="idm45983630073416"/>Memory</dt>&#13;
<dd>&#13;
<p>The amount of data that can be stored in main memory. While today’s systems can store incredible amounts of data on the order of tens or hundreds of gigabytes, even this can fall short, particularly for data-intensive systems that lean on memory to circumvent disk I/O speed limits. Scaling strategies include offloading data from memory to disk (at the expense of disk I/O) or an external dedicated cache (at the expense of network I/O), or simply increasing the amount of available memory.</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="disk I/O" id="idm45983630070920"/>Disk I/O</dt>&#13;
<dd>&#13;
<p>The speed at which data can be read from and written to a hard disk or other persistent storage medium. Disk I/O is a common bottleneck on highly parallel systems that read and write heavily to disk, such as databases. Scaling strategies include caching data in RAM (at the expense of memory) or using an external dedicated cache (at the expense of network I/O).</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="network I/O" id="idm45983630068552"/>Network I/O</dt>&#13;
<dd>&#13;
<p>The speed at which data can be sent across a network, either from a particular point or in aggregate. Network I/O translates directly into <em>how much</em> data the network can transmit per unit of time. Scaling strategies for network I/O are often limited,<sup><a data-type="noteref" id="idm45983630066296-marker" href="ch07.xhtml#idm45983630066296">5</a></sup> but network I/O is particularly amenable to various optimization strategies that we’ll discuss shortly.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>As the demand on a system increases, it’ll almost certainly find itself bottlenecked by one of these, and while there are efficiency strategies that can be applied, those tend to come at the expense of one or more other resources, so you’ll eventually find your system being bottlenecked <em>again</em> by another resource.<a data-type="indexterm" data-primary="scalability" data-secondary="bottlenecks in" data-startref="ch07_term4" id="idm45983630064088"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect1" data-pdf-bookmark="State and Statelessness"><div class="sect1" id="section_ch07_state_and_statelessness">&#13;
<h1>State and Statelessness</h1>&#13;
&#13;
<p><a data-type="indexterm" data-primary="scalability" data-secondary="states of" id="ch07_term5"/>We briefly touched on statelessness in <a data-type="xref" href="ch05.xhtml#sidebar_ch05_statelessness">“Application State Versus Resource State”</a>, where we described application state—server-side data about the application or how it’s being used by a client—as something to be avoided if at all possible. But this time, let’s spend a little more time discussing what state is, why it can be problematic, and what we can do about it.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="state" id="idm45983630058344"/>It turns out that “state” is strangely difficult to define, so I’ll do my best on my own. For the purposes of this book I’ll define state as the set of an application’s variables which, if changed, affect the behavior of the application.<sup><a data-type="noteref" id="idm45983630057256-marker" href="ch07.xhtml#idm45983630057256">6</a></sup></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="Application State Versus Resource State"><div class="sect2" id="idm45983630056488">&#13;
<h2>Application State Versus Resource State</h2>&#13;
&#13;
<p>Most applications have some form of state, but not all state is created equal. It comes in two kinds, one of which is far less desirable than the other.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="application state" id="idm45983630054520"/><a data-type="indexterm" data-primary="stateful application" data-see="application state" id="idm45983630053816"/>First, there’s <em>application state</em>, which exists any time an application needs to remember an event locally. Whenever somebody talks about a <em>stateful</em> application, they’re usually talking about an application that’s designed to use this kind of local state. “Local” is an operative word here.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="resource state" id="idm45983630051336"/><a data-type="indexterm" data-primary="stateless" id="ch07_term5_2"/>Second, there’s <em>resource state</em>, which is the same for every client and which has nothing to do with the actions of clients, like data stored in external data store or managed by configuration management. It’s misleading, but saying that an application is <em>stateless</em> doesn’t mean that it doesn’t have any data, just that it’s been designed in such a way that it’s free of any local persistent data. Its only state is resource state, often because all of its state is stored in some external data store.</p>&#13;
&#13;
<p>To illustrate the difference between the two, imagine an application that tracks client sessions, associating them with some application context. If users’ session data was maintained locally by the application, that would be considered “application state.” But if the data was stored in an external database, then it could be treated as a remote resource, and it would be “resource state.”</p>&#13;
&#13;
<p>Application state is something of the “anti-scalability.” Multiple instances of a stateful service will quickly find their individual states diverging due to different inputs being received by each replica. Server affinity provides a workaround to this specific condition by ensuring that each of a client’s requests are made to the same server, but this strategy poses a considerable data risk, since the failure of any single server is likely to result in a loss of data.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="Advantages of Statelessness"><div class="sect2" id="idm45983630046360">&#13;
<h2>Advantages of Statelessness</h2>&#13;
&#13;
<p>So far, we’ve discussed the differences between application state and resource state, and we’ve even suggested—without much evidence (yet)—that application state is bad. However, statelessness provides some very noticeable advantages:</p>&#13;
<dl>&#13;
<dt><a data-type="indexterm" data-primary="autoscaling" id="idm45983630044040"/>Scalability</dt>&#13;
<dd>&#13;
<p>The most visible and most often cited benefit is that stateless applications can handle each request or interaction independent of previous requests. This means that any service replica can handle any request, allowing applications to grow, shrink, or be restarted without losing data required to handle any in-flight sessions or requests. This is especially important when autoscaling your service, because the instances, nodes, or pods hosting the service can (and usually will) be created and destroyed unexpectedly.</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="durability" id="idm45983630041528"/>Durability</dt>&#13;
<dd>&#13;
<p>Data that lives in exactly one place (such as a single service replica) can (and, at some point, <em>will</em>) get lost when that replica goes away for any reason. Remember: everything in “the cloud” evaporates eventually.</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="simplicity" id="idm45983630038888"/>Simplicity</dt>&#13;
<dd>&#13;
<p>Without any application state, stateless services are freed from the need to… well… manage their state.<sup><a data-type="noteref" id="idm45983630037192-marker" href="ch07.xhtml#idm45983630037192">7</a></sup> Not being burdened with having to maintain service-side state synchronization, consistency, and recovery logic<sup><a data-type="noteref" id="idm45983630036376-marker" href="ch07.xhtml#idm45983630036376">8</a></sup> makes stateless APIs less complex, and therefore easier to design, build, and maintain.</p>&#13;
</dd>&#13;
<dt>Cacheability</dt>&#13;
<dd>&#13;
<p>APIs provided by stateless services are relatively easy to design for cacheability. If a service knows that the result of a particular request will always be the same, regardless of who’s making it or when, the result can be safely set aside for easy retrieval later, increasing efficiency and reducing response time.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>These might seem like four different things, but there’s overlap with respect to what they provide. Specifically, statelessness makes services both simpler and safer to build, deploy, and maintain.<a data-type="indexterm" data-primary="stateless" data-startref="ch07_term5_2" id="idm45983630033112"/><a data-type="indexterm" data-primary="scalability" data-secondary="states of" data-startref="ch07_term5" id="idm45983630032136"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect1" data-pdf-bookmark="Scaling Postponed: Efficiency"><div class="sect1" id="section_ch07_efficiency">&#13;
<h1>Scaling Postponed: Efficiency</h1>&#13;
&#13;
<p><a data-type="indexterm" data-primary="scalability" data-secondary="efficiency and" id="ch07_term6"/>In the context of cloud computing, we usually think of scalability in terms of the ability of a system to add network and computing resources. Often neglected, however, is the role of <em>efficiency</em> in scalability. Specifically, the ability for a system to handle changes in demand <em>without</em> having to add (or greatly over-provision) dedicated resources.</p>&#13;
&#13;
<p>While it can be argued that most people don’t care about program efficiency most of the time, this starts to become less true as demand on a service increases. If a language has a relatively high per-process concurrency overhead—often the case with dynamically typed languages—it will consume all available memory or compute resources much more quickly than a lighter-weight language, and consequently require resources and more scaling events to support the same demand.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="goroutine" id="idm45983630025192"/>This was a major consideration in the design of Go’s concurrency model, whose &#13;
<span class="keep-together">goroutines</span> aren’t threads at all but lightweight routines multiplexed onto multiple OS threads. Each costs little more than the allocation of stack space, allowing potentially millions of concurrently executing routines to be created.</p>&#13;
&#13;
<p>As such, in this section we’ll cover a selection of Go features and tooling that allow us to avoid common scaling problems, such as memory leaks and lock contention, and to identify and fix them when they do arise.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="Efficient Caching Using an LRU Cache"><div class="sect2" id="section_ch07_lru_cache">&#13;
<h2>Efficient Caching Using an LRU Cache</h2>&#13;
&#13;
<p><a data-type="indexterm" data-primary="caching" id="ch07_term7"/><a data-type="indexterm" data-primary="LRU cache" id="ch07_term8"/>Caching to memory is a very flexible efficiency strategy that can be used to relieve pressure on anything from CPU to disk I/O or network I/O, or even just to reduce latency associated with remote or otherwise slow-running operations.</p>&#13;
&#13;
<p>The concept of caching certainly <em>seems</em> straightforward. You have something you want to remember the value of—like the result of an expensive (but deterministic) calculation—and you put it in a map for later. Right?</p>&#13;
&#13;
<p>Well, you could do that, but you’ll soon start running into problems. What happens as the number of cores and goroutines increases? Since you didn’t consider concurrency, you’ll soon find your modifications stepping on one another, leading to some unpleasant results. Also, since we forgot to remove anything from our map, it’ll continue growing indefinitely until it consumes all of our memory.</p>&#13;
&#13;
<p>What we need is a cache that:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Supports concurrent read, write, and delete operations</p>&#13;
</li>&#13;
<li>&#13;
<p>Scales well as the number of cores and goroutines increase</p>&#13;
</li>&#13;
<li>&#13;
<p>Won’t grow without limit to consume all available memory</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Least Recently Used cache" data-see="LRE cache" id="idm45983630012408"/>One common solution to this dilemma is an LRU (Least Recently Used) cache: a particularly lovely data structure that tracks how recently each of its keys have been “used” (read or written). When a value is added to the cache such that it exceeds a predefined capacity, the cache is able to “evict” (delete) its least recently used value.</p>&#13;
&#13;
<p>A detailed discussion of how to implement an LRU cache is beyond the scope of this book, but I will say that it’s quite clever. As illustrated on <a data-type="xref" href="#img_ch07_lru_cache">Figure 7-2</a>, an LRU cache contains a doubly linked list (which actually contains the values), and a map that associates each key to a node in the linked list. Whenever a key is read or written, the appropriate node is moved to the bottom of the list, such that the least recently used node is always at the top.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="HashiCorp" id="idm45983630008920"/>There are a couple of Go LRU cache implementations available, though none in the core libraries (yet). Perhaps the most common can be found as part of the <a href="https://oreil.ly/Q5pzk">golang/groupcache</a> library. However, I prefer HashiCorp’s open source extension to <code>groupcache</code>, <a href="https://oreil.ly/25ESk"><code>hashicorp/golang-lru</code></a>, which is better documented and includes <code>sync.RWMutexes</code> for concurrency safety.</p>&#13;
&#13;
<figure><div id="img_ch07_lru_cache" class="figure">&#13;
<img src="Images/cngo_0702.png" alt="cngo 0702" width="846" height="350"/>&#13;
<h6><span class="label">Figure 7-2. </span>An LRU cache contains a map and a doubly linked list, which allows it to discard stale items when it exceeds its capacity</h6>&#13;
</div></figure>&#13;
&#13;
<p>HashiCorp’s library contains two construction functions, each of which returns a pointer of type <code>*Cache</code> and an <code>error</code>:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="c1">// New creates an LRU cache with the given capacity.</code>&#13;
<code class="kd">func</code> <code class="nx">New</code><code class="p">(</code><code class="nx">size</code> <code class="kt">int</code><code class="p">)</code> <code class="p">(</code><code class="o">*</code><code class="nx">Cache</code><code class="p">,</code> <code class="kt">error</code><code class="p">)</code>&#13;
&#13;
<code class="c1">// NewWithEvict creates an LRU cache with the given capacity, and also accepts</code>&#13;
<code class="c1">// an "eviction callback" function that's called when an eviction occurs.</code>&#13;
<code class="kd">func</code> <code class="nx">NewWithEvict</code><code class="p">(</code><code class="nx">size</code> <code class="kt">int</code><code class="p">,</code>&#13;
    <code class="nx">onEvicted</code> <code class="kd">func</code><code class="p">(</code><code class="nx">key</code> <code class="kd">interface</code><code class="p">{},</code> <code class="nx">value</code> <code class="kd">interface</code><code class="p">{}))</code> <code class="p">(</code><code class="o">*</code><code class="nx">Cache</code><code class="p">,</code> <code class="kt">error</code><code class="p">)</code></pre>&#13;
&#13;
<p>The <code>*Cache</code> struct has a number of attached methods, the most useful of which are as follows:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="c1">// Add adds a value to the cache and returns true if an eviction occurred.</code>&#13;
<code class="kd">func</code> <code class="p">(</code><code class="nx">c</code> <code class="o">*</code><code class="nx">Cache</code><code class="p">)</code> <code class="nx">Add</code><code class="p">(</code><code class="nx">key</code><code class="p">,</code> <code class="nx">value</code> <code class="kd">interface</code><code class="p">{})</code> <code class="p">(</code><code class="nx">evicted</code> <code class="kt">bool</code><code class="p">)</code>&#13;
&#13;
<code class="c1">// Check if a key is in the cache (without updating the recent-ness).</code>&#13;
<code class="kd">func</code> <code class="p">(</code><code class="nx">c</code> <code class="o">*</code><code class="nx">Cache</code><code class="p">)</code> <code class="nx">Contains</code><code class="p">(</code><code class="nx">key</code> <code class="kd">interface</code><code class="p">{})</code> <code class="kt">bool</code>&#13;
&#13;
<code class="c1">// Get looks up a key's value and returns (value, true) if it exists.</code>&#13;
<code class="c1">// If the value doesn't exist, it returns (nil, false).</code>&#13;
<code class="kd">func</code> <code class="p">(</code><code class="nx">c</code> <code class="o">*</code><code class="nx">Cache</code><code class="p">)</code> <code class="nx">Get</code><code class="p">(</code><code class="nx">key</code> <code class="kd">interface</code><code class="p">{})</code> <code class="p">(</code><code class="nx">value</code> <code class="kd">interface</code><code class="p">{},</code> <code class="nx">ok</code> <code class="kt">bool</code><code class="p">)</code>&#13;
&#13;
<code class="c1">// Len returns the number of items in the cache.</code>&#13;
<code class="kd">func</code> <code class="p">(</code><code class="nx">c</code> <code class="o">*</code><code class="nx">Cache</code><code class="p">)</code> <code class="nx">Len</code><code class="p">()</code> <code class="kt">int</code>&#13;
&#13;
<code class="c1">// Remove removes the provided key from the cache.</code>&#13;
<code class="kd">func</code> <code class="p">(</code><code class="nx">c</code> <code class="o">*</code><code class="nx">Cache</code><code class="p">)</code> <code class="nx">Remove</code><code class="p">(</code><code class="nx">key</code> <code class="kd">interface</code><code class="p">{})</code> <code class="p">(</code><code class="nx">present</code> <code class="kt">bool</code><code class="p">)</code></pre>&#13;
&#13;
<p>There are several other methods as well. Take a look at <a href="https://oreil.ly/ODcff">the GoDocs</a> for a complete list.</p>&#13;
&#13;
<p>In the following example, we create and use an LRU cache with a capacity of two. To better highlight evictions, we include a callback function that prints some output to <code>stdout</code> whenever an eviction occurs. Note that we’ve decided to initialize the <code>cache</code> variable in an <code>init</code> function, a special function that’s automatically called before the <code>main</code> function and after the variable declarations have evaluated their initializers:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="kn">package</code> <code class="nx">main</code>&#13;
&#13;
<code class="kn">import</code> <code class="p">(</code>&#13;
    <code class="s">"fmt"</code>&#13;
    <code class="nx">lru</code> <code class="s">"github.com/hashicorp/golang-lru"</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="kd">var</code> <code class="nx">cache</code> <code class="o">*</code><code class="nx">lru</code><code class="p">.</code><code class="nx">Cache</code>&#13;
&#13;
<code class="kd">func</code> <code class="nx">init</code><code class="p">()</code> <code class="p">{</code>&#13;
    <code class="nx">cache</code><code class="p">,</code> <code class="nx">_</code> <code class="p">=</code> <code class="nx">lru</code><code class="p">.</code><code class="nx">NewWithEvict</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code>&#13;
        <code class="kd">func</code><code class="p">(</code><code class="nx">key</code> <code class="kd">interface</code><code class="p">{},</code> <code class="nx">value</code> <code class="kd">interface</code><code class="p">{})</code> <code class="p">{</code>&#13;
            <code class="nx">fmt</code><code class="p">.</code><code class="nx">Printf</code><code class="p">(</code><code class="s">"Evicted: key=%v value=%v\n"</code><code class="p">,</code> <code class="nx">key</code><code class="p">,</code> <code class="nx">value</code><code class="p">)</code>&#13;
        <code class="p">},</code>&#13;
    <code class="p">)</code>&#13;
<code class="p">}</code>&#13;
&#13;
<code class="kd">func</code> <code class="nx">main</code><code class="p">()</code> <code class="p">{</code>&#13;
    <code class="nx">cache</code><code class="p">.</code><code class="nx">Add</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="s">"a"</code><code class="p">)</code>           <code class="c1">// adds 1</code>&#13;
    <code class="nx">cache</code><code class="p">.</code><code class="nx">Add</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="s">"b"</code><code class="p">)</code>           <code class="c1">// adds 2; cache is now at capacity</code>&#13;
&#13;
    <code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="nx">cache</code><code class="p">.</code><code class="nx">Get</code><code class="p">(</code><code class="mi">1</code><code class="p">))</code>   <code class="c1">// "a true"; 1 now most recently used</code>&#13;
&#13;
    <code class="nx">cache</code><code class="p">.</code><code class="nx">Add</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="s">"c"</code><code class="p">)</code>           <code class="c1">// adds 3, evicts key 2</code>&#13;
&#13;
    <code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="nx">cache</code><code class="p">.</code><code class="nx">Get</code><code class="p">(</code><code class="mi">2</code><code class="p">))</code>   <code class="c1">// "&lt;nil&gt; false" (not found)</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>In the preceding program, we create a <code>cache</code> with a capacity of two, which means that the addition of a third value will force the eviction of the least recently used value.</p>&#13;
&#13;
<p>After adding the values <code>{1:"a"}</code> and <code>{2:"b"}</code> to the cache, we call <code>cache.Get(1)</code>, which makes <code>{1:"a"}</code> more recently used than <code>{2:"b"}</code>. So, when we add <code>{3:"c"}</code> in the next step, <code>{2:"b"}</code> is evicted, so the next <code>cache.Get(2)</code> shouldn’t return a value.</p>&#13;
&#13;
<p>If we run this program we’ll be able to see this in action. We’ll expect the following output:</p>&#13;
&#13;
<pre data-type="programlisting">$ go run lru.go&#13;
a true&#13;
Evicted: key=2 value=b&#13;
&lt;nil&gt; false</pre>&#13;
&#13;
<p><a data-type="indexterm" data-primary="concurrency" data-secondary="levels of" id="idm45983629705768"/>The LRU cache is an excellent data structure to use as a global cache for most use cases, but it does have a limitation: at very high levels of concurrency—on the order of several million operations per second—it will start to experience some contention.</p>&#13;
&#13;
<p>Unfortunately, at the time of this writing, Go still doesn’t seem to have a <em>very</em> high throughput cache implementation.<sup><a data-type="noteref" id="idm45983629703592-marker" href="ch07.xhtml#idm45983629703592">9</a></sup><a data-type="indexterm" data-primary="caching" data-startref="ch07_term7" id="idm45983629701976"/><a data-type="indexterm" data-primary="LRU cache" data-startref="ch07_term8" id="idm45983629701032"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="Efficient Synchronization"><div class="sect2" id="section_ch07_efficient_synchronization">&#13;
<h2>Efficient Synchronization</h2>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Go" data-secondary="proverbs" id="idm45983629699000"/>A commonly repeated Go proverb is “don’t communicate by sharing memory; share memory by communicating.” In other words, channels are generally preferred over shared data structures.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="concurrency" data-secondary="primitives" data-seealso="channels, goroutine" id="idm45983629697432"/>This is a pretty powerful concept. After all, Go’s concurrency primitives—goroutines and channels—provide a powerful and expressive synchronization mechanism, such that a set of goroutines using channels to exchange references to data structures can often allow locks to be dispensed with altogether.</p>&#13;
&#13;
<p>(If you’re a bit fuzzy on the details of channels and goroutines, don’t stress. Take a moment to flip back to&#13;
<a data-type="xref" href="ch03.xhtml#section_ch03_goroutines">“Goroutines”</a>. It’s okay. I’ll wait.)</p>&#13;
&#13;
<p>That being said, Go <em>does</em> provide more traditional locking mechanisms by way of the <code>sync</code> package. But if channels are so great, why would we want to use something like a <code>sync.Mutex</code>, and when would we use it?</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="channels" data-secondary="features of" id="idm45983629692328"/><a data-type="indexterm" data-primary="mutex" id="idm45983629691352"/>Well, as it turns out, channels <em>are</em> spectacularly useful, but they’re not the solution to every problem. Channels shine when you’re working with many discrete values, and are the better choice for passing ownership of data, distributing units of work, or communicating asynchronous results. Mutexes, on the other hand, are ideal for synchronizing access to caches or other large stateful structures.</p>&#13;
&#13;
<p>At the end of the day, no tool solves every problem. Ultimately, the best option is to use whichever is most expressive and/or most simple.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect3" data-pdf-bookmark="Share memory by communicating"><div class="sect3" id="idm45983629689032">&#13;
<h3>Share memory by communicating</h3>&#13;
&#13;
<p>Threading is easy; locking is hard.</p>&#13;
&#13;
<p>In this section we’re going to use a classic example—originally presented in Andrew Gerrand’s classic <em>Go Blog</em> article “Share Memory By Communicating”<sup><a data-type="noteref" id="idm45983629686280-marker" href="ch07.xhtml#idm45983629686280">10</a></sup>—to demonstrate this truism and show how Go channels can make concurrency safer and easier to reason about.</p>&#13;
&#13;
<p>Imagine, if you will, a hypothetical program that polls a list of URLs by sending it a GET request and waiting for the response. The catch is that each request can spend quite a bit of time waiting for the service to respond: anywhere from milliseconds to seconds (or more), depending on the service. Exactly the kind of operation that can benefit from a bit of concurrency, isn’t it?</p>&#13;
&#13;
<p>In a traditional threading environment that depends on locking for synchronization you might structure its data something like the following:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="kd">type</code> <code class="nx">Resource</code> <code class="kd">struct</code> <code class="p">{</code>&#13;
    <code class="nx">url</code>        <code class="kt">string</code>&#13;
    <code class="nx">polling</code>    <code class="kt">bool</code>&#13;
    <code class="nx">lastPolled</code> <code class="kt">int64</code>&#13;
<code class="p">}</code>&#13;
&#13;
<code class="kd">type</code> <code class="nx">Resources</code> <code class="kd">struct</code> <code class="p">{</code>&#13;
    <code class="nx">data</code> <code class="p">[]</code><code class="o">*</code><code class="nx">Resource</code>&#13;
    <code class="nx">lock</code> <code class="o">*</code><code class="nx">sync</code><code class="p">.</code><code class="nx">Mutex</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>As you can see, instead of having a slice of URL strings, we have two structs—<code>Resource</code> and <code>Resources</code>—each of which is already saddled with a number of synchronization structures beyond the URL strings we really care about.</p>&#13;
&#13;
<p>To multithread the polling process in the traditional way, you might have a <code>Poller</code> function like the following running in multiple threads:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="kd">func</code> <code class="nx">Poller</code><code class="p">(</code><code class="nx">res</code> <code class="o">*</code><code class="nx">Resources</code><code class="p">)</code> <code class="p">{</code>&#13;
    <code class="k">for</code> <code class="p">{</code>&#13;
        <code class="c1">// Get the least recently polled Resource and mark it as being polled</code>&#13;
        <code class="nx">res</code><code class="p">.</code><code class="nx">lock</code><code class="p">.</code><code class="nx">Lock</code><code class="p">()</code>&#13;
&#13;
        <code class="kd">var</code> <code class="nx">r</code> <code class="o">*</code><code class="nx">Resource</code>&#13;
&#13;
        <code class="k">for</code> <code class="nx">_</code><code class="p">,</code> <code class="nx">v</code> <code class="o">:=</code> <code class="k">range</code> <code class="nx">res</code><code class="p">.</code><code class="nx">data</code> <code class="p">{</code>&#13;
            <code class="k">if</code> <code class="nx">v</code><code class="p">.</code><code class="nx">polling</code> <code class="p">{</code>&#13;
                <code class="k">continue</code>&#13;
            <code class="p">}</code>&#13;
            <code class="k">if</code> <code class="nx">r</code> <code class="o">==</code> <code class="kc">nil</code> <code class="o">||</code> <code class="nx">v</code><code class="p">.</code><code class="nx">lastPolled</code> <code class="p">&lt;</code> <code class="nx">r</code><code class="p">.</code><code class="nx">lastPolled</code> <code class="p">{</code>&#13;
                <code class="nx">r</code> <code class="p">=</code> <code class="nx">v</code>&#13;
            <code class="p">}</code>&#13;
        <code class="p">}</code>&#13;
&#13;
        <code class="k">if</code> <code class="nx">r</code> <code class="o">!=</code> <code class="kc">nil</code> <code class="p">{</code>&#13;
            <code class="nx">r</code><code class="p">.</code><code class="nx">polling</code> <code class="p">=</code> <code class="kc">true</code>&#13;
        <code class="p">}</code>&#13;
&#13;
        <code class="nx">res</code><code class="p">.</code><code class="nx">lock</code><code class="p">.</code><code class="nx">Unlock</code><code class="p">()</code>&#13;
&#13;
        <code class="k">if</code> <code class="nx">r</code> <code class="o">==</code> <code class="kc">nil</code> <code class="p">{</code>&#13;
            <code class="k">continue</code>&#13;
        <code class="p">}</code>&#13;
&#13;
        <code class="c1">// Poll the URL</code>&#13;
&#13;
        <code class="c1">// Update the Resource's polling and lastPolled</code>&#13;
        <code class="nx">res</code><code class="p">.</code><code class="nx">lock</code><code class="p">.</code><code class="nx">Lock</code><code class="p">()</code>&#13;
        <code class="nx">r</code><code class="p">.</code><code class="nx">polling</code> <code class="p">=</code> <code class="kc">false</code>&#13;
        <code class="nx">r</code><code class="p">.</code><code class="nx">lastPolled</code> <code class="p">=</code> <code class="nx">time</code><code class="p">.</code><code class="nx">Nanoseconds</code><code class="p">()</code>&#13;
        <code class="nx">res</code><code class="p">.</code><code class="nx">lock</code><code class="p">.</code><code class="nx">Unlock</code><code class="p">()</code>&#13;
    <code class="p">}</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>This does the job, but it has a lot of room for improvement. It’s about a page long, hard to read, hard to reason about, and doesn’t even include the URL polling logic or gracefully handle exhaustion of the <code>Resources</code> pool.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Go" data-secondary="channels" id="idm45983629653592"/>Now let’s take a look at the same functionality implemented using Go channels. In this example, <code>Resource</code> has been reduced to its essential component (the URL string), and <code>Poller</code> is a function that receives <code>Resource</code> values from an input channel, and sends them to an output channel when they’re done:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="kd">type</code> <code class="nx">Resource</code> <code class="kt">string</code>&#13;
&#13;
<code class="kd">func</code> <code class="nx">Poller</code><code class="p">(</code><code class="nx">in</code><code class="p">,</code> <code class="nx">out</code> <code class="kd">chan</code> <code class="o">*</code><code class="nx">Resource</code><code class="p">)</code> <code class="p">{</code>&#13;
    <code class="k">for</code> <code class="nx">r</code> <code class="o">:=</code> <code class="k">range</code> <code class="nx">in</code> <code class="p">{</code>&#13;
        <code class="c1">// Poll the URL</code>&#13;
&#13;
        <code class="c1">// Send the processed Resource to out</code>&#13;
        <code class="nx">out</code> <code class="o">&lt;-</code> <code class="nx">r</code>&#13;
    <code class="p">}</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>It’s so…simple. We’ve completely shed the clockwork locking logic in <code>Poller</code>, and our <code>Resource</code> data structure no longer contains bookkeeping data. In fact, all that’s left are the important parts.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="goroutine" id="idm45983629405848"/>But what if we wanted more than one <code>Poller</code> process? Isn’t that what we were trying to do in the first place? The answer is, once again, gloriously simple: goroutines. Take a look at the following:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="k">for</code> <code class="nx">i</code> <code class="o">:=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="p">&lt;</code> <code class="nx">numPollers</code><code class="p">;</code> <code class="nx">i</code><code class="o">++</code> <code class="p">{</code>&#13;
    <code class="k">go</code> <code class="nx">Poller</code><code class="p">(</code><code class="nx">in</code><code class="p">,</code> <code class="nx">out</code><code class="p">)</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>By executing <code>numPollers</code> goroutines, we’re creating <code>numPollers</code> concurrent processes, each reading from and writing to the same channels.</p>&#13;
&#13;
<p>A lot<a contenteditable="false" data-primary="Codewalk &quot;Share Memory by Communicating&quot;" data-type="indexterm" id="idm45983629392744"/> has been omitted from the previous examples to highlight the relevant bits. For a walkthrough of a complete, idiomatic Go program that uses these ideas, see the <a href="https://oreil.ly/HF1Ay">“Share Memory By Communicating”</a> Codewalk.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect3" data-pdf-bookmark="Reduce blocking with buffered channels"><div class="sect3" id="idm45983629688408">&#13;
<h3>Reduce blocking with buffered channels</h3>&#13;
&#13;
<p><a data-type="indexterm" data-primary="channels" data-secondary="buffered" id="ch07_term9"/>At some point in this chapter you’ve probably thought to yourself, “sure, channels are great, but writing to channels still blocks.” After all, every send operation on a channel blocks until there’s a corresponding receive, right? Well, as it turns out, this is only <em>mostly</em> true. At least, it’s true of default, unbuffered channels.</p>&#13;
&#13;
<p>However, as we first describe in <a data-type="xref" href="ch03.xhtml#section_ch03_channel_buffering">“Channel buffering”</a>, it’s possible to create channels that have an internal message buffer. Send operations on such buffered channels only block when the buffer is full and receives from a channel only block when the buffer is empty.</p>&#13;
&#13;
<p>You may recall that buffered channels can be created by passing an additional capacity parameter to the <code>make</code> function to specify the size of the buffer:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="nx">ch</code> <code class="o">:=</code> <code class="nb">make</code><code class="p">(</code><code class="kd">chan</code> <code class="kd">type</code><code class="p">,</code> <code class="nx">capacity</code><code class="p">)</code></pre>&#13;
&#13;
<p>Buffered channels are especially useful for handling “bursty” loads. In fact, we already used this strategy in <a data-type="xref" href="ch05.xhtml#chapter_5">Chapter 5</a> when we initialized our <code>FileTransactionLogger</code>. Distilling some of the logic that’s spread through that chapter produces something like the following:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="kd">type</code> <code class="nx">FileTransactionLogger</code> <code class="kd">struct</code> <code class="p">{</code>&#13;
    <code class="nx">events</code>       <code class="kd">chan</code><code class="o">&lt;-</code> <code class="nx">Event</code>       <code class="c1">// Write-only channel for sending events</code>&#13;
    <code class="nx">lastSequence</code> <code class="kt">uint64</code>             <code class="c1">// The last used event sequence number</code>&#13;
<code class="p">}</code>&#13;
&#13;
<code class="kd">func</code> <code class="p">(</code><code class="nx">l</code> <code class="o">*</code><code class="nx">FileTransactionLogger</code><code class="p">)</code> <code class="nx">WritePut</code><code class="p">(</code><code class="nx">key</code><code class="p">,</code> <code class="nx">value</code> <code class="kt">string</code><code class="p">)</code> <code class="p">{</code>&#13;
    <code class="nx">l</code><code class="p">.</code><code class="nx">events</code> <code class="o">&lt;-</code> <code class="nx">Event</code><code class="p">{</code><code class="nx">EventType</code><code class="p">:</code> <code class="nx">EventPut</code><code class="p">,</code> <code class="nx">Key</code><code class="p">:</code> <code class="nx">key</code><code class="p">,</code> <code class="nx">Value</code><code class="p">:</code> <code class="nx">value</code><code class="p">}</code>&#13;
<code class="p">}</code>&#13;
&#13;
<code class="kd">func</code> <code class="p">(</code><code class="nx">l</code> <code class="o">*</code><code class="nx">FileTransactionLogger</code><code class="p">)</code> <code class="nx">Run</code><code class="p">()</code> <code class="p">{</code>&#13;
    <code class="nx">l</code><code class="p">.</code><code class="nx">events</code> <code class="p">=</code> <code class="nb">make</code><code class="p">(</code><code class="kd">chan</code> <code class="nx">Event</code><code class="p">,</code> <code class="mi">16</code><code class="p">)</code>             <code class="c1">// Make an events channel</code>&#13;
&#13;
    <code class="k">go</code> <code class="kd">func</code><code class="p">()</code> <code class="p">{</code>&#13;
        <code class="k">for</code> <code class="nx">e</code> <code class="o">:=</code> <code class="k">range</code> <code class="nx">events</code> <code class="p">{</code>                 <code class="c1">// Retrieve the next Event</code>&#13;
            <code class="nx">l</code><code class="p">.</code><code class="nx">lastSequence</code><code class="o">++</code>                    <code class="c1">// Increment sequence number</code>&#13;
        <code class="p">}</code>&#13;
    <code class="p">}()</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p><a data-type="indexterm" data-primary="WritePut method" id="idm45983629297592"/>In this segment, we have a <code>WritePut</code> function that can be called to send a message to an <code>events</code> channel, which is received in the <code>for</code> loop inside the goroutine created in the <code>Run</code> function. If <code>events</code> was a standard channel, each send would block until the anonymous goroutine completed a receive operation. That might be fine most of the time, but if several writes came in faster than the goroutine could process them, the upstream client would be blocked.</p>&#13;
&#13;
<p>By using a buffered channel we made it possible for this code to handle small bursts of up to 16 closely clustered write requests. Importantly, however, the 17th write <em>would</em> block.</p>&#13;
&#13;
<p>It’s also important to consider that using buffered channels like this creates a risk of data loss should the program terminate before any consuming goroutines are able to clear the buffer.<a data-type="indexterm" data-primary="channels" data-secondary="buffered" data-startref="ch07_term19" id="idm45983629138760"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect3" data-pdf-bookmark="Minimizing locking with sharding"><div class="sect3" id="section_ch07_sharding">&#13;
<h3>Minimizing locking with sharding</h3>&#13;
&#13;
<p><a data-type="indexterm" data-primary="cache" id="idm45983629135816"/><a data-type="indexterm" data-primary="patterns" data-secondary="sharding" id="ch07_term9_2_2"/><a data-type="indexterm" data-primary="sharding" id="ch07_term9_2"/>As lovely as channels are, as we mentioned in <a data-type="xref" href="#section_ch07_efficient_synchronization">“Efficient Synchronization”</a> they don’t solve <em>every</em> problem. A common example of this is a large, central &#13;
<span class="keep-together">data structure,</span> such as a cache, that can’t be easily decomposed into discrete units of work.<sup><a data-type="noteref" id="idm45983629130744-marker" href="ch07.xhtml#idm45983629130744">11</a></sup></p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="mutex" id="idm45983629129896"/>When shared data structures have to be concurrently accessed, it’s standard to use a locking mechanism, such as the mutexes provided by the <code>sync</code> package, as we do in <a data-type="xref" href="ch05.xhtml#section_ch05_concurrency_safe">“Making Your Data Structure Concurrency-Safe”</a>. For example, we might create a struct that contains a map and an embedded <code>sync.RWMutex</code>:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="kd">var</code> <code class="nx">cache</code> <code class="p">=</code> <code class="kd">struct</code> <code class="p">{</code>&#13;
    <code class="nx">sync</code><code class="p">.</code><code class="nx">RWMutex</code>&#13;
    <code class="nx">data</code> <code class="kd">map</code><code class="p">[</code><code class="kt">string</code><code class="p">]</code><code class="kt">string</code>&#13;
<code class="p">}{</code><code class="nx">data</code><code class="p">:</code> <code class="nb">make</code><code class="p">(</code><code class="kd">map</code><code class="p">[</code><code class="kt">string</code><code class="p">]</code><code class="kt">string</code><code class="p">)}</code></pre>&#13;
&#13;
<p>When a routine wants to write to the cache, it would carefully use <code>cache.Lock</code> to establish the write lock, and <code>cache.Unlock</code> to release the lock when it’s done. We might even want to wrap it in a convenience function as follows:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="kd">func</code> <code class="nx">ThreadSafeWrite</code><code class="p">(</code><code class="nx">key</code><code class="p">,</code> <code class="nx">value</code> <code class="kt">string</code><code class="p">)</code> <code class="p">{</code>&#13;
    <code class="nx">cache</code><code class="p">.</code><code class="nx">Lock</code><code class="p">()</code>                                    <code class="c1">// Establish write lock</code>&#13;
    <code class="nx">cache</code><code class="p">.</code><code class="nx">data</code><code class="p">[</code><code class="nx">key</code><code class="p">]</code> <code class="p">=</code> <code class="nx">value</code>&#13;
    <code class="nx">cache</code><code class="p">.</code><code class="nx">Unlock</code><code class="p">()</code>                                  <code class="c1">// Release write lock</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p><a data-type="indexterm" data-primary="lock contention" id="idm45983629105096"/>By design, this restricts write access to whichever routine happens to have the lock. This pattern generally works just fine. However, as we discussed in <a data-type="xref" href="ch04.xhtml#chapter_4">Chapter 4</a>, as the number of concurrent processes acting on the data increases, the average amount of time that processes spend waiting for locks to be released also increases. You may remember the name for this unfortunate condition: lock contention.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="complexity" id="idm45983629022328"/><a data-type="indexterm" data-primary="latency" id="idm45983629021624"/><a data-type="indexterm" data-primary="vertical sharding" id="idm45983629020952"/>While this might be resolved in some cases by scaling the number of instances, this also increases complexity and latency, as distributed locks need to be established &#13;
<span class="keep-together">and writes</span>  need to establish consistency. An alternative strategy for reducing lock &#13;
<span class="keep-together">contention</span> around shared data structures within an instance of a service is <em>vertical sharding</em>, in which a large data structure is partitioned into two or more structures, each representing a part of the whole. Using this strategy, only a portion of the overall structure needs to be locked at a time, decreasing overall lock contention.</p>&#13;
&#13;
<p>You may recall that we discussed vertical sharding in some detail in <a data-type="xref" href="ch04.xhtml#section_ch04_sharding">“Sharding”</a>. If you’re unclear on vertical sharding theory or implementation, feel free to take some time to go back and review that section.<a data-type="indexterm" data-primary="patterns" data-secondary="sharding" data-startref="ch07_term9_2_2" id="idm45983629016552"/><a data-type="indexterm" data-primary="sharding" data-startref="ch07_term9_2" id="idm45983629015336"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="Memory Leaks Can…fatal error: runtime: out of memory"><div class="sect2" id="idm45983629699960">&#13;
<h2>Memory Leaks Can…fatal error: runtime: out of memory</h2>&#13;
&#13;
<p><a data-type="indexterm" data-primary="C++" id="idm45983629013192"/><a data-type="indexterm" data-primary="garbage-collected language" id="idm45983629012488"/><a data-type="indexterm" data-primary="bugs" id="idm45983629011848"/><a data-type="indexterm" data-primary="memory leaks" id="ch07_term10"/>Memory leaks are a class of bugs in which memory is not released even after it’s no longer needed. These bugs can be quite subtle and often plague languages like C++ in which memory is manually managed. But while garbage collection certainly helps by attempting to reclaim memory occupied by objects that are no longer in use by the program, garbage-collected languages like Go aren’t immune to memory leaks. Data structures can still grow unbounded, unresolved goroutines can still accumulate, and even unstopped <code>time.Ticker</code> values can get away from you.</p>&#13;
&#13;
<p>In this section we’ll review a few common causes of memory leaks particular to Go, and how to resolve them.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect3" data-pdf-bookmark="Leaking goroutines"><div class="sect3" id="idm45983629008520">&#13;
<h3>Leaking goroutines</h3>&#13;
&#13;
<p><a data-type="indexterm" data-primary="goroutine" data-seealso="leaking goroutine" id="idm45983629006920"/><a data-type="indexterm" data-primary="leaking goroutine" id="ch07_term11"/>I’m not aware of any actual data on the subject,<sup><a data-type="noteref" id="idm45983629004872-marker" href="ch07.xhtml#idm45983629004872">12</a></sup> but based purely on my own personal experience, I strongly suspect that goroutines are the single largest source of memory leaks in Go.</p>&#13;
&#13;
<p>Whenever a goroutine is executed, it’s initially allocated a small memory stack—2048 bytes—that can be dynamically adjusted up or down as it runs to suit the needs of the process. The precise maximum stack size depends on a lot of things,<sup><a data-type="noteref" id="idm45983629003416-marker" href="ch07.xhtml#idm45983629003416">13</a></sup> but it’s essentially reflective of the amount of available physical memory.</p>&#13;
&#13;
<p>Normally, when a goroutine returns, its stack is either deallocated or set aside for recycling.<sup><a data-type="noteref" id="idm45983629000568-marker" href="ch07.xhtml#idm45983629000568">14</a></sup> Whether by design or by accident, however, not every goroutine actually returns. For example:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="kd">func</code> <code class="nx">leaky</code><code class="p">()</code> <code class="p">{</code>&#13;
    <code class="nx">ch</code> <code class="o">:=</code> <code class="nb">make</code><code class="p">(</code><code class="kd">chan</code> <code class="kt">string</code><code class="p">)</code>&#13;
&#13;
    <code class="k">go</code> <code class="kd">func</code><code class="p">()</code> <code class="p">{</code>&#13;
        <code class="nx">s</code> <code class="o">:=</code> <code class="o">&lt;-</code><code class="nx">ch</code>&#13;
        <code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="s">"Message:"</code><code class="p">,</code> <code class="nx">s</code><code class="p">)</code>&#13;
    <code class="p">}()</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>In the previous example, the <code>leaky</code> function creates a channel and executes a goroutine that reads from that channel. The <code>leaky</code> function returns without error, but if you look closely you’ll see that no values are ever sent to <code>ch</code>, so the goroutine will never return and its stack will never be deallocated. There’s even collateral damage: because the goroutine references <code>ch</code>, that value can’t be cleaned up by the garbage collector.</p>&#13;
&#13;
<p>So we now have a bona fide memory leak. If such a function is called regularly the total amount of memory consumed will slowly increase over time until it’s completely exhausted.</p>&#13;
&#13;
<p>This is a contrived example, but there are good reasons why a programmer might want to create long-running goroutines, so it’s usually quite hard to know whether such a process was created intentionally.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Cheney, Dave" id="idm45983628944088"/>So what do we do about this? Dave Cheney offers some excellent advice here: “You should never start a goroutine without knowing how it will stop….Every time you use the <code>go</code> keyword in your program to launch a goroutine, you must know how, and when, that goroutine will exit. If you don’t know the answer, that’s a potential &#13;
<span class="keep-together">memory leak.</span>"<sup><a data-type="noteref" id="idm45983628941816-marker" href="ch07.xhtml#idm45983628941816">15</a></sup></p>&#13;
&#13;
<p>This may seem like obvious, even trivial advice, but it’s incredibly important. It’s all too easy to write functions that leak goroutines, and those leaks can be a pain to identify and find.<a data-type="indexterm" data-primary="leaking goroutine" data-startref="ch07_term11" id="idm45983628939384"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect3" data-pdf-bookmark="Forever ticking tickers"><div class="sect3" id="idm45983629007960">&#13;
<h3>Forever ticking tickers</h3>&#13;
&#13;
<p><a data-type="indexterm" data-primary="time dimension" id="idm45983628936968"/>Very often you’ll want to add some kind of time dimension to your Go code, to execute it at some point in the future or repeatedly at some interval, for example.</p>&#13;
&#13;
<p>The <code>time</code> package provides two useful tools to add such a time dimension to Go code execution: <code>time.Timer</code>, which fires at some point in the future, and <code>time.Ticker</code>, which fires repeatedly at some specified interval.</p>&#13;
&#13;
<p>However, where <code>time.Timer</code> has a finite useful life with a defined start and end, <code>time.Ticker</code> has no such limitation. A <code>time.Ticker</code> can live forever. Maybe you can see where this is going.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Timers" id="idm45983628932248"/><a data-type="indexterm" data-primary="Tickers" id="idm45983628931544"/>Both Timers and Tickers use a similar mechanism: each provides a channel that’s sent a value whenever it fires. The following example uses both:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="kd">func</code> <code class="nx">timely</code><code class="p">()</code> <code class="p">{</code>&#13;
    <code class="nx">timer</code> <code class="o">:=</code> <code class="nx">time</code><code class="p">.</code><code class="nx">NewTimer</code><code class="p">(</code><code class="mi">5</code> <code class="o">*</code> <code class="nx">time</code><code class="p">.</code><code class="nx">Second</code><code class="p">)</code>&#13;
    <code class="nx">ticker</code> <code class="o">:=</code> <code class="nx">time</code><code class="p">.</code><code class="nx">NewTicker</code><code class="p">(</code><code class="mi">1</code> <code class="o">*</code> <code class="nx">time</code><code class="p">.</code><code class="nx">Second</code><code class="p">)</code>&#13;
&#13;
    <code class="nx">done</code> <code class="o">:=</code> <code class="nb">make</code><code class="p">(</code><code class="kd">chan</code> <code class="kt">bool</code><code class="p">)</code>&#13;
&#13;
    <code class="k">go</code> <code class="kd">func</code><code class="p">()</code> <code class="p">{</code>&#13;
        <code class="k">for</code> <code class="p">{</code>&#13;
            <code class="k">select</code> <code class="p">{</code>&#13;
            <code class="k">case</code> <code class="o">&lt;-</code><code class="nx">ticker</code><code class="p">.</code><code class="nx">C</code><code class="p">:</code>&#13;
                <code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="s">"Tick!"</code><code class="p">)</code>&#13;
            <code class="k">case</code> <code class="o">&lt;-</code><code class="nx">done</code><code class="p">:</code>&#13;
                <code class="k">return</code>&#13;
            <code class="p">}</code>&#13;
        <code class="p">}</code>&#13;
    <code class="p">}()</code>&#13;
&#13;
    <code class="o">&lt;-</code><code class="nx">timer</code><code class="p">.</code><code class="nx">C</code>&#13;
    <code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="s">"It's time!"</code><code class="p">)</code>&#13;
    <code class="nb">close</code><code class="p">(</code><code class="nx">done</code><code class="p">)</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p><a data-type="indexterm" data-primary="timely function" id="idm45983628928808"/>The <code>timely</code> function executes a goroutine that loops at regular intervals by listening for signals from <code>ticker</code>—which occur every second—or from a <code>done</code> channel &#13;
<span class="keep-together">that returns</span> the goroutine. The line <code>&lt;-timer.C</code> blocks until the 5-second timer fires, allowing <code>done</code> to be closed, triggering the <code>case &lt;-done</code> condition and ending &#13;
<span class="keep-together">the loop.</span></p>&#13;
&#13;
<p>The <code>timely</code> function completes as expected, and the goroutine has a defined return, so you could be forgiven for thinking that everything’s fine. There’s a particularly sneaky bug here though: running <code>time.Ticker</code> values contain an active goroutine that can’t be cleaned up. Because we never stopped the timer, <code>timely</code> contains a memory leak.</p>&#13;
&#13;
<p>The solution: always be sure to stop your timers. A <code>defer</code> works quite nicely for this purpose:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="kd">func</code> <code class="nx">timelyFixed</code><code class="p">()</code> <code class="p">{</code>&#13;
    <code class="nx">timer</code> <code class="o">:=</code> <code class="nx">time</code><code class="p">.</code><code class="nx">NewTimer</code><code class="p">(</code><code class="mi">5</code> <code class="o">*</code> <code class="nx">time</code><code class="p">.</code><code class="nx">Second</code><code class="p">)</code>&#13;
    <code class="nx">ticker</code> <code class="o">:=</code> <code class="nx">time</code><code class="p">.</code><code class="nx">NewTicker</code><code class="p">(</code><code class="mi">1</code> <code class="o">*</code> <code class="nx">time</code><code class="p">.</code><code class="nx">Second</code><code class="p">)</code>&#13;
    <code class="k">defer</code> <code class="nx">ticker</code><code class="p">.</code><code class="nx">Stop</code><code class="p">()</code>                         <code class="c1">// Be sure to stop the ticker!</code>&#13;
&#13;
    <code class="nx">done</code> <code class="o">:=</code> <code class="nb">make</code><code class="p">(</code><code class="kd">chan</code> <code class="kt">bool</code><code class="p">)</code>&#13;
&#13;
    <code class="k">go</code> <code class="kd">func</code><code class="p">()</code> <code class="p">{</code>&#13;
        <code class="k">for</code> <code class="p">{</code>&#13;
            <code class="k">select</code> <code class="p">{</code>&#13;
            <code class="k">case</code> <code class="o">&lt;-</code><code class="nx">ticker</code><code class="p">.</code><code class="nx">C</code><code class="p">:</code>&#13;
                <code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="s">"Tick!"</code><code class="p">)</code>&#13;
            <code class="k">case</code> <code class="o">&lt;-</code><code class="nx">done</code><code class="p">:</code>&#13;
                <code class="k">return</code>&#13;
            <code class="p">}</code>&#13;
        <code class="p">}</code>&#13;
    <code class="p">}()</code>&#13;
&#13;
    <code class="o">&lt;-</code><code class="nx">timer</code><code class="p">.</code><code class="nx">C</code>&#13;
    <code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="s">"It's time!"</code><code class="p">)</code>&#13;
    <code class="nb">close</code><code class="p">(</code><code class="nx">done</code><code class="p">)</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>By calling <code>ticker.Stop()</code>, we shut down the underlying <code>Ticker</code>, allowing it to be recovered by the garbage collector and preventing a leak.<a data-type="indexterm" data-primary="memory leaks" data-startref="ch07_term10" id="idm45983628807816"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="On Efficiency"><div class="sect2" id="idm45983628937816">&#13;
<h2>On Efficiency</h2>&#13;
&#13;
<p>In this section, we covered a number of common methods for improving the efficiency of your programs, ranging from using an LRU cache rather than a map to constrain your cache’s memory footprint, to approaches for effectively synchronizing your processes, to preventing memory leaks. While these sections might not seem particularly closely connected, they’re all important for building programs that scale.</p>&#13;
&#13;
<p>Of course, there are countless other methods that I would have liked to include as well, but wasn’t able to given the fundamental limits imposed by time and space.</p>&#13;
&#13;
<p>In the next section, we’ll change themes once again to cover some common service architectures and their effects on scalability. These might be a little less focused on Go specifically, but they’re critical for a study of scalability, especially in a cloud native context.<a data-type="indexterm" data-primary="scalability" data-secondary="efficiency and" data-startref="ch07_term6" id="idm45983628674488"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect1" data-pdf-bookmark="Service Architectures"><div class="sect1" id="section_ch07_service_architectures">&#13;
<h1>Service Architectures</h1>&#13;
&#13;
<p><a data-type="indexterm" data-primary="service architectures" data-see="microservices architecture, monolith architecture, serverless architecture" id="idm45983628671464"/><a data-type="indexterm" data-primary="microservice" data-seealso="microservices architecture" id="ch07_term13"/><a data-type="indexterm" data-primary="service-oriented architecture (SOA)" id="idm45983628669176"/><a data-type="indexterm" data-primary="SOA (service-oriented architecture)" id="idm45983628668488"/><a data-type="indexterm" data-primary="monolith" data-seealso="monolith architecture" id="idm45983628667800"/><a data-type="indexterm" data-primary="scalability" data-secondary="service architectures and" id="ch07_term12"/>The concept of the <em>microservice</em> first appeared in the early 2010s as a refinement and simplification of the earlier service-oriented architecture (SOA) and a response to the <em>monoliths</em>—server-side applications contained within a single large executable—that were then the most common architectural model of choice.<sup><a data-type="noteref" id="idm45983628664360-marker" href="ch07.xhtml#idm45983628664360">16</a></sup></p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="microservices architecture" id="idm45983628663672"/>At the time, the idea of the microservice architecture—a single application composed of multiple small services, each running in its own process and communicating with lightweight mechanisms—was revolutionary. Unlike monoliths, which require the entire application to be rebuilt and deployed for any change to the system, microservices were independently deployable by fully automated deployment mechanisms. This sounds small, even trivial, but its implications were (and are) vast.</p>&#13;
&#13;
<p>If you ask most programmers to compare monoliths to microservices, most of the answers you get will probably be something about how monoliths are slow, sluggish, and bloated, while microservices are small, agile, and the new hotness. Sweeping generalizations are always wrong, though, so let’s take a moment to ask ourselves whether this is true, and whether monoliths might sometimes be the right choice.</p>&#13;
&#13;
<p>We will begin by defining what we mean when we talk about monoliths and &#13;
<span class="keep-together">microservices</span>.<a data-type="indexterm" data-primary="microservice" data-startref="ch07_term13" id="idm45983628660424"/></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="The Monolith System Architecture"><div class="sect2" id="idm45983628659288">&#13;
<h2>The Monolith System Architecture</h2>&#13;
&#13;
<p><a data-type="indexterm" data-primary="monolith architecture" id="ch07_term14"/>In a <em>monolith architecture</em>, all of the functionally distinguishable aspects of a service are coupled together in one place. A common example is a web application whose user interface, data layer, and business logic are all intermingled, often on a single server.</p>&#13;
&#13;
<p>Traditionally, enterprise applications have been built in three main parts, as illustrated in <a data-type="xref" href="#img_ch07_arch_monolith">Figure 7-3</a>: a client-side interface running on the user’s machine, a relational database where all of the application’s data lives, and a server-side application that handles all user input, executes all business logic, and reads and writes data to the database.</p>&#13;
&#13;
<figure><div id="img_ch07_arch_monolith" class="figure">&#13;
<img src="Images/cngo_0703.png" alt="cngo 0703" width="899" height="245"/>&#13;
<h6><span class="label">Figure 7-3. </span>In a monolith architecture, all of the functionally distinguishable aspects of a service are coupled together in one place</h6>&#13;
</div></figure>&#13;
&#13;
<p>At the time, this pattern made sense. All the business logic ran in a single process, making development easier, and you could even scale by running more monoliths behind a load balancer, usually using sticky sessions to maintain server affinity. Things were <em>perfectly fine</em>, and for many years this was by far the most common way of building web applications.</p>&#13;
&#13;
<p>Even today, for relatively small or simple applications (for some definition of “small” and “simple”) this works perfectly well (though I still strongly recommend statelessness over server affinity).</p>&#13;
&#13;
<p>However, as the number of features and general complexity of a monolith increases, difficulties start to arise:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Monoliths are usually deployed as a single artifact, so making even a small change generally requires a new version of the entire monolith to be built, tested, and deployed.</p>&#13;
</li>&#13;
<li>&#13;
<p>Despite even the best of intentions and efforts, monolith code tends to decrease in modularity over time, making it harder to make changes in one part of the service without affecting another part in unexpected ways.</p>&#13;
</li>&#13;
<li>&#13;
<p>Scaling the application means creating replicas of the entire application, not just the parts that need it.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The larger and more complex the monolith gets, the more pronounced these effects tend to become. By the early- to mid-2000s, these issues were well known, leading frustrated programmers to experiment with breaking their big, complex services into smaller, independently deployable and scalable components. By 2012, this pattern even had a name: microservices architecture.<a data-type="indexterm" data-primary="monolith architecture" data-startref="ch07_term14" id="idm45983628645640"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="The Microservices System Architecture"><div class="sect2" id="section_ch07_microservices">&#13;
<h2>The Microservices System Architecture</h2>&#13;
&#13;
<p><a data-type="indexterm" data-primary="microservices architecture" id="ch07_term15"/>The defining characteristic of a <em>microservices architecture</em> is a service whose functional components have been divided into a set of discrete sub-services that can be independently built, tested, deployed, and scaled.</p>&#13;
&#13;
<p>This is illustrated in <a data-type="xref" href="#img_ch07_arch_microservices">Figure 7-4</a>, in which a user interface service—perhaps an HTML-serving web application or a public API—interacts with clients, but rather than handling the business logic locally, it makes secondary requests of one or more component services to handle some specific functionality. Those services might in turn even make further requests of yet more services.</p>&#13;
&#13;
<figure><div id="img_ch07_arch_microservices" class="figure">&#13;
<img src="Images/cngo_0704.png" alt="cngo 0704" width="898" height="563"/>&#13;
<h6><span class="label">Figure 7-4. </span>In a microservices architecture, functional components are divided into discrete subservices</h6>&#13;
</div></figure>&#13;
&#13;
<p>While the microservices architecture has a number of advantages over the monolith, there are significant costs to consider. On one hand, microservices provide some significant benefits:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>A clearly-defined separation of concerns supports and reinforces modularity, which can be very useful for larger or multiple teams.</p>&#13;
</li>&#13;
<li>&#13;
<p>Microservices should be independently deployable, making them easier to manage and making it possible to isolate errors and failures.</p>&#13;
</li>&#13;
<li>&#13;
<p>In a microservices system, it’s possible for different services to use the technology—language, development framework, data storage, etc—that’s most appropriate to its function.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>These benefits shouldn’t be underestimated: the increased modularity and functional isolation of microservices tends to produce components that are themselves generally far more maintainable than a monolith with the same functionality. The resulting system isn’t just easier to deploy and manage, but easier to understand, reason about, and extend for a larger number of programmers and teams.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Mixing different technologies may sound appealing in theory, but use restraint. Each adds new requirements for tooling and expertise. The pros and cons of adopting a new technology—any new technology<sup><a data-type="noteref" id="idm45983628631080-marker" href="ch07.xhtml#idm45983628631080">17</a></sup>—should always be carefully considered.</p>&#13;
</div>&#13;
&#13;
<p>The discrete nature of microservices makes them far easier to maintain, deploy, and scale than monoliths. However, while these are real benefits that can pay real dividends, there are some downsides as well:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The distributed nature of microservices makes them subject to the Fallacies of Distributed Computing (see <a data-type="xref" href="ch04.xhtml#chapter_4">Chapter 4</a>), which makes them significantly harder to program and debug.<a data-type="indexterm" data-primary="Fallacies of Distributed Computing" id="idm45983628627608"/></p>&#13;
</li>&#13;
<li>&#13;
<p>Sharing any kind of state between your services can often be extremely difficult.</p>&#13;
</li>&#13;
<li>&#13;
<p>Deploying and managing multiple services can be quite complex and tends to demand a high level of operational maturity.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>So given these, which do you choose? The relative simplicity of the monolith, or the flexibility and scalability of microservices? You might have noticed that most of the benefits of microservices pay off as the application gets larger or the number of teams working on it increases. For this reason many authors advocate starting with a monolith and decomposing it later.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="monolith" id="idm45983628623656"/>On a personal note, I’ll mention that I’ve never seen any organization successfully break apart a large monolith, but I’ve seen many try. That’s not to say it’s impossible, just that it’s hard. I can’t tell you whether you should start your system as microservices, or with a monolith and break it up later. I’d certainly get a lot of angry emails if I tried. But please, whatever you do, stay stateless.<a data-type="indexterm" data-primary="microservices architecture" data-startref="ch07_term15" id="idm45983628622392"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="Serverless Architectures"><div class="sect2" id="idm45983628643944">&#13;
<h2>Serverless Architectures</h2>&#13;
&#13;
<p><a data-type="indexterm" data-primary="serverless architecture" id="ch07_term16"/><a data-type="indexterm" data-primary="serverless computing" id="idm45983628619080"/>Serverless computing is a pretty popular topic in web application architecture, and a lot of (digital) ink has been spilled about it. Much of this hype has been driven by the major cloud providers, which have invested heavily in serverlessness, but not all of it.</p>&#13;
&#13;
<p>But what is serverless computing, really?</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="FaaS" id="idm45983628617336"/><a data-type="indexterm" data-primary="AWS Lambda" id="idm45983628616632"/><a data-type="indexterm" data-primary="GCP Cloud Functions" id="idm45983628615960"/>Well, as is often the case, it depends on who you ask. For the purposes of this book, however, we’re defining it as a form of utility computing in which some server-side logic, written by a programmer, is transparently executed in a managed ephemeral environment in response to some predefined trigger. This is also sometimes referred to as “functions as a service,” or “FaaS.” All of the major cloud providers offer FaaS implementations, such as AWS’s Lambda or GCP’s Cloud Functions.</p>&#13;
&#13;
<p>Such functions are quite flexible and can be usefully incorporated into many architectures. In fact, as we’ll discuss shortly, entire <em>serverless architectures</em> can even be built that don’t use traditional services at all, but are instead built entirely from FaaS resources and third-party managed services.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45983628613352">&#13;
<h5>Be Suspicious of Hype</h5>&#13;
<p>I may sound like a grizzled old dinosaur here, but I’ve learned to be wary of new technologies that nobody really understands claiming to solve all of our problems.</p>&#13;
&#13;
<p>According to the research and advisory firm Gartner, which specializes in studying IT and technology trends, serverless infrastructure is hovering at or near the “Peak of Inflated Expectations”<sup><a data-type="noteref" id="idm45983628611016-marker" href="ch07.xhtml#idm45983628611016">18</a></sup> of its <a href="https://oreil.ly/fNuG8">“hype cycle”</a>. This is eventually, but inevitably, followed by the “Trough of Disillusionment.”</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Slope of Enlightenment" id="idm45983628607576"/><a data-type="indexterm" data-primary="Plateau of Productivity" id="idm45983628606872"/>In time, people start to figure out what the technology is really useful for (not <em>everything</em>) and when to use it (not <em>always</em>), and it enters the “Slope of Enlightenment” and “Plateau of Productivity.” I’ve learned the hard way that it’s usually best to wait until a technology has entered these two later phases before investing heavily in its use.</p>&#13;
&#13;
<p>That being said: serverless computing <em>is</em> intriguing, and it <em>does</em> seem appropriate for some use-cases.</p>&#13;
</div></aside>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect3" data-pdf-bookmark="The pros and cons of serverlessness"><div class="sect3" id="idm45983628603464">&#13;
<h3>The pros and cons of serverlessness</h3>&#13;
&#13;
<p>As with any other architectural decision, the choice to go with a partially or entirely serverless architecture should be carefully weighed against all available options. While serverlessness provides some clear benefits—some obvious (no servers to manage!), others less so (cost and energy savings)—it’s very different from traditional architectures, and carries its own set of downsides.</p>&#13;
&#13;
<p>That being said, let’s start weighing. Let’s start with the advantages:</p>&#13;
<dl>&#13;
<dt>Operational management</dt>&#13;
<dd>&#13;
<p>Perhaps the most obvious benefit of serverless architectures is that there’s considerably less operational overhead.<sup><a data-type="noteref" id="idm45983628559272-marker" href="ch07.xhtml#idm45983628559272">19</a></sup> There are no servers to provision and maintain, no licenses to buy, and no software to install.</p>&#13;
</dd>&#13;
<dt>Scalability</dt>&#13;
<dd>&#13;
<p>When using serverless functions, it’s the provider—not the user—who’s responsible for scaling capacity to meet demand. As such, the implementor can spend less time and effort considering and implementing scaling rules.</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="FaaS" id="idm45983628556136"/>Reduced costs</dt>&#13;
<dd>&#13;
<p>FaaS providers typically use a “pay-as-you-go” model, charging only for the time and memory allocated when the function is run. This can be considerably more cost-effective than deploying traditional services to (likely underutilized) servers.</p>&#13;
</dd>&#13;
<dt>Productivity</dt>&#13;
<dd>&#13;
<p>In a FaaS model, the unit of work is an event-driven function. This model tends to encourage a “function first” mindset, resulting in code that’s often simpler, more readable, and easier to test.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>It’s not all roses, though. There are very some real downsides to serverless architectures that need to be taken into consideration as well:</p>&#13;
<dl>&#13;
<dt><a data-type="indexterm" data-primary="cold start" id="idm45983628551240"/>Startup latency</dt>&#13;
<dd>&#13;
<p>When a function is first called, it has to be “spun up” by the cloud provider. This typically takes less than a second, but in some cases can add 10 or more seconds to the initial requests. This is known as the <em>cold start</em> delay. What’s more, if the function isn’t called for several minutes—the exact time varies between providers—it’s “spun down” by the provider so that it has to endure another cold start when it’s called again. This usually isn’t a problem if your function doesn’t have enough idle time to get spun down, but can be a significant issue if your load is particularly “bursty.”</p>&#13;
</dd>&#13;
<dt>Observability</dt>&#13;
<dd>&#13;
<p>While most of the cloud vendors provide some basic monitoring for their FaaS offerings, it’s usually quite rudimentary. While third-party providers have been working to fill the void, the quality and quantity of data available from your ephemeral functions is often less than desired.</p>&#13;
</dd>&#13;
<dt>Testing</dt>&#13;
<dd>&#13;
<p>While unit testing tends to be pretty straightforward for serverless functions, integration testing is quite hard. It’s often difficult or impossible to simulate the serverless environment, and mocks are approximations at best.</p>&#13;
</dd>&#13;
</dl>&#13;
<dl>&#13;
<dt>Cost</dt>&#13;
<dd>&#13;
<p>Although the “pay-as-you-go” model can be considerably cheaper when demand is lower, there is a point at which this is no longer true. In fact, very high levels of load can grow to be quite expensive.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Clearly, there’s quite a lot to consider—on both sides—and while there <em>is</em> a great deal of hype around serverless at the moment, to some degree I think it’s merited. However, while serverlessness promises (and largely delivers) scalability and reduced costs, it does have quite a few gotchas, including, but not limited to, testing and debugging challenges. Not to mention the increased burden on operations around observability!<sup><a data-type="noteref" id="idm45983628542136-marker" href="ch07.xhtml#idm45983628542136">20</a></sup></p>&#13;
&#13;
<p>Finally, as we’ll see in the next section, serverless architectures also require quite a lot more up-front planning than traditional architectures. While some people might call this a positive feature, it can add significant complexity.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect3" data-pdf-bookmark="Serverless services"><div class="sect3" id="idm45983628540808">&#13;
<h3>Serverless services</h3>&#13;
&#13;
<p><a data-type="indexterm" data-primary="FaaS" id="idm45983628539208"/>As mentioned previously, functions as a service (FaaS) are flexible enough to serve as the foundation of entire serverless architectures that don’t use traditional services &#13;
<span class="keep-together">at all,</span> but are instead built entirely from FaaS resources and third-party managed &#13;
<span class="keep-together">services</span>.</p>&#13;
&#13;
<p class="pagebreak-before less_space"><a data-type="indexterm" data-primary="monolith architecture" id="idm45983628536232"/>Let’s take, as an example, the familiar three-tier system in which a client issues a request to a service, which in turn interacts with a database. A good example is the key-value store we started in <a data-type="xref" href="ch05.xhtml#chapter_5">Chapter 5</a>, whose (admittedly primitive) monolithic architecture might look something like what’s shown in <a data-type="xref" href="#img_ch07_kv_monolith">Figure 7-5</a>.</p>&#13;
&#13;
<figure><div id="img_ch07_kv_monolith" class="figure">&#13;
<img src="Images/cngo_0705.png" alt="cngo 0705" width="1028" height="245"/>&#13;
<h6><span class="label">Figure 7-5. </span>The monolithic architecture of our primitive key/value store</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="indexterm" data-primary="API gateway" id="idm45983628531208"/>To convert this monolith into a serverless architecture, we’ll need to use an <em>API gateway</em>: a managed service that’s configured to expose specific HTTP endpoints and to direct requests to each endpoint to a specific resource—typically a FaaS functions—that handles requests and issue responses. Using this architecture, our key/value store might look something like what’s shown in <a data-type="xref" href="#img_ch07_arch_serverless_api">Figure 7-6</a>.</p>&#13;
&#13;
<figure><div id="img_ch07_arch_serverless_api" class="figure">&#13;
<img src="Images/cngo_0706.png" alt="cngo 0706" width="1349" height="445"/>&#13;
<h6><span class="label">Figure 7-6. </span>An API gateway routes HTTP calls to serverless handler functions</h6>&#13;
</div></figure>&#13;
&#13;
<p>In this example, we’ve replaced the monolith with an API gateway that supports three endpoints: <code>GET /v1/{key}</code>,  <code>PUT /v1/{key}</code>, and <code>DELETE /v1/{key}</code> (the <code>{key}</code> component indicates that this path will match any string, and refer to the value as <code>key</code>).</p>&#13;
&#13;
<p>The API gateway is configured so that requests to each of its three endpoints are directed to a different handler function—<code>getKey</code>, <code>putKey</code>, and <code>deleteKey</code>, respectively—which performs all of the logic for handling that request and interacting with the backing database.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Auth0" id="idm45983628521720"/><a data-type="indexterm" data-primary="Okta" id="idm45983628521016"/>Granted, this is an incredibly simple application and doesn’t account for things like authentication (which can be provided by a number of excellent third-party services like Auth0 or Okta), but some things are immediately evident.</p>&#13;
&#13;
<p>First, there are a greater number of moving parts that you have to get your head around, which necessitates quite a bit more up-front planning and testing. For example, what happens if there’s an error in a handler function? What happens to the request? Does it get forwarded to some other destination, or is it perhaps sent to a dead-letter queue for further processing?</p>&#13;
&#13;
<p>Do not underestimate the significance of this increase in complexity! Replacing in-process interactions with distributed, fully managed components tends to introduce a variety of problems and failure cases that simply don’t exist in the former. You may well have turned a relatively simple problem into an enormously complex one. Complexity kills; simplicity scales.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="FaaS" id="idm45983628518104"/>Second, with all of these different components, there’s a need for more sophisticated distributed monitoring than you’d need with a monolith or small microservices system. Due to the fact that FaaS relies heavily on the cloud provider, this may be challenging or, at least, awkward.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Redis" id="idm45983628516712"/><a data-type="indexterm" data-primary="Amazon S3" id="idm45983628516008"/>Finally, the ephemeral nature of FaaS means that ALL state, even short-lived optimizations like caches, has to be externalized to a database, an external cache (like Redis), or network file/object store (like S3). Again, this can be argued to be a Good Thing, but it does add to up-front complexity.<a data-type="indexterm" data-primary="scalability" data-secondary="service architectures and" data-startref="ch07_term12" id="idm45983628514888"/><a data-type="indexterm" data-primary="serverless architecture" data-startref="ch07_term16" id="idm45983628513656"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm45983628540248">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>This was a very difficult chapter to write, not because there isn’t much to say, but because scalability is such a huge topic with so many different things I could have drilled down into. Every one of these battled in my brain for weeks.</p>&#13;
&#13;
<p>I even ended up throwing away some perfectly good architecture content that, in retrospect, simply wasn’t appropriate for this book. Fortunately, I was able to salvage a whole other chunk of work about messaging that ended up getting moved into <a data-type="xref" href="ch08.xhtml#chapter_8">Chapter 8</a>. I think it’s happier there anyway.</p>&#13;
&#13;
<p>In those weeks, I spent a lot of time thinking about what scalability really is, and about the role that efficiency plays in it. Ultimately, I think that the decision to spend so much time on programmatic—rather than infrastructural—solutions to scaling problems was the right one.</p>&#13;
&#13;
<p>All told, I think the end result is a good one. We certainly covered a lot of ground:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>We reviewed the different axes of scaling, and how scaling out is often the best long-term strategy.</p>&#13;
</li>&#13;
<li>&#13;
<p>We discussed state and statelessness, and why application state is essentially “anti-scalability.”</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<ul class="pagebreak-before">&#13;
<li>&#13;
<p>We learned a few strategies for efficient in-memory caching and for avoiding memory leaks.</p>&#13;
</li>&#13;
<li>&#13;
<p>We compared and contrasted monolithic, microservice, and serverless architectures.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>That’s quite a lot, and although I wish I’d been able to drill down in some more detail, I’m pleased to have been able to touch on the things I did.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45983630129928"><sup><a href="ch07.xhtml#idm45983630129928-marker">1</a></sup> Kanat-Alexander, Max. <em>Code Simplicity: The Science of Software Design</em>. O’Reilly Media, 23 March 2012.</p><p data-type="footnote" id="idm45983630123800"><sup><a href="ch07.xhtml#idm45983630123800-marker">2</a></sup> Honestly, if we had autoscaling in place I probably wouldn’t even remember that this happened.</p><p data-type="footnote" id="idm45983630119912"><sup><a href="ch07.xhtml#idm45983630119912-marker">3</a></sup> If you want to know more about cloud native infrastructure and architecture, a bunch of excellent books on the subject have already been written. I particularly recommend <em>Cloud Native Infrastructure</em> by Justin Garrison and Kris Nova, and <em>Cloud Native Transformation</em> by Pini Reznik, Jamie Dobson, and Michelle Gienow (both O’Reilly Media).</p><p data-type="footnote" id="idm45983630115000"><sup><a href="ch07.xhtml#idm45983630115000-marker">4</a></sup> This is my definition. I acknowledge that it diverges from other common definitions.</p><p data-type="footnote" id="idm45983630066296"><sup><a href="ch07.xhtml#idm45983630066296-marker">5</a></sup> Some cloud providers impose lower network I/O limits on smaller instances. Increasing the size of the instance may increase these limits in some cases.</p><p data-type="footnote" id="idm45983630057256"><sup><a href="ch07.xhtml#idm45983630057256-marker">6</a></sup> If you have a better definition, let me know. I’m already thinking about the second edition.</p><p data-type="footnote" id="idm45983630037192"><sup><a href="ch07.xhtml#idm45983630037192-marker">7</a></sup> I know I said the word “state” a bunch of times there. Writing is hard.</p><p data-type="footnote" id="idm45983630036376"><sup><a href="ch07.xhtml#idm45983630036376-marker">8</a></sup> See also: idempotence.</p><p data-type="footnote" id="idm45983629703592"><sup><a href="ch07.xhtml#idm45983629703592-marker">9</a></sup> However, if you’re interested in learning more about high-performance caching in Go, take a look at Manish Rai Jain’s excellent post on the subject, “The State of Caching in Go,” on the <a href="https://oreil.ly/N6lrh"><em>Dgraph Blog</em></a>.</p><p data-type="footnote" id="idm45983629686280"><sup><a href="ch07.xhtml#idm45983629686280-marker">10</a></sup> Gerrand, Andrew. “Share Memory By Communicating.” <em>The Go Blog</em>, 13 July 2010. <a href="https://oreil.ly/GTURp"><em class="hyperlink">https://oreil.ly/GTURp</em></a> Portions of this section are modifications based on work created and <a href="https://oreil.ly/D8ntT">shared by Google</a> and used according to terms described in the <a href="https://oreil.ly/la3YW">Creative Commons 4.0 Attribution License</a>.</p><p data-type="footnote" id="idm45983629130744"><sup><a href="ch07.xhtml#idm45983629130744-marker">11</a></sup> You could probably shoehorn channels into a solution for interacting with a cache, but you might find it difficult to make it simpler than locking.</p><p data-type="footnote" id="idm45983629004872"><sup><a href="ch07.xhtml#idm45983629004872-marker">12</a></sup> If you are, let me know!</p><p data-type="footnote" id="idm45983629003416"><sup><a href="ch07.xhtml#idm45983629003416-marker">13</a></sup> Dave Cheney wrote an excellent article on this topic called <a href="https://oreil.ly/PUCLF"><em>Why is a Goroutine’s stack infinite?</em></a> that I recommend you take a look at if you’re interested in the dynamics of goroutine memory  <span class="keep-together">allocation</span>.</p><p data-type="footnote" id="idm45983629000568"><sup><a href="ch07.xhtml#idm45983629000568-marker">14</a></sup> There’s a very good article by Vincent Blanchon on the subject of goroutine recycling entitled <a href="https://oreil.ly/GnoV2"><em>How Does Go Recycle Goroutines?</em></a></p><p data-type="footnote" id="idm45983628941816"><sup><a href="ch07.xhtml#idm45983628941816-marker">15</a></sup> Cheney, Dave. “Never Start a Goroutine without Knowing How It Will Stop.” dave.cheney.net, 22 Dec. 2016. <a href="https://oreil.ly/VUlrY"><em class="hyperlink">https://oreil.ly/VUlrY</em></a>.</p><p data-type="footnote" id="idm45983628664360"><sup><a href="ch07.xhtml#idm45983628664360-marker">16</a></sup> Not that they’ve gone away.</p><p data-type="footnote" id="idm45983628631080"><sup><a href="ch07.xhtml#idm45983628631080-marker">17</a></sup> Yes, even Go.</p><p data-type="footnote" id="idm45983628611016"><sup><a href="ch07.xhtml#idm45983628611016-marker">18</a></sup> Bowers, Daniel, et al. “Hype Cycle for Compute Infrastructure, 2019.” <em>Gartner</em>, Gartner Research, 26 July 2019, <a href="https://oreil.ly/3gkJh"><em class="hyperlink">https://oreil.ly/3gkJh</em></a>.</p><p data-type="footnote" id="idm45983628559272"><sup><a href="ch07.xhtml#idm45983628559272-marker">19</a></sup> It’s <em>right in the name!</em></p><p data-type="footnote" id="idm45983628542136"><sup><a href="ch07.xhtml#idm45983628542136-marker">20</a></sup> Sorry, there’s no such thing as NoOps.</p></div></div></section></div></body></html>