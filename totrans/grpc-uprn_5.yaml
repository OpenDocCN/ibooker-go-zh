- en: 'Chapter 5\. gRPC: Beyond the Basics'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章：gRPC：超越基础知识
- en: When you build real-world gRPC applications you may have to augment them with
    various capabilities to meet requirements such as intercepting incoming and outgoing
    RPC, handling network delays resiliently, handling errors, sharing metadata between
    services and consumers, and so on.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 构建实际的 gRPC 应用程序时，您可能需要增加各种功能以满足需求，如拦截进出的 RPC、弹性地处理网络延迟、处理错误、在服务和消费者之间共享元数据等。
- en: Note
  id: totrans-2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: To maintain consistency, all samples in this chapter are explained using Go.
    If you’re more familiar with Java, you can refer to the Java samples in the source
    code repository for the same use cases.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持一致性，本章中的所有示例都是用 Go 语言解释的。如果您更熟悉 Java，可以参考源代码存储库中的 Java 示例，以实现相同的用例。
- en: In this chapter, you will learn some key advanced gRPC capabilities including
    using gRPC interceptors to intercept RPCs on the server and client sides, using
    deadlines to specify the wait time for an RPC to complete, error-handling best
    practices on the server and client sides, using multiplexing to run multiple services
    on the same server, sharing custom metadata between applications, using load-balancing
    and name resolution techniques when calling other services, and compressing RPC
    calls to effectively use the network bandwidth.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解一些关键的高级 gRPC 能力，包括使用 gRPC 拦截器在服务器和客户端上拦截 RPC、使用截止时间指定 RPC 完成的等待时间、服务器和客户端上的错误处理最佳实践、使用多路复用在同一服务器上运行多个服务、在调用其他服务时共享自定义元数据、使用负载均衡和名称解析技术、压缩
    RPC 调用以有效利用网络带宽。
- en: Let’s begin our discussion with gRPC interceptors.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 gRPC 拦截器开始讨论。
- en: Interceptors
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拦截器
- en: As you build gRPC applications, you may want to execute some common logic before
    or after the execution of the remote function, for either client or server applications.
    In gRPC you can intercept that RPC’s execution to meet certain requirements such
    as logging, authentication, metrics, etc., using an extension mechanism called
    an *interceptor*. gRPC provides simple APIs to implement and install interceptors
    in your client and server gRPC applications. They are one of the key extension
    mechanisms in gRPC and are quite useful in use cases such as logging, authentication,
    authorization, metrics, tracing, and any other customer requirements.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建 gRPC 应用程序时，您可能希望在远程函数执行前或执行后执行一些常见逻辑，无论是客户端还是服务器应用程序。在 gRPC 中，您可以拦截该 RPC
    的执行，以满足诸如日志记录、身份验证、度量等要求，使用一种称为*拦截器*的扩展机制。gRPC 提供了简单的 API 来在客户端和服务器 gRPC 应用程序中实现和安装拦截器。它们是
    gRPC 中的关键扩展机制之一，在日志记录、身份验证、授权、度量、跟踪以及其他客户需求等用例中非常有用。
- en: Note
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Interceptors are not supported in all languages that support gRPC, and the implementation
    of interceptors in each language may be different. In this book we only cover
    Go and Java.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有支持 gRPC 的语言都支持拦截器，并且每种语言中拦截器的实现可能有所不同。在本书中，我们只涵盖了 Go 和 Java 两种语言。
- en: gRPC interceptors can be categorized into two types based on the type of RPC
    calls they intercept. For unary RPC you can use *unary interceptors*, while for
    streaming RPC you can use *streaming interceptors*. These interceptors can be
    used on the gRPC server side or on the gRPC client side. First, let’s start by
    looking at using interceptors on the server side.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC 拦截器可以根据它们拦截的 RPC 调用类型分为两种类型。对于一元 RPC，您可以使用*一元拦截器*，而对于流式 RPC，您可以使用*流式拦截器*。这些拦截器可以在
    gRPC 服务器端或 gRPC 客户端端使用。首先，让我们从在服务器端使用拦截器开始。
- en: Server-Side Interceptors
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务器端拦截器
- en: When a client invokes a remote method of a gRPC service, you can execute a common
    logic prior to the execution of the remote methods by using a server-side interceptor.
    This helps when you need to apply certain features such as authentication prior
    to invoking the remote method. As shown in [Figure 5-1](#server_side_interceptors),
    you can plug one or more interceptors into any gRPC server that you develop. For
    example, to plug a new server-side interceptor into your `OrderManagement` gRPC
    service, you can implement the interceptor and register it when you create the
    gRPC server.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当客户端调用 gRPC 服务的远程方法时，您可以通过使用服务器端拦截器在调用远程方法之前执行一些常见逻辑。当您需要在调用远程方法之前应用某些功能（例如身份验证）时，这将非常有帮助。如图
    [5-1](#server_side_interceptors) 所示，您可以将一个或多个拦截器插入到您开发的任何 gRPC 服务器中。例如，要将一个新的服务器端拦截器插入到您的
    `OrderManagement` gRPC 服务中，您可以实现该拦截器，并在创建 gRPC 服务器时进行注册。
- en: '![Server-side interceptors ](assets/grpc_0501.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![服务器端拦截器](assets/grpc_0501.png)'
- en: Figure 5-1\. Server-side interceptors
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. 服务器端拦截器
- en: On the server side, the unary interceptor allows you to intercept the unary
    RPC call while the streaming interceptor intercepts the streaming RPC. Let’s first
    discuss server-side unary interceptors.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务器端，一元拦截器允许你拦截一元 RPC 调用，而流拦截器则拦截流式 RPC。让我们先讨论服务器端的一元拦截器。
- en: Unary interceptor
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一元拦截器
- en: 'If you want to intercept the unary RPC of your gRPC service at the server side,
    you’ll need to implement a unary interceptor for your gRPC server. As shown in
    the Go code snippet in [Example 5-1](#EX5-1), you can do this by implementing
    a function of type `UnaryServerInterceptor` and registering that function when
    you create a gRPC server. `UnaryServerInterceptor` is the type for a server-side
    unary interceptor with the following signature:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在服务器端拦截你的 gRPC 服务的一元 RPC，你需要为你的 gRPC 服务器实现一个一元拦截器。如 Go 代码片段中所示的 [示例 5-1](#EX5-1)，你可以通过实现一个类型为
    `UnaryServerInterceptor` 的函数并在创建 gRPC 服务器时注册该函数来实现这一点。`UnaryServerInterceptor`
    是具有以下签名的服务器端一元拦截器类型：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Inside this function you get full control of all unary RPC calls that are coming
    to your gRPC server.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数内部，你可以完全控制所有发送到你的 gRPC 服务器的一元 RPC 调用。
- en: Example 5-1\. gRPC server-side unary interceptor
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-1\. gRPC 服务器端一元拦截器
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO1-1)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO1-1)'
- en: 'Preprocessing phase: this is where you can intercept the message prior to invoking
    the respective RPC.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理阶段：这是你可以在调用相应 RPC 之前拦截消息的地方。
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO1-2)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO1-2)'
- en: Invoking the RPC method via `UnaryHandler`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `UnaryHandler` 调用 RPC 方法。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO1-3)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO1-3)'
- en: 'Postprocessing phase: you can process the response from the RPC invocation.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理阶段：您可以处理来自 RPC 调用的响应。
- en: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO1-4)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO1-4)'
- en: Sending back the RPC response.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 RPC 响应。
- en: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO1-5)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO1-5)'
- en: Registering the unary interceptor with the gRPC server.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 gRPC 服务器注册一元拦截器。
- en: 'The implementation of a server-side unary interceptor can usually be divided
    into three parts: preprocessing, invoking the RPC method, and postprocessing.
    As the name implies, the preprocessor phase is executed prior to invoking the
    remote method intended in the RPC call. In the preprocessor phase, users can get
    info about the current RPC call by examining the args passed in, such as RPC context,
    RPC request, and server information. Thus, during the preprocessor phase you can
    even modify the RPC call.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器端一元拦截器的实现通常可以分为三个部分：预处理、调用 RPC 方法和后处理。顾名思义，预处理阶段在调用 RPC 调用中指定的远程方法之前执行。在预处理阶段，用户可以通过检查传递的
    args 获取有关当前 RPC 调用的信息，例如 RPC 上下文、RPC 请求和服务器信息。因此，在预处理阶段，甚至可以修改 RPC 调用。
- en: Then, in the invoker phase, you have to call the gRPC `UnaryHandler` to invoke
    the RPC method. Once you invoke the RPC, the postprocessor phase is executed.
    This means that the response for the RPC call goes through the postprocessor phase.
    In the phase, you can deal with the returned reply and error when required. Once
    the postprocessor phase is completed, you need to return the message and the error
    as the return parameters of your interceptor function. If no postprocessing is
    required, you can simply return the handler call (`handler(ctx, req)`).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在调用程序阶段，您必须调用 gRPC `UnaryHandler` 来调用 RPC 方法。一旦调用 RPC，将执行后处理阶段。这意味着 RPC 调用的响应经过后处理阶段。在此阶段，您可以在需要时处理返回的回复和错误。完成后处理阶段后，您需要将消息和错误作为拦截器函数的返回参数返回。如果不需要后处理，则可以简单地返回处理程序调用（`handler(ctx,
    req)`）。
- en: Next, let’s discuss streaming interceptors.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论流拦截器。
- en: Stream interceptor
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流拦截器
- en: The server-side streaming interceptor intercepts any streaming RPC calls that
    the gRPC server deals with. The stream interceptor includes a preprocessing phase
    and a stream operation interception phase.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器端流拦截器拦截 gRPC 服务器处理的任何流式 RPC 调用。流拦截器包括预处理阶段和流操作拦截阶段。
- en: 'As shown in the Go code snippet in [Example 5-2](#EX5-2), suppose that we want
    to intercept streaming RPC calls of the `OrderManagement` service. `StreamServerInterceptor`
    is the type for server-side stream interceptors. `orderServerStreamInterceptor`
    is an interceptor function of type `StreamServerInterceptor` with the signature:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[示例 5-2](#EX5-2)中的Go代码片段中所示，假设我们想拦截`OrderManagement`服务的流式RPC调用。`StreamServerInterceptor`是服务器端流拦截器的类型。`orderServerStreamInterceptor`是一个类型为`StreamServerInterceptor`的拦截器函数，其签名为：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Similar to a unary interceptor, in the preprocessor phase, you can intercept
    a streaming RPC call before it goes to the service implementation. After the preprocessor
    phase, you can then invoke the `StreamHandler` to complete the execution of RPC
    invocation of the remote method. After the preprocessor phase, you can intercept
    the streaming RPC message by using an interface known as a wrapper stream that
    implements the `grpc.ServerStream` interface. You can pass this wrapper structure
    when you invoke `grpc.StreamHandler` with `handler(srv, newWrappedStream(ss))`.
    The wrapper of `grpc.ServerStream` intercepts the streaming messages sent or received
    by the gRPC service. It implements the `SendMsg` and `RecvMsg` functions, which
    will be invoked when the service receives or sends an RPC streaming message.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于一元拦截器，在预处理阶段，您可以在服务实现之前拦截流式RPC调用。在预处理阶段之后，您可以调用`StreamHandler`来完成远程方法的RPC调用执行。在预处理阶段之后，您可以通过使用实现`grpc.ServerStream`接口的包装器流来拦截流式RPC消息。当您使用`handler(srv,
    newWrappedStream(ss))`调用时，可以传递此包装器结构。`grpc.ServerStream`的包装器拦截gRPC服务发送或接收的流式消息。它实现了`SendMsg`和`RecvMsg`函数，这些函数在服务接收或发送RPC流式消息时将被调用。
- en: Example 5-2\. gRPC server-side streaming interceptor
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-2\. gRPC服务器端流式拦截器
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO2-1)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO2-1)'
- en: Wrapper stream of the `grpc.ServerStream`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`grpc.ServerStream`的包装器流。'
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO2-2)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO2-2)'
- en: Implementing the `RecvMsg` function of the wrapper to process messages received
    with stream RPC.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 实现包装器的`RecvMsg`函数以处理使用流RPC接收的消息。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO2-3)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO2-3)'
- en: Implementing the `SendMsg` function of the wrapper to process messages sent
    with stream RPC.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 实现包装器的`SendMsg`函数以处理使用流RPC发送的消息。
- en: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO2-4)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO2-4)'
- en: Creating an instance of the new wrapper stream.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新包装器流的实例。
- en: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO2-5)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO2-5)'
- en: Streaming interceptor implementation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 流式拦截器实现。
- en: '[![6](assets/6.png)](#co_grpc__beyond_the_basics_CO2-6)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_grpc__beyond_the_basics_CO2-6)'
- en: Preprocessor phase.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理阶段。
- en: '[![7](assets/7.png)](#co_grpc__beyond_the_basics_CO2-7)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_grpc__beyond_the_basics_CO2-7)'
- en: Invoking the streaming RPC with the wrapper stream.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用包装器流调用流式RPC。
- en: '[![8](assets/8.png)](#co_grpc__beyond_the_basics_CO2-8)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](assets/8.png)](#co_grpc__beyond_the_basics_CO2-8)'
- en: Registering the interceptor.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注册拦截器。
- en: 'To understand the behavior of the streaming interceptor on the server side,
    look at the following output from the gRPC server logs. Based on the order in
    which each log message is printed you can identify the behavior of the streaming
    interceptor. The streaming remote method that we have invoked here is `SearchOrders`,
    which is a server-streaming RPC:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解服务器端流式拦截器的行为，请查看gRPC服务器日志中的以下输出。根据每条日志消息打印的顺序，您可以识别流式拦截器的行为。我们在这里调用的流式远程方法是`SearchOrders`，这是一个服务器流式RPC：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Client-side interceptor terminology is quite similar to that of server-side
    interceptors, with some subtle variations as to the interfaces and function signatures.
    Let’s move on to the details of client-side interceptors.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端拦截器术语与服务器端拦截器非常相似，但在接口和函数签名上有一些微妙的变化。让我们继续了解客户端拦截器的详细信息。
- en: Client-Side Interceptors
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 客户端拦截器
- en: When a client invokes an RPC call to invoke a remote method of a gRPC service,
    you can intercept those RPC calls on the client side. As shown in [Figure 5-2](#client_side_interceptors),
    with client-side interceptors, you can intercept unary RPC calls as well as streaming
    RPC calls.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当客户端调用RPC调用来调用gRPC服务的远程方法时，您可以在客户端拦截这些RPC调用。如图[5-2](#client_side_interceptors)所示，使用客户端拦截器，您可以拦截一元RPC调用以及流式RPC调用。
- en: '![Client-side Interceptors](assets/grpc_0502.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![客户端拦截器](assets/grpc_0502.png)'
- en: Figure 5-2\. Client-side interceptors
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. 客户端拦截器
- en: This is particularly useful when you need to implement certain reusable features,
    such as securely calling a gRPC service outside the client application code.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要在客户端应用程序代码之外安全地调用 gRPC 服务时，这是特别有用的。
- en: Unary interceptor
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一元拦截器
- en: 'A client-side unary RPC interceptor is used for intercepting the unary RPC
    client side. `UnaryClientInterceptor` is the type for a client-side unary interceptor
    that has a function signature as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 用于拦截客户端单一 RPC 的客户端一元拦截器。`UnaryClientInterceptor` 是一个具有以下函数签名的客户端一元拦截器的类型：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As we saw with the server-side unary interceptor, the client-side unary interceptor
    has different phases. [Example 5-3](#EX5-3) shows the basic Go implementation
    of a unary interceptor on the client side. In the preprocessor phase, you can
    intercept the RPC calls before invoking the remote method. Here you will have
    access to the information about the current RPC call by examining the args passed
    in, such as RPC context, method string, request to be sent, and `CallOptions`
    configured. So, you can even modify the original RPC call before it is sent to
    the server application. Then using the `UnaryInvoker` argument you can invoke
    the actual unary RPC. In the postprocessor phase, you can access the response
    or the error results of the RPC invocation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在服务器端一元拦截器中看到的那样，客户端一元拦截器具有不同的阶段。[示例 5-3](#EX5-3) 展示了在客户端实现一元拦截器的基本 Go 代码。在预处理阶段，你可以在调用远程方法之前拦截
    RPC 调用。在这里，你可以通过检查传入的参数（如 RPC 上下文、方法字符串、要发送的请求和配置的 `CallOptions`）来访问当前 RPC 调用的信息。因此，甚至可以修改原始
    RPC 调用，然后使用 `UnaryInvoker` 参数调用实际的一元 RPC。在后处理阶段，你可以访问 RPC 调用的响应或错误结果。
- en: Example 5-3\. gRPC client-side unary interceptor
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-3\. gRPC 客户端单一拦截器
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO3-1)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO3-1)'
- en: Preprocessing phase has access to the RPC request prior to sending it out to
    the server.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 RPC 请求发送到服务器之前，预处理阶段可以访问 RPC 请求。
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO3-2)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO3-2)'
- en: Invoking the RPC method via `UnaryInvoker`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `UnaryInvoker` 调用 RPC 方法。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO3-3)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO3-3)'
- en: Postprocessing phase where you can process the response or error results.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理阶段可以处理响应或错误结果。
- en: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO3-4)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO3-4)'
- en: Returning an error back to the gRPC client application along with a reply, which
    is passed as an argument.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 将错误返回到 gRPC 客户端应用程序，并将回复作为参数传递。
- en: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO3-5)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO3-5)'
- en: Setting up a connection to the server by passing a unary interceptor as a dial
    option.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将一元拦截器作为拨号选项来建立与服务器的连接。
- en: Registering the interceptor function is done inside the `grpc.Dial` operation
    using `grpc.WithUnaryInterceptor`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `grpc.Dial` 操作中通过 `grpc.WithUnaryInterceptor` 注册拦截器函数。
- en: Stream interceptor
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流拦截器
- en: 'The client-side streaming interceptor intercepts any streaming RPC calls that
    the gRPC client deals with. The implementation of the client-side stream interceptor
    is quite similar to that of the server side. `StreamClientInterceptor` is the
    type for a client-side stream interceptor; it is a function type with this signature:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端流拦截器用于拦截 gRPC 客户端处理的任何流式 RPC 调用。客户端流拦截器的实现与服务器端非常相似。`StreamClientInterceptor`
    是客户端流拦截器的类型，其函数类型如下：
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As shown in [Example 5-4](#EX5-4), the client-side stream interceptor implementation
    includes preprocessing and stream operation interception.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如示例 5-4 所示，客户端流拦截器的实现包括预处理和流操作拦截。
- en: Example 5-4\. gRPC client-side stream interceptor
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-4\. gRPC 客户端流拦截器
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO4-1)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO4-1)'
- en: Preprocessing phase has access to the RPC request prior to sending it out to
    the server.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 RPC 请求发送到服务器之前，预处理阶段可以访问 RPC 请求。
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO4-2)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO4-2)'
- en: Calling the passed-in streamer to get a `ClientStream`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 调用传入的流程以获取一个`ClientStream`。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO4-3)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO4-3)'
- en: Wrapping around the `ClientStream`, overloading its methods with intercepting
    logic, and returning it to the client application.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 包装 `ClientStream`，通过拦截逻辑重载其方法，并将其返回给客户端应用程序。
- en: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO4-4)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO4-4)'
- en: Wrapper stream of `grpc.ClientStream`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`grpc.ClientStream`的包装流。'
- en: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO4-5)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO4-5)'
- en: Function to intercept messages received from streaming RPC.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 拦截从流式 RPC 接收到的消息的函数。
- en: '[![6](assets/6.png)](#co_grpc__beyond_the_basics_CO4-6)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_grpc__beyond_the_basics_CO4-6)'
- en: Function to intercept messages sent from streaming RPC.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 拦截从流式 RPC 发送的消息的函数。
- en: '[![7](assets/7.png)](#co_grpc__beyond_the_basics_CO4-7)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_grpc__beyond_the_basics_CO4-7)'
- en: Registering a streaming interceptor.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注册流拦截器。
- en: Intercepting for stream operations is done via a wrapper implementation of the
    stream where you have to implement a new structure wrapping `grpc.ClientStream`.
    Here you implement two wrapped stream methods, `RecvMsg` and `SendMsg`, that can
    be used to intercept streaming messages received or sent from the client side.
    The registration of the interceptor is the same as for the unary interceptor and
    is done with the `grpc.Dial` operation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过包装 `grpc.ClientStream` 的实现来拦截流操作是通过一个新结构来实现的，该结构包装了两个被拦截的流方法 `RecvMsg` 和 `SendMsg`，这些方法可用于拦截从客户端发送或接收的流式消息。拦截器的注册与一元拦截器相同，并且在
    `grpc.Dial` 操作中完成。
- en: Let’s look at deadlines, another capability you’ll often need to apply when
    calling gRPC services from the client application.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看截止时间，这是在从客户端应用程序调用 gRPC 服务时经常需要应用的另一种能力。
- en: Deadlines
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 截止时间
- en: Deadlines and timeouts are two commonly used patterns in distributed computing.
    *Timeouts* allow you to specify how long a client application can wait for an
    RPC to complete before it terminates with an error. A timeout is usually specified
    as a duration and locally applied at each client side. For example, a single request
    may consist of multiple downstream RPCs that chain together multiple services.
    So we can apply timeouts, relative to each RPC, at each service invocation. Therefore,
    timeouts cannot be directly applied for the entire life cycle of the request.
    That’s where we need to use deadlines.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 截止时间和超时是分布式计算中常用的两种模式。*超时* 允许您指定客户端应用程序在终止并返回错误之前可以等待 RPC 完成的时间。超时通常以持续时间指定，并且在每个客户端端本地应用。例如，单个请求可能由多个下游
    RPC 组成，这些 RPC 将多个服务链在一起。因此，我们可以在每个服务调用处相对于每个 RPC 应用超时。因此，超时不能直接应用于请求的整个生命周期。这就是我们需要使用截止时间的地方。
- en: A *deadline* is expressed in absolute time from the beginning of a request (even
    if the API presents them as a duration offset) and applied across multiple service
    invocations. The application that initiates the request sets the deadline and
    the entire request chain needs to respond by the deadline. gRPC APIs supports
    using deadlines for your RPC. For many reasons, it is always good practice to
    use deadlines in your gRPC applications. gRPC communication happens over the network,
    so there can be delays between the RPC calls and responses. Also, in certain cases
    the gRPC service itself can take more time to respond depending on the service’s
    business logic. When client applications are developed without using deadlines,
    they infinitely wait for a response for RPC requests that are initiated and resources
    will be held for all in-flight requests. This puts the service as well as the
    client at risk of running out of resources, increasing the latency of the service;
    this could even crash the entire gRPC service.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*截止时间* 是从请求开始的绝对时间来表达的（即使 API 将它们表示为持续时间偏移），并且在多个服务调用之间应用。发起请求的应用程序设置截止时间，并且整个请求链需要在截止时间之前响应。gRPC
    API 支持在 RPC 中使用截止时间。出于多种原因，在您的 gRPC 应用程序中始终使用截止时间是个好习惯。gRPC 通信通过网络进行，因此在 RPC 调用和响应之间可能会存在延迟。此外，在某些情况下，根据服务的业务逻辑，gRPC
    服务本身可能需要更长时间来响应。当客户端应用程序在不使用截止时间的情况下开发时，它们会无限期地等待发起的 RPC 请求的响应，并且资源将被保留用于所有正在进行的请求。这会使服务和客户端面临资源耗尽的风险，增加服务的延迟；甚至可能导致整个
    gRPC 服务崩溃。'
- en: The example scenario shown in [Figure 5-3](#using_deadlines_when_calling_services)
    illustrates a gRPC client application calling a product management service that
    again invokes the inventory service.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 5-3](#using_deadlines_when_calling_services) 中展示的示例场景说明了一个 gRPC 客户端应用程序调用产品管理服务，该服务再次调用库存服务。
- en: The client application sets a deadline offset (i.e., deadline = current time
    + offset) of 50 ms. The network latency between the client and `ProductMgt` service
    is 0 ms and the processing latency of the `ProductMgt` service is 20 ms. The product
    management service has to set a deadline offset of 30 ms. Since the inventory
    service takes 30 ms to respond, the deadline event would occur on both client
    sides (`ProductMgt` invokes the `Inventory` service and the client application).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端应用程序设置了截止时间偏移量（即，截止时间 = 当前时间 + 偏移量），为50毫秒。客户端与`ProductMgt`服务之间的网络延迟为0毫秒，而`ProductMgt`服务的处理延迟为20毫秒。产品管理服务必须设置30毫秒的截止时间偏移量。由于库存服务需要30毫秒来响应，截止事件将在客户端双方发生（`ProductMgt`调用`Inventory`服务和客户端应用程序）。
- en: The latency added from the business logic of the `ProductMgt` service is 20
    ms. Then the `ProductMgt` service’s invocation logic triggers the deadline-exceeded
    scenario and propagates it back to the client application as well. Therefore,
    when using deadlines, make sure that they are applied across all services.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从`ProductMgt`服务的业务逻辑中添加的延迟为20毫秒。然后，`ProductMgt`服务的调用逻辑触发了超过截止日期的情况，并将其传播回客户端应用程序。因此，在使用截止日期时，请确保它们应用于所有服务。
- en: '![Using Deadlines when calling services](assets/grpc_0503.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![调用服务时使用截止日期](assets/grpc_0503.png)'
- en: Figure 5-3\. Using deadlines when calling services
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. 调用服务时使用截止日期
- en: A client application can set a deadline when it initiates a connection with
    a gRPC service. Once the RPC call is made, the client application waits for the
    duration specified by the deadline; if the response for the RPC call is not received
    within that time, the RPC call is terminated with a `DEADLINE_EXCEEDED` error.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端应用程序在初始化与 gRPC 服务的连接时可以设置截止日期。一旦发起 RPC 调用，客户端应用程序将等待由截止日期指定的持续时间；如果未在该时间内收到
    RPC 调用的响应，则以`DEADLINE_EXCEEDED`错误终止 RPC 调用。
- en: Let’s look at a real-world example of using deadlines when invoking gRPC services.
    In the same `OrderManagement` service use case, suppose the `AddOrder` RPC takes
    a significant amount of time to complete (we’ve simulated this with the introduction
    of a delay into the `AddOrder` method of the `OrderManagement` gRPC service).
    But the client application only waits until the response is no longer useful to
    it. For example, the duration that `AddOrder` takes to respond is two seconds,
    while the client only waits two seconds for a response. To implement this (as
    shown in the Go code snippet shown in [Example 5-5](#EX5-5)), the client application
    can set the two-second timeout with the `context.WithDeadline` operation. We have
    used the `status` package to process error code; we’ll discuss this in detail
    in the error-handling section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个使用 gRPC 服务调用截止日期的真实示例。在相同的`OrderManagement`服务用例中，假设`AddOrder` RPC花费了大量时间来完成（我们通过在`OrderManagement`
    gRPC服务的`AddOrder`方法中引入延迟来模拟这一点）。但是客户端应用程序只等待直到响应对其不再有用。例如，`AddOrder`响应所需的持续时间为两秒，而客户端只等待两秒钟以获取响应。为了实现这一点（如在[示例 5-5](#EX5-5)中所示的
    Go 代码片段中），客户端应用程序可以使用`context.WithDeadline`操作设置两秒钟的超时时间。我们已使用`status`包来处理错误代码；我们将在错误处理部分详细讨论这一点。
- en: Example 5-5\. gRPC deadlines for the client application
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-5\. 客户端应用程序的 gRPC 截止日期
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO5-1)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO5-1)'
- en: Setting a two-second deadline on the current context.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前上下文中设置两秒钟的截止日期。
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO5-2)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO5-2)'
- en: Invoking the `AddOrder` remote method and capturing any possible errors into
    `addErr`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`AddOrder`远程方法，并将可能的任何错误捕获到`addErr`中。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO5-3)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO5-3)'
- en: Using the status package to determine the error code.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`status`包来确定错误代码。
- en: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO5-4)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO5-4)'
- en: If the invocation exceeds the specified deadline, it should return an error
    of the type `DEADLINE_EXCEEDED`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果调用超过指定的截止时间，应返回类型为`DEADLINE_EXCEEDED`的错误。
- en: So how should we determine the ideal value for the deadline? There is no single
    answer to that question, but you need to consider several factors in making that
    choice; mainly, the end-to-end latency of each service that we invoke, which RPCs
    are serial and which RPCs can be made in parallel, the latency of the underlying
    network, and the deadline values of the downstream services. Once you are able
    to come up with the initial value for the deadline, fine-tune it based on the
    operating condition of the gRPC applications.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们应该如何确定截止时间的理想值呢？对于这个问题没有单一的答案，但你需要考虑几个因素来做出选择；主要是我们调用的每个服务的端到端延迟，哪些RPC是串行的，哪些可以并行执行，底层网络的延迟以及下游服务的截止时间值。一旦你能够确定截止时间的初始值，根据gRPC应用程序的操作条件进行微调。
- en: Note
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Setting the gRPC deadline in Go is done through Go’s [`context` package](https://oreil.ly/OTrmY),
    where `WithDeadline` is a built-in function. In Go, context is often used to pass
    down common data that can be used by all downstream operations. Once this is called
    from the gRPC client application, the gRPC library at the client side creates
    a required gRPC header to represent the deadline between the client and server
    applications. In Java, this is slightly different, as the implementation directly
    comes from the `io.grpc.stub.*` package’s stub implementation where you will set
    the gRPC deadline with `blockingStub.withDeadlineAfter(long, java.util.concurrent.TimeUnit)`.
    Please refer to the code repository for details of the Java implementation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 设置在Go语言中gRPC的截止时间是通过Go的[`context`包](https://oreil.ly/OTrmY)完成的，其中`WithDeadline`是一个内置函数。在Go中，上下文经常用于传递可以被所有下游操作使用的共同数据。一旦这个函数从gRPC客户端应用程序调用，客户端侧的gRPC库将创建一个必需的gRPC头来表示客户端和服务器应用程序之间的截止时间。在Java中，这有些不同，因为实现直接来自`io.grpc.stub.*`包的存根实现，你可以使用`blockingStub.withDeadlineAfter(long,
    java.util.concurrent.TimeUnit)`来设置gRPC的截止时间。详细的Java实现请参考代码库。
- en: When it comes to deadlines in gRPC, both the client and server can make their
    own independent and local determination about whether the RPC was successful;
    this means their conclusions may not match. For instance, in our example, when
    the client meets the `DEADLINE_EXCEEDED` condition, the service may still try
    to respond. So, the service application needs to determine whether the current
    RPC is still valid or not. From the server side, you can also detect when the
    client has reached the deadline specified when invoking the RPC. Inside the `AddOrder`
    operation, you can check for `ctx.Err() == context.DeadlineExceeded` to find out
    whether the client has already met the deadline exceeded state, and then abandon
    the RPC at the server side and return an error (this is often implemented using
    a nonblocking `select` construct in Go).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到gRPC中的截止时间时，客户端和服务器都可以独立地进行关于RPC是否成功的本地决定；这意味着它们的结论可能不匹配。例如，在我们的例子中，当客户端遇到`DEADLINE_EXCEEDED`条件时，服务可能仍然尝试响应。因此，服务应用程序需要确定当前RPC是否仍然有效。从服务器端来看，你还可以检测客户端在调用RPC时已达到的截止时间。在`AddOrder`操作中，你可以检查`ctx.Err()
    == context.DeadlineExceeded`来判断客户端是否已经遇到截止时间过期状态，然后在服务器端放弃RPC并返回错误（这通常使用非阻塞的`select`结构在Go中实现）。
- en: Similar to deadlines, there can be certain situations in which your client or
    server application wants to terminate the ongoing gRPC communication. This is
    where gRPC cancellation becomes useful.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于截止时间，有些情况下你的客户端或服务器应用程序希望终止正在进行的gRPC通信。这时候gRPC的取消功能就变得非常有用。
- en: Cancellation
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 取消
- en: In a gRPC connection between a client and server application, both the client
    and server make independent and local determinations of the success of the call.
    For instance, you could have an RPC that finishes successfully on the server side
    but fails on the client side. Similarly, there can be various conditions where
    the client and server may end up with different conclusions on the results of
    an RPC. When either the client or server application wants to terminate the RPC
    this can be done by *canceling* the RPC. Once the RPC is canceled, no further
    RPC-related messaging can be done and the fact that one party has canceled the
    RPC is propagated to the other side.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端和服务器应用程序之间的 gRPC 连接中，客户端和服务器都会独立和本地地确定调用的成功情况。例如，服务器端可以成功完成一个 RPC，但客户端端可能失败。同样地，客户端和服务器可能会因为各种条件而对
    RPC 的结果得出不同的结论。当客户端或服务器应用程序想要终止 RPC 时，可以通过 *取消* RPC 来实现。一旦 RPC 被取消，将无法进行更多与 RPC
    相关的消息传递，并且取消 RPC 的一方会向另一方传播这一事实。
- en: Note
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: In Go, similar to deadlines, the cancellation capability is provided via the
    [`context` package](https://oreil.ly/OTrmY) where `WithCancel` is a built-in function.
    Once this is called from the gRPC application, the gRPC library on the client
    side creates a required gRPC header to represent the gRPC termination between
    the client and server applications.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Go 中，类似于截止日期，取消功能是通过 [`context` 包](https://oreil.ly/OTrmY) 提供的，其中 `WithCancel`
    是一个内置函数。一旦从 gRPC 应用程序中调用它，客户端的 gRPC 库会创建一个必要的 gRPC 头来表示客户端和服务器应用程序之间的 gRPC 终止。
- en: Let’s take the example of bidirectional streaming between the client and server
    applications. In the Go code sample shown in [Example 5-6](#EX5-6), you can obtain
    the `cancel` function from the `context.WithTimeout` call. Once you have the reference
    to `cancel`, you can call it at any location where you intend to terminate the
    RPC.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以客户端和服务器应用程序之间的双向流式处理为例。在所示的 Go 代码示例中 [Example 5-6](#EX5-6) 中，您可以从 `context.WithTimeout`
    调用中获取 `cancel` 函数。一旦获得了 `cancel` 的引用，您可以在任何想要终止 RPC 的地方调用它。
- en: Example 5-6\. gRPC cancellation
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 5-6\. gRPC 取消
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO6-1)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO6-1)'
- en: Obtaining the reference to cancel.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 获取取消的引用。
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO6-2)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO6-2)'
- en: Invoking the streaming RPC.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 调用流式 RPC。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO6-3)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO6-3)'
- en: Sending messages to the service via the stream.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 通过流向服务发送消息。
- en: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO6-4)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO6-4)'
- en: Canceling RPC/terminating RPC from the client side.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 从客户端取消 RPC/终止 RPC。
- en: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO6-5)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO6-5)'
- en: Status of the current context.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当前上下文的状态。
- en: '[![6](assets/6.png)](#co_grpc__beyond_the_basics_CO6-6)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_grpc__beyond_the_basics_CO6-6)'
- en: Returning context canceled error when trying to receive messages from a canceled
    context.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当尝试从取消的上下文接收消息时返回上下文取消错误。
- en: When one party cancels the RPC, the other party can determine it by checking
    the context. In this example, the server application can check whether the current
    context is canceled by using `stream.Context().Err() == context.Canceled`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当一方取消 RPC 时，另一方可以通过检查上下文确定。在这个例子中，服务器应用程序可以通过 `stream.Context().Err() == context.Canceled`
    来检查当前上下文是否已取消。
- en: As you have seen in the application of deadlines as well as cancellation, handling
    errors with RPC is a very common requirement. In the next section, we look at
    gRPC error-handling techniques in detail.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在截止日期和取消应用中所见，处理 RPC 错误是一个非常常见的要求。在接下来的章节中，我们将详细讨论 gRPC 错误处理技术。
- en: Error Handling
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 错误处理
- en: When we invoke a gRPC call, the client receives a response with a successful
    status or an error with the corresponding error status. The client application
    needs to be written in such a way that you handle all the potential errors and
    error conditions. The server application requires you to handle errors as well
    as generate the appropriate errors with corresponding status codes.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们调用 gRPC 时，客户端会收到一个带有成功状态或相应错误状态的响应。客户端应用程序需要编写成这样的方式，能够处理所有可能的错误和错误条件。服务器应用程序需要处理错误，并生成相应的状态码。
- en: When an error occurs, gRPC returns one of its error-status codes with an optional
    error message that provides more details of the error condition. The status object
    is composed of an integer code and a string message that are common to all gRPC
    implementations for different languages.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当发生错误时，gRPC 返回其错误状态码之一，并可选择性地提供错误消息，以提供更多错误条件的详细信息。状态对象由整数代码和字符串消息组成，这在所有不同语言的
    gRPC 实现中都是通用的。
- en: 'gRPC uses a set of well-defined gRPC-specific status codes. This includes status
    codes such as the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC 使用一组明确定义的特定于 gRPC 的状态代码。这包括诸如以下状态代码：
- en: '`OK`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`成功`'
- en: Successful status; not an error.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 成功状态；不是错误。
- en: '`CANCELLED`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`已取消`'
- en: The operation was canceled, typically by the caller.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 操作通常由调用者取消。
- en: '`DEADLINE_EXCEEDED`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`超出期限`'
- en: The deadline expired before the operation could complete.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 操作在完成之前已超出期限。
- en: '`INVALID_ARGUMENT`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`无效参数`'
- en: The client specified an invalid argument.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端指定了无效参数。
- en: '[Table 5-1](#grpc_error_codes) shows the available gRPC error codes and the
    description of each error code. The complete list of error codes can be found
    [in the gRPC official documentation](https://oreil.ly/LiNLn), or in the documentation
    for [Go](https://oreil.ly/E61Q0) and [Java](https://oreil.ly/Ugtg0).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 5-1](#grpc_error_codes) 显示了可用的 gRPC 错误代码及每个错误代码的描述。完整的错误代码列表可以在 [gRPC 官方文档](https://oreil.ly/LiNLn)
    中找到，或者在 [Go](https://oreil.ly/E61Q0) 和 [Java](https://oreil.ly/Ugtg0) 的文档中找到。'
- en: Table 5-1\. gRPC error codes
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-1\. gRPC 错误代码
- en: '| Code | Number | Description |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 代码 | 编号 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| OK | 0 | Success status. |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 成功 | 0 | 成功状态。 |'
- en: '| CANCELLED | 1 | The operation was canceled (by the caller). |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 已取消 | 1 | 操作已被取消（由调用者）。 |'
- en: '| UNKNOWN | 2 | Unknown error. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 未知错误 | 2 | 未知错误。 |'
- en: '| INVALID_ARGUMENT | 3 | The client specified an invalid argument. |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 无效参数 | 3 | 客户端指定了无效参数。 |'
- en: '| DEADLINE_EXCEEDED | 4 | The deadline expired before the operation could complete.
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 超出期限 | 4 | 操作在完成之前已超出期限。 |'
- en: '| NOT_FOUND | 5 | Some requested entity was not found. |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 未找到 | 5 | 未找到某些请求的实体。 |'
- en: '| ALREADY_EXISTS | 6 | The entity that a client attempted to create already
    exists. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 已存在 | 6 | 客户端试图创建的实体已存在。 |'
- en: '| PERMISSION_DENIED | 7 | The caller does not have permission to execute the
    specified operation. |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 拒绝许可 | 7 | 调用者无权限执行指定的操作。 |'
- en: '| UNAUTHENTICATED | 16 | The request does not have valid authentication credentials
    for the operation. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 未经身份验证 | 16 | 请求没有为操作提供有效的身份验证凭据。 |'
- en: '| RESOURCE_EXHAUSTED | 8 | Some resource has been exhausted. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 资源耗尽 | 8 | 某些资源已耗尽。 |'
- en: '| FAILED_PRECONDITION | 9 | The operation was rejected because the system is
    not in a state required for the operation’s execution. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 前提条件失败 | 9 | 操作被拒绝，因为系统不处于操作执行所需的状态。 |'
- en: '| ABORTED | 10 | The operation was aborted. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 已中止 | 10 | 操作已中止。 |'
- en: '| OUT_OF_RANGE | 11 | The operation was attempted past the valid range. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 超出范围 | 11 | 尝试的操作超出了有效范围。 |'
- en: '| UNIMPLEMENTED | 12 | The operation is not implemented or is not supported/enabled
    in this service. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 未实现 | 12 | 操作未实现或不受此服务支持/启用。 |'
- en: '| INTERNAL | 13 | Internal errors. |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 内部错误 | 13 | 内部错误。 |'
- en: '| UNAVAILABLE | 14 | The service is currently unavailable. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 服务不可用 | 14 | 当前服务不可用。 |'
- en: '| DATA_LOSS | 15 | Unrecoverable data loss or corruption. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 数据丢失 | 15 | 不可恢复的数据丢失或损坏。 |'
- en: The error model provided with gRPC out of the box is quite limited and independent
    of the underlying gRPC data format (where the most common format is protocol buffers).
    If you are using protocol buffers as your data format then you can leverage the
    richer error model the Google APIs provide under the `google.rpc` package. However,
    the error model is supported only in the C++, Go, Java, Python, and Ruby libraries,
    so be mindful of this if you plan to use other languages than these.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC 提供的默认错误模型相当有限，并且独立于底层 gRPC 数据格式（其中最常见的格式是协议缓冲区）。如果您正在使用协议缓冲区作为数据格式，则可以利用
    Google API 在 `google.rpc` 包下提供的更丰富的错误模型。然而，该错误模型仅在 C++、Go、Java、Python 和 Ruby 库中受支持，因此如果您计划使用其他语言，请注意这一点。
- en: Let’s look at how these concepts can be used in a real-world gRPC error-handling
    use case. In our order management use case, suppose that we need to handle a request
    with invalid order IDs in the `AddOrder` remote method. As shown in [Example 5-7](#EX5-7),
    suppose that if the given order ID equals `-1` then you need to generate an error
    and return it to the consumer.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在实际的 gRPC 错误处理用例中使用这些概念。在我们的订单管理用例中，假设在`AddOrder`远程方法中处理请求时，我们需要处理一个具有无效订单
    ID 的请求。如[示例 5-7](#EX5-7)所示，假设给定的订单 ID 等于`-1`，则需要生成一个错误并返回给消费者。
- en: Example 5-7\. Error creation and propagation on the server side
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-7\. 服务器端错误创建和传播
- en: '[PRE11]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO7-1)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO7-1)'
- en: Invalid request, needs to generate an error and send it back to the client.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 无效请求，需要生成错误并将其发送回客户端。
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO7-2)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO7-2)'
- en: Create a new error status with error code `InvalidArgument`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个带有错误码`InvalidArgument`的新错误状态。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO7-3)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO7-3)'
- en: Include any error details with an error type `BadRequest_FieldViolation` from
    `google.golang.org/genproto/googleapis/rpc/errdetails`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 包含来自`google.golang.org/genproto/googleapis/rpc/errdetails`的错误类型为`BadRequest_FieldViolation`的任何错误详细信息。
- en: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO7-4)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO7-4)'
- en: Returning the generated error.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 返回生成的错误。
- en: You can simply create an error status from `grpc.status` packages with the required
    error code and details. In the example here we have used `status.New(codes.InvalidArgument,
    "Invalid information received")`. You just need to send this error back to the
    client with `return nil, errorStatus.Err()`. However, to include a richer error
    model, you can use Google API’s `google.rpc` package. In this example, we have
    set an error detail with a specific error type from *google.golang.org/genproto/googleapis/rpc/errdetails*.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以简单地从`grpc.status`包创建一个错误状态，包括所需的错误码和详细信息。在这个例子中，我们使用了`status.New(codes.InvalidArgument,
    "接收到无效信息")`。您只需将此错误发送回客户端`return nil, errorStatus.Err()`。但是，如果需要包含更丰富的错误模型，您可以使用Google
    API的`google.rpc`包。在本例中，我们设置了来自*google.golang.org/genproto/googleapis/rpc/errdetails*的特定错误类型的错误详细信息。
- en: For error handling on the client side, you simply process the error returned
    as part of your RPC invocation. For example, in [Example 5-8](#EX5-8), you can
    find the Go implementation of the client application of this order management
    use case. Here we invoked the `AddOrder` method and assigned the returned error
    to the `addOrderError` variable. So, the next step is to inspect the results of
    `addOrderError` and gracefully handle the error. For that, you can obtain the
    error code and specific error type that we have set from the server side.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对于客户端端错误处理，您只需处理作为RPC调用的一部分返回的错误。例如，在[示例 5-8](#EX5-8)中，您可以找到该订单管理用例的客户端应用程序的
    Go 实现。在这里，我们调用了`AddOrder`方法并将返回的错误赋给`addOrderError`变量。因此，下一步是检查`addOrderError`的结果并优雅地处理错误。为此，您可以获取从服务器端设置的错误码和特定错误类型。
- en: Example 5-8\. Error handling on the client side
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-8\. 客户端端错误处理
- en: '[PRE12]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO8-1)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO8-1)'
- en: This is an invalid order.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个无效的订单。
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO8-2)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO8-2)'
- en: Invoke the `AddOrder` remote method and assign the error to `addOrderError`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`AddOrder`远程方法并将错误赋给`addOrderError`。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO8-3)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO8-3)'
- en: Obtain the error code using the `grpc/status` package.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`grpc/status`包获取错误码。
- en: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO8-4)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO8-4)'
- en: Check for `InvalidArgument` error code.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 检查`InvalidArgument`错误码。
- en: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO8-5)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO8-5)'
- en: Obtain the error status from the error.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 从错误中获取错误状态。
- en: '[![6](assets/6.png)](#co_grpc__beyond_the_basics_CO8-6)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_grpc__beyond_the_basics_CO8-6)'
- en: Check for `BadRequest_FieldViolation` error type.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 检查`BadRequest_FieldViolation`错误类型。
- en: It’s always good practice to use the appropriate gRPC error codes and a richer
    error model whenever possible for your gRPC applications. gRPC error status and
    details are normally sent via the trailer headers at the transport protocol level.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的 gRPC 应用程序中，始终使用适当的 gRPC 错误码和更丰富的错误模型是一个良好的实践。gRPC 错误状态和详细信息通常通过传输协议级别的尾部标头发送。
- en: Now let’s look at multiplexing, a service-hosting mechanism on the same gRPC
    server runtime.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看多路复用，这是在同一个 gRPC 服务器运行时的一种服务托管机制。
- en: Multiplexing
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多路复用
- en: In terms of gRPC services and client applications, we’ve seen so far a given
    gRPC server with a gRPC service registered on it and a gRPC client connection
    being used by a single client stub only. However, gRPC allows you to run multiple
    gRPC services on the same gRPC server (see [Figure 5-4](#muliplexing_multiple_grpc_services_in_the_same_server_application)),
    as well as use the same gRPC client connection for multiple gRPC client stubs.
    This capability is known as *multiplexing*.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 就 gRPC 服务和客户端应用程序而言，我们已经看到了一个特定的 gRPC 服务器上注册了一个 gRPC 服务，并且一个 gRPC 客户端连接仅被单个客户端存根使用的情况。但是，gRPC
    允许您在同一个 gRPC 服务器上运行多个 gRPC 服务（请参见 [Figure 5-4](#muliplexing_multiple_grpc_services_in_the_same_server_application)），并且可以为多个
    gRPC 客户端存根使用相同的 gRPC 客户端连接。这种能力被称为*多路复用*。
- en: '![Multiplexing multiple gRPC services in the same server application](assets/grpc_0504.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![在同一服务器应用程序中复用多个 gRPC 服务](assets/grpc_0504.png)'
- en: Figure 5-4\. Multiplexing multiple gRPC services in the same server application
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. 在同一服务器应用程序中复用多个 gRPC 服务
- en: For example, in our `OrderManagement` service example, suppose that you want
    to run another service that is required for order-management purposes on the same
    gRPC server, so that the client application can reuse the same connection to invoke
    both the services as required. Then you can register both services on the same
    gRPC server by using their respective server register functions (i.e., `ordermgt_pb.RegisterOrderManagementServer`
    and `hello_pb.RegisterGreeterServer`). Using this method, you can register one
    or more gRPC services on the same gRPC server (as shown in [Example 5-9](#EX5-9)).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的 `OrderManagement` 服务示例中，假设您希望在同一个 gRPC 服务器上运行另一个服务，以便客户端应用程序可以重用同一连接来调用两个服务，那么您可以通过使用它们各自的服务器注册函数（即
    `ordermgt_pb.RegisterOrderManagementServer` 和 `hello_pb.RegisterGreeterServer`）来同时在同一个
    gRPC 服务器上注册这两个服务。使用这种方法，您可以在同一个 gRPC 服务器上注册一个或多个 gRPC 服务（如示例 [Example 5-9](#EX5-9)
    所示）。
- en: Example 5-9\. Two gRPC services sharing the same grpc.Server
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-9\. 两个共享相同 grpc.Server 的 gRPC 服务
- en: '[PRE13]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO9-1)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO9-1)'
- en: Creating the gRPC server.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 gRPC 服务器。
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO9-2)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO9-2)'
- en: Registering the `OrderManagement` service with the gRPC server.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在 gRPC 服务器上注册 `OrderManagement` 服务。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO9-3)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO9-3)'
- en: Registering the `Hello` service with the same gRPC server.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一个 gRPC 服务器上注册 `Hello` 服务。
- en: Similarly, from the client side you can share the same gRPC connection between
    two gRPC client stubs.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，从客户端方面，您可以在两个 gRPC 客户端存根之间共享相同的 gRPC 连接。
- en: As shown in [Example 5-10](#EX5-10), since both gRPC services are running in
    the same gRPC server, you can create a gRPC connection and use it when creating
    the gRPC client instance for different services.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如示例 [Example 5-10](#EX5-10) 所示，由于两个 gRPC 服务在同一个 gRPC 服务器上运行，您可以创建一个 gRPC 连接，并在为不同服务创建
    gRPC 客户端实例时使用它。
- en: Example 5-10\. Two gRPC client stubs sharing the same grpc.ClientConn
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-10\. 两个共享相同 grpc.ClientConn 的 gRPC 客户端存根
- en: '[PRE14]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO10-1)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO10-1)'
- en: Creating a gRPC connection.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 gRPC 连接。
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO10-2)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO10-2)'
- en: Using the created gRPC connection to create an `OrderManagement` client.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 使用创建的 gRPC 连接创建 `OrderManagement` 客户端。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO10-3)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO10-3)'
- en: Using the same gRPC connection to create the `Hello` service client.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 使用同一 gRPC 连接创建 `Hello` 服务客户端。
- en: Running multiple services or using the same connection between multiple stubs
    is a design choice that is independent of gRPC concepts. In most everyday use
    cases such as microservices, it is quite common to not share the same gRPC server
    instance between two services.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个服务运行或在多个存根之间使用相同连接是独立于 gRPC 概念的设计选择。在大多数日常用例中，如微服务，将同一 gRPC 服务器实例共享给两个服务是非常常见的。
- en: Note
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: One powerful use for gRPC multiplexing in a microservice architecture is to
    host multiple major versions of the same service in one server process. This allows
    a service to accommodate legacy clients after a breaking API change. Once the
    old version of the service contract is no longer in use, it can be removed from
    the server.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在微服务架构中，gRPC多路复用的一个强大用途是在一个服务器进程中托管同一服务的多个主要版本。这样一来，在API变更后，服务可以为旧客户端提供支持。一旦不再使用旧版本的服务契约，就可以从服务器中删除。
- en: In the next section, we’ll talk about how to exchange data that is not part
    of RPC parameters and responses between client and service applications.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将讨论如何在客户端和服务应用程序之间交换不属于RPC参数和响应的数据。
- en: Metadata
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元数据
- en: gRPC applications usually share information via RPC calls between gRPC services
    and consumers. In most cases, information directly related to the service’s business
    logic and consumer is part of the remote method invocation arguments. However,
    in certain conditions, you may want to share information about the RPC calls that
    are not related to the business context of the RPC, so they shouldn’t be part
    of the RPC arguments. In such cases, you can use *gRPC metadata* that you can
    send or receive from either the gRPC service or the gRPC client. As illustrated
    in [Figure 5-5](#exchanging_grpc_metadata_between_client_and_server_applications),
    the metadata that you create on either the client or server side can be exchanged
    between the client and server applications using gRPC headers. Metadata is structured
    in the form of a list of key(string)/value pairs.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC应用程序通常通过RPC调用在gRPC服务和消费者之间共享信息。在大多数情况下，与服务的业务逻辑和消费者直接相关的信息是远程方法调用参数的一部分。然而，在某些情况下，您可能希望共享与RPC调用不相关的关于RPC调用的信息，因此它们不应作为RPC参数的一部分。在这种情况下，您可以使用*gRPC元数据*，您可以从gRPC服务或gRPC客户端发送或接收它们。正如在[图5-5](#exchanging_grpc_metadata_between_client_and_server_applications)中所示，您在客户端或服务器端创建的元数据可以通过gRPC头在客户端和服务器应用程序之间进行交换。元数据的结构形式为键（字符串）/值对的列表。
- en: One of the most common usages of metadata is to exchange security headers between
    gRPC applications. Similarly, you can use it to exchange any such information
    between gRPC applications. Often gRPC metadata APIs are heavily used inside the
    interceptors that we develop. In the next section, we’ll explore how gRPC supports
    sending metadata between the client and server.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据的最常见用途之一是在gRPC应用程序之间交换安全头信息。同样，您可以用它来在gRPC应用程序之间交换任何此类信息。通常，gRPC元数据API在我们开发的拦截器中被大量使用。在接下来的部分中，我们将探讨gRPC如何支持在客户端和服务器之间发送元数据。
- en: '![Exchanging gRPC metadata between client and server applications. ](assets/grpc_0505.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![在客户端和服务器应用程序之间交换gRPC元数据。](assets/grpc_0505.png)'
- en: Figure 5-5\. Exchanging gRPC metadata between client and server applications
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-5\. 在客户端和服务器应用程序之间交换gRPC元数据
- en: Creating and Retrieving Metadata
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建和检索元数据
- en: 'The creation of metadata from a gRPC application is quite straightforward.
    In the following Go code snippet, you will find two ways of creating metadata.
    Metadata is represented as a normal map in Go and can be created with the format
    `metadata.New(map[string]string{"key1": "val1", "key2": "val2"})`. Also, you can
    use `metadata.Pairs` to create metadata in pairs, so that metadata with the same
    key will get merged into a list:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '从gRPC应用程序创建元数据非常简单。在以下Go代码片段中，您将找到创建元数据的两种方法。在Go中，元数据被表示为普通的映射，并且可以使用`metadata.New(map[string]string{"key1":
    "val1", "key2": "val2"})`格式创建。此外，您还可以使用`metadata.Pairs`以键值对的方式创建元数据，因此具有相同键的元数据将合并为列表：'
- en: '[PRE15]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can also set binary data as metadata values. The binary data that we set
    as metadata values will be base64 encoded before sending, and will be decoded
    after being transferred.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将二进制数据设置为元数据值。在发送之前，我们设置为元数据值的二进制数据将进行base64编码，并在传输后进行解码。
- en: 'Reading metadata from either the client or server side can be done using the
    incoming context of the RPC call with `metadata.FromIncomingContext(ctx)`, which
    returns the metadata map in Go:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 从客户端或服务器端读取元数据可以通过RPC调用的传入上下文和`metadata.FromIncomingContext(ctx)`来完成，该方法在Go语言中返回元数据映射：
- en: '[PRE16]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now let’s dive into how metadata sending and receiving happens on the client
    or server side for different unary and streaming RPC styles.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入探讨客户端或服务器端不同一元和流式RPC样式中的元数据发送和接收过程。
- en: 'Sending and Receiving Metadata: Client Side'
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发送和接收元数据：客户端端
- en: You can send metadata from the client side to the gRPC service by creating metadata
    and setting it into the context of the RPC call. In a Go implementation, you can
    do this in two different ways. As shown in [Example 5-11](#EX5-11), you can create
    a new context with the new metadata using `NewOutgoingContext`, or simply append
    the metadata to the existing context using `AppendToOutgoingContext`. Using `NewOutgoingContext`,
    however, replaces any existing metadata in the context. Once you create a context
    with the required metadata, it can be used either for unary or streaming RPC.
    As you learned in [Chapter 4](ch04.html#ch_04), the metadata that you set in the
    context is translated into gRPC headers (on HTTP/2) or trailers at the wire level.
    So when the client sends those headers they are received by the recipient as headers.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过创建元数据并将其设置到 RPC 调用的上下文中，从客户端向 gRPC 服务发送元数据。在 Go 实现中，您可以通过两种不同的方式完成这个操作。如
    [示例 5-11](#EX5-11) 所示，您可以使用 `NewOutgoingContext` 创建带有新元数据的新上下文，或者只需将元数据附加到现有上下文中使用
    `AppendToOutgoingContext`。但是，使用 `NewOutgoingContext` 将替换上下文中的任何现有元数据。一旦您创建了带有所需元数据的上下文，它可以用于一元或流式
    RPC。正如您在 [第四章](ch04.html#ch_04) 中学到的那样，您在上下文中设置的元数据会被转换为 gRPC 标头（在 HTTP/2 中）或者在传输层级别上的尾部。因此，当客户端发送这些标头时，接收方将其作为标头接收。
- en: Example 5-11\. Sending metadata from the gRPC client side
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-11\. 从 gRPC 客户端端发送元数据
- en: '[PRE17]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO11-1)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO11-1)'
- en: Creating metadata.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 创建元数据。
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO11-2)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO11-2)'
- en: Creating a new context with the new metadata.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用新元数据创建一个新上下文。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO11-3)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO11-3)'
- en: Appending some more metadata to the existing context.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 将一些额外的元数据附加到现有上下文中。
- en: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO11-4)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO11-4)'
- en: Unary RPC using the new context with the metadata.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 使用新元数据的一元 RPC。
- en: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO11-5)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO11-5)'
- en: The same context can be used for a streaming RPC, too.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的上下文也可以用于流式 RPC。
- en: Therefore, when it comes to receiving metadata from the client side, you need
    to treat them as either headers or trailers. In [Example 5-12](#EX5-12), you can
    find Go code examples on receiving metadata for both unary and streaming RPC styles.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在从客户端接收元数据时，您需要将它们视为标头或尾部。在 [示例 5-12](#EX5-12) 中，您可以找到有关一元和流式 RPC 样式接收元数据的
    Go 代码示例。
- en: Example 5-12\. Reading metadata on the gRPC client side
  id: totrans-268
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-12\. 在 gRPC 客户端端读取元数据
- en: '[PRE18]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO12-1)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO12-1)'
- en: Variable to store header and trailer returned from the RPC call.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 用于存储从 RPC 调用返回的标头和尾部的变量。
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO12-2)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO12-2)'
- en: Pass header and trailer reference to store the returned values for unary RPC.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 传递标头和尾部引用以存储一元 RPC 返回的值。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO12-3)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO12-3)'
- en: Getting the headers from the stream.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 从流中获取标头。
- en: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO12-4)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO12-4)'
- en: Getting the trailers from the stream. Trailers are used to send status codes
    and the status message.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 从流中获取尾部。尾部用于发送状态码和状态消息。
- en: Once the values are obtained from the respective RPC operations, you can process
    them as a generic map and process the required metadata.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦从各自的 RPC 操作中获取了值，您可以将它们作为通用映射处理并处理所需的元数据。
- en: Now let’s move on to metadata handling on the server side.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向服务器端的元数据处理。
- en: 'Sending and Receiving Metadata: Server Side'
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发送和接收元数据：服务器端
- en: Receiving metadata on the server side is quite straightforward. Using Go, you
    can simply obtain the metadata with `metadata.FromIncomingContext(ctx)` inside
    your remote method implementations (see [Example 5-13](#EX13)).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务器端接收元数据非常简单。在 Go 中，您可以在远程方法实现中使用 `metadata.FromIncomingContext(ctx)` 简单获取元数据（参见
    [示例 5-13](#EX13)）。
- en: Example 5-13\. Reading metadata on the gRPC server side
  id: totrans-282
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-13\. 在 gRPC 服务器端读取元数据
- en: '[PRE19]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO13-1)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO13-1)'
- en: Unary RPC.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 一元 RPC。
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO13-2)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO13-2)'
- en: Read the metadata map from the incoming context of the remote method.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 从远程方法的传入上下文中读取元数据映射。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO13-3)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO13-3)'
- en: Streaming RPC.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 流式 RPC。
- en: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO13-4)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO13-4)'
- en: Obtain the context from the stream and read metadata from it.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 从流中获取上下文并从中读取元数据。
- en: To send metadata from the server side, send a header with metadata or set a
    trailer with metadata. The metadata creation method is the same as what we discussed
    in the previous section. In [Example 5-14](#EX5-14), you can find Go code examples
    of sending metadata from a unary and a streaming remote method implementation
    on the server side.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 要从服务器端发送元数据，请使用带有元数据的头部或设置带有元数据的尾部。元数据创建方法与我们在前一节中讨论的相同。在[示例 5-14](#EX5-14)中，您可以找到有关在服务器端实现一元和流式远程方法发送元数据的
    Go 代码示例。
- en: Example 5-14\. Sending metadata from the gRPC server side
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-14\. 从 gRPC 服务器端发送元数据
- en: '[PRE20]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO14-1)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO14-1)'
- en: Send metadata as a header.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在流的头部发送元数据。
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO14-2)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO14-2)'
- en: Send metadata along with the trailer.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在流的尾部发送元数据。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO14-3)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO14-3)'
- en: Send metadata as a header in the stream.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在流中作为头部发送元数据。
- en: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO14-4)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO14-4)'
- en: Send metadata along with the trailer of the stream.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在流的尾部发送元数据。
- en: In both the unary and streaming cases, you can send metadata using the `grpc.SendHeader`
    method. If you want to send metadata as part of the trailer, you need to set the
    metadata as part of the trailer of the context using the `grpc.SetTrailer` or
    `SetTrailer` method of the respective stream.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是一元的还是流式的情况，你都可以使用`grpc.SendHeader`方法发送元数据。如果你希望将元数据作为尾部的一部分发送，则需要使用上下文的尾部设置元数据，使用相应流的`grpc.SetTrailer`或`SetTrailer`方法。
- en: 'Now let’s discuss another commonly used technique when calling gRPC applications:
    name resolving.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论调用 gRPC 应用程序时另一种常用的技术：名称解析。
- en: Name Resolver
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 名称解析器
- en: A *name resolver* takes a service name and returns a list of IPs of the backends.
    The resolver used in [Example 5-15](#EX5-16) resolves `lb.example.grpc.io` to
    `localhost:50051` and `localhost:50052`.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '*名称解析器*接受服务名称并返回后端的 IP 地址列表。在[示例 5-15](#EX5-16)中使用的解析器将`lb.example.grpc.io`解析为`localhost:50051`和`localhost:50052`。'
- en: Example 5-15\. gRPC name resolver implementation in Go
  id: totrans-307
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-15\. 在 Go 中实现的 gRPC 名称解析器
- en: '[PRE21]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO15-1)'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO15-1)'
- en: Name resolver builder that creates the resolver.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 创建解析器的名称解析器构建器。
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO15-2)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO15-2)'
- en: Creating the example resolver that resolves `lb.example.grpc.io`.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 创建解析`lb.example.grpc.io`的示例解析器。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO15-3)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO15-3)'
- en: This resolves `lb.example.grpc.io` to `localhost:50051` and `localhost:50052`.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这将`lb.example.grpc.io`解析为`localhost:50051`和`localhost:50052`。
- en: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO15-4)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_grpc__beyond_the_basics_CO15-4)'
- en: This resolver is created for scheme `example`.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 此解析器适用于 scheme `example`。
- en: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO15-5)'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_grpc__beyond_the_basics_CO15-5)'
- en: Structure of the name resolver.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 名称解析器的结构。
- en: Thus, based on this name resolver implementation, you can implement resolvers
    for any service registry of your choice such as [Consul](https://www.consul.io),
    [etcd](https://etcd.io), and [Zookeeper](https://zookeeper.apache.org). The gRPC
    load-balancing requirements may be quite dependent on the deployment patterns
    that you use or on the use cases. With the increasing adoption of container orchestration
    platforms such as Kubernetes and more higher-level abstractions such as service
    mesh, the need to implement load-balancing logic on the client side is becoming
    quite rare. We’ll explore some best practices for deploying gRPC applications
    locally on containers, as well as Kubernetes, in [Chapter 7](ch07.html#ch_07).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基于此名称解析器的实现，您可以为任何服务注册表（如[Consul](https://www.consul.io)、[etcd](https://etcd.io)和[Zookeeper](https://zookeeper.apache.org)）实现解析器。
    gRPC 负载均衡需求可能与您使用的部署模式或用例密切相关。随着像 Kubernetes 这样的容器编排平台和更高级别的抽象（如服务网格）的日益普及，客户端端实现负载均衡逻辑的需求变得非常罕见。我们将探讨在容器和
    Kubernetes 上本地部署 gRPC 应用程序的一些最佳实践，详见[第 7 章](ch07.html#ch_07)。
- en: Now let’s discuss one of the most common requirements of your gRPC applications,
    load balancing, in which we can use name resolvers in certain cases.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论你的 gRPC 应用程序中最常见的要求之一，即负载均衡，在某些情况下可以使用名称解析器。
- en: Load Balancing
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡
- en: 'Often when you develop production-ready gRPC applications, you need to make
    sure that your application can cater to high availability and scalability needs.
    Therefore, you always run more than one gRPC server in production. So, distributing
    RPC calls between these services needs to be taken care of by some entity. That’s
    where load balancing comes into play. Two main load-balancing mechanisms are commonly
    used in gRPC: a *load-balancer (LB) proxy* and *client-side load balancing*. Let’s
    start by discussing the LB proxy.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发可用于生产的 gRPC 应用程序时，通常需要确保应用程序能够满足高可用性和可扩展性需求。因此，您始终会在生产环境中运行多个 gRPC 服务器。因此，需要有一个实体来负责在这些服务之间分发
    RPC 调用。这就是负载均衡发挥作用的地方。gRPC 中通常使用两种主要的负载均衡机制：*负载均衡（LB）代理* 和 *客户端负载均衡*。让我们先讨论 LB
    代理。
- en: Load-Balancer Proxy
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负载均衡代理
- en: In proxy load balancing ([Figure 5-6](#client_application_invokes_a_load_balancer_which_is_fronting_multiple_grpc_services)),
    the client issues RPCs to the LB proxy. Then the LB proxy distributes the RPC
    call to one of the available backend gRPC servers that implements the actual logic
    for serving the call. The LB proxy keeps track of load on each backend server
    and offers a different load-balancing algorithm for distributing the load among
    the backend services.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理负载均衡中（[图 5-6](#client_application_invokes_a_load_balancer_which_is_fronting_multiple_grpc_services)），客户端向
    LB 代理发起 RPC 调用。然后 LB 代理将 RPC 调用分发给一个实现实际调用服务逻辑的可用后端 gRPC 服务器。LB 代理跟踪每个后端服务器的负载，并为在后端服务之间分发负载提供不同的负载均衡算法。
- en: '![Client application invokes a  load balancer which is fronting multiple gRPC
    services.](assets/grpc_0506.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![客户端应用调用一个负载均衡器，该负载均衡器前端有多个 gRPC 服务。](assets/grpc_0506.png)'
- en: Figure 5-6\. Client application invokes a load balancer that fronts multiple
    gRPC services
  id: totrans-326
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-6\. 客户端应用调用一个负载均衡器，该负载均衡器前端有多个 gRPC 服务
- en: The topology of the backend services is not transparent to the gRPC clients,
    and they are only aware of the load balancer’s endpoint. Therefore, on the client
    side, you don’t need to make any changes to cater to a load-balancing use case,
    apart from using the load balancer’s endpoint as the destination for all your
    gRPC connections. The backend services can report the load status back to the
    load balancer so that it can use that information for the load-balancing logic.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 后端服务的拓扑对 gRPC 客户端不透明，它们只知道负载均衡器的终端节点。因此，在客户端，除了将负载均衡器的终端点作为所有 gRPC 连接的目的地外，不需要为负载均衡使用情况做任何其他更改。后端服务可以向负载均衡器报告负载状态，以便负载均衡器可以利用该信息进行负载均衡逻辑。
- en: In theory, you can select any load balancer that supports HTTP/2 as the LB proxy
    for your gRPC applications. However, it must have full HTTP/2 support. Thus it’s
    always a good idea to specifically choose load balancers that explicitly offer
    gRPC support. For instance, you can use load-balancing solutions such as [Nginx](https://oreil.ly/QH_1c),
    [Envoy proxy](https://www.envoyproxy.io), etc., as the LB proxy for your gRPC
    applications.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，您可以选择任何支持 HTTP/2 的负载均衡器作为 gRPC 应用程序的 LB 代理。但是，它必须完全支持 HTTP/2。因此，明确选择支持 gRPC
    的负载均衡器总是一个好主意。例如，您可以使用像 [Nginx](https://oreil.ly/QH_1c)、[Envoy proxy](https://www.envoyproxy.io)
    等负载均衡解决方案作为 gRPC 应用程序的 LB 代理。
- en: If you don’t use a gRPC load balancer, then you can implement the load-balancing
    logic as part of the client applications you write. Let’s look more closely at
    client-side load balancing.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不使用 gRPC 负载均衡器，则可以将负载均衡逻辑实现为您编写的客户端应用程序的一部分。让我们更详细地了解客户端负载均衡。
- en: Client-Side Load Balancing
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 客户端负载均衡
- en: Rather than having an intermediate proxy layer for load balancing, you can implement
    the load-balancing logic at the gRPC client level. In this method, the client
    is aware of multiple backend gRPC servers and chooses one to use for each RPC.
    As illustrated in [Figure 5-7](#client_side_load_balancing), the load-balancing
    logic may be entirely developed as part of the client application (also known
    as *thick client*) or it can be implemented in a dedicated server known as lookaside
    load balancer. Then the client can query it to obtain the best gRPC server to
    connect to. The client directly connects to the gRPC server address obtained by
    the lookaside load balancer.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 与具有负载均衡中间代理层不同，您可以在 gRPC 客户端级别实现负载均衡逻辑。在这种方法中，客户端知道多个后端 gRPC 服务器，并选择一个用于每个 RPC。正如在
    [图 5-7](#client_side_load_balancing) 中所示，负载均衡逻辑可以完全作为客户端应用程序的一部分开发（也称为*厚客户端*），或者可以在专用服务器上实现，称为外部负载均衡器。然后，客户端可以查询它以获取连接的最佳
    gRPC 服务器。客户端直接连接到外部负载均衡器获取的 gRPC 服务器地址。
- en: '![Client-side load balancing](assets/grpc_0507.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![客户端负载均衡](assets/grpc_0507.png)'
- en: Figure 5-7\. Client-side load balancing
  id: totrans-333
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-7\. 客户端负载均衡
- en: To understand how you can implement client-side load balancing, let’s look at
    an example of a thick client implemented using Go. In this use case, suppose we
    have two backend gRPC services running an echo server on :50051 and :50052\. These
    gRPC services will include the serving address of the server as part of the RPC
    response. So we can consider these two servers as two members of an echo gRPC
    service cluster. Now, suppose we want to build a gRPC client application that
    uses the round-robin (executed in turn against every other) algorithm when selecting
    the gRPC server endpoint and another client that uses the first endpoint of the
    server endpoint list. [Example 5-16](#EX5-15) shows the thick client load-balancing
    implementation. Here you can observe that the client is dialing *example:///lb.example.grpc.io*.
    So, we are using the `example` scheme name and `lb.example.grpc.io` as the server
    name. Based on this scheme, it will look for a name resolver to discover the absolute
    value for the backend service address. Based on the list of values the name resolver
    returns, gRPC runs different load-balancing algorithms against those servers.
    The behavior is configured with `grpc.WithBalancerName("round_robin")`.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何实现客户端负载均衡，让我们看一个使用 Go 实现的厚客户端的示例。在这个用例中，假设我们有两个后端 gRPC 服务，在 :50051 和 :50052
    上运行 echo 服务器。这些 gRPC 服务将服务器地址作为 RPC 响应的一部分包括进去。因此，我们可以将这两个服务器视为 echo gRPC 服务集群的两个成员。现在，假设我们想要构建一个使用轮询（依次执行对每个其他的）算法选择
    gRPC 服务器端点的 gRPC 客户端应用程序，另一个客户端使用服务器端点列表的第一个端点。[示例 5-16](#EX5-15) 展示了厚客户端负载均衡的实现。在这里，你可以观察到客户端正在拨号
    *example:///lb.example.grpc.io*。因此，我们使用 `example` 方案名和 `lb.example.grpc.io` 作为服务器名。基于此方案，它将查找名解析器以发现后端服务地址的绝对值。根据名解析器返回的值列表，gRPC
    对这些服务器运行不同的负载均衡算法。该行为通过 `grpc.WithBalancerName("round_robin")` 进行配置。
- en: Example 5-16\. Client-side load balancing with a thick client
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-16\. 带有厚客户端的客户端负载均衡
- en: '[PRE22]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO16-1)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_grpc__beyond_the_basics_CO16-1)'
- en: Creating a gRPC connection with a *scheme* and the service name. The scheme
    is resolved from a scheme resolver, which is part of the client application.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *scheme* 和服务名称创建 gRPC 连接。该方案从方案解析器解析，该解析器是客户端应用程序的一部分。
- en: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO16-2)'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_grpc__beyond_the_basics_CO16-2)'
- en: Specifying a load-balancing algorithm that picks the first server on the server
    endpoint list.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 指定选择服务器端点列表中的第一个服务器的负载均衡算法。
- en: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO16-3)'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_grpc__beyond_the_basics_CO16-3)'
- en: Using the round-robin load-balancing algorithm.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 使用轮询负载均衡算法。
- en: 'There are two load-balancing policies supported in gRPC by default: `pick_first`
    and `round_robin`. `pick_first` tries to connect to the first address, uses it
    for all RPCs if it connects, or tries the next address if it fails. `round_robin`
    connects to all the addresses it sees and sends an RPC to each backend one at
    a time in order.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC 默认支持两种负载均衡策略：`pick_first` 和 `round_robin`。`pick_first` 尝试连接第一个地址，如果连接成功，则用于所有
    RPC；如果失败，则尝试下一个地址。`round_robin` 连接它看到的所有地址，并依次向每个后端发送一个 RPC。
- en: In the client-side load-balancing scenario in [Example 5-16](#EX5-15), we have
    a name resolver to resolve scheme `example`, which contains the logic of discovering
    the actual values of the endpoint URLs. Now let’s talk about compression, another
    commonly used feature of gRPC, for sending large amounts of content over RPC.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端负载均衡的场景中，在[示例 5-16](#EX5-15)中，我们有一个名称解析器来解析包含发现端点URL实际值的逻辑的`scheme example`。现在让我们谈谈压缩，这是gRPC的另一个常用特性，用于在RPC上发送大量内容。
- en: Compression
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 压缩
- en: To use network bandwidth efficiently, use compression when performing RPCs between
    client and services. Using gRPC compression on the client side can be implemented
    by setting a compressor when you do the RPC. For example, in Go, this is as easy
    as using `client.AddOrder(ctx, &order1, grpc.UseCompressor(gzip.Name))`, where
    `"google.golang.org/grpc/encoding/gzip"` provides the gzip package.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效利用网络带宽，在客户端和服务之间执行RPC时，请使用压缩。在客户端使用gRPC压缩可以通过在进行RPC时设置压缩器来实现。例如，在Go中，可以简单地使用`client.AddOrder(ctx,
    &order1, grpc.UseCompressor(gzip.Name))`来实现，其中`"google.golang.org/grpc/encoding/gzip"`提供了gzip包。
- en: From the server side, registered compressors will be used automatically to decode
    request messages and encode the responses. In Go, registering a compressor is
    as simple as importing `"google.golang.org/grpc/encoding/gzip"` into your gRPC
    server application. The server always responds using the same compression method
    specified by the client. If the corresponding compressor has not been registered,
    an `Unimplemented` status will be returned to the client.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 从服务器端，注册的压缩器将自动用于解码请求消息和编码响应。在Go中，注册压缩器就像将`"google.golang.org/grpc/encoding/gzip"`导入到您的gRPC服务器应用程序中一样简单。服务器始终使用客户端指定的相同压缩方法进行响应。如果未注册相应的压缩器，则会向客户端返回`Unimplemented`状态。
- en: Summary
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Building production-ready, real-world gRPC applications often requires you to
    include various capabilities besides defining the service interface, generating
    the server and client code, and implementing the business logic. As you saw in
    this chapter, gRPC offers a wide range of capabilities that you will need when
    building gRPC applications, including interceptors, deadlines, cancellations,
    and error handling.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 构建可供生产使用的真实世界gRPC应用程序通常需要包括除定义服务接口、生成服务器和客户端代码以及实现业务逻辑之外的各种能力。正如您在本章中看到的，gRPC提供了多种能力，在构建gRPC应用程序时将会用到，包括拦截器、截止时间、取消和错误处理。
- en: However, we haven’t yet discussed how to secure gRPC applications and how to
    consume them. So, in the next chapter we’ll cover this topic in detail.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还没有讨论如何保护gRPC应用程序以及如何消费它们。因此，在下一章中，我们将详细讨论这个主题。
