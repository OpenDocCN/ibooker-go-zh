- en: Chapter 5\. How Go Uses Memory Resource
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html#ch-hardware), we started looking under the hood of
    the modern computer. We discussed the efficiency aspects of using the CPU resource.
    Efficient execution of instructions in the CPU is important, but the sole purpose
    of performing those instructions is to modify the data. Unfortunately, the path
    of changing data is not always trivial. For example, in [Chapter 4](ch04.html#ch-hardware)
    we learned that in the von Neumann architecture (presented in [Figure 4-1](ch04.html#img-uma)),
    we experience the CPU and memory wall problem when accessing data from the main
    memory (RAM).
  prefs: []
  type: TYPE_NORMAL
- en: The industry invented numerous technologies and optimization layers to overcome
    challenges like that, including memory safety and ensuring large memory capacities.
    As a result of those inventions, accessing eight bytes from RAM to the CPU register
    might be represented as a simple `MOVQ <destination register> <address XYZ>` instruction.
    However, the actual process done by the CPU to get that information from the physical
    chip storing those bytes is very complex. We discussed mechanisms like the hierarchical
    cache system, but there is much more.
  prefs: []
  type: TYPE_NORMAL
- en: In some ways, those mechanisms are abstracted from programmers as much as possible.
    So, for example, when we define a variable in Go code, we don’t need to think
    about how much memory has to be reserved, where, and in how many L-caches it has
    to fit. This is great for development speed, but sometimes it might surprise us
    when we need to process a lot of data. In those cases, we need to revive our [mechanical
    sympathy](https://oreil.ly/Co2IM) toward memory resource, optimizing TFBO flow
    ([“Efficiency-Aware Development Flow”](ch03.html#ch-conq-eff-flow)), and good
    tooling.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will focus on understanding the RAM resource. We will start by
    exploring overall memory relevance. Then we will set the context in [“Do We Have
    a Memory Problem?”](#ch-hw-memory). Next, we will explain the patterns and consequences
    of each element involved in the memory access from bottom to top. The data journey
    for memory starts in [“Physical Memory”](#ch-hw-memory-ph), the hardware memory
    chips. Then we will move to operating system (OS) memory management techniques
    that allow managing limited physical memory space in multiprocess systems: [“Virtual
    Memory”](#ch-hw-memory-vt) and [“OS Memory Mapping”](#ch-hw-memory-mmap-os), with
    a more detailed explanation of the [“mmap Syscall”](#ch-hw-memory-mmap).'
  prefs: []
  type: TYPE_NORMAL
- en: With the lower layers of memory access explained, we can move to the key knowledge
    for Go programmers looking to optimize memory efficiency—the explanation of [“Go
    Memory Management”](#ch-hw-go-mem). This includes the necessary elements like
    memory layout, what [“Values, Pointers, and Memory Blocks”](#ch-hw-allocations)
    mean, and the basics of the [“Go Allocator”](#ch-hw-allocator) with its measurable
    consequences. Finally, we will explore [“Garbage Collection”](#ch-hw-garbage).
  prefs: []
  type: TYPE_NORMAL
- en: We will go into many details about memory in this chapter, but the key aim is
    to build an instinct toward the patterns and behavior of Go programs when it comes
    to memory usage. For example, what problems can occur while accessing memory?
    How do we measure memory usage? What does it mean to allocate memory? How can
    we release it? We will explore answers to those questions in this chapter. But
    let’s start this chapter by clarifying why RAM is relevant to our program execution.
    What makes it so important?
  prefs: []
  type: TYPE_NORMAL
- en: Memory Relevance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All Linux programs require more resources than just the CPU to perform their
    programmed functionalities. For example, let’s take a web server like [NGINX](https://oreil.ly/7F0cZ)
    (written in C) or [Caddy](https://oreil.ly/MpHMZ) (written in Go). Those programs
    allow serving static content from disk or proxy HTTP requests, among other functionalities.
    They use the CPU to execute written code. However, a web server like this also
    interacts with other resources, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: With RAM to cache basic HTTP responses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a disk to load configuration, static content, or write log lines for observability
    needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a network to serve HTTP requests from remote clients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a result, the CPU resource is only one part of the equation. This is the
    same for most programs—they are created to save, read, manage, operate, and transform
    data from different mediums.
  prefs: []
  type: TYPE_NORMAL
- en: One would argue that the “memory” resource, often called RAM,^([1](ch05.html#idm45606834619920))
    sits at the core of those interactions. The RAM is the backbone of the computer
    because every external piece of data (bytes from disk, network, or another device)
    has to be buffered in memory to be accessible to the CPU. So, for example, the
    first thing the OS does to start a new process is load part of the program’s machine
    code and initial data to memory for the CPU to execute it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, we must be aware of three main caveats when using memory in
    our programs:'
  prefs: []
  type: TYPE_NORMAL
- en: RAM access is significantly slower than CPU operational speed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is always a finite amount of RAM in our machines (typically from a few
    GB to hundreds of GB per machine), which forces us to care about space efficiency.^([2](ch05.html#idm45606834615344))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unless [the persistent type of memory](https://oreil.ly/uaPiN) will be commoditized
    with RAM-like speeds, pricing, and robustness, our main memory is strictly volatile.
    When the computer power goes down, all information is completely lost.^([3](ch05.html#idm45606834611488))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ephemeral characteristics of memory and its finite size are why we are forced
    to add an auxiliary, persistent I/O resource to our computer, i.e., a disk. These
    days we have relatively fast solid state drive (SSD) disks (yet still around 10x
    slower than RAM) with a limited lifetime (~five years). On the other hand, we
    have a slower and cheaper hard disk drive (HDD). While cheaper than RAM, the disk
    resource is also a scarce resource.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, for scalability and reliability reasons, our computers rely
    on data from remote locations. Industry invented different networks and protocols
    that allow us to communicate with remote software (e.g., databases) or even remote
    hardware (via iSCSI or NFS protocols). We typically abstract this type of I/O
    as a network resource usage. Unfortunately, the network is one of the most challenging
    resources to work with because of its unpredictable nature, limited bandwidth,
    and bigger latencies.
  prefs: []
  type: TYPE_NORMAL
- en: While using any of those resources, we use it through the memory resource. As
    a result, it is essential to understand its mechanics. There are many things a
    programmer can do to impact the application’s memory usage. But unfortunately,
    without proper education, our implementations tend to be prone to inefficiencies
    and unnecessary waste of computer resources or execution time. This problem is
    amplified by the vast amount of data our programs have to process these days.
    This is why we often say that efficient programming is all about the data.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Inefficiency Is Usually the Most Common Problem in Go Programs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Go is a garbage collected language, which allows Go to be an extremely productive
    language. However, the garbage collector (GC) sacrifices some visibility and control
    over memory management (more on that in [“Garbage Collection”](#ch-hw-garbage)).
  prefs: []
  type: TYPE_NORMAL
- en: But even when we forget about GC overhead, for cases where we need to process
    a significant amount of data or are under some resource constraints, we have to
    take more care with how our program uses memory. Therefore, I recommend reading
    this chapter with extra care since most first-level optimizations are usually
    around memory resources.
  prefs: []
  type: TYPE_NORMAL
- en: When should we start the memory optimization process? A few common symptoms
    might reveal that we might have a memory efficiency issue.
  prefs: []
  type: TYPE_NORMAL
- en: Do We Have a Memory Problem?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s useful to understand how Go uses the computer’s main memory and its efficiency
    consequences, but we must also follow the pragmatic approach. As with any optimizations,
    we should refrain from optimizing memory until we know there is a problem. We
    can define a set of situations that should trigger our interest in Go memory usage
    and potential optimizations in this area:'
  prefs: []
  type: TYPE_NORMAL
- en: Our physical computer, virtual machine, container, or process crashed because
    of an out-of-memory (OOM) signal, or our process is about to hit that memory limit.^([4](ch05.html#idm45606834596960))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our Go program is executing slower than usual, while the memory usage is higher
    than average. Spoiler: our system might be under memory pressure causing trashing
    or swapping, as explained in [“OS Memory Mapping”](#ch-hw-memory-mmap-os).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our Go program is executing slower than usual, with high spikes of CPU utilization.
    Spoiler: allocation or releasing memory slows our programs if an excessive number
    of short-lived objects is created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you encounter any of those situations, it might be time to debug and optimize
    the memory usage of your Go program. As I will teach you in [“Complexity Analysis”](ch07.html#ch-hw-complexity),
    if you know what you are looking for, a set of early warning signals can indicate
    huge memory problems that could be avoided easily. Moreover, building such a proactive
    instinct can make you a valuable team asset!
  prefs: []
  type: TYPE_NORMAL
- en: But we can’t build anything without good foundations. As with the CPU resource,
    you won’t be able to apply optimizations without actually understanding them!
    We have to understand the reasons behind those optimizations. For example, [Example 4-1](ch04.html#code-sum)
    allocates 30.5 MB of memory for 1 million integers in the input. But what does
    it mean? Where was that space reserved? Does it mean we used exactly 30.5 MB of
    physical memory, or more? Was this memory released at some point? This chapter
    aims to give you awareness, allowing you to answer all of these questions. We
    will learn why memory is often the issue and what we can do about it.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the basics of memory management from the point of view of hardware
    (HW), operating system (OS), and the Go runtime. Let’s start with essential details
    about physical memory directly impacting our program execution. On top of that,
    this knowledge might help you better understand the specifications and documentation
    of modern physical memory!
  prefs: []
  type: TYPE_NORMAL
- en: Physical Memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We store information digitally in the form of bits, the basic computer storage
    unit. A bit can have one of two values, 0 or 1\. With enough bits, we can represent
    any information: integer, floating value, letters, messages, sounds, images, videos,
    programs, [metaverses](https://oreil.ly/il8Tz), etc.'
  prefs: []
  type: TYPE_NORMAL
- en: The main physical memory that we use when we execute our programs (RAM) is based
    on dynamic random-access memory ([DRAM](https://oreil.ly/hbo59)). These chips
    are soldered into modules, often referred to as RAM “sticks.” When connected to
    the motherboard, these chips allow us to store and read data bits as long as the
    DRAM is continuously powered.
  prefs: []
  type: TYPE_NORMAL
- en: DRAM contains billions of memory cells (as many cells as the number of bits
    DRAM can store). Each memory cell comprises one access transistor acting as a
    switch and one storage capacitor. The transistor guards the access to the capacitor,
    which is charged to the store 1 or drained to keep the 0 value. This allows each
    memory cell to store a single bit of information. This architecture is much simpler
    and cheaper to produce and use than Static RAM (SRAM), which is generally faster
    and used for smaller types of memory like registers and hierarchical caches in
    the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of this writing, the most popular memory used for RAM is the simpler,
    synchronous (clock) version in the DRAM family—[SDRAM](https://oreil.ly/07efG).
    Particularly, the fifth generation of SDRAM called DDR4.
  prefs: []
  type: TYPE_NORMAL
- en: Eight bits form a “byte.” That number came from the fact that in the past, the
    smallest number of bits that could hold a text character was eight.^([5](ch05.html#idm45606834576192))
    The industry standardized a “byte” as the smallest meaningful unit of information.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, most hardware is byte addressable. This means that, from a software
    programmer’s point of view, there are instructions to access data through individual
    bytes. If you want to access a single bit, you need to access the whole byte and
    use [bitmasks](https://oreil.ly/pFoxI) to get or write the bit you want.
  prefs: []
  type: TYPE_NORMAL
- en: The byte addressability makes developer life easier when working with data from
    different mediums like memory, disk, network, etc. Unfortunately, that creates
    a certain illusion that the data is always accessible with byte granularity. Don’t
    let that mislead you. More often than not, the underlying hardware has to transfer
    a much larger chunk of data to give you the desired byte.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in [“Hierachical Cache System”](ch04.html#ch-hw-lcache), we learned
    that CPU registers are typically 64 bits (8 bytes), and the cache line is even
    bigger (64 bytes). Yet we have CPU instructions that can copy a single byte from
    memory to the CPU register. However, an experienced developer will notice that
    to copy that single byte, in many cases, the CPU will fetch not 1 byte but at
    least a complete cache line (64 bytes) from physical memory.
  prefs: []
  type: TYPE_NORMAL
- en: From a high-level point of view, physical memory (RAM) can also be seen as byte
    addressable, as presented in [Figure 5-1](#img-physical-addr).
  prefs: []
  type: TYPE_NORMAL
- en: Memory space can be seen as a contiguous set of one-byte slots with a unique
    address. Each address is a number from zero to the total memory capacity in the
    system in bytes. For this reason, 32-bit systems that use only 32-bit integers
    for memory addresses typically could not handle RAM with more capacity than 4
    GB—the largest number we can represent with 32 bits is <math alttext="2 Superscript
    32"><msup><mn>2</mn> <mn>32</mn></msup></math> . This limitation was removed with
    the introduction of the 64-bit operating systems that use 64-bit (8-byte)^([6](ch05.html#idm45606834567216))
    integers for memory addressing.
  prefs: []
  type: TYPE_NORMAL
- en: '![efgo 0501](assets/efgo_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Physical memory addresses space
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We discussed in [“CPU and Memory Wall Problem”](ch04.html#ch-hw-mem-wall) that
    memory access is not that fast compared to, for example, CPU speed. But there
    is more. Addressability, in theory, should allow fast, random access to bytes
    from the main memory. After all, this is why that main memory is called “random-access
    memory.” Unfortunately, if we look at our napkin math in [Appendix A](app01.html#appendix-napkin-math),
    sequential memory access can be 10 times (or more) faster than random access!
  prefs: []
  type: TYPE_NORMAL
- en: 'But there is more—we don’t expect any improvements in this area in the future.
    Within the last few decades, we only improved the speed (bandwidth) of the sequential
    read. We did not improve random access latency at all! The lack of improvement
    on the latency side is not a mistake. It is a strategic choice—the internal designs
    of the modern RAM modules have to work against various requirements and limitations,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: Capacity
  prefs: []
  type: TYPE_NORMAL
- en: There is a strong demand for bigger capacities of RAM, e.g., to compute more
    data or run more realistic games.
  prefs: []
  type: TYPE_NORMAL
- en: Bandwidth and latency
  prefs: []
  type: TYPE_NORMAL
- en: We want to wait less time to access memory while writing or reading large chunks
    of data since memory access is the major slowdown for CPU operations.
  prefs: []
  type: TYPE_NORMAL
- en: Voltage
  prefs: []
  type: TYPE_NORMAL
- en: There is a demand for a lower voltage requirement for each memory chip, which
    would allow for running more of them while maintaining low power consumption and
    manageable thermal characteristics (more time on battery for our laptops and smartphones!).
  prefs: []
  type: TYPE_NORMAL
- en: Cost
  prefs: []
  type: TYPE_NORMAL
- en: RAM is a fundamental piece of the computer required in large quantities; thus,
    production and usage costs must be kept low.
  prefs: []
  type: TYPE_NORMAL
- en: Slower random access has many implications for the layers of many managers we
    will learn about in this chapter. For example, this is why the CPU with L-caches
    fetches and caches bigger chunks of memory up front, even if only one byte is
    needed for computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s summarize a few things worth remembering about modern generations of
    hardware for RAM like DDR4 SDRAM:'
  prefs: []
  type: TYPE_NORMAL
- en: Random access of the memory is relatively slow, and generally, there aren’t
    many good ideas to improve that soon. If anything, lower power consumption, larger
    capacity, and bandwidth only increase that delay.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Industry is improving overall memory bandwidth by allowing us to transfer bigger
    chunks of adjacent (sequential) memory. This means that efforts to align Go data
    structures and knowing how they are stored in memory matter—ensuring we can access
    them faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether sequentially or randomly, our programs never directly access physical
    memory—the OS manages the RAM space. This is great for developers, as we don’t
    need to understand low-level memory access details. But there are more important
    reasons why there has to be an OS between our programs and hardware. So let’s
    discuss why and what it means for our Go programs.
  prefs: []
  type: TYPE_NORMAL
- en: OS Memory Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What are the operating system’s goals for memory management? Hiding complexities
    of physical memory access is only one thing. The other, more important, goal is
    to allow using the same physical memory simultaneously and securely across thousands
    of processes and their OS threads.^([7](ch05.html#idm45606834540672)) The problem
    of multiprocess execution on common memory space is nontrivial for multiple reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Dedicated memory space for each process
  prefs: []
  type: TYPE_NORMAL
- en: Programs are compiled assuming nearly full and continuous access to the RAM.
    As a result, the OS must track which slots from the physical memory from our address
    space (shown in [Figure 5-1](#img-physical-addr)) belong to which process. Then
    we need to find a way to coordinate those “reservations” to the processes so only
    allocated addresses are accessed.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding external fragmentation
  prefs: []
  type: TYPE_NORMAL
- en: Having thousands of processes with dynamic memory usage poses a great risk of
    waste in memory due to inefficient packing. We call this problem [the external
    fragmentation of memory](https://oreil.ly/lBfRq).
  prefs: []
  type: TYPE_NORMAL
- en: Memory isolation
  prefs: []
  type: TYPE_NORMAL
- en: We have to ensure that no process touches the physical memory address reserved
    for other processes running on the same machine (e.g., operating system processes!).
    This is because any accidental write or read from outside of process memory (out-of-bounds
    memory access) can crash other processes, malform data on persistent mediums (e.g.,
    disk), or crash the whole machine (e.g., if you corrupt the memory used by the
    OS).
  prefs: []
  type: TYPE_NORMAL
- en: Memory safety
  prefs: []
  type: TYPE_NORMAL
- en: Operating systems are usually multiuser systems, which means processes can have
    different permissions to different resources (e.g., files on disk or other process
    memory space). This is why the mentioned out-of-bounds memory accesses have serious
    security risks.^([8](ch05.html#idm45606834529664)) Imagine a malicious process
    with no permissions reading credentials from other process memory, or causing
    a Denial-of-Service (DoS) attack.^([9](ch05.html#idm45606834527872)) This is especially
    important for virtualized environments, where a single memory unit can be shared
    across different operating systems and even more users.
  prefs: []
  type: TYPE_NORMAL
- en: Efficient memory usage
  prefs: []
  type: TYPE_NORMAL
- en: Programs never use all the memory they asked for at the same time. For example,
    instruction code and statically allocated data (e.g., constant variables) can
    be as large as dozens of megabytes. But for single-threaded applications, a maximum
    of a few kilobytes of data is used in a given second. Instructions for error handling
    are rarely used. Arrays are often oversized for worst-case scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve all those challenges, modern OS manages memory using three fundamental
    mechanisms we will learn about in this section: paged virtual memory, memory mapping,
    and hardware address translation. Let’s start by explaining virtual memory.'
  prefs: []
  type: TYPE_NORMAL
- en: Virtual Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key idea behind [virtual memory](https://oreil.ly/RBiCV) is that every process
    is given its own logical, simplified view of the RAM. As a result, programming
    language designers and developers can effectively manage process memory space
    as if they had an entire memory space for themselves. Even more, with virtual
    memory, the process can use a full range of addresses from 0 to <math alttext="2
    Superscript 64 Baseline minus 1"><mrow><msup><mn>2</mn> <mn>64</mn></msup> <mo>-</mo>
    <mn>1</mn></mrow></math> for its data, even if the physical memory has, for example,
    the capacity to accommodate only <math alttext="2 Superscript 35"><msup><mn>2</mn>
    <mn>35</mn></msup></math> addresses (32 GB of memory). This frees the process
    from coordinating the memory among other processes, bin packing challenges, and
    other important tasks (e.g., physical memory defragmentation, security, limits,
    and swap). Instead, all of these complex and error-prone memory management tasks
    can be delegated to the kernel (a core part of the Linux operating system).
  prefs: []
  type: TYPE_NORMAL
- en: There are a few ways of implementing virtual memory, but the most popular technique
    is called *paging*.^([10](ch05.html#idm45606834509584)) The OS divides physical
    and virtual memory into fixed-size chunks of memory. The virtual memory chunks
    are called [*pages*](https://oreil.ly/JTWoU), whereas physical memory chunks are
    called *frames*. Both pages and frames can be then individually managed. The default
    page size is usually 4 KB,^([11](ch05.html#idm45606834504144)) but it can be changed
    to larger page sizes with respect to specific CPU capabilities.^([12](ch05.html#idm45606834502960))
    It is also possible to use 4 KB pages for normal workloads and dedicated (sometimes
    transparent to processes!) [huge pages](https://oreil.ly/7KuGx) from 2 MB to 1
    GB.
  prefs: []
  type: TYPE_NORMAL
- en: The Importance of Page Size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The 4 KB number was chosen in the 1980s, and many say that it’s time to bump
    this number up, given modern hardware and cheaper RAM (in terms of dollars per
    byte).
  prefs: []
  type: TYPE_NORMAL
- en: Yet the choice of page size is a game of trade-offs. Larger pages inevitably
    waste more memory space,^([13](ch05.html#idm45606834497952)) which is often referred
    to as [the internal memory fragmentation](https://oreil.ly/PnOuT). On the other
    hand, keeping a 4 KB page size or making it smaller makes memory access slower
    and memory management more expensive, eventually blocking the ability to use larger
    RAM modules in our computers.
  prefs: []
  type: TYPE_NORMAL
- en: The OS can dynamically map pages in virtual memory to specific physical memory
    frames (or other mediums like chunks of disk space), mostly transparently to the
    processes. The mapping, state, permissions, and additional metadata of the page
    are stored in the page entry in the many hierarchical page tables maintained by
    the OS.^([14](ch05.html#idm45606834494864))
  prefs: []
  type: TYPE_NORMAL
- en: To achieve an easy-to-use and dynamic virtual memory, we need to have a versatile
    address translation mechanism. The problem is that only the OS knows about the
    current memory space mapping between virtual and physical space (or lack of it).
    Our running program’s process only knows about virtual memory addresses, so all
    CPU instructions in machine code use virtual addresses. Our programs will be even
    slower if we try to consult the OS for every memory access to translate each address,
    so the industry figured out dedicated hardware support for translating memory
    pages.
  prefs: []
  type: TYPE_NORMAL
- en: From the 1980s, almost every CPU architecture started to include the Memory
    Management Unit (MMU) used for every memory access. MMU translates each memory
    address referenced by CPU instructions to a physical address based on the OS page
    table entries. To avoid accessing RAM to search for the relevant page tables,
    engineers added the Translation Lookaside Buffer (TLB). TLB is a small cache that
    can cache a few thousand page table entries (typically 4 KB of entries). The overall
    flow looks like [Figure 5-2](#img-mem-vm-mmu).
  prefs: []
  type: TYPE_NORMAL
- en: '![efgo 0502](assets/efgo_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Address translation mechanism done by MMU and TLB in CPU. OS has
    to inject the relevant page tables so MMU knows what virtual addresses correspond
    to physical addresses.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TLB is very fast, but it has limited capacity. If MMU cannot find the accessed
    virtual address in the TLB, we have a TLB miss. This means that either the CPU
    (hardware TLB management) or OS (software-managed TLB) has to walk through page
    tables in RAM, which causes significant latency (around one hundred CPU clock
    cycles)!
  prefs: []
  type: TYPE_NORMAL
- en: It is essential to mention that not every “allocated” virtual memory page will
    have a reserved physical memory page behind it. In fact, most of the virtual memory
    is not backed up by RAM at all. As a result, we can almost always see large amounts
    of virtual memory used by the process (called `VSS` or `VSZ` in various Linux
    tools like `ps`). Still, the actual physical memory (often called `RSS` or `RES`
    from “resident memory”) reserved for this process might be tiny. There are often
    cases where a single process allocates more virtual memory than is available to
    the whole machine! See an example situation like this on my machine in [Figure 5-3](#img-mem-vss).
  prefs: []
  type: TYPE_NORMAL
- en: '![efgo 0503](assets/efgo_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. First few lines of `htop` output, showing the current usage of
    a few Chrome browser processes, sorted by virtual memory size
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see in [Figure 5-3](#img-mem-vss), my machine has 32 GB of physical
    memory, with 16.2 GB currently used. Yet we see Chrome processes using 45.7 GB
    of virtual memory each! However, if you look at the `RES` column, it has only
    507 MB resident, with 126 MB of it shared with other processes. So how this is
    possible? How can the process think that it has 45.7 GB of RAM available, given
    the machine has only 32 GB and the system actually allocated just a few hundred
    MBs in RAM?
  prefs: []
  type: TYPE_NORMAL
- en: We can call such a situation a [memory overcommitment](https://oreil.ly/wbZGf),
    and it exists because of the very same reasons [airlines often overbook seats
    for their flights](https://oreil.ly/El9iy). On average, many travelers cancel
    their trips at the last minute or do not show up for their flight. As a result,
    to maximize the plane’s used capacity, it is more profitable for airlines to sell
    more tickets than seats in the airplane and handle the rare “out of seats” situations
    “gracefully” (e.g., by moving the unlucky customer to another flight). This means
    that the true “allocation” of seats happens when travelers actually “access” them
    during the flight onboarding process.
  prefs: []
  type: TYPE_NORMAL
- en: The OS performs the same overcommitment strategy by default^([15](ch05.html#idm45606834472688))
    for processes trying to allocate physical memory. The physical memory is only
    allocated when our program accesses it, not when it “creates” a big object, for
    example, `make([]byte, 1024)` (you will see a practical example of this in [“Go
    Allocator”](#ch-hw-allocator)).
  prefs: []
  type: TYPE_NORMAL
- en: Overcommitment is implemented with the pages and memory mapping techniques.
    Typically, memory mapping refers to a low-level memory management capability offered
    with the [`mmap`](https://oreil.ly/m5n7A) system call on Linux (and the similar
    `MapViewOfFile` function in Windows).
  prefs: []
  type: TYPE_NORMAL
- en: Developers Can Utilize mmap Explicitly in Programs for Specific Use Cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `mmap` call is used extensively in almost every database software, e.g.,
    in [MySQL](https://oreil.ly/o8a5o) and [PostgreSQL](https://oreil.ly/scByc) as
    well as those written in Go, like [Prometheus](https://oreil.ly/2Sa3P), [Thanos](https://oreil.ly/tFBUf),
    and [M3db](https://oreil.ly/Jg3wb) projects. The `mmap` (among other memory allocation
    techniques) is also what Go runtime and other programming languages use under
    the hood to allocate memory from OS, e.g., for the heap (discussed in [“Go Memory
    Management”](#ch-hw-go-mem)).
  prefs: []
  type: TYPE_NORMAL
- en: Using explicit `mmap` for most Go applications is not recommended. Instead,
    we should stick to the Go runtime’s standard allocation mechanisms, which we will
    learn in [“Go Memory Management”](#ch-hw-go-mem). As our [“Efficiency-Aware Development
    Flow”](ch03.html#ch-conq-eff-flow) said, only if we see indications through benchmarking
    that this is not enough, might we consider moving to more advanced methods like
    `mmap`. This is why `mmap` is not even on my [Chapter 11](ch11.html#ch-opt2) list!
  prefs: []
  type: TYPE_NORMAL
- en: However, there is a reason why I explain `mmap` at the start of our journey
    with the memory resource. Even if we don’t use it explicitly, the OS uses the
    same memory mapping mechanism to manage all allocated pages in our system. The
    data structures we use in our Go programs are indirectly saved to certain virtual
    memory pages, which are then `mmap`-like managed by the OS or Go runtime. As a
    result, understanding the explicit `mmap` syscall will conveniently explain the
    on-demand paging and mapping techniques Linux OS uses to manage virtual memory.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s focus on the Linux `mmap` syscall next.
  prefs: []
  type: TYPE_NORMAL
- en: mmap Syscall
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To learn about OS memory mapping patterns, let’s discuss the [`mmap`](https://oreil.ly/m5n7A)
    syscall. [Example 5-1](#code-mmap) shows a simplified abstraction, using `mmap`
    OS syscall, that allows allocating a byte slice in our process virtual memory
    without Go memory management coordination.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-1\. The adapted snippet of Linux-specific [Prometheus `mmap` abstraction](https://oreil.ly/KJ4dD)
    that allows creating and maintaining read-only memory-mapped byte arrays
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_how_go_uses_memory_resource_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`OpenFileBacked` creates explicit memory mapped backed up by the file from
    the provided path.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_how_go_uses_memory_resource_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: '`unix.Mmap` is a Unix-specific Go helper that uses the `mmap` syscall to create
    a direct mapping between bytes from the file on disk (between 0 and the `size`
    address) and virtual memory allocated by the returned `[]byte` array in the `b`
    variable. We also pass the read-only flag (`PROT_READ`) and shared flag (`MAP_SHARED`).^([16](ch05.html#idm45606834159392))
    We can also skip the passing file descriptor, and pass 0 as the first argument
    and `MAP_ANON` as the last argument to create anonymous mapping (more on that
    later).^([17](ch05.html#idm45606834158000))'
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_how_go_uses_memory_resource_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: We use the [`merrors`](https://oreil.ly/lnrJM) package to ensure the we capture
    both errors if `Close` also returns an error.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_how_go_uses_memory_resource_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: '`unix.Munmap` is one of the few ways to remove mapping and de-allocate `mmap`-ed
    bytes from virtual memory.'
  prefs: []
  type: TYPE_NORMAL
- en: The returned byte slice from the open-ed `MemoryMap.Bytes` structure can be
    read as a regular byte slice acquired in typical ways, e.g., `make([]byte, size)`.
    However, since we marked this memory-mapped location as read-only (`unix.PROT_READ`),
    writing to such a slice will cause the OS to terminate the Go process with the
    `SIGSEGV` reason.^([18](ch05.html#idm45606834147584)) Furthermore, a segmentation
    fault will also happen if we read from this slice after doing `Close` (`Unmap`)
    on it.
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, the `mmap`-ed byte array looks like a regular byte slice with
    extra steps and constraints. So what’s unique about it? It’s best to explain that
    using an example! Imagine that we want to buffer a 600 MB file in the `[]byte`
    slice so we can quickly access a couple of bytes on demand from random offsets
    of that file. The 600 MB might sound excessive, but such a requirement is commonly
    seen in databases or caches where reading from a disk on demand might be too slow.
  prefs: []
  type: TYPE_NORMAL
- en: The naive solution without an explicit `mmap` could look like [Example 5-2](#code-naive-read-usage).
    Every few instructions, we will look at what the OS memory statistics told us
    about the allocated pages on physical RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-2\. Buffering 600 MB from a file to access three bytes from three
    different locations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_how_go_uses_memory_resource_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We open the 600+ MB file. At this point, if you ran the `ls -l /proc/$PID/fd`
    (where `$PID` is the process ID of this executed program) command on a Linux machine,
    you would see file descriptors telling you that this process has used these files.
    One of the descriptors is a symbolic link to our `test686mbfile.out` file we just
    opened. The process will hold that file descriptor until the file is closed.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_how_go_uses_memory_resource_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We read 600 MB into a pre-allocated `[]byte` slice. After the `f.Read` method
    execution, the RSS of the process shows 621 MB.^([19](ch05.html#idm45606833899392))
    This means that we need over 600 MB of free physical RAM to run this program.
    The virtual memory size (VSZ) increased too, hitting 1.3 GB.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_how_go_uses_memory_resource_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: No matter what bytes we access from our buffer, our program will not allocate
    any more bytes on RSS for our buffer (however, it might need extra bytes for the
    `Println` logic).
  prefs: []
  type: TYPE_NORMAL
- en: Generally, [Example 5-2](#code-naive-read-usage) proves that without an explicit
    `mmap`, we would need to reserve at least 600 MB of memory (~150,000 pages) on
    physical RAM from the very beginning. We also keep all of them reserved for our
    process until it is collected by the garbage collection process.
  prefs: []
  type: TYPE_NORMAL
- en: What would the same functionality look like with the explicit `mmap`? Let’s
    do something similar in [Example 5-3](#code-mmap-usage) using the [Example 5-1](#code-mmap)
    abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-3\. Memory mapping 600 MB from file to access three bytes from three
    different locations, using [Example 5-1](#code-mmap)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_how_go_uses_memory_resource_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We open our test file and memory map 600 MB of its content into the `[]byte`
    slice. At this point, similar to [Example 5-2](#code-naive-read-usage), we see
    a related file descriptor for our `test686mbfile.out` file in the *fd* directory.
    More importantly, however, if you executed the `ls -l /proc/$PID>/map_files` (again,
    `$PID` is the process ID) command, you would also have another symbolic link to
    the `test686mbfile.out` file we just referenced. This represents a file-backed
    memory map.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_how_go_uses_memory_resource_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: After this statement, we have the byte buffer `b` with the file content. However,
    if we check the memory statistics for this process, the OS did not allocate any
    page in physical memory for our slice elements.^([20](ch05.html#idm45606833705184))
    So the total RSS is as small as 1.6 MB, despite having 600 MB of content accessible
    in `b`! The VSZ, on the other hand, is around 1.3 GB, which indicates the OS is
    telling the Go program that it can access this space.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_how_go_uses_memory_resource_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: After accessing a single byte from our slice, we see an increase in RSS, around
    48–70 KB worth of RAM pages for this mapping. This means that the OS only allocated
    a few (10 or so) pages on RAM when our code wanted to access a single, concrete
    byte from `b`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_how_go_uses_memory_resource_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing a different byte far away from already allocated pages triggers the
    allocation of extra pages. RSS reading would show 100–128 KB.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_how_go_uses_memory_resource_CO3-5)'
  prefs: []
  type: TYPE_NORMAL
- en: If we access a single byte 4,000 bytes away from the previous read, OS does
    not allocate any additional pages. This might be for a few reasons.^([21](ch05.html#idm45606833635152))
    For instance, when our program read the file’s contents at offset 100,000, the
    OS already allocated a 4 KB page with the byte we accessed here. Thus RSS reading
    would still show 100–128 KB.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_how_go_uses_memory_resource_CO3-6)'
  prefs: []
  type: TYPE_NORMAL
- en: If we remove the memory mapping, all our related pages will eventually be unmapped
    from RAM. This means our process total RSS number should be smaller.^([22](ch05.html#idm45606833630880))
  prefs: []
  type: TYPE_NORMAL
- en: An Underrated Way to Learn More About Your Process and OS Resource Behavior
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linux provides amazing statistics and debugging information for the current
    process or thread state. Everything is accessible as special files inside */proc/`<PID>`*.
    The ability to debug each detailed statistic (e.g., every little memory mapping
    status) and configuration was eye-opening for me. Learn more about what you can
    do by reading the [proc](https://oreil.ly/jxBig) (process pseudofilesystem) documentation.
  prefs: []
  type: TYPE_NORMAL
- en: I recommend getting familiar with the Linux pseudofilesystem or the tools using
    it if you plan to work more on low-level Linux software.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the main behaviors highlighted when we used explicit `mmap` in [Example 5-3](#code-mmap-usage)
    is called on-demand paging. When the process asks the OS for any virtual memory
    using `mmap`, the OS will not allocate any page on RAM, no matter how large. Instead,
    the OS will only give the process the virtual address range. Further along, when
    the CPU performs the first instruction that accesses memory from that virtual
    address range (e.g., our `fmt.Println("Reading the 5000th byte," b[5000])` in
    [Example 5-3](#code-mmap-usage)), the MMU will generate a page fault. Page fault
    is a hardware interrupt that is handled by the OS kernel. The OS can then respond
    in various ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Allocate more RAM frames
  prefs: []
  type: TYPE_NORMAL
- en: If we have free frames (physical memory pages) in RAM, the OS can mark some
    of them as used and map them to the process that triggered the page fault. This
    is the only moment when the OS actually “allocates” RAM (and increases the `RSS`
    metric).
  prefs: []
  type: TYPE_NORMAL
- en: De-allocate unused RAM frames and reuse them
  prefs: []
  type: TYPE_NORMAL
- en: If no free frame exists (high memory usage on the machine), the OS can remove
    a couple of frames that belong to file-backed mappings for any process as long
    as the frames are not currently accessed. As a result, many pages can be unmapped
    from physical frames before OS has to resort to more brutal methods. Still, this
    will potentially cause other processes to generate another page fault. If this
    situation happens very often, our whole OS with all processes will be seriously
    slowed down (memory trashing situation).
  prefs: []
  type: TYPE_NORMAL
- en: Triggering out-of-memory (OOM) situation
  prefs: []
  type: TYPE_NORMAL
- en: 'If the situation worsens and all unused file-backed memory-mapped pages are
    freed, and we still have no free pages, the OS is essentially out of memory. Handling
    that situation can be configured in the OS, but generally, there are three options:'
  prefs: []
  type: TYPE_NORMAL
- en: The OS can start unmapping pages from physical memory for memory mappings backed
    by anonymous files. To avoid data loss, a swap disk partition can be configured
    (the `swapon --show` command will show you the existence and usage of swap partitions
    in your Linux system). This disk space is then used to back up virtual memory
    pages from the anonymous file memory map. As you can imagine, this can cause a
    similar (if not worse) memory trashing situation and overall system slowdown.^([23](ch05.html#idm45606833609392))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A second option for the OS is to simply reboot the system, generally known as
    [the system-level OOM crash](https://oreil.ly/BboW0).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last option is to recover from the OOM situation by immediately terminating
    a few lower-priority processes (e.g., from the user space). This is typically
    done by the OS sending the [`SIGKILL` signal](https://oreil.ly/SLWOv). The detection
    of what processes to kill varies,^([24](ch05.html#idm45606833605136)) but if we
    want more determinisms, the system administrator can configure specific memory
    limits per process or group of processes using, for example, [`cgroups`](https://oreil.ly/E72wh)^([25](ch05.html#idm45606833602304))
    or [`ulimit`](https://oreil.ly/fF12F).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On top of the on-demand paging strategy, it’s worth mentioning that the OS never
    releases any frame pages from RAM at the moment of process termination or when
    it explicitly releases some virtual memory. Only virtual mapping is updated at
    that point. Instead, physical memory is mainly reclaimed lazily (on demand) with
    the help of [a page frame reclaiming algorithm (PFRA)](https://oreil.ly/ruKUM)
    that we won’t discuss in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, the `mmap` syscall might seem complex to use and understand. Yet,
    it explains what it means when our program allocates some RAM by asking the OS.
    Let’s now compose what we learned into the big picture of how the OS manages the
    RAM and talk about the consequences we developers might observe when dealing with
    a memory resource.
  prefs: []
  type: TYPE_NORMAL
- en: OS Memory Mapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The explicit memory mapping presented in [Example 5-3](#code-mmap-usage) is
    just one example of the possible OS memory mapping techniques. Besides, rare file-backed
    mapping and advanced off-heap solutions, there is almost no need to explicitly
    use such `mmap` syscalls in our Go programs. However, to manage virtual memory
    efficiently, the OS is transparently using the same technique of page memory mapping
    for nearly all the RAM! The example memory mappings situation is presented in
    [Figure 5-4](#img-mem-vm), which pulls into one graphic a few common page mapping
    situations we could have in our machine.
  prefs: []
  type: TYPE_NORMAL
- en: '![efgo 0504](assets/efgo_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. Example MMU translation of a few memory pages from the virtual
    memory of two processes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The situation in [Figure 5-4](#img-mem-vm) might look complicated, but we have
    already discussed some of those cases. Let’s enumerate them from the perspective
    of Process 1 or 2:'
  prefs: []
  type: TYPE_NORMAL
- en: Page `A`
  prefs: []
  type: TYPE_NORMAL
- en: Represents the simplest case of *anonymous file mapping* that has already mapped
    the frame on RAM. So, for example, if Process 1 writes or reads a byte from an
    address between `0x2000` and `0x2FFF` in its virtual space, the MMU will translate
    the address to RAM physical address `0x9000`, plus the required offset. As a result,
    the CPU will be able to fetch or write it as a cache line to its L-caches and
    desired register.
  prefs: []
  type: TYPE_NORMAL
- en: Page `B`
  prefs: []
  type: TYPE_NORMAL
- en: Represents a *file-based memory page* mapped to a physical frame like we created
    in [Example 5-3](#code-mmap-usage). This frame is also shared with another process
    since there is no need to keep two copies of the same data as both mappings map
    to the same file on a disk. This is only allowed if the mapping is not set as
    `MAP_PRIVATE`.
  prefs: []
  type: TYPE_NORMAL
- en: Page `C`
  prefs: []
  type: TYPE_NORMAL
- en: This is an anonymous file mapping that wasn’t yet accessed. For example, if
    Process 1 writes a byte to an address between `0x0` and `0xFFF`, a page fault
    hardware interrupt is generated by the CPU, and the OS will need to find a free
    frame.
  prefs: []
  type: TYPE_NORMAL
- en: Page `D`
  prefs: []
  type: TYPE_NORMAL
- en: This is an anonymous page like `C`, but some data was already written on it.
    Yet the OS seems to have `swap` enabled and unmaps it from RAM because this page
    was not used for a long time by Process 2, or the system is under memory pressure.
    The OS backed the data to swap files in the swap partition to avoid data loss.
    Process 2 accessing any byte from a virtual address between `0x1000` and `0x1FFF`
    would result in a page fault, which will tell the OS to find a free frame on RAM
    and read page `D` content from the swap file. Only then can data be available
    to Process 2\. Note that such swap logic for anonymous pages is disabled by default
    on most operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should now have a clearer view of OS memory management basics and virtual
    memory patterns. So let’s now go through a list of important consequences those
    pose on Go (and any other programming language):'
  prefs: []
  type: TYPE_NORMAL
- en: Practically speaking, observing the size of virtual memory is never useful.
  prefs: []
  type: TYPE_NORMAL
- en: On-demand paging is why we always see larger virtual memory usage (represented
    by virtual set size, or VSS) than resident memory usage (RSS) for a process (e.g.,
    the browser memory usage in [Figure 5-3](#img-mem-vss)). While the process thinks
    that all pages it sees on the virtual address space are in RAM, most of them might
    be currently unmapped and stored on disk (mapped file or swap partition). In most
    cases, you [can ignore](https://oreil.ly/u9l5k) the VSS metric when assessing
    the amount of memory your Go program uses.
  prefs: []
  type: TYPE_NORMAL
- en: It is impossible to tell precisely how much memory a process (or system) has
    used in a given time.
  prefs: []
  type: TYPE_NORMAL
- en: What metric can we use if the VSS metric does not help assess process memory
    usage? For Go developers interested in the memory efficiency of their programs,
    knowing the current and past memory usage is essential information. It tells how
    efficient our code is and if our optimizations work as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, because of the on-demand paging and memory mapping behavior we
    learned in this section, this is currently very hard—we can only roughly estimate.
    We will discuss the best available metrics in [“Memory Usage”](ch06.html#ch-obs-mem-usage),
    but don’t be surprised if the RSS metric shows a few kilobytes or even megabytes
    more or less than you expected.
  prefs: []
  type: TYPE_NORMAL
- en: OS memory usage expands to all available RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Due to lazy release and page caches, even if our Go process released all memory,
    sometimes the RSS will still look very high if there’s generally low memory pressure
    on the system. This means that there’s enough physical RAM to satisfy the rest
    of the processes, so the OS doesn’t bother to release our pages. This is often
    why the RSS metric is not very reliable, as discussed in [“Memory Usage”](ch06.html#ch-obs-mem-usage).
  prefs: []
  type: TYPE_NORMAL
- en: Tail latency of our Go program memory access is much slower than just physical
    DRAM access latency.
  prefs: []
  type: TYPE_NORMAL
- en: There is a high price to pay for using OS with virtual memory. In the worst
    cases, already slow memory access caused by DRAM design (mentioned in [“Physical
    Memory”](#ch-hw-memory-ph)) is even slower. If we stack up things that can happen,
    like TLB miss, page fault, looking for a free page, or on-demand memory loading
    from disk, we have extreme latency, which can waste thousands of CPU cycles. The
    OS does as much as possible to ensure those bad cases rarely happen, so the amortized
    (average) access latency is as low as possible.
  prefs: []
  type: TYPE_NORMAL
- en: As Go developers, we have some control to reduce the risk of those extra latencies
    happening more often. For example, we can use less memory in our programs or prefer
    sequential memory access (more on that later).
  prefs: []
  type: TYPE_NORMAL
- en: High usage of RAM might cause slow program execution.
  prefs: []
  type: TYPE_NORMAL
- en: When our system executes many processes that want to access large quantities
    of pages close to RAM capacity, memory access latencies and OS cleanup routines
    can take most of the CPU cycles. Furthermore, as we discussed, things like memory
    trashing, constant memory swaps, and page reclaim mechanisms will slow the whole
    system. As a result, if your program latency is high, it is not necessarily doing
    too much work on the CPU or executing slow operations (e.g., I/O), it might just
    use a lot of the memory!
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, you understand the impact of OS memory management on how we should
    think about the memory resource. As in [“Physical Memory”](#ch-hw-memory-ph),
    I only explained the basics of memory management. This is because the kernel algorithms
    evolve, and different OSes manage memory differently. The information I provided
    should give you a rough understanding of the standard techniques and their consequences.
    Such a foundation should also give you a kick-start toward learning more from
    materials like [*Understanding the Linux Kernel*](https://oreil.ly/Wr1nY) by Daniel
    P. Bovet and Marco Cesati (O’Reilly) or [LWN.net](https://lwn.net).
  prefs: []
  type: TYPE_NORMAL
- en: With that knowledge, let’s discuss how Go has chosen to leverage the memory
    functionalities the OS and hardware offer. It should help us find the right optimizations
    to try in our TFBO flow if we have to focus on the memory efficiency of our Go
    program.
  prefs: []
  type: TYPE_NORMAL
- en: Go Memory Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The programming language task here is to ensure that developers who write programs
    can create variables, abstractions, and operations that use memory safely, efficiently,
    and (ideally) without fuss! So let’s dig into how the Go language enables that.
  prefs: []
  type: TYPE_NORMAL
- en: Go uses a relatively standard internal process memory management pattern that
    other languages (e.g., C/C++) share, with some unique elements. As we learned
    in [“Operating System Scheduler”](ch04.html#ch-hw-os-scheduler), when a new process
    starts, the operating system creates various metadata about the process, including
    a new dedicated virtual address space. The OS also creates initial memory mappings
    for a few starting segments based on information stored in the program binary.
    Once the process starts, it uses `mmap` or [`brk/sbrk`](https://oreil.ly/31emh)^([26](ch05.html#idm45606833533232))
    to dynamically allocate more pages on virtual memory when needed. An example organization
    of the virtual memory in Go is presented in [Figure 5-5](#img-mem-layout).
  prefs: []
  type: TYPE_NORMAL
- en: '![efgo 0505](assets/efgo_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. Memory layout of an executed Go program in virtual address space
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can enumerate a couple of common sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.text`, `.data`, and shared libraries'
  prefs: []
  type: TYPE_NORMAL
- en: Program code and all global data like global variables are automatically memory
    mapped by the OS when the process starts (whether it takes 1 MB or 100 GB of virtual
    memory). This data is read-only, backed up by the binary file. Additionally, only
    a small contiguous part of the program is executed at a time by the CPU so that
    the OS can keep a minimal amount of pages with code and data in the physical memory.
    Those pages are also heavily shared (more processes are started using the same
    binary, plus some dynamically linked shared libraries).
  prefs: []
  type: TYPE_NORMAL
- en: Block starting symbol (`.bss`)
  prefs: []
  type: TYPE_NORMAL
- en: When OS starts a process, it also allocates anonymous pages for uninitialized
    data (`.bss)`. The amount of space used by `.bss` is known in advance—for example,
    the `http` package defines the [`DefaultTransport`](https://oreil.ly/7m0Wv) global
    variable. While we don’t know the value of this variable, we know it will be a
    pointer, so we need to prepare eight bytes of memory for it. This type of memory
    allocation is called static allocation. This space is allocated once, backed by
    anonymous pages, and is never freed (from virtual memory at least; if swapping
    is enabled, it can be unmapped from RAM).
  prefs: []
  type: TYPE_NORMAL
- en: Heap
  prefs: []
  type: TYPE_NORMAL
- en: 'The first (and probably the most important) dynamic segment in [Figure 5-5](#img-mem-layout)
    is the memory reserved for dynamic allocations, typically called the *heap* (do
    not confuse it with the [data structure](https://oreil.ly/740nv) with the same
    name). Dynamic allocations are required for program data (e.g., variables) that
    have to be available outside a single function scope. As a result, such allocations
    are unknown in advance and must be stored in memory for an unpredictable time.
    When the process starts, the OS prepares the initial number of anonymous pages
    for the heap. After that, the OS gives the process some control over that space.
    It can then increase or decrease its size using the `sbrk` syscall or by preparing
    or removing extra virtual memory using the `mmap` and `unmmap` syscalls. It’s
    up to the process to organize and manage the heap in the best possible way, and
    different languages do that differently:'
  prefs: []
  type: TYPE_NORMAL
- en: C forces the programmer to manually allocate and free memory for variables (using
    `malloc` and `free` functions).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C++ adds smart pointers like [`std::unique_ptr`](https://oreil.ly/QS9zj) and
    [`std::shared_ptr`](https://oreil.ly/QbQqQ), which offer simple counting mechanisms
    to track the object lifecycle (reference counting).^([27](ch05.html#idm45606833508688))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rust has a powerful [memory ownership mechanism](https://oreil.ly/MajFo), but
    it makes programming much more difficult for nonmemory critical code areas.^([28](ch05.html#idm45606833506480))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, languages like Python, C#, Java, and others implement advanced heap
    allocators and garbage collector mechanisms. Garbage collectors periodically check
    if any memory is unused and can be released.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this sense, Go is closer to Java with memory management than C. Go implicitly
    (transparently to the programmer) allocates memory that requires dynamic allocation
    on the heap. For that purpose, Go has its unique components (implemented in Go
    and Assembly); see [“Go Allocator”](#ch-hw-allocator) and [“Garbage Collection”](#ch-hw-garbage).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Most of the Time, It’s Enough to Optimize the Heap Usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Heap is the memory that usually stores the largest amounts of data in physical
    memory pages. It is so significant that it’s enough to look at the heap size to
    assess the Go process memory usage in most cases. On top of that, the overhead
    of heap management with runtime garbage collection is significant too. Both make
    the heap our first choice to analyze when optimizing memory use.
  prefs: []
  type: TYPE_NORMAL
- en: Manual process mappings
  prefs: []
  type: TYPE_NORMAL
- en: Both Go runtime and the developer writing Go code can manually allocate additional
    memory-mapped regions (e.g., using our [Example 5-1](#code-mmap) abstraction).
    Of course, it’s up to the process what kind of memory mapping to use (private
    or shared, read or write, anonymous or file backed), but all of them have a dedicated
    space in the process’s virtual memory, presented in [Figure 5-5](#img-mem-layout).
  prefs: []
  type: TYPE_NORMAL
- en: Stack
  prefs: []
  type: TYPE_NORMAL
- en: The last section of the Go memory layout is reserved for function stacks. The
    stack is a simple yet fast structure allowing accessing values in last in, first
    out (LIFO) order. Programming languages use them to store all the elements (e.g.,
    variables) that can use automatic allocation. As opposed to dynamic allocations
    fulfilled by the heap, automatic allocations work well for local data like local
    variables, function input, or return arguments. Allocations of those elements
    can be “automatic” because the compiler can deduce their lifespan before the program
    starts.
  prefs: []
  type: TYPE_NORMAL
- en: Some programming languages might have a single stack or a stack per thread.
    Go is a bit unique here. As we learned in [“Go Runtime Scheduler”](ch04.html#ch-hw-concurrency),
    the Go execution flow is designed around goroutines. Thus Go maintains a single
    dynamically sized stack per Go routine. This might even mean [hundreds of thousands
    of stacks](https://oreil.ly/zrqhj). Whenever the goroutine invokes another function,
    we can push its local variables and arguments to stack in a stack frame. We can
    pop those elements (de-allocate the stack frame) from the stack when we leave
    the function. If stack structures require more space than what’s reserved in virtual
    memory, Go will ask the OS for more memory attributed to the stack segment, e.g.,
    via the `mmap` syscall.
  prefs: []
  type: TYPE_NORMAL
- en: Stacks are incredibly fast as there is no extra overhead to figure out when
    memory used by certain elements must be removed (no usage tracking). Thus ideally,
    we write our algorithms so that they allocate primarily on the stack instead of
    the heap. Unfortunately, this is impossible in many cases due to stack limitations
    (we can’t allocate too-large objects) or when the variable has to live longer
    than the function’s scope. Therefore, the compiler decides which data can be allocated
    automatically (on the stack) and which must be allocated dynamically (on the heap).
    This process is called escape analysis, which you saw in [Example 4-3](ch04.html#code-comp-sum).
  prefs: []
  type: TYPE_NORMAL
- en: 'All the mechanisms discussed (except manual mappings) are helping Go developers.
    We don’t need to care where and how we should allocate memory for our variables.
    That is a huge win—for example, when we want to make some HTTP calls, we simply
    create an HTTP client using a standard library, e.g., with the `client := http.Cli⁠ent{}`
    code statement. As a result of Go’s memory design, we can immediately start using
    `client`, focusing on our code’s functionality, readability, and reliability.
    In particular:'
  prefs: []
  type: TYPE_NORMAL
- en: We don’t need to ensure that the OS has a free virtual memory page to hold the
    `client` variable. Likewise, we don’t need to find a valid segment and virtual
    address for it. Both will be done automatically by the compiler (if the variable
    can be stored on the stack) or runtime allocator (dynamic allocation on the heap).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We don’t need to remember to release memory kept by the `client` variable when
    we stop using it. Instead, suppose the `client` would go beyond code reach (nothing
    references it). In that case, the data in Go will be released—immediately when
    stored on the stack or in the next garbage collection execution cycle if stored
    on the heap (more on that in [“Garbage Collection”](#ch-hw-garbage)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Such automation is much less error-prone to potential memory leaks (“I forgot
    to release memory for `client`”) or dangling pointers (“I released memory for
    `client`, but actually some code still uses it”).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Generally, we don’t need to care what segment is used for our objects for everyday
    use of the Go language.
  prefs: []
  type: TYPE_NORMAL
- en: How do I know whether a variable is allocated on the heap or the stack? From
    a correctness standpoint, you don’t need to know. Each variable in Go exists as
    long as there are references to it. The storage location chosen by the implementation
    is irrelevant to the semantics of the language.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The storage location does have an effect on writing efficient programs.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The Go Team, [“Go: Frequently Asked Questions (FAQ)”](https://oreil.ly/UUGgI)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: However, since allocations are so effortless, there is a risk of not noticing
    the memory waste.
  prefs: []
  type: TYPE_NORMAL
- en: Transparent Allocations Mean There Is a Risk of Overdoing Them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Allocations are implicit in Go, making coding much easier, but there are trade-offs.
    One is around memory efficiency: if we don’t see explicit memory allocations and
    releases, it’s easier to miss apparent high memory usage in our code.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s similar to going shopping with cash versus a credit card. You will likely
    overspend with a credit card than with cash since you don’t see that money flowing.
    With a credit card, money spent is almost transparent to us—it is the same with
    allocations in Go.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, Go is a very productive language because, when programming, we don’t
    need to worry about where and how the data held by our variables and abstractions
    is stored. Yet sometimes when our measurements indicate efficiency problems, it’s
    useful to have a basic awareness of the parts of our program that might allocate
    some memory, how this occurs, and how the memory is released. So let’s uncover
    that.
  prefs: []
  type: TYPE_NORMAL
- en: Values, Pointers, and Memory Blocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s get this straight before we start—you don’t need to know what type of
    statements trigger memory allocation, where (on a stack or heap), and how much
    memory was allocated. But, as you will learn in Chapters [7](ch07.html#ch-observability2)
    and [9](ch09.html#ch-observability3), many robust tools can tell us all that accurately
    and quickly. In most cases, we can find what code line and roughly how much was
    allocated within seconds. Thus, there is generally a common theme: we should not
    guess that information (since humans tend to guess wrong) because there are tools
    for that.'
  prefs: []
  type: TYPE_NORMAL
- en: This is generally true, but there is no harm in building some basic allocation
    awareness. On the contrary, it might make us more effective while using those
    tools to analyze memory usage. The aim is to build a healthy instinct for what
    pieces of code can potentially allocate the suspicious amount of memory and where
    we need to be careful.
  prefs: []
  type: TYPE_NORMAL
- en: Many books try to teach this by listing examples of common statements that allocate.
    This is great, but it’s a bit like giving someone [a fish instead of a fishing
    rod](https://oreil.ly/utQIG). So again, it’s helpful, but only for “common” statements.
    Ideally, I want you to understand the underlying rules for why something allocates.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into how we reference objects in Go to start noticing that allocation
    more quickly. Our code can perform certain operations on objects stored in some
    memory. Therefore, we must link those objects to operations, and we typically
    do that via variables. We describe those variables using Go’s type system to make
    it even easier for the compiler and developers.
  prefs: []
  type: TYPE_NORMAL
- en: However, Go is [value oriented](https://oreil.ly/lgy2S) rather than reference
    oriented (like many [managed runtime](https://oreil.ly/ben85) languages). This
    means that Go variables never reference objects. Instead, the variables always
    store the whole *value* of the object. There is no exception to this rule!
  prefs: []
  type: TYPE_NORMAL
- en: To understand this better, the memory representation of three variables is shown
    in [Figure 5-6](#img-mem-blocks).
  prefs: []
  type: TYPE_NORMAL
- en: '![efgo 0506](assets/efgo_0506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. Representation of three variables allocated on the process’s virtual
    memory
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Think About Variables as Boxes Holding Values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever the compiler sees a definition of the `var` variable or function arguments
    (including parameters) in the invocation scope, it allocates a contiguous “memory
    block” for a box. The box is big enough to contain the whole value of the given
    type. For example, `var var1 int` and `var var2 int` will need a box for eight
    bytes.^([29](ch05.html#idm45606833447648))
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to our available space in “boxes,” we can copy some values. In [Figure 5-6](#img-mem-blocks),
    we can copy an integer `1` to `var1`. Now, Go does not have reference variables,
    so even if we assign the `var1` value to another box named `var2`, this is yet
    another box with unique space. We can confirm that by printing `&var1` and `&var2`.
    It should print `0xA040` and `0xA038`, respectively. As a result, a simple assignment
    is always a copy, which adds latency proportional to the value’s size.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike C++, each variable defined in a Go program occupies a unique memory location.
    It is not possible to create a Go program where two variables share the same storage
    location in memory. It is possible to create two variables whose contents point
    to the same storage location, but that is not the same thing.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dave Cheney, [“There Is No Pass-By-Reference in Go”](https://oreil.ly/iPu5w)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The `var3` box is a pointer to the integer type. A “pointer” variable is a box
    that stores the value representing the memory address. The type of memory address
    is just `uintptr` or `unsafe.Pointer`, so simply a 64-bit unsigned integer that
    allows pointing to another value in memory. As a result, any pointer variable
    needs a box for eight bytes.
  prefs: []
  type: TYPE_NORMAL
- en: The pointer can also be `nil` (Go’s NULL value), a special value indicating
    that the pointer does not point to anything. In [Figure 5-6](#img-mem-blocks),
    we can see that the `var3` box contains a value too—a memory address of the `var1`
    box.
  prefs: []
  type: TYPE_NORMAL
- en: This is also consistent with more complex types. For example, both `var var4`
    and `var var5` require boxes for only 24 bytes. This is because the `slice` struct
    value has three integers.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Structure for Go Slice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Slice allows easy dynamic behavior of the underlying array of a given type.
    A slice data structure requires a memory block that can hold `length`, `capacity`,
    and `pointer` to the desired array.^([30](ch05.html#idm45606833426272))
  prefs: []
  type: TYPE_NORMAL
- en: Generally, the slice is just a more complex struct. You can think about a struct
    as a cabinet—it is full of drawers (struct fields) that are simply boxes that
    share a memory block with other drawers in the same cabinet. So, for example,
    the `slice` type has three drawers. One of them is of pointer type.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two special behaviors of `slice` and a few other special types:'
  prefs: []
  type: TYPE_NORMAL
- en: You can use the [`make`](https://oreil.ly/Mlx6Q) built-in function that only
    works for `map`, `chan`, and `slice` types. It returns the type’s value^([31](ch05.html#idm45606833418784))
    and allocates underlying structures, like an array for slices, a buffer for channels,
    and a hashmap for maps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can put `nil` into boxes of types, like `func`, `map`, `chan`, or `slice`,
    although they are not strictly pointers, e.g., `[]byte(nil)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One drawer of the `var4` and `var5` cabinets is a type of pointer that holds
    the memory address. Thanks to `make([]byte, 5000)` in `var5`, it points to another
    memory block containing a 5,000-element byte array.
  prefs: []
  type: TYPE_NORMAL
- en: Structure Padding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The slice structure with three 64-bit fields requires a 24-byte long memory
    block. But the memory block size for a structure type is not always the sum of
    the size of its fields!
  prefs: []
  type: TYPE_NORMAL
- en: Smart compilers like in Go might attempt to align type sizes to the typical
    cache lines or the OS or internal Go allocator page sizes. For this reason, Go
    compilers sometimes add padding between fields.^([32](ch05.html#idm45606833407008))
  prefs: []
  type: TYPE_NORMAL
- en: 'To reinforce that knowledge, let’s ask a common question when designing a new
    function or method: should my arguments be pointers of values? Of course, the
    first thing we should answer is obviously, if we want the caller to see the modifications
    of that value. But there is an efficiency aspect as well. Let’s discuss the difference
    in [Example 5-4](#code-copy-ptr), assuming we don’t need to see modifications
    of those arguments from outside.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-4\. Different arguments highlight the differences using values, pointers,
    and special types like `slice`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_how_go_uses_memory_resource_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Function arguments are like any newly declared variable: boxes. So for `arg1`,
    it will create an eight-byte box (most likely allocate it on the stack) and copy
    the passed integer during the `myFunction` invocation. For `arg2`, it will create
    a similar eight-byte box that will copy the pointer instead.'
  prefs: []
  type: TYPE_NORMAL
- en: For such simple types, avoiding the pointer makes more sense if you don’t need
    to modify the value. You use the same amount of memory and the same copying overhead.
    The only difference is that the value pointed to by `arg2` has to live on the
    heap, which is more expensive and, in many cases, can be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_how_go_uses_memory_resource_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The rule is the same for custom `struct` arguments, but the size and copying
    overhead might matter more. For example, `arg3` is of `biggie` `struct`, which
    is of extraordinary size. Because of the static array with 100 million elements,
    the type requires a ~100 MB memory block.
  prefs: []
  type: TYPE_NORMAL
- en: For bigger types like this, we should consider using a pointer when passing
    through functions. This is because every `myFunction` invocation will allocate
    100 MB on the heap for the `arg3` box (it’s too large to be on the stack)! On
    top of that, it will spend CPU time copying large objects between boxes. So, `arg4`
    will allocate eight bytes on the stack (and copy only that) and point to memory
    on the heap with the `biggie` object, which can be reused across function calls.
  prefs: []
  type: TYPE_NORMAL
- en: Note that despite `biggie` being copied in `arg3`, the copy is *shallow*, i.e.,
    `arg3.other` will share a memory with the previous box!
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_how_go_uses_memory_resource_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The `slice` type behaves like the `biggie` type. We must remember the [underlying
    `struct` type of the slice](https://oreil.ly/Tla4w).
  prefs: []
  type: TYPE_NORMAL
- en: As a result, `arg5` will allocate a 24-byte box and copy three integers. In
    contrast, `arg6` will allocate an eight-byte box and copy only one integer (pointer).
    From the efficiency point of view, it does not matter. It only matters if we want
    to expose modifications of the underlying array (both `arg5` and `arg6` allow
    that) or if we want to also expose changes to the `pointer`, `len`, and `cap`
    fields as `arg6` allows.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_how_go_uses_memory_resource_CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Special types like `chan`, `map`, and `func()` can be treated similarly to pointers.
    They share memory through the heap, and the only cost is to allocate and copy
    the pointer value into `arg7`, `arg8`, or `arg9` boxes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same decision flow can be applied to decide about pointer versus value
    types for:'
  prefs: []
  type: TYPE_NORMAL
- en: Return arguments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `struct` fields
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elements of map, slice, or channels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The method receiver (e.g., `func (receiver) Method()`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hopefully, the preceding information will give you an understanding of which
    Go code statements allocate memory and roughly how much. Generally:'
  prefs: []
  type: TYPE_NORMAL
- en: Every variable declaration (including function arguments, return arguments,
    and method receiver) allocates the whole type or just a pointer to it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`make` allocates special types and their underlying (pointed) structures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new(<type>)` is the same as `&<type>`, so it allocates a pointer box and the
    type on the heap in the separate memory block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Most program memory allocations are only known in runtime; thus, dynamic allocation
    (in a heap) is needed. Therefore, when we optimize memory in Go programs, 99%
    of the time we just focus on the heap. Go comes with two important runtime components:
    Allocator and GC, responsible for heap management. Those components are nontrivial
    pieces of software that often introduce certain waste in terms of extra CPU cycles
    by the program runtime and some memory waste. Given its nondeterministic and nonimmediate
    memory release nature, it’s worth discussing this in detail. Let’s do that in
    the next two sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Go Allocator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s far from easy to manage the heap, as it poses similar challenges as the
    OS has toward physical memory. For example, the Go program runs multiple goroutines,
    and each wants a few (dynamically sized!) segments of the heap memory for a different
    amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: The Go Allocator is a piece of internal runtime Go code maintained by the Go
    team. As the name suggests, it can dynamically (in runtime) allocate the memory
    blocks required to operate on objects. In addition, it is optimized to avoid locking
    and fragmentation, and to mitigate slow syscalls to the OS.
  prefs: []
  type: TYPE_NORMAL
- en: During compilation, the Go compiler performs a complex stack escape analysis
    to detect if the memory for objects can be automatically allocated (mentioned
    in [Example 4-3](ch04.html#code-comp-sum)). If yes, it adds appropriate CPU instructions
    that store related memory blocks in the stack segment of the memory layout. However,
    in most cases the compiler can’t avoid putting most of our memory on the heap.
    In these cases, it generates different CPU instructions invoking the Go Allocator
    code.
  prefs: []
  type: TYPE_NORMAL
- en: The Go Allocator is responsible for [bin packing](https://oreil.ly/l27Jv) the
    memory blocks in the virtual memory space. It also asks for more space from the
    OS if needed using `mmap` with private, anonymous pages, which are initialized
    by zero.^([33](ch05.html#idm45606833200480)) As we learned in [“OS Memory Mapping”](#ch-hw-memory-mmap-os),
    those pages are also allocated on the physical RAM only when accessed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, the Go developer can live without learning details about Go Allocator
    internals. However, it’s enough to remember that:'
  prefs: []
  type: TYPE_NORMAL
- en: It is based on a custom Google `C++` `malloc` implementation called [TCMalloc](https://oreil.ly/AZ5S7).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is OS virtual memory page aware, but it operates with 8 KB pages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It mitigates fragmentation by allocating memory blocks to certain spans that
    hold one or multiple 8 KB pages. Each span is created for class memory block sizes.
    For example, in Go 1.18, there are 67 different [size classes](https://oreil.ly/tMlnv)
    (size buckets), the largest being 32 KB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory blocks for objects that do not contain a pointer are marked with the
    `noscan` type, making it easier to track nested objects in the garbage collection
    phase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objects with over 32 KB memory block (e.g., 600 MB byte array) are treated specially
    (allocated directly without span).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If runtime needs more virtual space from OS for the heap, it allocates a bigger
    chunk of memory at once (at least 1 MB), which amortizes the latency of the syscall.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the preceding points are constantly changing, with the open source community
    and Go team adding various small optimizations and features.
  prefs: []
  type: TYPE_NORMAL
- en: They say one code snippet is worth a thousand words, so let’s visualize and
    explain some of these allocation characteristics caused by a mix of Go, OS, and
    hardware using an example. [Example 5-5](#code-mem-alloc-slice) shows the same
    functionality as [Example 5-3](#code-mmap-usage), but instead of explicit `mmap`,
    we will rely on Go memory management and no underlying file.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-5\. Allocation of a large `[]byte` slice followed by different access
    patterns
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_how_go_uses_memory_resource_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `b` variable is declared as a `[]byte` slice. The following `make` statement
    is tasked to create a byte array with 600 MB of data (~600 million elements in
    the array). This memory block is allocated on the heap.^([34](ch05.html#idm45606833118576))
  prefs: []
  type: TYPE_NORMAL
- en: 'If we would analyze this situation closely, the Go Allocator seemed to create
    three contiguous anonymous mappings for that slice with different (virtual) memory
    sizes: 2 MB, 598 MB, and 4 MB. (The total size is usually bigger than the requested
    600 MB because of the Go Allocator internal bucketed algorithm.) Let’s summarize
    the interesting statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The RSS for three memory mappings used by our slice: 548 KB, 0 KB, and 120
    KB (much lower than VSS numbers).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total RSS of the whole process shows 21 MB. Profiling shows that most of this
    comes from outside the heap.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go reports 600.15 MB of the heap size (despite RSS being significantly lower).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_how_go_uses_memory_resource_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Only after we start accessing the slice elements (either by writing or reading)
    will the OS start reserving actual physical memory surrounding those elements.
    Our statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The RSS for three memory mappings: 556 KB, (still) 0 KB, and 180 KB (only a
    few KB more than before accessing).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total RSS still shows 21 MB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go reports 600.16 MB of the heap size (actually a few KB more, probably due
    to background goroutines).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_how_go_uses_memory_resource_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: 'After we loop over all elements to access it, we will see that the OS mapped
    on demand all pages for our `b` slice in physical memory. Our statistics prove
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The RSS for three memory mappings: 1.5 MB, (fully mapped) 598 MB, and 1.2 MB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total RSS of the whole process shows 621.7 MB (finally, same as heap size).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go reports the same 600.16 MB of the heap size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This example might feel similar to Examples [5-2](#code-naive-read-usage) and
    [5-3](#code-mmap-usage), but it’s a bit different. Notice that in [Example 5-5](#code-mem-alloc-slice),
    there is no (explicit) file involved that could store some data if the page is
    not mapped. We also utilize the Go Allocator to organize and manage different
    anonymous page mappings most efficiently, whereas in [Example 5-3](#code-mmap-usage),
    the Go Allocator is unaware of that memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Internal Go Runtime Knowledge Versus OS Knowledge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Go Allocator tracks certain information we can collect through different
    observability mechanisms discussed in [Chapter 6](ch06.html#ch-observability).
  prefs: []
  type: TYPE_NORMAL
- en: Be mindful when using those. In the preceding example, we saw that the heap
    size tracked by the Go Allocator was significantly larger than the actual amount
    of memory used on physical RAM (RSS)!^([35](ch05.html#idm45606833047648)) Similarly,
    the memory used by explicit `mmap`, as in [Example 5-3](#code-mmap-usage), is
    not reflected in any Go runtime metrics. This is why it’s good to rely on more
    than one metric on our TFBO journey, as discussed in [“Memory Usage”](ch06.html#ch-obs-mem-usage).
  prefs: []
  type: TYPE_NORMAL
- en: The behavior of Go heap management backed up by on-demand paging tends to be
    indeterministic and fuzzy. We cannot control it directly either. For instance,
    if you tried to reproduce [Example 5-5](#code-mem-alloc-slice) on your machine,
    you would most likely observe slightly different mappings, more or less different
    RSS numbers (with a tolerance of few MBs), and different heap sizes. It all depends
    on the Go version you build a program with, the kernel version, the RAM capacity
    and model, and the load on your system. This poses important challenges to the
    assessment step of our TFBO process, which we will discuss in [“Reliability of
    Experiments”](ch07.html#ch-obs-rel).
  prefs: []
  type: TYPE_NORMAL
- en: Don’t Be Bothered by a Small Memory Increase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Don’t try to understand where every hundred bytes or kilobytes of your process
    RSS memory came from. In most cases, it is impossible to tell or control at that
    low level. Heap management overhead, speculative page allocations by both the
    OS and the Go Allocator, dynamic OS mapping behavior, and eventual memory collection
    (we will learn about that in the next section) make things indeterministic on
    such a “micro” kilobyte level.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you spot some pattern in one environment, it will be different in others
    unless we talk about bigger numbers like hundreds of megabytes or more!
  prefs: []
  type: TYPE_NORMAL
- en: The lesson here is that we have to adjust our mindsets. There will always be
    a few unknowns. What matters is to understand bigger unknowns that contribute
    the most to the potentially too-high memory usage situation. Together with this
    allocator awareness, you will learn how to do that in Chapters [6](ch06.html#ch-observability)
    and [9](ch09.html#ch-observability3).
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have discussed how to efficiently reserve memory for our memory blocks
    through the Go Allocator and how to access it. However, we can’t just reserve
    more memory indefinitely if there is no logic for removing the memory blocks our
    code doesn’t need anymore. That’s why it’s critical to understand the second part
    of heap management responsible for releasing unused objects from the heap—garbage
    collection. Let’s explore that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Garbage Collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You pay for memory allocation more than once. The first is obviously when you
    allocate it. But you also pay every time the garbage collection runs.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Damian Gryski, [“go-perfbook”](https://oreil.ly/yg1LK)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The second part of heap management is similar to vacuuming your house. It is
    related to a process that removes the proverbial garbage—unused objects from the
    program’s heap. Generally speaking, the garbage collector (GC) is an additional
    background routine that executes “collection” at certain moments. The cadence
    of collections is critical:'
  prefs: []
  type: TYPE_NORMAL
- en: If the GC runs less often, we risk allocating a significant amount of new RAM
    space without the ability to reuse the memory pages currently allocated by garbage
    (unused objects).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the GC runs too often, we risk spending most of the program time and CPU
    on GC work instead of moving our functionality forward. As we will learn later,
    the GC is relatively fast but can directly or indirectly impact the execution
    of other goroutines in the system, especially if we have many objects in a heap
    (if we allocate a lot).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The interval of the GC runs is not based on time. Instead, two configuration
    variables (working independently) define the pace: `GOGC` and, from Go 1.19, `GOMEMLIMIT`.
    To learn more about them, read [an official detailed guide about GC tuning](https://oreil.ly/f2F6H).
    For this book, let’s explain both very briefly:'
  prefs: []
  type: TYPE_NORMAL
- en: The `GOGC` option represents the “GC percentage.”
  prefs: []
  type: TYPE_NORMAL
- en: '`GOGC` is enabled by default with a 100 value. It means that the next GC collection
    will be done when the heap size expands to 100% of the size it has at the end
    of the last GC cycle. GC’s pacing algorithm estimates when that goal will be reached
    based on current heap growth. It can also be set programmatically with the [`debug.SetGCPercent`
    function](https://oreil.ly/7khRe).'
  prefs: []
  type: TYPE_NORMAL
- en: The `GOMEMLIMIT` option controls the soft memory limit.
  prefs: []
  type: TYPE_NORMAL
- en: The `GOMEMLIMIT` option was introduced in Go 1.19\. It is disabled by default
    (set to `math.MaxInt64`), and offers running GC more often when we are close (or
    above) the set memory limit. It can be used with `GOGC=off` (disabled) or together
    with `GOGC`. This option can also be set programmatically with the [`debug.Set​Me⁠moryLimit`
    function](https://oreil.ly/etDUv).
  prefs: []
  type: TYPE_NORMAL
- en: GOMEMLIMIT Does Not Prevent Your Program from Allocating More than the Set Value!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The GC’s soft memory limit configuration is called “soft” for a reason. It tells
    the GC how much memory overhead space there is for the GC “laziness” to save the
    CPU.
  prefs: []
  type: TYPE_NORMAL
- en: However, when your program allocates and uses more memory than the desired limit,
    with the `GOMEMLIMIT` option set, it will only make things worse. This is because
    the GC will run nearly continuously, taking up 25% of the precious CPU time from
    other functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: We still have to optimize the memory efficiency of our programs!
  prefs: []
  type: TYPE_NORMAL
- en: Manual trigger.
  prefs: []
  type: TYPE_NORMAL
- en: Programmers can also trigger another GC collection on demand by invoking [`runtime.GC()`](https://oreil.ly/znoCL).
    It is mostly used in testing or benchmarking code, as it can block the entire
    program. Other pacing configurations like `GOGC` and `GOMEMLIMIT` might run in
    between.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Go GC implementation can be described as [the concurrent, nongenerational,
    tricolor mark and sweep collector](https://oreil.ly/vvOgl) implementation. Whether
    invoked by the programmer or by the runtime-based `GOGC` or `GOMEMLIMIT` option,
    the `runtime.GC()` implementation comprises a few phases. The first one is a mark
    phase that has to:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform a “stop the world” (STW) event to inject an essential [write barrier](https://oreil.ly/Sl9PI)
    (a lock on writing data) into all goroutines. Even though STW is relatively fast
    (10–30 microseconds on average), it is pretty impactful—it suspends the execution
    of all goroutines in our process for that time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to use 25% of the CPU capacity given to the process to concurrently mark
    all objects in the heap that are still in use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Terminate marking by removing the write barrier from the goroutines. This requires
    another STW event.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After the mark phase, the GC function is generally complete. As interesting
    as it sounds, the GC doesn’t release any memory! Instead, the sweeping phase releases
    objects that were not marked as in use. It is done lazily: every time a goroutine
    wants to allocate memory through the Go Allocator, it must perform a sweeping
    work first, then allocate. This is counted as an `allocation` latency, even though
    it is technically a garbage collection functionality—worth noting!'
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, the Go Allocator and GC compose a sophisticated implementation
    of bucketed [object pooling](https://oreil.ly/r1K18), where each pool of slots
    of different sizes are prepared for incoming allocations. When an allocation is
    not needed anymore, it is eventually released. The memory space for this allocation
    is not immediately released to the OS since it can be assigned to another incoming
    allocation soon (this is similar to the pooling pattern using `sync.Pool` we will
    discuss in [“Memory Reuse and Pooling”](ch11.html#ch-basic-pool)). When the number
    of free buckets is big enough, Go releases memory to the OS. But even then, it
    does not necessarily mean that runtime deletes mapped regions straight away. For
    example, on Linux, Go runtime typically “releases” memory through the [`madvise`
    syscall](https://oreil.ly/pxXum) with the `MADV_DONTNEED` argument by default.^([36](ch05.html#idm45606832989440))
    This is because our mapped region might be needed again pretty soon, so it’s faster
    to keep them just in case and ask the OS to take them back only if other processes
    require this physical memory.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, when applied to shared mappings, `MADV_DONTNEED` might not lead to
    immediate freeing of the pages in the range. The kernel is free to delay freeing
    the pages until an appropriate moment. The resident set size (RSS) of the calling
    process will be immediately reduced, however.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Linux Community, ["`madvise(2)`, Linux Manual Page”](https://oreil.ly/JDuS7)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With the theory behind the GC algorithm, it will be easier for us to understand
    in [Example 5-6](#code-mem-dealloc-slice) what happens if we try to clean the
    memory used for the large, 600 MB byte slice we created in [Example 5-5](#code-mem-alloc-slice).
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-6\. Memory release (de-allocation) of large slice created in [Example 5-5](#code-mem-alloc-slice)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_how_go_uses_memory_resource_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we discussed in [Example 5-5](#code-mem-alloc-slice), the statistics after
    allocating a large slice and accessing all elements might look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Slice is allocated in three memory mappings with the corresponding virtual
    memory size (VSS) numbers: 2 MB, 598 MB, and 4 MB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The RSS for three memory mappings: 1.5 MB, 598 MB, and 1.2 MB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total RSS of the whole process shows 621.7 MB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go reports 600.16 MB of the heap size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_how_go_uses_memory_resource_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: After the last statement where data from `b` is accessed, even before `b = nil`,
    the `Mark` phase of GC would consider `b` as a “garbage” to clean. Yet, the GC
    has its own pace; thus, immediately after this statement, no memory will be released—memory
    statistics will be the same.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_how_go_uses_memory_resource_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: In typical cases when you no longer use the `b` value and the function scope
    ends, or you will replace `b` content with a pointer to a different object, there
    is no need for an explicit `b = nil` statement. The GC will know that the array
    pointed to by `b` is garbage. Yet sometimes, especially on long-living functions
    (e.g., a goroutine that performs background job items delivered by the Go channel),
    it is useful to set the variable to `nil` to make sure the next GC run will mark
    it for cleaning earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_how_go_uses_memory_resource_CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our tests, let’s invoke the GC manually to see what happens. After this
    statement, the statistics will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: All three memory mappings still exist, with the same VSS values. This proves
    what we mentioned about the Go Allocator only advising on memory mappings, not
    removing those straightaway!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The RSS for three memory mappings: 1.5 MB, 0 (RSS released), and 60 KB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total RSS of the whole process shows 21 MB (back to the initial number).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go reports 159 KB of the heap size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_how_go_uses_memory_resource_CO6-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s allocate another twice smaller slice. The following memory statistics
    prove the theory that Go will try to reuse previous memory mappings!
  prefs: []
  type: TYPE_NORMAL
- en: Same three memory mappings still exist, with the same VSS values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The RSS for three memory mappings: 1.5 MB, 300 MB, and 60 KB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total RSS of the whole process shows 321 MB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go reports 300.1 KB of the heap size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we mentioned earlier, the beauty of GC is that it simplifies programmer life
    thanks to carefree allocations, memory safety, and solid efficiency for most applications.
    Unfortunately, it also makes our life a bit harder when our program violates our
    efficiency expectations, and the reason is not what you might think. The main
    problem with the Go Allocator and GC pair is that they hide the root cause of
    our memory efficiency problems—in almost all cases, our code allocates too much
    memory!
  prefs: []
  type: TYPE_NORMAL
- en: 'Think of a garbage collector like a Roomba: Just because you have one does
    not mean you tell your children not to drop arbitrary pieces of garbage onto the
    floor.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Halvar Flake, [Twitter](https://oreil.ly/ukXDV)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s explore the potential symptoms we might notice in Go when we are not
    careful with the number and type of the allocations:'
  prefs: []
  type: TYPE_NORMAL
- en: CPU overhead
  prefs: []
  type: TYPE_NORMAL
- en: First and foremost, the GC must go through all the objects stored on the heap
    to tell which ones are in use. This can use a significant portion of the CPU resource,
    especially if there are many objects in heap.^([37](ch05.html#idm45606832784288))
  prefs: []
  type: TYPE_NORMAL
- en: This is especially visible if the objects stored on the heap are rich in pointer
    types, which forces the GC to traverse them to check if they don’t point to an
    object that was not yet marked as “in use.” Given the limited CPU resources in
    our computers, the more work we have to do for the GC, the less work we can perform
    toward the core program functionality, which translates to higher program latency.
  prefs: []
  type: TYPE_NORMAL
- en: In platforms with garbage collection, memory pressure naturally translates into
    increased CPU consumption.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Google Teams, [*Site Reliability Engineering*](https://oreil.ly/PhZaD)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Additional increase in program latency
  prefs: []
  type: TYPE_NORMAL
- en: CPU time spent on GC is one thing, but there is more. First, the STW event performed
    twice slows down all goroutines. This is because the GC must stop all goroutines
    and inject (and then remove) a write barrier. It also prevents some goroutines
    that have to store some data in memory from doing any further work for the moment
    of GC marking.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a second, often missed effect. The GC collection runs are destructive
    to the hierarchical cache system efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: For your program to be fast, you want everything you’re doing to be in the cache.
    ... There are technical and physical reasons in the silicon why allocating memory,
    throwing it away and GC cleaning that for you, is going to not only slow your
    program down, because GC is doing its work, but it slows the rest of your program
    down, because it kicked everything out of [the CPU] cache.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bryan Boreham, [“Make Your Go Go Faster!”](https://oreil.ly/cDw6c)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Memory overhead
  prefs: []
  type: TYPE_NORMAL
- en: Since Go 1.19, there has been a way to set a soft memory limit for the GC. This
    still means that we have to often implement on our side checks against unbounded
    allocations (e.g., rejecting reading too-large HTTP body requests), but at least
    the GC is more prompt if you need to avoid that overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Still, the collection phase is eventual. This means we might be unable to release
    some memory blocks before new allocations come in. Changing the `GOGC` option
    to run GC less often only amplifies the problem but might be a good trade-off
    if you optimize for the CPU resource and have spare RAM on your machines.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in extreme cases, our program might even leak memory if [the GC
    is not fast enough to deal with all new allocations](https://oreil.ly/4giW6)!
  prefs: []
  type: TYPE_NORMAL
- en: The GC can sometimes have surprising effects on our program efficiency. Hopefully,
    after this section, you will be able to notice when you are affected. You will
    also be able to notice the GC bottlenecks with the observability tools explained
    in [Chapter 9](ch09.html#ch-observability3).
  prefs: []
  type: TYPE_NORMAL
- en: The Solution to Most Memory Efficiency Issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Produce less garbage!
  prefs: []
  type: TYPE_NORMAL
- en: It’s easy to overallocate memory in Go. This is why the best way to solve GC
    bottleneck or other memory efficiency issues is to allocate less. I will introduce
    [“The Three Rs Optimization Method”](ch11.html#ch-hw-rrr), which goes through
    different optimizations that help with those efficiency problems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It was a long chapter, but you made it! Unfortunately, memory resource is one
    of the hardest to explain and master. Probably that’s why there are so many opportunities
    to reduce the size or number of our Go program’s allocations.
  prefs: []
  type: TYPE_NORMAL
- en: You learned the long, multilayer path between our code that needs to allocate
    bits on memory and bits landing on the DRAM chip. You learned about many memory
    trade-offs, behaviors, and consequences on the OS level. Finally, you now know
    how Go uses those mechanisms and why memory allocations in Go are so transparent.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps you can already figure out the root causes of why [Example 4-1](ch04.html#code-sum)
    was using 30.5 MB of the heap for every single operation when the input file was
    3 MB large. In [“Optimizing Memory Usage”](ch10.html#ch-opt-mem-example), I will
    propose the algorithm and code improvements to [Example 4-1](ch04.html#code-sum)
    that allow it to use memory in numbers that are a fraction of the input file size,
    while also improving the latency.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that this space is evolving. Go compiler, Go garbage
    collector, and Go Allocator are constantly being improved, changed, and scaled
    for the needs of Go users. Yet most of the incoming changes will likely be only
    iterations of what we have now in Go.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ahead of us are Chapters [6](ch06.html#ch-observability) and [7](ch07.html#ch-observability2),
    which I consider two of the most crucial chapters in the book. I have already
    mentioned many tools I used to explain the main concepts in past chapters: metrics,
    benchmarking, and profiling. It’s time to learn them in detail!'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch05.html#idm45606834619920-marker)) In this book when I say “memory,”
    I mean RAM and vice versa. Other mediums offer “memorizing” data in computer architecture
    (e.g., L-caches), but we tend to treat RAM as the “main” memory resource.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch05.html#idm45606834615344-marker)) Not only because of physical limitations
    like not enough chip pins, space, and energy for transistors, but also because
    managing large memory poses huge overhead as we will learn in [“OS Memory Management”](#ch-hw-memory-os).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch05.html#idm45606834611488-marker)) In some way, RAM volatility can sometimes
    be treated as a feature, not a bug! Have you ever wondered why restarting a computer
    or process often fixes your problem? The memory volatility forces programmers
    to implement robust initialization techniques that rebuild the state from backup
    mediums, enhancing reliability and mitigating potential program bugs. In extreme
    cases, [crash-only software](https://oreil.ly/DAbDs) with the restart is the primary
    way of failure handling.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch05.html#idm45606834596960-marker)) We can resolve that problem by simply
    adding more memory to the system or switching to the server (or virtual machine)
    with more memory resource. That might be a solid solution if we are willing to
    pay additionally if it’s not a memory leak and if such a resource can be increased
    (e.g., the cloud has virtual machines with more memory). Yet I suggest investigating
    your program memory usage, especially if you continuously have to expand the system
    memory. Then there might be easy wins, thanks to trivially wasted space we could
    optimize.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch05.html#idm45606834576192-marker)) Nowadays, popular encodings like
    UTF-8 can dynamically use from one up to four bytes of memory per single character.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch05.html#idm45606834567216-marker)) By just doubling the “pointer” size,
    we moved the limit to how many elements we can address to extreme sizes. We could
    even estimate that 64-bit is enough to [address all grains of sand from all beaches
    on Earth](https://oreil.ly/By1J3)!
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch05.html#idm45606834540672-marker)) I introduced the *process* and *thread*
    terms in [“Operating System Scheduler”](ch04.html#ch-hw-os-scheduler).
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch05.html#idm45606834529664-marker)) Many Common Vulnerabilities and Exposures
    (CVE) issues exist due to various bugs that allow [out-of-bounds memory access](https://oreil.ly/iSbqk).
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch05.html#idm45606834527872-marker)) It might be less intuitive, but the
    malicious process can perform a DoS if access to another process memory is not
    restricted. For example, by setting counters to incorrect values or breaking loop
    invariants, the victim program might error out or exhaust machine resources.
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch05.html#idm45606834509584-marker)) In the past, [segmentation](https://oreil.ly/8BFmb)
    was used to implement virtual memory. This has proven to have less versatility,
    especially the inability to move this space around for defragmentation (better
    packing of memory). Still, even with paging, segmentation is applied to virtual
    memory by the process itself (with underlying paging). Plus, the kernel sometimes
    still uses nonpaged segmentation for its part of critical kernel memory.
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch05.html#idm45606834504144-marker)) You can check the current page size
    on the Linux system using the `getconf PAGESIZE` command.
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch05.html#idm45606834502960-marker)) For example, typically, Intel CPUs
    are capable of hardware-supported [4 KB, 2 MB, or 1 GB pages](https://oreil.ly/mxlry).
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch05.html#idm45606834497952-marker)) Even naive and conservative calculations
    indicate around [24% of total memory is wasted for 2 MB pages](https://oreil.ly/iklRd).
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch05.html#idm45606834494864-marker)) We won’t discuss the implementation
    of page tables since it’s pretty complex and not something Go developers have
    to worry about. Yet this topic is quite interesting as the trivial implementation
    of paging would have a massive overhead in memory usage (what’s the point of memory
    management that would take the majority of memory space it manages?). You can
    learn more [here](https://oreil.ly/jU9Is).
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch05.html#idm45606834472688-marker)) There is also an option to [disable
    an overcommitment mechanism](https://oreil.ly/h82uS) on Linux. When disabled,
    the virtual memory size (VSS) is not allowed to be bigger than the physical memory
    used by the process (RSS). You might want to do this so the process will have
    generally faster memory accesses, but the waste of memory is enormous. As a result,
    I have never seen such an option used in practice.
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch05.html#idm45606834159392-marker)) `MAP_SHARED` means that any other
    process can reuse the same physical memory page if it accesses the same file.
    This is harmless if the mapped file does not change over time, but it has more
    complex nuances for mapping modifiable content.
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch05.html#idm45606834158000-marker)) A full list of options can be found
    in the [`mmap` documentation](https://oreil.ly/m5n7A).
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch05.html#idm45606834147584-marker)) `SIGSEV` means a segmentation fault.
    This tells us that the process wants to access an invalid memory address.
  prefs: []
  type: TYPE_NORMAL
- en: ^([19](ch05.html#idm45606833899392-marker)) On Linux, you can find this information
    by doing `ps -ax --format=pid,rss,vsz | grep $PID`, where `$PID` is process ID.
  prefs: []
  type: TYPE_NORMAL
- en: ^([20](ch05.html#idm45606833705184-marker)) How do I know? We can have exact
    statistics for each memory mapping process we use on Linux thanks to the `/proc/*<PID>*/smaps`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: ^([21](ch05.html#idm45606833635152-marker)) There are many reasons why accessing
    nearby bytes might not need allocating more pages on RAM in the memory-mapped
    situation. For example, the cache hierarchy (discussed in [“Hierachical Cache
    System”](ch04.html#ch-hw-lcache)), the OS, and compiler deciding to pull more
    at once, or such a page being already a shared or private page because of previous
    accesses.
  prefs: []
  type: TYPE_NORMAL
- en: ^([22](ch05.html#idm45606833630880-marker)) Note that physical frames for this
    file can still be allocated on physical memory by the OS (just not accounted for
    our process). This is called `page cache` and can be useful if any process tries
    to memorize the same file. Page cache is stored as best effort in the memory that
    would otherwise not be used. It can be released when the system is under high
    memory pressure or manually by the administrator, e.g., with `sysctl -w vm.drop_caches=1`.
  prefs: []
  type: TYPE_NORMAL
- en: ^([23](ch05.html#idm45606833609392-marker)) Swapping is usually turned off by
    default on most machines.
  prefs: []
  type: TYPE_NORMAL
- en: ^([24](ch05.html#idm45606833605136-marker)) [“Teaching the OOM killer”](https://oreil.ly/AFDh0)
    explains some problems in choosing what process to kill first. The lesson here
    is that the global OOM killer is often hard to [predict](https://oreil.ly/4rPzk).
  prefs: []
  type: TYPE_NORMAL
- en: ^([25](ch05.html#idm45606833602304-marker)) Exact implementation of memory controller
    can be found [here](https://oreil.ly/Ken3G).
  prefs: []
  type: TYPE_NORMAL
- en: ^([26](ch05.html#idm45606833533232-marker)) Remember, whatever type or amount
    of virtual memory the OS is giving to the process, it uses the memory mapping
    technique. `sbrk` allows simpler resizing of the virtual memory section typically
    covered by the heap. However, it behaves like any other `mmap` using anonymous
    pages.
  prefs: []
  type: TYPE_NORMAL
- en: ^([27](ch05.html#idm45606833508688-marker)) Of course no one blocks anyone from
    implementing external garbage collection on top of those mechanisms in C and C++.
  prefs: []
  type: TYPE_NORMAL
- en: ^([28](ch05.html#idm45606833506480-marker)) It’s hard that the ownership model
    in Rust requires the programmer to be hyperaware of every memory allocation and
    what part owns it. Despite that, I am a huge fan of the Rust ownership model if
    we could scope this memory management only to a certain part of our code. I believe
    it would be beneficial to bring some ownership pattern to Go, where a small amount
    of code could use that, whereas the rest would use GC. Wish list for someday?
    :)
  prefs: []
  type: TYPE_NORMAL
- en: ^([29](ch05.html#idm45606833447648-marker)) You can reveal the box size with
    the [`unsafe.Sizeof`](https://oreil.ly/QtpSf) function.
  prefs: []
  type: TYPE_NORMAL
- en: ^([30](ch05.html#idm45606833426272-marker)) See the handy [`reflect.SliceHeader`](https://oreil.ly/9unR4)
    struct that represents a slice.
  prefs: []
  type: TYPE_NORMAL
- en: ^([31](ch05.html#idm45606833418784-marker)) Technically speaking, the type `map`
    variable is a pointer to the hashmap. However, to avoid always typing `*map`,
    the Go team decided to [hide that detail](https://oreil.ly/mfwDa).
  prefs: []
  type: TYPE_NORMAL
- en: ^([32](ch05.html#idm45606833407008-marker)) We won’t cover [struct padding](https://oreil.ly/1gx5O)
    in this edition. There is also an amazing utility that helps you to notice the
    waste [introduced by struct misalignment](https://oreil.ly/WtYFZ).
  prefs: []
  type: TYPE_NORMAL
- en: ^([33](ch05.html#idm45606833200480-marker)) This is one of the reasons why in
    Go, every new structure has defined zero value or nil at the start, instead of
    random value.
  prefs: []
  type: TYPE_NORMAL
- en: '^([34](ch05.html#idm45606833118576-marker)) We know that because `go build
    -gcflags="-m=1" slice.go` outputs the `./slice.go:11:11: make([]byte, size) escapes
    to heap` line.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([35](ch05.html#idm45606833047648-marker)) This behavior was often leveraged
    by more advanced memory ballasting, which generally is less needed after Go 1.19
    introduced the memory soft limit discussed in [“Garbage Collection”](#ch-hw-garbage).
  prefs: []
  type: TYPE_NORMAL
- en: ^([36](ch05.html#idm45606832989440-marker)) It’s also possible to change Go
    memory release strategy by changing the `GODEBUG` [environment variable](https://oreil.ly/ynNXr).
    For example, we can set `GODEBUG=madvdontneed=0`, so `MADV_FREE` will be used
    instead to notify the OS about unneeded memory space. The difference between `MADV_DONTNEED`
    and `MADV_FREE` is precisely around the point mentioned in the Linux Community
    quote. For `MADV_FREE`, memory release is even faster for Go programs, but the
    resident set size (RSS) metric of the calling process might not be immediately
    reduced until the OS reclaims that space. This has proven to cause a massive problem
    on some systems (e.g., lightly virtualized systems like Kubernetes) that rely
    on RSS to manage the processes. This happened in 2019 when Go defaulted to `MADV_FREE`
    for a couple of versions. More on that is explained in my [blog post](https://oreil.ly/UYXJy).
  prefs: []
  type: TYPE_NORMAL
- en: ^([37](ch05.html#idm45606832784288-marker)) To be strict, Go [ensures that a
    maximum of 25% of the total CPU assigned for the process is used for the GC](https://oreil.ly/9rtOs).
    This is, however, not a silver-bullet solution. By reducing the maximum CPU time
    used, we simply use the same amount, just over longer periods.
  prefs: []
  type: TYPE_NORMAL
