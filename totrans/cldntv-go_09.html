<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. It’s All About Dependability"><div class="chapter" id="chapter_6">&#13;
<h1><span class="label">Chapter 6. </span>It’s All About Dependability</h1>&#13;
&#13;
<blockquote>&#13;
<p>The most important property of a program is whether it accomplishes the intention of its user.<sup><a data-type="noteref" id="idm45983630648552-marker" href="ch06.xhtml#idm45983630648552">1</a></sup></p>&#13;
<p data-type="attribution">C.A.R. Hoare, <cite>Communications of the ACM (October 1969)</cite></p>&#13;
</blockquote>&#13;
&#13;
<p><a data-type="indexterm" data-primary="CSP" id="idm45983630645064"/><a data-type="indexterm" data-primary="Hoare, Tony" id="idm45983630644360"/><a data-type="indexterm" data-primary="quicksort" id="idm45983630643688"/><a data-type="indexterm" data-primary="Hoare Logic" id="idm45983630643016"/>Professor Sir Charles Antony Richard (Tony) Hoare is a brilliant guy. He invented quicksort, authored Hoare Logic for reasoning about the correctness of computer programs, and created the formal language “communicating sequential processes” (CSP) that inspired Go’s beloved concurrency model. Oh, and he developed the structured programming paradigm<sup><a data-type="noteref" id="idm45983630641848-marker" href="ch06.xhtml#idm45983630641848">2</a></sup> that forms the foundation of all modern programming languages in common use today. He also invented the null reference. Please don’t hold that against him, though. He publicly apologized<sup><a data-type="noteref" id="idm45983630640824-marker" href="ch06.xhtml#idm45983630640824">3</a></sup> for it in 2009, calling it his “billion-dollar mistake.”</p>&#13;
&#13;
<p>Tony Hoare literally invented programming as we know it. So when he says that the single most important property of a program is whether it accomplishes the intention of its user, you can take that on some authority. Think about this for a second: Hoare specifically (and quite rightly) points out that it’s the intention of a program’s <em>users</em>—not its <em>creators</em>—that dictates whether a program is performing correctly. How inconvenient that the intentions of a program’s users aren’t always the same as those of its creator!</p>&#13;
&#13;
<p>Given this assertion, it stands to reason that a user’s first expectation about a program is that <em>the program works</em>. But when is a program “working”? This is actually a pretty big question, one that lies at the heart of cloud native design. The first goal of this chapter is to explore that very idea, and in the process, introduce concepts like “dependability” and “reliability” that we can use to better describe (and meet) user expectations. Finally, we’ll briefly review a number of practices commonly used in cloud native development to ensure that services meet the expectations of its users. We’ll discuss each of these in-depth throughout the remainder of this book.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect1" data-pdf-bookmark="What’s the Point of Cloud Native?"><div class="sect1" id="idm45983630618760">&#13;
<h1>What’s the Point of Cloud Native?</h1>&#13;
&#13;
<p><a data-type="indexterm" data-primary="cloud native" data-secondary="purpose of" id="idm45983630617544"/>In <a data-type="xref" href="ch01.xhtml#chapter_1">Chapter 1</a> we spent a few pages defining “cloud native,” starting with the Cloud Native Computing Foundation’s definition and working forward to the properties of an ideal cloud native service. We spent a few more pages talking about the pressures that have driven cloud native to be a thing in the first place.</p>&#13;
&#13;
<p>What we didn’t spend so much time on, however, was the <em>why</em> of cloud native. Why does the concept of cloud native even exist? Why would we even want our systems to be cloud native? What’s its purpose? What makes it so special? Why should I care?</p>&#13;
&#13;
<p>So, why <em>does</em> cloud native exist? The answer is actually pretty straightforward: it’s all about dependability. In the first part of this chapter, we’ll dig into the concept of dependability, what it is, why it’s important, and how it underlies all the patterns and techniques that we call cloud native.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect1" data-pdf-bookmark="It’s All About Dependability"><div class="sect1" id="idm45983630612776">&#13;
<h1>It’s All About Dependability</h1>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Cummins, Holly" id="idm45983630611320"/><a data-type="indexterm" data-primary="idempotency" id="idm45983630610616"/>Holly Cummins, the worldwide development community practice lead for the IBM Garage, famously said that “if cloud native has to be a synonym for anything, it would be idempotent.”<sup><a data-type="noteref" id="idm45983630609624-marker" href="ch06.xhtml#idm45983630609624">4</a></sup> Cummins is absolutely brilliant, and has said a lot of absolutely brilliant things,<sup><a data-type="noteref" id="idm45983630608616-marker" href="ch06.xhtml#idm45983630608616">5</a></sup> but I think she only has half of the picture on this one. I think that idempotence is very important—perhaps even necessary for cloud native—but not sufficient. I’ll elaborate.</p>&#13;
&#13;
<p>The history of software, particularly the network-based kind, has been one of struggling to meet the expectations of increasingly sophisticated users. Long gone are the days when a service could go down at night “for maintenance.” Users today rely heavily on the services they use, and they expect those services to be available and to respond promptly to their requests. Remember the last time you tried to start a Netflix movie and it took the longest five seconds of your life? Yeah, that.</p>&#13;
&#13;
<p>Users don’t care that your services have to be maintained. They won’t wait patiently while you hunt down that mysterious source of latency. They just want to finish binge-watching the second season of Breaking Bad.<sup><a data-type="noteref" id="idm45983630606248-marker" href="ch06.xhtml#idm45983630606248">6</a></sup></p>&#13;
&#13;
<p>All of the patterns and techniques that we associate with cloud native—<em>every single one</em>—exist to allow services to be deployed, operated, and maintained at scale in unreliable environments, driven by the need to produce dependable services that keep users happy.</p>&#13;
&#13;
<p>In other words, I think that if “cloud native” has to be a synonym for anything, it would be “dependability.”</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect1" data-pdf-bookmark="What Is Dependability and Why Is It So Important?"><div class="sect1" id="idm45983630603944">&#13;
<h1>What Is Dependability and Why Is It So Important?</h1>&#13;
&#13;
<p><a data-type="indexterm" data-primary="dependability" id="ch06_term1"/><a data-type="indexterm" data-primary="systems engineering" id="idm45983630601400"/><a data-type="indexterm" data-primary="Laprie, Jean-Claude" id="idm45983630600728"/>I didn’t choose the word “dependability” arbitrarily. It’s actually a core concept in the field of <em>systems engineering</em>, which is full of some very smart people who say some very smart things about the design and management of complex systems. The concept of dependability in a computing context was first rigorously defined by Jean-Claude Laprie about 35 years ago,<sup><a data-type="noteref" id="idm45983630599160-marker" href="ch06.xhtml#idm45983630599160">7</a></sup> who defined a system’s dependability according to the expectations of its users. Laprie’s original definition has been tweaked and extended over the years by various authors, but here’s my favorite:</p>&#13;
<blockquote>&#13;
<p>The dependability of a computer system is its ability to avoid failures that are more frequent or more severe, and outage durations that are longer, than is acceptable to the user(s).<sup><a data-type="noteref" id="idm45983630595768-marker" href="ch06.xhtml#idm45983630595768">8</a></sup></p>&#13;
<p data-type="attribution">Fundamental Concepts of Computer System Dependability (2001)</p>&#13;
</blockquote>&#13;
&#13;
<p>In other words, a dependable system consistently does what its users expect and can be quickly fixed when it doesn’t.</p>&#13;
&#13;
<p>By this definition, a system is dependable only when it can <em>justifiably</em> be trusted. Obviously, a system can’t be considered dependable if it falls over any time one of its components glitch, or if it requires hours to recover from a failure. Even if it’s been running for months without interruption, an undependable system may still be one bad day away from catastrophe: lucky isn’t dependable.</p>&#13;
&#13;
<p>Unfortunately, it’s hard to objectively gauge “user expectations.” For this reason, as illustrated in <a data-type="xref" href="#img_ch06_dependability_tree">Figure 6-1</a>, dependability is an umbrella concept encompassing several more specific and quantifiable attributes—availability, reliability, and maintainability—all of which are subject to similar threats that may be overcome by similar means.</p>&#13;
&#13;
<figure><div id="img_ch06_dependability_tree" class="figure">&#13;
<img src="Images/cngo_0601.png" alt="cngo 0601" width="740" height="1250"/>&#13;
<h6><span class="label">Figure 6-1. </span>The system attributes and means that contribute to dependability</h6>&#13;
</div></figure>&#13;
&#13;
<p>So while the concept of “dependability” alone might be a little squishy and subjective, the attributes that contribute to it are quantitative and measurable enough to be &#13;
<span class="keep-together">useful</span>:</p>&#13;
<dl>&#13;
<dt><a data-type="indexterm" data-primary="availability" id="idm45983630584856"/>Availability</dt>&#13;
<dd>&#13;
<p>The ability of a system to perform its intended function at a random moment in time. This is usually expressed as the probability that a request made of the system will be successful, defined as uptime divided by total time.</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="reliability" id="idm45983630582632"/><a data-type="indexterm" data-primary="mean time between failures (MTBF)" id="idm45983630581928"/><a data-type="indexterm" data-primary="MTBF (mean time between failures)" id="idm45983630581288"/><a data-type="indexterm" data-primary="failure rate" id="idm45983630580600"/>Reliability</dt>&#13;
<dd>&#13;
<p>The ability of a system to perform its intended function for a given time interval. This is often expressed as either the mean time between failures (MTBF: total time divided by the number of failures) or failure rate (number of failures divided by total time).</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="maintainability" id="idm45983630578376"/>Maintainability</dt>&#13;
<dd>&#13;
<p>The ability of a system to undergo modifications and repairs. There are a variety of indirect measures for maintainability, ranging from calculations of cyclomatic complexity to tracking the amount of time required to change a system’s behavior to meet new requirements or to restore it to a functional state.</p>&#13;
</dd>&#13;
</dl>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Later authors extended Laprie’s definition of dependability to include several security-related properties, including safety, confidentiality, and integrity. I’ve reluctantly omitted these, not because security isn’t important (it’s <em>SO</em> important!), but for brevity. A worthy discussion of security would require an entire book of its own.</p>&#13;
</div>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45983630574312">&#13;
<h5>Dependability Is Not Reliability</h5>&#13;
<p>If you’ve read any of O’Reilly’s Site Reliability Engineering (SRE) books<sup><a data-type="noteref" id="idm45983630572776-marker" href="ch06.xhtml#idm45983630572776">9</a></sup> you’ve already heard quite a lot about reliability. However, as illustrated in <a data-type="xref" href="#img_ch06_dependability_tree">Figure 6-1</a>, reliability is just one property that contributes to overall dependability.</p>&#13;
&#13;
<p>If that’s true, though, then why has reliability become the standard metric for service functionality? Why are there “site reliability engineers” but no “site dependability engineers”?</p>&#13;
&#13;
<p>There are probably several answers to these questions, but perhaps the most definitive is that the definition of “dependability” is purely qualitative. There’s no measure for it, and when you can’t measure something it’s very hard to construct a set of rules around it.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="reliability" id="idm45983630568728"/>Reliability, on the other hand, is quantitative. Given a robust definition<sup><a data-type="noteref" id="idm45983630567896-marker" href="ch06.xhtml#idm45983630567896">10</a></sup> for what it means for a system to provide “correct” service, it becomes relatively straightforward to calculate that system’s “reliability,” making it a powerful (if indirect) measure of user experience.</p>&#13;
</div></aside>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="Dependability: It’s Not Just for Ops Anymore"><div class="sect2" id="idm45983630566792">&#13;
<h2>Dependability: It’s Not Just for Ops Anymore</h2>&#13;
&#13;
<p>Since the introduction of networked services, it’s been the job of developers to build services, and of systems administrators (“operations”) to deploy those services onto servers and keep them running. This worked well enough for a time, but it had the unfortunate side-effect of incentivizing developers to prioritize feature development at the expense of stability and operations.</p>&#13;
&#13;
<p>Fortunately, over the past decade or so—coinciding with the DevOps movement—a new wave of technologies has become available with the potential to completely change the way technologists of all kinds do their jobs.</p>&#13;
&#13;
<p>On the operations side, with the availability of infrastructure and platforms as a service (IaaS/PaaS) and tools like Terraform and Ansible, working with infrastructure has never been more like writing software.</p>&#13;
&#13;
<p>On the development side, the popularization of technologies like containers and serverless functions has given developers an entire new set of “operations-like” capabilities, particularly around virtualization and deployment.</p>&#13;
&#13;
<p>As a result, the once-stark line between software and infrastructure is getting increasingly blurry. One could even argue that with the growing advancement and adoption of infrastructure abstractions like virtualization, container orchestration frameworks like Kubernetes, and software-defined behavior like service meshes, we may even be at the point where they could be said to have merged. Everything is software now.</p>&#13;
&#13;
<p>The ever-increasing demand for service dependability has driven the creation of a whole new generation of cloud native technologies. The effects of these new technologies and the capabilities they provide has been considerable, and the traditional developer and operations roles are changing to suit them. At long last, the silos are crumbling, and, increasingly, the rapid production of dependable, high-quality &#13;
<span class="keep-together">services</span> is a fully collaborative effort of all of its designers, implementors, and &#13;
<span class="keep-together">maintainers</span>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect1" data-pdf-bookmark="Achieving Dependability"><div class="sect1" id="idm45983630603320">&#13;
<h1>Achieving Dependability</h1>&#13;
&#13;
<p>This is where the rubber meets the road. If you’ve made it this far, congratulations.</p>&#13;
&#13;
<p>So far we’ve discussed Laprie’s definition of “dependability,” which can be (very) loosely paraphrased as “happy users,” and we’ve discussed the attributes—availability, reliability, and maintainability—that contribute to it. This is all well and good, but without actionable advice for how to achieve dependability the entire discussion is purely academic.</p>&#13;
&#13;
<p>Laprie thought so too, and defined four broad categories of techniques that can be used together to improve a system’s dependability (or which, by their absence, can reduce it):</p>&#13;
<dl>&#13;
<dt><a data-type="indexterm" data-primary="fault prevention" id="ch06_term2"/>Fault prevention</dt>&#13;
<dd>&#13;
<p>Fault prevention techniques are used during system construction to prevent the occurrence or introduction of faults.</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="fault tolerance" id="idm45983630553320"/>Fault tolerance</dt>&#13;
<dd>&#13;
<p>Fault tolerance techniques are used during system design and implementation to prevent service failures in the presence of faults.&#13;
<a data-type="indexterm" data-primary="fault removal" id="idm45983630551608"/></p>&#13;
</dd>&#13;
<dt>Fault removal</dt>&#13;
<dd>&#13;
<p>Fault removal techniques are used to reduce the number and severity of faults.</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="fault forecasting" id="idm45983630549208"/>Fault forecasting</dt>&#13;
<dd>&#13;
<p>Fault forecasting techniques are used to identify the presence, the creation, and the consequences of faults.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Means of Dependability pyramid" id="idm45983630547000"/>Interestingly, as illustrated in <a data-type="xref" href="#img_ch06_means_pyramid">Figure 6-2</a>, these four categories correspond surprisingly well to the five cloud native attributes that we introduced all the way back in <a data-type="xref" href="ch01.xhtml#chapter_1">Chapter 1</a>.</p>&#13;
&#13;
<figure><div id="img_ch06_means_pyramid" class="figure">&#13;
<img src="Images/cngo_0602.png" alt="cngo 0602" width="847" height="524"/>&#13;
<h6><span class="label">Figure 6-2. </span>The four means of achieving dependability, and their corresponding cloud native attributes</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="indexterm" data-primary="scalability" data-secondary="efficiency and" id="idm45983630542088"/><a data-type="indexterm" data-primary="resilience" data-secondary="building" id="idm45983630541112"/><a data-type="indexterm" data-primary="dependability procurement" id="idm45983630540168"/>Fault prevention and fault tolerance make up the bottom two layers of the pyramid, corresponding with scalability, loose coupling, and resilience. Designing a system for scalability prevents a variety of faults common among cloud native applications, and resiliency techniques allow a system to tolerate faults when they do inevitably arise. Techniques for loose coupling can be said to fall into both categories, preventing and enhancing a service’s fault tolerance. Together these can be said to contribute to what Laprie terms <em>dependability procurement</em>: the means by which a system is provided with the ability to perform its designated function.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="manageability" data-secondary="contributions to" id="idm45983630538056"/><a data-type="indexterm" data-primary="dependability validation" id="idm45983630537080"/>Techniques and designs that contribute to manageability are intended to produce a system that can be easily modified, simplifying the process of removing faults when they’re identified. Similarly, observability naturally contributes to the ability to forecast faults in a system. Together fault removal and forecasting techniques contribute to what Laprie termed <em>dependability validation</em>: the means by which confidence is gained in a system’s ability to perform its designated function.</p>&#13;
&#13;
<p>Consider the implications of this relationship: what was a purely academic exercise 35 years ago has essentially been rediscovered—apparently independently—as a natural consequence of years of accumulated experience building reliable production systems. Dependability has come full-circle.</p>&#13;
&#13;
<p>In the subsequent sections we’ll explore these relationships more fully and preview later chapters, in which we discuss exactly how these two apparently disparate systems actually correspond quite closely.<a data-type="indexterm" data-primary="dependability" data-startref="ch06_term1" id="idm45983630534008"/></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="Fault Prevention"><div class="sect2" id="idm45983630532904">&#13;
<h2>Fault Prevention</h2>&#13;
&#13;
<p>At the base of our “Means of Dependability” pyramid are techniques that focus on preventing the occurrence or introduction of faults. As veteran programmers can attest, many—if not most—classes of errors and faults can be predicted and prevented during the earliest phases of development. As such, many fault prevention techniques come into play during the design and implementation of a service.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect3" data-pdf-bookmark="Good programming practices"><div class="sect3" id="idm45983630530824">&#13;
<h3>Good programming practices</h3>&#13;
&#13;
<p>Fault prevention is one of the primary goals of software engineering in general, and is the explicit goal of any development methodology, from pair programming to test-driven development and code review practices. Many such techniques can really be grouped into what might be considered to be “good programming practice,” about which innumerable excellent books and articles have already been written, so we won’t explicitly cover it here.<a data-type="indexterm" data-primary="fault prevention" data-startref="ch06_term2" id="idm45983630529016"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect3" data-pdf-bookmark="Language features"><div class="sect3" id="idm45983630527784">&#13;
<h3>Language features</h3>&#13;
&#13;
<p>Your choice of language can also greatly affect your ability to prevent or fix faults. Many language features that some programmers have sometimes come to expect, such as dynamic typing, pointer arithmetic, manual memory management, and thrown exceptions (to name a few) can easily introduce unintended behaviors that are difficult to find and fix, and may even be maliciously exploitable.</p>&#13;
&#13;
<p>These kinds of features strongly motivated many of the design decisions for Go, resulting in the strongly typed garbage-collected language we have today. For a refresher for why Go is particularly well suited for the development of cloud native services, take a look back at <a data-type="xref" href="ch02.xhtml#chapter_2">Chapter 2</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect3" data-pdf-bookmark="Scalability"><div class="sect3" id="idm45983630524296">&#13;
<h3>Scalability</h3>&#13;
&#13;
<p>We briefly introduced the concept of scalability way back in <a data-type="xref" href="ch01.xhtml#chapter_1">Chapter 1</a>, where it was defined as the ability of a system to continue to provide correct service in the face of significant changes in demand.</p>&#13;
&#13;
<p>In that section we introduced two different approaches to scaling—vertical scaling (scaling up) by resizing existing resources, and horizontal scaling (scaling out) by adding (or removing) service instances—and some of the pros and cons of each.</p>&#13;
&#13;
<p>We’ll go quite a bit deeper into each of these in <a data-type="xref" href="ch07.xhtml#chapter_7">Chapter 7</a>, especially into the gotchas and downsides. We’ll also talk a lot about the problems posed by state.<sup><a data-type="noteref" id="idm45983630519656-marker" href="ch06.xhtml#idm45983630519656">11</a></sup> For now, though, it’ll suffice to say that having to scale your service adds quite a bit of overhead, including but not limited to cost, complexity, and debugging.</p>&#13;
&#13;
<p>While scaling resources is eventually often inevitable, it’s often better (and cheaper!) to resist the temptation to throw hardware at the problem and postpone scaling events as long as possible by considering runtime efficiency and algorithmic scaling. As such, we’ll cover a number of Go features and tooling that allow us to identify and fix common problems like memory leaks and lock contention that tend to plague systems at scale.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect3" data-pdf-bookmark="Loose coupling"><div class="sect3" id="idm45983630517848">&#13;
<h3>Loose coupling</h3>&#13;
&#13;
<p><a data-type="indexterm" data-primary="loose coupling" data-secondary="definition of" id="idm45983630516760"/>Loose coupling, which we first defined in <a data-type="xref" href="ch01.xhtml#section_ch01_loose_coupling">“Loose Coupling”</a>, is the system property and design strategy of ensuring that a system’s components have as little knowledge of other components as possible. The degree of coupling between services can have an enormous—and too often under-appreciated—impact on a system’s ability to scale and to isolate and tolerate failures.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="distributed monolith" id="idm45983630514200"/>Since the beginning of microservices there have been dissenters who point to the difficulty of deploying and maintaining microservice-based systems as evidence that such architectures are just too complex to be viable. I don’t agree, but I can see where they’re coming from, given how incredibly easy it is to build a <em>distributed monolith</em>. The hallmark of a distributed monolith is the tight coupling between its components, which results in an application saddled with all of the complexity of microservices plus the all of the tangled dependencies of the typical monolith. If you have to deploy most of your services together, or if a failed health check sends cascading failures through your entire system, you probably have a distributed monolith.</p>&#13;
&#13;
<p>Building a loosely coupled system is easier said than done, but is possible with a little discipline and reasonable boundaries. In <a data-type="xref" href="ch08.xhtml#chapter_8">Chapter 8</a> we’ll cover how to use data exchange contracts to establish those boundaries, and different synchronous and asynchronous communication models and architectural patterns and packages used to implement them and avoid the dreaded distributed monolith.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="Fault Tolerance"><div class="sect2" id="section_ch06_fault_tolerance">&#13;
<h2>Fault Tolerance</h2>&#13;
&#13;
<p><a data-type="indexterm" data-primary="fault tolerance" id="idm45983630508792"/><a data-type="indexterm" data-primary="error detection" id="idm45983630508088"/><a data-type="indexterm" data-primary="recovery" id="idm45983630507416"/>Fault tolerance has a number of synonyms—self-repair, self-healing, resilience—that all describe a system’s ability to detect errors and prevent them from cascading into a full-blown failure. Typically, this consists of two parts: <em>error detection</em>, in which an error is discovered during normal service, and <em>recovery</em>, in which the system is returned to a state where it can be activated again.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="redundancy" id="idm45983630505192"/>Perhaps the most common strategy for providing resilience is redundancy: the duplication of critical components (having multiple service replicas) or functions (retrying service requests). This is a broad and very interesting field with a number of subtle gotchas that we’ll dig into in <a data-type="xref" href="ch09.xhtml#chapter_9">Chapter 9</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="Fault Removal"><div class="sect2" id="idm45983630503000">&#13;
<h2>Fault Removal</h2>&#13;
&#13;
<p><a data-type="indexterm" data-primary="fault removal" id="ch06_term3"/>Fault removal, the third of the four dependability means, is the process of reducing the number and severity of faults—latent software flaws that can cause errors—before they manifest as errors.</p>&#13;
&#13;
<p>Even under ideal conditions, there are plenty of ways that a system can error or otherwise misbehave. It might fail to perform an expected action, or perform the wrong action entirely, perhaps maliciously. Just to make things even more complicated, conditions aren’t always—or often—ideal.</p>&#13;
&#13;
<p>Many faults can be identified by testing, which allows you to verify that the system (or at least its components) behaves as expected under known test conditions.</p>&#13;
&#13;
<p>But what about unknown conditions? Requirements change, and the real world doesn’t care about your test conditions. Fortunately, with effort, a system can be designed to be manageable enough that its behavior can often be adjusted to keep it secure, running smoothly, and compliant with changing requirements.</p>&#13;
&#13;
<p>We’ll briefly discuss these next.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect3" data-pdf-bookmark="Verification and testing"><div class="sect3" id="idm45983630497688">&#13;
<h3>Verification and testing</h3>&#13;
&#13;
<p><a data-type="indexterm" data-primary="testability" id="ch06_term4"/>There are exactly four ways of finding latent software faults in your code: testing, testing, testing, and bad luck.</p>&#13;
&#13;
<p>Yes, I joke, but that’s not so far from the truth: if you don’t find your software faults, your users will. If you’re lucky. If you’re not, then they’ll be found by bad actors seeking to take advantage of them.</p>&#13;
&#13;
<p>Bad jokes aside, there are two common approaches to finding software faults in development:</p>&#13;
<dl>&#13;
<dt><a data-type="indexterm" data-primary="static analysis" id="idm45983630493240"/>Static analysis</dt>&#13;
<dd>&#13;
<p>Automated, rule-based code analysis performed without actually executing programs. Static analysis is useful for providing early feedback, enforcing consistent practices, and finding common errors and security holes without depending on human knowledge or effort.</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="dynamic analysis" id="idm45983630490888"/>Dynamic analysis</dt>&#13;
<dd>&#13;
<p>Verifying the correctness of a system or subsystem by executing it under controlled conditions and evaluating its behavior. More commonly referred to simply as “testing.”</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p><a data-type="indexterm" data-primary="degrees of freedom" id="idm45983630488584"/><a data-type="indexterm" data-primary="search space" id="idm45983630487720"/>Key to software testing is having software that’s <em>designed for testability</em> by minimizing the <em>degrees of freedom</em>—the range of possible states—of its components. Highly testable functions have a single purpose, with well-defined inputs and outputs and few or no <em>side effects</em>; that is, they don’t modify variables outside of their scope. If you’ll forgive the nerdiness, this approach minimizes the <em>search space</em>—the set of all possible solutions—of each function.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Go" data-secondary="design of" id="idm45983630484728"/>Testing is a critical step in software development that’s all too often neglected. The Go creators understood this and baked unit testing and benchmarking into the language itself in the form of the <code>go test</code> command and the <a href="https://oreil.ly/PrhXq">testing package</a>. Unfortunately, a deep dive into testing theory is well beyond the scope of this book, but we’ll do our best to scratch the surface in <a data-type="xref" href="ch09.xhtml#chapter_9">Chapter 9</a>.<a data-type="indexterm" data-primary="testability" data-startref="ch06_term4" id="idm45983630481304"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect3" data-pdf-bookmark="Manageability"><div class="sect3" id="idm45983630480200">&#13;
<h3>Manageability</h3>&#13;
&#13;
<p><a data-type="indexterm" data-primary="manageability" data-secondary="designs for" id="idm45983630478792"/>Faults exist when your system doesn’t behave according to requirements. But what happens when those requirements change?</p>&#13;
&#13;
<p>Designing for <em>manageability</em>, first introduced back in <a data-type="xref" href="ch01.xhtml#section_ch01_manageability">“Manageability”</a>, allows a system’s behavior to be adjusted without code changes. A manageable system essentially has “knobs” that allow real-time control to keep your system secure, running smoothly, and compliant with changing requirements.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="feature flags" id="idm45983630475352"/>Manageability can take a variety of forms, including (but not limited to!) adjusting and configuring resource consumption, applying on-the-fly security remediations, <em>feature flags</em> that can turn features on or off, or even loading plug-in-defined behaviors.<a data-type="indexterm" data-primary="fault removal" data-startref="ch06_term3" id="idm45983630473960"/></p>&#13;
&#13;
<p>Clearly, manageability is a broad topic. We’ll review a few of the mechanisms Go provides for it in <a data-type="xref" href="ch10.xhtml#chapter_10">Chapter 10</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="Fault Forecasting"><div class="sect2" id="idm45983630471528">&#13;
<h2>Fault Forecasting</h2>&#13;
&#13;
<p><a data-type="indexterm" data-primary="fault forecasting" id="idm45983630470024"/>At the peak of our “Means of Dependability” pyramid (<a data-type="xref" href="#img_ch06_means_pyramid">Figure 6-2</a>) is <em>fault forecasting</em>, which builds on the knowledge gained and solutions implemented in the levels below it to attempt to estimate the present number, the future incidence, and the likely consequence of faults.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Failure Mode and Effects Analysis" id="idm45983630467400"/>Too often this consists of guesswork and gut feelings instead, generally resulting in unexpected failures when a starting assumption stops being true. More systematic approaches include <a href="https://oreil.ly/sNe6P">Failure Mode and Effects Analysis</a> and stress testing, which are very useful for understanding a system’s possible failure modes.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="observability" data-secondary="techniques" id="idm45983630465320"/>In a system designed for <em>observability</em>, which we’ll discuss in depth in <a data-type="xref" href="ch11.xhtml#chapter_11">Chapter 11</a>, failure mode indicators can be tracked so that they can be forecast and corrected before they manifest as errors. Furthermore, when unexpected failures occur—as they inevitably will—observable systems allow the underlying faults to be quickly identified, isolated, and corrected.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect1" data-pdf-bookmark="The Continuing Relevance of the Twelve-Factor App"><div class="sect1" id="section_ch06_12_factor_app">&#13;
<h1>The Continuing Relevance of the Twelve-Factor App</h1>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Twelve-Factor App" id="ch06_term6"/><a data-type="indexterm" data-primary="Heroku" id="idm45983630459800"/><a data-type="indexterm" data-primary="platform as a service (PaaS)" id="idm45983630459128"/><a data-type="indexterm" data-primary="PaaS (platform as a service)" id="idm45983630458440"/>In the early 2010s, developers at Heroku, a platform as a service (PaaS) company and early cloud pioneer, realized that they were seeing web applications being developed again and again with the same fundamental flaws.</p>&#13;
&#13;
<p>Motivated by what they felt were systemic problems in modern application development, they drafted <em>The Twelve-Factor App</em>. This was a set of twelve rules and guidelines constituting a development methodology for building web applications, and by extension, cloud native applications (although “cloud native” wasn’t a commonly used term at the&#13;
time). The methodology was for building web applications that: <sup><a data-type="noteref" id="idm45983630456168-marker" href="ch06.xhtml#idm45983630456168">12</a></sup></p>&#13;
<ul>&#13;
<li>Use declarative formats for setup automation, to minimize time and cost for new developers joining the project</li>&#13;
<li>Have a clean contract with the underlying operating system, offering maximum portability between execution environments</li>&#13;
<li>Are suitable for deployment on modern cloud platforms, obviating the need for servers and systems administration</li>&#13;
<li class="pagebreak-before">Minimize divergence between development and production, enabling continuous deployment for maximum agility</li>&#13;
<li>Can scale up without significant changes to tooling, architecture, or development practices</li>&#13;
</ul>&#13;
&#13;
<p>While not fully appreciated when it was first published in 2011, as the complexities of cloud native development have become more widely understood (and felt), <em>The Twelve Factor App</em> and the properties it advocates have started to be cited as the bare minimum for any service to be cloud native.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="I. Codebase"><div class="sect2" id="idm45983630449064">&#13;
<h2>I. Codebase</h2>&#13;
<blockquote>&#13;
<p>One codebase tracked in revision control, many deploys.</p>&#13;
<p data-type="attribution">The Twelve-Factor App</p>&#13;
</blockquote>&#13;
&#13;
<p><a data-type="indexterm" data-primary="codebase" id="idm45983630446040"/>For any given service, there should be exactly one codebase that’s used to produce any number of immutable releases for multiple deployments to multiple environments. These environments typically include a production site, and one or more staging and development sites.</p>&#13;
&#13;
<p>Having multiple services sharing the same code tends to lead to a blurring of the lines between modules, trending in time to something like a monolith, making it harder to make changes in one part of the service without affecting another part (or another service!) in unexpected ways. Instead, shared code should be refactored into libraries that can be individually versioned and included through a dependency manager.</p>&#13;
&#13;
<p>Having a single service spread across multiple repositories, however, makes it nearly impossible to automatically apply the build and deploy phases of your service’s life cycle.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="II. Dependencies"><div class="sect2" id="idm45983630443368">&#13;
<h2>II. Dependencies</h2>&#13;
<blockquote>&#13;
<p>Explicitly declare and isolate (code) dependencies.</p>&#13;
<p data-type="attribution">The Twelve-Factor App</p>&#13;
</blockquote>&#13;
&#13;
<p><a data-type="indexterm" data-primary="dependencies" id="ch06_term7"/>For any given version of the codebase, <code>go build</code>, <code>go test</code>, and <code>go run</code> should be deterministic: they should have the same result, however they’re run, and the product should always respond the same way to the same inputs.</p>&#13;
&#13;
<p>But what if a dependency—an imported code package or installed system tool beyond the programmer’s control—changes in such a way that it breaks the build, introduces a bug, or becomes incompatible with the service?</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Go" data-secondary="modules" id="idm45983630436840"/>Most programming languages offer a packaging system for distributing support libraries, and Go is no different.<sup><a data-type="noteref" id="idm45983630435608-marker" href="ch06.xhtml#idm45983630435608">13</a></sup> By using <a href="https://oreil.ly/68ds1">Go modules</a> to declare all dependencies, completely and exactly, you can ensure that imported packages won’t change out from under you and break your build in unexpected ways.</p>&#13;
&#13;
<p>To extend this somewhat, services should generally try to avoid using the <code>os/exec</code> package’s <code>Command</code> function to shell out to external tools like ImageMagick or <code>curl</code>.</p>&#13;
&#13;
<p>Yes, your target tool might be available on all (or most) systems, but there’s no way to <em>guarantee</em> that they both exist and are fully compatible with the service everywhere that it might run in the present or future. Ideally, if your service requires an external tool, that tool should be <em>vendored</em> into the service by including it in the service’s repository.<a data-type="indexterm" data-primary="dependencies" data-startref="ch06_term7" id="idm45983630430792"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="III. Configuration"><div class="sect2" id="section_ch06_12_factor_app_configuration">&#13;
<h2>III. Configuration</h2>&#13;
<blockquote>&#13;
<p>Store configuration in the environment.</p>&#13;
<p data-type="attribution">The Twelve-Factor App</p>&#13;
</blockquote>&#13;
&#13;
<p><a data-type="indexterm" data-primary="configuration" id="ch06_term8"/>Configuration—anything that’s likely to vary between environments (staging, production, developer environments, etc)—should always be cleanly separated from the code. Under no circumstances should an application’s configuration be baked into the code.</p>&#13;
&#13;
<p>Configuration items may include, but certainly aren’t limited to:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>URLs or other resource handles to a database or other upstream service dependencies—even if it’s not likely to change any time soon.</p>&#13;
</li>&#13;
<li>&#13;
<p>Secrets of <em>any</em> kind, such as passwords or credentials for external services.<a data-type="indexterm" data-primary="secrets" id="idm45983630421784"/></p>&#13;
</li>&#13;
<li>&#13;
<p>Per-environment values, such as the canonical hostname for the deploy.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>A common means of extracting configuration from code is by <em>externalizing</em> them into some configuration file—often YAML<sup><a data-type="noteref" id="idm45983630418920-marker" href="ch06.xhtml#idm45983630418920">14</a></sup>—which may or may not be checked into the repository alongside the code. This is certainly an improvement over configuration-in-code, but it’s also less than ideal.</p>&#13;
&#13;
<p>First, if your configuration file lives outside of the repository, it’s all too easy to accidentally check it in. What’s more, such files tend to proliferate, with different versions for different environments living in different places, making it hard to see and manage configurations with any consistency.</p>&#13;
&#13;
<p>Alternatively, you <em>could</em> have different versions of your configurations for each environment in the repository, but this can be unwieldy and tends to lead to some awkward repository acrobatics.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45983630416152">&#13;
<h5>There Are No Partially Compromised Secrets</h5>&#13;
<p>It’s worth emphasizing that while configuration values should never be in code, passwords or other sensitive secrets should <em>absolutely, never ever</em> be in code. It’s all too easy for those secrets, in a moment of forgetfulness, to get shared with the whole world.</p>&#13;
&#13;
<p>Once a secret is out, it’s out. There are no partially compromised secrets.</p>&#13;
&#13;
<p>Always treat your repository—and the code it contains—as if it can be made public at any time. Which, of course, it can.</p>&#13;
</div></aside>&#13;
&#13;
<p><a data-type="indexterm" data-primary="environment variable" data-secondary="uses of" id="idm45983630412712"/>Instead of configurations as code or even as external configurations, <em>The Twelve Factor App</em> recommends that configurations be stored as <em>environment variables</em>. Using environment variables in this way actually has a lot of advantages:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>They are standard and largely OS and language agnostic.</p>&#13;
</li>&#13;
<li>&#13;
<p>That are easy to change between deploys without changing any code.</p>&#13;
</li>&#13;
<li>&#13;
<p>They’re very easy to inject into containers.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Go" data-secondary="tools of" id="idm45983630406936"/>Go has several tools for doing this.</p>&#13;
&#13;
<p>The first—and most basic—is the <code>os</code> package, which provides the <code>os.Getenv</code> function for this purpose:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="nx">name</code> <code class="o">:=</code> <code class="nx">os</code><code class="p">.</code><code class="nx">Getenv</code><code class="p">(</code><code class="s">"NAME"</code><code class="p">)</code>&#13;
<code class="nx">place</code> <code class="o">:=</code> <code class="nx">os</code><code class="p">.</code><code class="nx">Getenv</code><code class="p">(</code><code class="s">"CITY"</code><code class="p">)</code>&#13;
&#13;
<code class="nx">fmt</code><code class="p">.</code><code class="nx">Printf</code><code class="p">(</code><code class="s">"%s lives in %s.\n"</code><code class="p">,</code> <code class="nx">name</code><code class="p">,</code> <code class="nx">place</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="indexterm" data-primary="Viper" id="idm45983630402296"/>For more sophisticated configuration options, there are several excellent packages available. Of these, <a href="https://oreil.ly/8giE4"><code>spf13/viper</code></a> seems to be particularly popular. A snippet of Viper in action might look like the following:</p>&#13;
&#13;
<pre data-type="programlisting" data-code-language="go"><code class="nx">viper</code><code class="p">.</code><code class="nx">BindEnv</code><code class="p">(</code><code class="s">"id"</code><code class="p">)</code>             <code class="c1">// Will be uppercased automatically</code>&#13;
<code class="nx">viper</code><code class="p">.</code><code class="nx">SetDefault</code><code class="p">(</code><code class="s">"id"</code><code class="p">,</code> <code class="s">"13"</code><code class="p">)</code>    <code class="c1">// Default value is "13"</code>&#13;
&#13;
<code class="nx">id1</code> <code class="o">:=</code> <code class="nx">viper</code><code class="p">.</code><code class="nx">GetInt</code><code class="p">(</code><code class="s">"id"</code><code class="p">)</code>&#13;
<code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="nx">id1</code><code class="p">)</code>                <code class="c1">// 13</code>&#13;
&#13;
<code class="nx">os</code><code class="p">.</code><code class="nx">Setenv</code><code class="p">(</code><code class="s">"ID"</code><code class="p">,</code> <code class="s">"50"</code><code class="p">)</code>           <code class="c1">// Typically done outside of the app!</code>&#13;
&#13;
<code class="nx">id2</code> <code class="o">:=</code> <code class="nx">viper</code><code class="p">.</code><code class="nx">GetInt</code><code class="p">(</code><code class="s">"id"</code><code class="p">)</code>&#13;
<code class="nx">fmt</code><code class="p">.</code><code class="nx">Println</code><code class="p">(</code><code class="nx">id2</code><code class="p">)</code>                <code class="c1">// 50</code></pre>&#13;
&#13;
<p><a data-type="indexterm" data-primary="etcd" id="idm45983630380472"/><a data-type="indexterm" data-primary="Consul" id="idm45983630293832"/>Additionally, Viper provides a number of features that the standard packages do not, such as default values, typed variables, and reading from command-line flags, variously formatted configuration files, and even remote configuration systems like etcd and Consul.<a data-type="indexterm" data-primary="configuration" data-startref="ch06_term8" id="idm45983630292824"/></p>&#13;
&#13;
<p>We’ll dive more deeply into Viper and other configuration topics in <a data-type="xref" href="ch10.xhtml#chapter_10">Chapter 10</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="IV. Backing Services"><div class="sect2" id="idm45983630429016">&#13;
<h2>IV. Backing Services</h2>&#13;
<blockquote>&#13;
<p>Treat backing services as attached resources.</p>&#13;
<p data-type="attribution">The Twelve-Factor App</p>&#13;
</blockquote>&#13;
&#13;
<p><a data-type="indexterm" data-primary="backing services" id="idm45983630287672"/>A backing service is any downstream dependency that a service consumes across the network as part of its normal operation (see <a data-type="xref" href="ch01.xhtml#sidebar_ch01_dependencies">“Upstream and Downstream Dependencies”</a>).&#13;
A service should make no distinction between backing services of the same type.&#13;
Whether it’s an internal service that’s managed within the same organization or a remote service managed by a third party should make no difference.</p>&#13;
&#13;
<p>To the service, each distinct upstream service should be treated as just another resource, each addressable by a configurable URL or some other resource handle, as illustrated oi <a data-type="xref" href="#img_ch06_backing_services">Figure 6-3</a>. All resources should be treated as equally subject to the <em>Fallacies of Distributed Computing</em> (see <a data-type="xref" href="ch04.xhtml#chapter_4">Chapter 4</a> for a refresher, if necessary).</p>&#13;
&#13;
<figure><div id="img_ch06_backing_services" class="figure">&#13;
<img src="Images/cngo_0603.png" alt="cngo 0603" width="885" height="566"/>&#13;
<h6><span class="label">Figure 6-3. </span>Each upstream service should be treated as just another resource, each addressable by a configurable URL or some other resource handle, each equally subject to the Fallacies of Distributed Computing</h6>&#13;
</div></figure>&#13;
&#13;
<p>In other words, a MySQL database run by your own team’s sysadmins should be treated no differently than an AWS-managed RDS instance. The same goes for <em>any</em> upstream service, whether it’s running in a data center in another hemisphere or in a Docker container on the same server.</p>&#13;
&#13;
<p>A service that’s able to swap out any resource at will with another one of the same kind—internally managed or otherwise—just by changing a configuration value can be more easily deployed to different environments, can be more easily tested, and more easily maintained.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="V. Build, Release, Run"><div class="sect2" id="idm45983630278936">&#13;
<h2>V. Build, Release, Run</h2>&#13;
<blockquote>&#13;
<p>Strictly separate build and run stages.</p>&#13;
<p data-type="attribution">The Twelve-Factor App</p>&#13;
</blockquote>&#13;
&#13;
<p>Each (nondevelopment) deployment—the union of a specific version of the built code and a configuration—should be immutable and uniquely labeled. It should be possible, if necessary, to precisely recreate a deployment if (heaven forbid) it is necessary to roll a deployment back to an earlier version.</p>&#13;
&#13;
<p>Typically, this is accomplished in three distinct stages, illustrated in <a data-type="xref" href="#img_ch06_build_release_run">Figure 6-4</a> and described in the following:</p>&#13;
<dl>&#13;
<dt><a data-type="indexterm" data-primary="build stage" id="idm45983630273400"/>Build</dt>&#13;
<dd>&#13;
<p>In the build stage, an automated process retrieves a specific version of the code, fetches dependencies, and compiles an executable artifact we call a <em>build</em>. Every build should always have a unique identifier, typically a timestamp or an incrementing build number.</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="release stage" id="idm45983630270712"/>Release</dt>&#13;
<dd>&#13;
<p>In the release stage, a specific build is combined with a configuration specific to the target deployment. The resulting <em>release</em> is ready for immediate execution in the execution environment. Like builds, releases should also have a unique identifier. Importantly, producing releases with same version of a build shouldn’t involve a rebuild of the code: to ensure environment parity, each environment-specific configuration should use the same build artifact.</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="run stage" id="idm45983630267816"/>Run</dt>&#13;
<dd>&#13;
<p>In the run stage, the release is delivered to the deployment environment and executed by launching the service’s processes.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Ideally, a new versioned build will be automatically produced whenever new code is deployed.</p>&#13;
&#13;
<figure><div id="img_ch06_build_release_run" class="figure">&#13;
<img src="Images/cngo_0604.png" alt="cngo 0604" width="793" height="277"/>&#13;
<h6><span class="label">Figure 6-4. </span>The process of deploying a codebase to a (nondevelopment) environment should be performed in distinct build, release, and run stages</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="VI. Processes"><div class="sect2" id="idm45983630263144">&#13;
<h2>VI. Processes</h2>&#13;
<blockquote>&#13;
<p>Execute the app as one or more stateless processes.</p>&#13;
<p data-type="attribution">The Twelve-Factor App</p>&#13;
</blockquote>&#13;
&#13;
<p><a data-type="indexterm" data-primary="service processes" id="idm45983630260216"/>Service processes should be stateless and share nothing. Any data that has to be persisted should be stored in a stateful backing service, typically a database or external cache.</p>&#13;
&#13;
<p>We’ve already spent some time talking about statelessness—and we’ll spend more in the next chapter—so we won’t dive into this point any further.</p>&#13;
&#13;
<p>However, if you’re interested in reading ahead, feel free to take a look at <a data-type="xref" href="ch07.xhtml#section_ch07_state_and_statelessness">“State and Statelessness”</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="VII. Data Isolation"><div class="sect2" id="idm45983630257240">&#13;
<h2>VII. Data Isolation</h2>&#13;
<blockquote>&#13;
<p>Each service manages its own data.</p>&#13;
<p data-type="attribution">Cloud Native, <cite>Data Isolation</cite></p>&#13;
</blockquote>&#13;
&#13;
<p><a data-type="indexterm" data-primary="data isolation" id="idm45983630253928"/>Each service should be entirely <em>self-contained</em>. That is, it should manage its own data, and make its data accessible only via an API designed for that purpose. If this sounds familiar to you, good! This is actually one of the core principles of microservices, which we’ll discuss more in <a data-type="xref" href="ch07.xhtml#section_ch07_microservices">“The Microservices System Architecture”</a>.</p>&#13;
&#13;
<p>Very often this will be implemented as a request-response service like a RESTful API or RPC protocol that’s exported by listening to requests coming in on a port, but this can also take the form of an asynchronous, event-based service using a publish-subscribe messaging pattern. Both of these patterns will be described in more detail in <a data-type="xref" href="ch08.xhtml#chapter_8">Chapter 8</a>.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45983630249832">&#13;
<h5>Historical Note</h5>&#13;
<p>The actual title of the seventh section of <em>The Twelve Factor App</em> is “Port Binding,” and is summarized as “export services via port binding.”<sup><a data-type="noteref" id="idm45983630247848-marker" href="ch06.xhtml#idm45983630247848">15</a></sup></p>&#13;
&#13;
<p>At the time, this advice certainly made sense, but this title obscures its main point: that a service should encapsulate and manage its own data, and only share that data via an API.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="functions as a service" data-see="FaaS" id="idm45983630244856"/><a data-type="indexterm" data-primary="FaaS" id="idm45983630243880"/>While many (or even most) web applications do, in fact, expose their APIs via ports, the increasing popularity of functions as a service (FaaS) and event-driven architectures means this is no longer necessarily always the case.</p>&#13;
&#13;
<p>So, instead of the original text, I’ve decided to use the more up-to-date (and true-to-intent) summary provided by Boris Scholl, Trent Swanson, and Peter Jausovec in <a href="https://oreil.ly/uvvsa" class="orm:hideurl"><em>Cloud Native: Using Containers, Functions, and Data to Build Next-Generation <span class="keep-together">Applications</span></em></a> (O’Reilly).</p>&#13;
</div></aside>&#13;
&#13;
<p><a data-type="indexterm" data-primary="testability" id="idm45983630208040"/><a data-type="indexterm" data-primary="portability" id="idm45983630207448"/><a data-type="indexterm" data-primary="data isolation" id="idm45983630206776"/><a data-type="indexterm" data-primary="environment agnosticism" id="idm45983630206104"/>And finally, although this is something you don’t see in the Go world, some languages and frameworks allow the runtime injection of an application server into the execution environment to create a web-facing service. This practice limits testability and portability by breaking data isolation and environment agnosticism, and is <em>very strongly</em> discouraged.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="VIII. Scalability"><div class="sect2" id="idm45983630204440">&#13;
<h2>VIII. Scalability</h2>&#13;
<blockquote>&#13;
<p>Scale out via the process model.</p>&#13;
<p data-type="attribution">The Twelve-Factor App</p>&#13;
</blockquote>&#13;
&#13;
<p><a data-type="indexterm" data-primary="horizontally scaling" id="idm45983630201384"/>Services should be able to scale horizontally by adding more instances.</p>&#13;
&#13;
<p>We talk about scalability quite a bit in this book. We even dedicated all of <a data-type="xref" href="ch07.xhtml#chapter_7">Chapter 7</a> to it. With good reason: the importance of scalability can’t be understated.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="vertical scaling" id="idm45983630199080"/>Sure, it’s certainly convenient to just beef up the one server your service is running on—and that’s fine in the (very) short term—but vertical scaling is a losing strategy in the long run. If you’re lucky, you’ll eventually hit a point where you simply can’t scale up any more. It’s more likely that your single server will either suffer load spikes faster than you can scale up, or just die without warning and without a redundant failover.<sup><a data-type="noteref" id="idm45983630197768-marker" href="ch06.xhtml#idm45983630197768">16</a></sup> Both scenarios end with a lot of unhappy users.&#13;
We’ll discuss scalability quite a bit more in <a data-type="xref" href="ch07.xhtml#chapter_7">Chapter 7</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="IX. Disposability"><div class="sect2" id="idm45983630196024">&#13;
<h2>IX. Disposability</h2>&#13;
<blockquote>&#13;
<p>Maximize robustness with fast startup and graceful shutdown.</p>&#13;
<p data-type="attribution">The Twelve-Factor App</p>&#13;
</blockquote>&#13;
&#13;
<p><a data-type="indexterm" data-primary="disposability" id="idm45983630193160"/>Cloud environments are fickle: provisioned servers have a funny way of disappearing at odd times. Services should account for this by being <em>disposable</em>: service instances should be able to be started or stopped—intentionally or not—at any time.</p>&#13;
&#13;
<p>Services should strive to minimize the time it takes to start up to reduce the time it takes for the service to be deployed (or redeployed) to elastically scale. Go, having no virtual machine or other significant overhead, is especially good at this.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="containers" id="idm45983630190696"/>Containers provide fast startup time and are also very useful for this, but care must be taken to keep image sizes small to minimize the data transfer overhead incurred with each initial deploy of a new image. This is another area in which Go excels: its self-sufficient binaries can generally be installed into <code>SCRATCH</code> images, without requiring an external language runtime or other external dependencies. We demonstrated this in the previous chapter, in <a data-type="xref" href="ch05.xhtml#section_ch05_containerizing">“Containerizing Your Key-Value Store”</a>.</p>&#13;
&#13;
<p>Services should also be capable of shutting down when they receive a <code>SIGTERM</code> signal by saving all data that needs to be saved, closing open network connections, or finishing any in-progress work that’s left or by returning the current job to the work queue.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="X. Development/Production Parity"><div class="sect2" id="idm45983630186936">&#13;
<h2>X. Development/Production Parity</h2>&#13;
<blockquote>&#13;
<p>Keep development, staging, and production as similar as possible.</p>&#13;
<p data-type="attribution">The Twelve-Factor App</p>&#13;
</blockquote>&#13;
&#13;
<p><a data-type="indexterm" data-primary="development/production parity" id="idm45983630183864"/>Any possible differences between development and production should be kept as small as possible. This includes code differences, of course, but it extends well beyond that:</p>&#13;
<dl>&#13;
<dt><a data-type="indexterm" data-primary="code divergence" id="idm45983630182024"/>Code divergence</dt>&#13;
<dd>&#13;
<p>Development branches should be small and short-lived, and should be tested and deployed into production as quickly as possible. This minimizes functional differences between environments and reduces the risk of both deploys and &#13;
<span class="keep-together">rollbacks</span>.</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="stack divergence" id="idm45983630178984"/>Stack divergence</dt>&#13;
<dd>&#13;
<p>Rather than having different components for development and production (say, SQLite on OS X versus MySQL on Linux), environments should remain as similar as possible. Lightweight containers are an excellent tool for this. This minimizes the possibility that inconvenient differences between almost-but-not-quite-the-same implementations will emerge to ruin your day.</p>&#13;
</dd>&#13;
<dt><a data-type="indexterm" data-primary="personnel divergence" id="idm45983630176616"/>Personnel divergence</dt>&#13;
<dd>&#13;
<p>Once it was common to have programmers who wrote code and operators who deployed code, but that arrangement created conflicting incentives and counter-productive adversarial relationships. Keeping code authors involved in deploying their work and responsible for its behavior in production helps break down development/operations silos and aligns incentives around stability and velocity.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Taken together, these approaches help to keep the gap between development and production small, which in turn encourages rapid, automated, continuous &#13;
<span class="keep-together">deployment</span>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="XI. Logs"><div class="sect2" id="section_ch06_12_factor_app_logs">&#13;
<h2>XI. Logs</h2>&#13;
<blockquote>&#13;
<p>Treat logs as event streams.</p>&#13;
<p data-type="attribution">The Twelve-Factor App</p>&#13;
</blockquote>&#13;
&#13;
<p><a data-type="indexterm" data-primary="logs" id="idm45983630169944"/>Logs—a service’s never-ending stream of consciousness—are incredibly useful things, particularly in a distributed environment. By providing visibility into the behavior of a running application, good logging can greatly simplify the task of locating and diagnosing misbehavior.</p>&#13;
&#13;
<p>Traditionally, services wrote log events to a file on the local disk. At cloud scale, however, this just makes valuable information awkward to find, inconvenient to access, and impossible to aggregate. In dynamic, ephemeral environments like Kubernetes your service instances (and their log files) may not even exist by the time you get around to viewing them.</p>&#13;
&#13;
<p>Instead, a cloud native service should treat log information as nothing more than a stream of events, writing each event, unbuffered, directly to <code>stdout</code>. It shouldn’t concern itself with implementation trivialities like routing or storage of its log events, and allow the executor to decide what happens to them.</p>&#13;
&#13;
<p>Though seemingly simple (and perhaps somewhat counterintuitive), this small change provides a great deal of freedom.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="ELK" id="idm45983630166040"/><a data-type="indexterm" data-primary="Elasticsearch" id="idm45983630165336"/><a data-type="indexterm" data-primary="Logstash" id="idm45983630164664"/><a data-type="indexterm" data-primary="Kibana" data-see="ELK" id="idm45983630163992"/><a data-type="indexterm" data-primary="Splunk" id="idm45983630163048"/>During local development, a programmer can watch the event stream in a terminal to observe the service’s behavior. In deployment, the output stream can be captured by the execution environment and forwarded to one or more destinations, such as a log indexing system like Elasticsearch, Logstash, and Kibana (ELK) or Splunk for review and analysis, or a data warehouse for long-term storage.</p>&#13;
&#13;
<p>We’ll discuss logs and logging, in the context of observability, in more detail in &#13;
<span class="keep-together"><a data-type="xref" href="ch11.xhtml#chapter_11">Chapter 11</a>.</span></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect2" data-pdf-bookmark="XII. Administrative Processes"><div class="sect2" id="idm45983630160056">&#13;
<h2>XII. Administrative Processes</h2>&#13;
<blockquote>&#13;
<p>Run administrative/management tasks as one-off processes.</p>&#13;
<p data-type="attribution">The Twelve-Factor App</p>&#13;
</blockquote>&#13;
&#13;
<p><a data-type="indexterm" data-primary="administrative processes" id="idm45983630157016"/>Of all of the original Twelve Factors, this is the one that most shows its age. For one thing, it explicitly advocates shelling into an environment to manually execute tasks.</p>&#13;
&#13;
<p><a data-type="indexterm" data-primary="snowflakes" id="ch06_term12"/>To be clear: <em>making manual changes to a server instance creates snowflakes. This is a bad thing.</em> See <a data-type="xref" href="#sidebar_ch06_snowflakes">“Special Snowflakes”</a>.</p>&#13;
&#13;
<p>Assuming you even have an environment that you can shell into, you should assume that it can (and eventually will) be destroyed and re-created any moment.</p>&#13;
&#13;
<p>Ignoring all of that for a moment, let’s distill the point to its original intent: administrative and management tasks should be run as one-off processes. This could be interpreted in two ways, each requiring its own approach:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>If your task is an administrative process, like a data repair job or database migration, it should be run as a short-lived process. Containers and functions are excellent vehicles for such purposes.</p>&#13;
</li>&#13;
<li>&#13;
<p>If your change is an update to your service or execution environment, you should instead modify your service or environment construction/configuration scripts, respectively.<a data-type="indexterm" data-primary="Twelve-Factor App" data-startref="ch06_term6" id="idm45983630149640"/></p>&#13;
</li>&#13;
</ul>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_ch06_snowflakes">&#13;
<h5>Special Snowflakes</h5>&#13;
<p>Keeping servers healthy can be a challenge. At 3 a.m., when things aren’t working quite right, it’s really tempting to make a quick change and go back to bed. Congratulations, you’ve just created a <em>snowflake</em>: a special server instance with manual changes that give it unique, usually undocumented, behaviors.</p>&#13;
&#13;
<p>Even minor, seemingly harmless changes can lead to significant problems. Even if the changes are documented—which is rarely the case—snowflake servers are hard to reproduce exactly, particularly if you need to keep an entire cluster in sync. This can lead to a bad time when you have to redeploy your service onto new hardware and can’t figure out why it’s not working.</p>&#13;
&#13;
<p>Furthermore, because your testing environment no longer matches production, you can no longer trust your development environments to reliably reproduce your production deployment.</p>&#13;
&#13;
<p>Instead, servers and containers should be treated as <em>immutable</em>. If something needs to be updated, fixed, or modified in any way, changes should be made by updating the appropriate build scripts, baking<sup><a data-type="noteref" id="idm45983630143304-marker" href="ch06.xhtml#idm45983630143304">17</a></sup> a new common image, and provisioning new server or container instances to replace the old ones.</p>&#13;
&#13;
<p>As the expression goes, instances should be treated as “cattle, not pets.”<a data-type="indexterm" data-primary="snowflakes" data-startref="ch06_term12" id="idm45983630141992"/></p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm45983630462008">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In this chapter we considered the question “what’s the point of cloud native?” The common answer is “a computer system that works in the cloud.” But “work” can mean anything. Surely we can do better.</p>&#13;
&#13;
<p>So we went back to thinkers like Tony Hoare and J-C Laprie, who provided the first part of the answer: <em>dependability</em>. That is, to paraphrase, computer systems that behave in ways that users find acceptable, despite living in a fundamentally unreliable environment.</p>&#13;
&#13;
<p>Obviously, that’s more easily said than done, so we reviewed three schools of thought regarding how to achieve it:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Laprie’s academic “means of dependability,” which include preventing, tolerating, removing, and forecasting faults</p>&#13;
</li>&#13;
<li>&#13;
<p>Adam Wiggins’ <em>Twelve Factor App</em>, which took a more prescriptive (and slightly dated, in spots) approach</p>&#13;
</li>&#13;
<li>&#13;
<p>Our own “cloud native attributes,” based on the Cloud Native Computing Foundation’s definition of “cloud native,” that we introduced in <a data-type="xref" href="ch01.xhtml#chapter_1">Chapter 1</a> and organized this entire book around</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Although this chapter was essentially a short survey of theory, there’s a lot of important, foundational information here that describes the motivations and means used to achieve what we call “cloud native.”</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45983630648552"><sup><a href="ch06.xhtml#idm45983630648552-marker">1</a></sup> Hoare, C.A.R. “An Axiomatic Basis for Computer Programming.”. <em>Communications of the ACM</em>, vol. 12, no. 10, October 1969, pp. 576–583. <a href="https://oreil.ly/jOwO9"><em class="hyperlink">https://oreil.ly/jOwO9</em></a>.</p><p data-type="footnote" id="idm45983630641848"><sup><a href="ch06.xhtml#idm45983630641848-marker">2</a></sup> When Edsger W. Dijkstra coined the expression “GOTO considered harmful,” he was referencing Hoare’s work in structured programming.</p><p data-type="footnote" id="idm45983630640824"><sup><a href="ch06.xhtml#idm45983630640824-marker">3</a></sup> Hoare, Tony. “Null References: The Billion Dollar Mistake.” <em>InfoQ.com</em>. 25 August 2009. <a href="https://oreil.ly/4QWS8"><em class="hyperlink">https://oreil.ly/4QWS8</em></a>.</p><p data-type="footnote" id="idm45983630609624"><sup><a href="ch06.xhtml#idm45983630609624-marker">4</a></sup> <em>Cloud Native Is About Culture, Not Containers</em>. Cummins, Holly. Cloud Native London 2018.</p><p data-type="footnote" id="idm45983630608616"><sup><a href="ch06.xhtml#idm45983630608616-marker">5</a></sup> If you ever have a chance to see her speak, I strongly recommend you take it.</p><p data-type="footnote" id="idm45983630606248"><sup><a href="ch06.xhtml#idm45983630606248-marker">6</a></sup> Remember what Walt did to Jane that time? That was so messed up.</p><p data-type="footnote" id="idm45983630599160"><sup><a href="ch06.xhtml#idm45983630599160-marker">7</a></sup> Laprie, J-C. “Dependable Computing and Fault Tolerance: Concepts and Terminology.” <em>FTCS-15 The 15th Int’l Symposium on Fault-Tolerant Computing</em>, June 1985, pp. 2–11. <a href="https://oreil.ly/UZFFY"><em class="hyperlink">https://oreil.ly/UZFFY</em></a>.</p><p data-type="footnote" id="idm45983630595768"><sup><a href="ch06.xhtml#idm45983630595768-marker">8</a></sup> A. Avižienis, J. Laprie, and B. Randell. “Fundamental Concepts of Computer System Dependability.” <em>Research Report No. 1145, LAAS-CNRS</em>, April 2001. <a href="https://oreil.ly/4YXd1"><em class="hyperlink">https://oreil.ly/4YXd1</em></a>.</p><p data-type="footnote" id="idm45983630572776"><sup><a href="ch06.xhtml#idm45983630572776-marker">9</a></sup> If you haven’t, start with <a href="https://oreil.ly/OJn99"><em>Site Reliability Engineering: How Google Runs Production Systems</em></a>. It really is very good.</p><p data-type="footnote" id="idm45983630567896"><sup><a href="ch06.xhtml#idm45983630567896-marker">10</a></sup> Many organizations use service-level objectives (SLOs) for precisely this purpose.</p><p data-type="footnote" id="idm45983630519656"><sup><a href="ch06.xhtml#idm45983630519656-marker">11</a></sup> Application state is hard, and when done wrong it’s poison to scalability.</p><p data-type="footnote" id="idm45983630456168"><sup><a href="ch06.xhtml#idm45983630456168-marker">12</a></sup> Wiggins, Adam. <em>The Twelve-Factor App.</em> 2011. <a href="https://12factor.net"><em class="hyperlink">https://12factor.net</em></a>.</p><p data-type="footnote" id="idm45983630435608"><sup><a href="ch06.xhtml#idm45983630435608-marker">13</a></sup> Although it was for too long!</p><p data-type="footnote" id="idm45983630418920"><sup><a href="ch06.xhtml#idm45983630418920-marker">14</a></sup> The world’s worst configuration language (except for all the other ones).</p><p data-type="footnote" id="idm45983630247848"><sup><a href="ch06.xhtml#idm45983630247848-marker">15</a></sup> Wiggins, Adam. “Port Binding.” <em>The Twelve-Factor App.</em> 2011. <a href="https://oreil.ly/bp8lC"><em class="hyperlink">https://oreil.ly/bp8lC</em></a>.</p><p data-type="footnote" id="idm45983630197768"><sup><a href="ch06.xhtml#idm45983630197768-marker">16</a></sup> Probably at three in the morning.</p><p data-type="footnote" id="idm45983630143304"><sup><a href="ch06.xhtml#idm45983630143304-marker">17</a></sup> “Baking” is a term sometimes used to refer to the process of creating a new container or server image.</p></div></div></section></div></body></html>