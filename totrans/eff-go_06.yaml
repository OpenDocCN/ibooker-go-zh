- en: Chapter 6\. Efficiency Observability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [“Efficiency-Aware Development Flow”](ch03.html#ch-conq-eff-flow), you learned
    to follow the TFBO (test, fix, benchmark, and optimize) flow to validate and achieve
    the required efficiency results with the least effort. Around the elements of
    the efficiency phase, observability takes one of the key roles, especially in
    Chapters [7](ch07.html#ch-observability2) and [9](ch09.html#ch-observability3).
    We focus on that phase in [Figure 6-1](#img-obs-intro).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![efgo 0601](assets/efgo_0601.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. An excerpt from [Figure 3-5](ch03.html#img-opt-flow) focusing on
    the part that requires good observability
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this chapter, I will explain the required observability and monitoring tools
    for this part of the flow. First, we will learn what observability is and what
    problems it solves. Then, we will discuss different observability signals, typically
    divided into logs, tracing, metrics, and, recently, profiles. Next, we will explain
    the first three signals in [“Example: Instrumenting for Latency”](#ch-obs-signals),
    which takes latency as an example of the efficiency information we might want
    to measure (profiling is explained in [Chapter 9](ch09.html#ch-observability3)).
    Last but not least, we will go through the specific semantics and sources of metrics
    related to our program efficiency in [“Efficiency Metrics Semantics”](#ch-obs-semantics).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: You Can’t Improve What You Don’t Measure!
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This quote, often attributed to Peter Drucker, is a key to improving anything:
    business revenues, car efficiency, family budget, body fat, or [even happiness](https://oreil.ly/eKiIR).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Especially when it comes to invisible waste that our inefficient software is
    producing, we can say that it’s impossible to optimize software without assessing
    and measuring before and after the change. Every decision must be data driven,
    as our guesses in this virtual space are often wrong.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: With no further ado, let’s learn how to measure the efficiency of our software
    in the easiest possible way—with the concept the industry calls observability.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To control software efficiency, we first need to find a structured and reliable
    way to measure the latency and resource usage of our Go applications. The key
    is to count these as accurately as possible and present them at the end as easy
    to understand numeric values. This is why for consumption measurements, we sometimes
    (not always!) use a “metric signal,” which is a pillar of the essential software
    (or system) characteristics called observability.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the cloud-native infrastructure world, we often talk about the observability
    of our applications. Unfortunately, observability is a very overloaded word.^([1](ch06.html#idm45606832729952))
    It can be summarized as follows: an ability to deduce the state of a system inferred
    from external signals.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'The external signals the industry uses nowadays can be generally categorized
    into four types: metrics, logs, traces, and profiling.^([2](ch06.html#idm45606832725840))'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当今行业使用的外部信号通常可以大致分为四类：指标、日志、追踪和性能分析。^([2](ch06.html#idm45606832725840))
- en: Observability is a huge topic nowadays as it can help us in many situations
    while developing and operating our software. Observability patterns allow us to
    debug failures or unexpected behaviors of our programs, find root causes of incidents,
    monitor healthiness, alert on unforeseen situations, perform billing, measure
    [SLIs (service level indicators)](https://oreil.ly/hsdXJ), run analytics, and
    much more. Naturally, we will focus only on the parts of observability that will
    help us ensure that our software efficiency matches our requirements (the RAERs
    mentioned in [“Efficiency Requirements Should Be Formalized”](ch03.html#ch-conq-req-formal)).
    So what is an observability signal?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 可观测性是当今一个重要的话题，因为它在开发和运行软件过程中能帮助我们应对许多情况。可观测性模式使我们能够调试程序的失败或意外行为，找到事故的根本原因，监控健康状况，对未预料到的情况进行警报，执行计费，测量[SLI（服务水平指标）](https://oreil.ly/hsdXJ)，进行分析等等。当然，我们只会专注于可观测性的那些部分，这些部分将帮助我们确保软件的效率与我们的需求相匹配（即RAERs在[“效率要求应被正式化”](ch03.html#ch-conq-req-formal)中提到的）。那么，什么是可观测性信号？
- en: Metrics are a numeric representation of data measured over intervals of time.
    Metrics can harness the power of mathematical modeling and prediction to derive
    knowledge of the behavior of a system over intervals of time in the present and
    future.
  id: totrans-15
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标是对时间间隔内数据的数值表示。指标可以利用数学建模和预测的能力，从而推导出系统在当前和未来时间间隔内的行为知识。
- en: 'An event log is an immutable, timestamped record of discrete events that happened
    over time. Event logs in general come in three forms but are fundamentally the
    same: a timestamp and a payload of some context.'
  id: totrans-16
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件日志是一种不可变的、带有时间戳的记录，记录了随时间发生的离散事件。一般来说，事件日志有三种形式，但本质上是相同的：时间戳和一些上下文信息的负载。
- en: A trace is a representation of a series of causally related distributed events
    that encode the end-to-end request flow through a distributed system. Traces are
    a representation of logs; the data structure of traces looks almost like that
    of an event log. A single trace can provide visibility into both the path traversed
    by a request as well as the structure of a request.
  id: totrans-17
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 追踪是一个代表一系列因果相关的分布式事件的表示形式，它编码了通过分布式系统的端到端请求流。追踪是日志的一种表示形式；追踪的数据结构几乎与事件日志相似。一个单独的追踪可以提供对请求经过的路径以及请求结构的可见性。
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Cindy Sridharan, [*Distributed Systems Observability*](https://oreil.ly/YrSIE)
    (O’Reilly, 2018)
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Cindy Sridharan，《分布式系统可观测性》（O’Reilly，2018）
- en: Generally, all those signals can be used to observe our Go applications’ latency
    and resource consumption for optimization purposes. For example, we can measure
    the latency of a specific operation and expose it as a metric. We can send that
    value encoded into a log line or trace annotations (e.g., [“baggage”](https://oreil.ly/V5sQ6)
    items). We can calculate latency by subtracting the timestamps of two log lines—when
    the operation started and when it finished. We can use trace spans, which track
    the latency of a span (individual unit of work done) by design.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，所有这些信号都可以用来观察我们的Go应用程序的延迟和资源消耗，以进行优化。例如，我们可以测量特定操作的延迟，并将其作为指标暴露出来。我们可以将该值编码为日志行或追踪注释（例如，“行李”项）。我们可以通过减去两个日志行的时间戳来计算延迟——操作开始和操作完成的时间戳。我们可以使用追踪跨度来跟踪跨度的延迟（完成的个体工作单元）。
- en: 'However, whatever we use to deliver that information to us (via metric-specific
    tools, logs, traces, or profiles), in the end, it has to have metric semantics.
    We need to derive information to a numeric value so we can gather it over time;
    subtract; find max, min, or average; and aggregate over dimensions. We need the
    information to visualize and analyze. We need it to allow tools to reactively
    alert us when required, potentially build further automation that will consume
    it, and compare other metrics. This is why an efficiency discussion will mostly
    navigate through metric aggregations: the tail latency of our application, maximum
    memory usage over time, etc.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，无论我们用什么方式将这些信息传递给我们（通过特定于度量的工具、日志、追踪或概要文件），最终都必须具有度量语义。我们需要将信息推导为数值，以便随时间收集它；进行减法运算；查找最大值、最小值或平均值；以及按维度聚合。我们需要这些信息来进行可视化和分析。我们需要它允许工具在需要时做出反应并提醒我们，可能构建进一步消费它的自动化，并比较其他指标。这就是为什么效率讨论通常会通过度量聚合进行导航的原因：我们应用程序的尾延迟、随时间的最大内存使用等。
- en: As we discussed, to optimize anything, you have to start measuring it, so the
    industry has developed many metrics and instruments to capture the usage of various
    resources. The process of observing or measuring always starts with the instrumentation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论的那样，为了优化任何东西，您必须开始测量它，因此行业已经开发出许多指标和工具来捕获各种资源的使用情况。观察或测量的过程始终从仪器化开始。
- en: Instrumentation
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仪器化
- en: Instrumentation is a process of adding or enabling instruments for our code
    that will expose the observability signals we need.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 仪器化是向我们的代码添加或启用仪器，以公开我们所需的可观察信号的过程。
- en: 'Instrumentation can have many forms:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 仪器化可以采用多种形式：
- en: Manual instrumentation
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 手动仪器化
- en: We can add a few statements to our code that import a Go module that generates
    an observability signal (for example, [Prometheus client for metrics](https://oreil.ly/AoWkJ),
    [go-kit logger](https://oreil.ly/adTO3), or [a tracing](https://oreil.ly/o7uYH)
    library) and hook it to the operations we do. Of course, this requires modifying
    our Go code, but it usually leads to more personalized and rich signals with more
    context. Usually, it represents [open box](https://oreil.ly/qMjUP) information
    because we can collect information tailored to the program functionality.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以向我们的代码添加几个语句，导入一个生成可观测信号的Go模块（例如，[Prometheus客户端用于度量](https://oreil.ly/AoWkJ)，[go-kit日志记录器](https://oreil.ly/adTO3)，或者[跟踪库](https://oreil.ly/o7uYH)），并将其连接到我们执行的操作中。当然，这需要修改我们的Go代码，但通常会产生更个性化和丰富的信号以及更多的上下文。通常，它代表了[开箱即用](https://oreil.ly/qMjUP)信息，因为我们可以收集针对程序功能定制的信息。
- en: Autoinstrumentation
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 自动仪器化
- en: Sometimes instrumentation means installing (and configuring) a tool that can
    derive useful information by looking at outside effects. For example, a service
    mesh gathers observability by looking at HTTP requests and responses, or a tool
    hooks to the operating system and gathers information through [cgroups](https://oreil.ly/aCe6S)
    or [eBPF](https://oreil.ly/QjxV9).^([3](ch06.html#idm45606832699376)) Autoinstrumentation
    does not require changing and rebuilding code and usually represents [closed box
    information](https://oreil.ly/UO0gK).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，仪器化意味着安装（和配置）一个工具，该工具可以通过观察外部效果来获取有用信息。例如，服务网格通过观察HTTP请求和响应来收集可观察性，或者工具钩入操作系统并通过[cgroups](https://oreil.ly/aCe6S)或[eBPF](https://oreil.ly/QjxV9)收集信息。^([3](ch06.html#idm45606832699376))
    自动仪器化不需要更改和重新构建代码，并且通常代表了[封闭盒](https://oreil.ly/UO0gK)信息。
- en: 'On top of that, it’s helpful to categorize instrumentation based on the granularity
    of the information:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，基于信息的粒度对仪器化进行分类也很有帮助：
- en: Capturing raw events
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 捕获原始事件
- en: Instrumentation in this category will try to deliver a separate piece of information
    for each event in our process. For example, suppose we would like to know how
    many and what errors are happening in all HTTP requests served by our process.
    In that case, we could have instrumentation that delivers a separate piece of
    information about each request (e.g., as a log line). Furthermore, this information
    usually has some metadata about its context, like the status code, user IP, timestamp,
    and the process and code statement in which it happened (target metadata).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此类仪器化将尝试为我们过程中的每个事件提供单独的信息。例如，假设我们想知道我们的进程为所有HTTP请求提供了多少次以及发生了什么错误。在这种情况下，我们可以有一个为每个请求提供单独信息的仪器化（例如，作为日志行）。此外，此信息通常包含有关其上下文的一些元数据，例如状态代码、用户IP、时间戳以及发生错误的进程和代码语句（目标元数据）。
- en: Once ingested to some observability backend, such raw data is very rich in context
    and, in theory, allows any ad hoc analysis. For example, we can scan through all
    events to find an average number of errors or the percentile distributions (more
    on that in [“Latency”](#ch-obs-latency)). We can navigate to every individual
    error representing a single event to inspect it in detail. Unfortunately, this
    kind of data is generally the most expensive to use, ingest, and store. We often
    risk an inaccuracy here since it’s likely we’ll miss an individual event or two.
    In extreme cases, it requires complex skills and automation for big data and data
    mining explorations to find the information you want.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦被摄取到某个可观测性后端，这些原始数据在内容上非常丰富，并且理论上允许任何的即席分析。例如，我们可以扫描所有事件以找到错误的平均数或百分位分布（更多内容请参阅[“延迟”](#ch-obs-latency)）。我们可以导航到每个表示单个事件的错误，以便详细检查。不幸的是，这种类型的数据通常是使用、摄取和存储成本最高的数据。我们往往会因此而冒失误的风险，因为可能会错过一个或两个个体事件。在极端情况下，需要复杂的大数据和数据挖掘探索技能和自动化来找到您想要的信息。
- en: Capturing aggregated information
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 捕获聚合信息
- en: We can capture pre-aggregated data instead of raw events. Every piece of information
    delivered by such instrumentation represents certain information about a group
    of events. In our HTTP server example, we could count successful and failed requests,
    and periodically deliver that information. Before forwarding this information,
    we could go even further and pre-calculate the error ratio inside our code. It’s
    worth mentioning that this kind of information also requires metadata, so we can
    summarize, aggregate further, compare, and analyze those aggregated pieces of
    information.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以捕获预聚合数据而不是原始事件。由这种仪器化交付的每一条信息都代表一组事件的某些信息。在我们的 HTTP 服务器示例中，我们可以计算成功和失败的请求，并定期传递该信息。在转发此信息之前，我们甚至可以进一步计算代码内的错误比率。值得一提的是，这种类型的信息也需要元数据，以便我们可以总结、进一步聚合、比较和分析这些聚合的信息片段。
- en: Pre-aggregated instrumentation forces Go processes or autoinstrumentation tools
    to do more work, but the results are generally easier to use. On top of this,
    because of the smaller amount of data, the complexity of the instrumentation,
    signal delivery, and backend is lower, thereby increasing reliability and decreasing
    cost significantly. There are trade-offs here as well. We lose some information
    (commonly called the cardinality). The decision of what information to prebuild
    is made up front, and is coded into instrumentation. If you suddenly have different
    questions to be answered (e.g., how many errors an individual user had across
    your processes) and your instrumentation was not set to pre-aggregate that information,
    you have to change it, which takes time and resources. Yet if you roughly know
    what you will be asking for ahead of time, aggregated type of information is an
    amazing win and a more pragmatic approach.^([4](ch06.html#idm45606832688544))
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 预聚合的仪表化迫使 Go 进程或自动仪器化工具承担更多工作，但通常结果更易使用。此外，由于数据量较小，仪表化、信号传递和后端的复杂性更低，从而显著提高了可靠性并降低了成本。在这里也存在一些权衡。我们会丢失一些信息（通常称为基数）。预建的信息决策是事先做出的，并且编码到仪表化中。如果突然有不同的问题需要回答（例如，跨多个进程的单个用户有多少错误），而您的仪器化未设置为预聚合该信息，那么您就需要进行更改，这需要时间和资源。然而，如果您大致知道将来会询问什么，聚合类型的信息就是一个了不起的胜利和更加务实的方法。^([4](ch06.html#idm45606832688544))
- en: 'Last but not least, generally speaking we can design our observability flows
    into push-and-pull collection models:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，一般而言，我们可以将我们的可观测流程设计成推拉式采集模型：
- en: Push
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 推送
- en: A system where a centralized remote process collects observability signals from
    your applications (including your Go programs).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个中心化的远程进程系统从您的应用程序（包括您的 Go 程序）中收集可观测信号。
- en: Pull
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 拉取
- en: A system where application processes push the signal to a remote centralized
    observability system.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个应用程序进程将信号推送到远程集中的可观测性系统的系统。
- en: Push Versus Pull
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推送与拉取
- en: Each of the conventions has its pros and cons. You can push your metrics, logs,
    and traces, but you can also pull all of them from your process. We can also use
    a mixed approach, different for each observability signal.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每种惯例都有其利弊。您可以推送您的度量、日志和跟踪，但也可以从您的进程中拉取所有这些。我们还可以使用混合方法，每种可观测信号使用不同的方法。
- en: Push versus pull method is sometimes a controversial topic. The industry is
    polarized as to what is generally better, not only in observability but also for
    any other architectures. We will discuss the pros and cons in [“Metrics”](#ch-obs-metrics),
    but the difficult truth is that both ways can scale equally well, just with different
    solutions, tools, and best practices.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'After learning about those three categories, we should be ready to dive further
    into observability signals. To measure and deliver observability information for
    efficiency optimizations, we can’t avoid learning more about instrumenting the
    three common observability signals: logging, tracing, and metrics. In the next
    section, let’s do that while keeping a practical goal in mind—measuring latency.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Instrumenting for Latency'
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All three signals you will learn in this section can be used to build observability
    that will fit in any of the three categorizations we discussed. Each signal can:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Be manually or autoinstrumented
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Give aggregated information or raw events
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be pulled (collected, tailed, or scraped) from the process or pushed (uploaded)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yet every signal—logging, tracing, or metric—might be better or worse fitted
    in any of those jobs. In this section, we will discuss these predispositions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: The best way to learn how to use observability signals and their trade-offs
    is to focus on the practical goal. Let’s imagine we want to measure the latency
    of a specific operation in our code. As mentioned in the introduction, we need
    to start measuring the latency to assess it and decide if our code needs more
    optimizations during every optimization iteration. As you will learn in this section,
    we can get latency results using any of those observability signals. The details
    around how information is presented, how complex instrumentation is, and so on
    will help you understand what to choose in your journey. Let’s dive in!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logging might be the clearest signal to understand an instrument. So let’s explore
    the most basic instrumentation that we might categorize as logging to collect
    latency measurements. Taking basic latency measurements for a single operation
    in Go code is straightforward, thanks to the standard [`time` package](https://oreil.ly/t9FDr).
    Whether you do it by hand or use standard or third-party libraries to obtain latencies,
    if they are written in Go, they use the pattern presented in [Example 6-1](#code-latency-simplest)
    using the `time` package.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-1\. Manual and simplest latency measurement of a single operation
    in Go
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_efficiency_observability_CO1-1)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '`time.Now()` captures the current wall time (clock time) from our operating
    system clock in the form `time.Time`. Note the `xTime`, example variable that
    specifies the desired number of runs.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_efficiency_observability_CO1-2)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: After our `cooperation` functions finish, we can capture the time between `start`
    and current time using `time.Since(start)`, which returns the handy `time.Duration`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`协作`函数完成后，我们可以使用`time.Since(start)`捕获从`start`到当前时间的时间，该方法返回方便的`time.Duration`。
- en: '[![3](assets/3.png)](#co_efficiency_observability_CO1-3)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_efficiency_observability_CO1-3)'
- en: We can leverage such an instrument to deliver our metric sample. For example,
    we can print the duration in nanoseconds to the standard output using the `.Nanoseconds()`
    method.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用这样的工具来传递我们的度量样本。例如，我们可以使用`.Nanoseconds()`方法将持续时间以纳秒打印到标准输出。
- en: Arguably, [Example 6-1](#code-latency-simplest) represents the simplest form
    of instrumentation and observability. We take a latency measurement and deliver
    it by printing the result into standard output. Given that every operation will
    output a new line, [Example 6-1](#code-latency-simplest) represents manual instrumentation
    of raw event information.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，[示例 6-1](#code-latency-simplest)代表了最简单的仪器化和可观测性形式。我们进行延迟测量，并通过将结果打印到标准输出来传递它。鉴于每次操作都将输出一行新数据，[示例 6-1](#code-latency-simplest)代表了原始事件信息的手动仪器化。
- en: Unfortunately, this is a little naive. First of all, as we will learn in [“Reliability
    of Experiments”](ch07.html#ch-obs-rel), a single measurement of anything can be
    misleading. We have to capture more of those—ideally hundreds or thousands for
    statistical purposes. When we have one process, and only one functionality we
    want to test or benchmark, [Example 6-1](#code-latency-simplest) will print hundreds
    of results that we can later analyze. However, to simplify the analysis, we could
    try to pre-aggregate some results. Instead of logging raw events, we could pre-aggregate
    using a mathematical average function and output that. [Example 6-2](#code-latency-simplest-aggr)
    presents a modification of [Example 6-1](#code-latency-simplest) that aggregates
    events into an easier-to-consume result.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这有点幼稚。首先，正如我们将在[“实验的可靠性”](ch07.html#ch-obs-rel)中学到的那样，任何单一的测量都可能具有误导性。我们必须捕获更多这样的测量结果，理想情况下是数百或数千次，以供统计目的使用。当我们有一个进程，只有一个我们想要测试或基准测试的功能时，[示例 6-1](#code-latency-simplest)将打印出我们稍后可以分析的数百个结果。然而，为了简化分析，我们可以尝试预先聚合一些结果。与其记录原始事件，我们可以使用数学平均函数进行预先聚合并输出结果。[示例 6-2](#code-latency-simplest-aggr)展示了将事件聚合成更易于消费结果的修改版本。
- en: Example 6-2\. Instrumenting Go to log the average latency of an operation in
    Go
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-2\. 仪器化Go，记录Go操作的平均延迟
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_efficiency_observability_CO2-1)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_efficiency_observability_CO2-1)'
- en: Instead of printing raw latency, we can gather a sum and number of operations
    in the sum.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是打印原始延迟，我们可以收集总和和操作数的数量。
- en: '[![2](assets/2.png)](#co_efficiency_observability_CO2-2)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_efficiency_observability_CO2-2)'
- en: Those two pieces of information can be used to calculate the accurate average
    and present that for a group of events instead of the unique latency. For example,
    one run printed the `188324467 ns/op` string on my machine.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个信息片段可用于计算组事件的准确平均值，并呈现给用户，而不是唯一的延迟。例如，一次运行在我的机器上打印了`188324467 ns/op`字符串。
- en: Given that we stop presenting latency for raw events, [Example 6-2](#code-latency-simplest-aggr)
    represents a manual, aggregated information observability. This method allows
    us to quickly get the information we need without complex (and time-consuming)
    tools analyzing our logging outputs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们停止了对原始事件的延迟呈现，[示例 6-2](#code-latency-simplest-aggr) 表示了一种手动的、聚合的信息可观测性。这种方法允许我们快速获取所需的信息，而无需复杂（且耗时的）工具分析我们的日志输出。
- en: This example is how the Go benchmarking tool will do the average latency calculations.
    We can achieve exactly the same logic as in [Example 6-2](#code-latency-simplest-aggr)
    using the snippet in [Example 6-3](#code-latency-go-bench) in a file with the
    *_test.go* suffix.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例展示了Go基准测试工具如何进行平均延迟计算。我们可以使用带有*_test.go*后缀的文件中的[示例 6-3](#code-latency-go-bench)片段实现与[示例 6-2](#code-latency-simplest-aggr)完全相同的逻辑。
- en: Example 6-3\. Simplest Go benchmark that will measure average latency per operation
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-3\. 最简单的Go基准测试，将测量每次操作的平均延迟
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_efficiency_observability_CO3-1)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_efficiency_observability_CO3-1)'
- en: The `for` loop with the `N` variable is essential in the benchmarking framework.
    It allows the Go framework to try different `N` values to perform enough test
    runs to fulfill the configured number of runs or test duration. For example, by
    default, the Go benchmark runs to fit one second, which is often too short for
    meaningful output reliability.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在基准测试框架中，带有变量 `N` 的 `for` 循环是必不可少的。它允许 Go 框架尝试不同的 `N` 值来执行足够的测试运行，以满足配置的运行次数或测试持续时间。例如，默认情况下，Go
    基准测试运行时间为一秒，这通常对于可靠的输出来说太短了。
- en: Once we run [Example 6-3](#code-latency-go-bench) using `go test` (explained
    in detail in [“Go Benchmarks”](ch08.html#ch-obs-micro-go)), it will print certain
    output. One part of the information is a result line with a number of runs and
    average nanoseconds per operation. One of the runs on my machine gave an output
    latency of `197999371 ns/op`, which generally matches the result from [Example 6-2](#code-latency-simplest-aggr).
    We can say that the Go benchmark is an autoinstrumentation with aggregated information
    using logging signals for things like latency.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用 `go test` 运行 [示例 6-3](#code-latency-go-bench) 时（详细解释见 [“Go 基准测试”](ch08.html#ch-obs-micro-go)），它会输出特定的信息。信息的一部分是一个包含运行次数和每次操作的平均纳秒数的结果行。在我的机器上，其中一个运行的输出延迟为
    `197999371 ns/op`，这通常与 [示例 6-2](#code-latency-simplest-aggr) 的结果相匹配。我们可以说，Go 基准测试是使用日志记录信号进行自动仪器化，用于诸如延迟等的聚合信息。
- en: On top of collecting latency about the whole operation, we can gain a lot of
    insight from having different granularity of those measurements. For example,
    we might wish to capture the latency of a few suboperations inside our single
    operation. Finally, for more complex deployments, when our Go program is part
    of a distributed system, as discussed in [“Macrobenchmarks”](ch08.html#ch-obs-macro),
    we have potentially many processes we have to measure across. For those cases,
    we have to use more sophisticated logging that will give us more metadata and
    ways to deliver a logging signal, not only by simply printing to a file, but by
    other means too.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 除了收集整个操作的延迟信息之外，我们还可以从这些测量的不同粒度中获得许多见解。例如，我们可能希望捕获单个操作内几个子操作的延迟。最后，在更复杂的部署中，当我们的
    Go 程序是分布式系统的一部分时，正如 [“宏基准测试”](ch08.html#ch-obs-macro) 中讨论的那样，我们可能有许多进程需要跨系统进行测量。对于这些情况，我们必须使用更复杂的日志记录器，它们会为我们提供更多的元数据和传递日志信号的方法，不仅仅是简单地打印到文件，还可以通过其他方式实现。
- en: 'The amount of information we have to attach to our logging signal results in
    the pattern called a logger in Go (and other programming languages). A logger
    is a structure that allows us to manually instrument our Go application with logs
    in the easiest and most readable way. A logger hides complexities like:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须附加到日志信号的信息量会导致一种称为 Go 中的日志记录器模式的模式。日志记录器是一种结构，允许我们以最简单和最可读的方式手动为我们的 Go 应用程序加入日志。日志记录器隐藏了复杂性，比如：
- en: Formatting of the log lines.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志行的格式化。
- en: Deciding if we should log or not based on the logging level (e.g., debug, warning,
    error, or more).
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据日志级别（例如调试、警告、错误或更多）决定是否记录。
- en: Delivering the log line to a configured place, such as the output file. Optionally,
    more complex, push-based logging delivery is possible to remote backends, which
    must support back-off retries, authorization, service discovery, etc.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将日志行传递到配置的位置，例如输出文件。还可以选择更复杂的基于推送的日志传递方式，可以将日志传送到远程后端，必须支持退避重试、授权、服务发现等功能。
- en: Adding context-based metadata and timestamps.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加基于上下文的元数据和时间戳。
- en: The Go standard library is very rich with many useful utilities, including logging.
    For example, the [`log` package](https://oreil.ly/JEUjT) contains a simple logger.
    It can work well for many applications, but it is prone to some usage pitfalls.^([5](ch06.html#idm45606832254464))
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Go 标准库非常丰富，包含许多有用的工具，包括日志记录。例如，[`log` 包](https://oreil.ly/JEUjT)包含一个简单的日志记录器。它对许多应用程序都能很好地工作，但也容易出现一些使用上的陷阱。^([5](ch06.html#idm45606832254464))
- en: Be Mindful While Using the Go Standard Library Logger
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在使用 Go 标准库的日志记录器时请谨慎。
- en: 'There are a few things to remember if you want to use the standard Go logger
    from the `log` package:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用 `log` 包中的标准 Go 日志记录器，有几件事需要记住：
- en: Don’t use the global `log.Default()` logger, so `log.Print` functions, and so
    on. Sooner or later, it will bite you.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要使用全局的 `log.Default()` 日志记录器，也不要使用 `log.Print` 等函数。迟早会让你后悔。
- en: Never store or consume `*log.Logger` directly in your functions and structures,
    especially when you write a library.^([6](ch06.html#idm45606832248016)) If you
    do, users will be forced to use a very limited `log` logger instead of their own
    logging libraries. Use a custom interface instead (e.g., [go-kit logger](https://oreil.ly/tCs2g)),
    so users can adapt their loggers to what you use in your code.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 永远不要直接在你的函数和结构中存储或使用`*log.Logger`，特别是当你编写一个库时。^([6](ch06.html#idm45606832248016))
    如果你这样做，用户将被迫使用非常有限的`log`日志记录器，而不是他们自己的日志记录库。相反，使用自定义接口（例如[go-kit日志记录器](https://oreil.ly/tCs2g)），这样用户可以将他们的日志记录器适配到你在代码中使用的日志记录器。
- en: Never use the `Fatal` method outside the main function. It panics, which should
    not be your default error handling.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 永远不要在主函数之外使用`Fatal`方法。它会引发panic，这不应该是你的默认错误处理方式。
- en: To not accidentally get hit by these pitfalls, in the projects I worked on,
    we decided to use the third-party popular [go-kit](https://oreil.ly/ziBdb)^([7](ch06.html#idm45606832242864))
    logger. An additional advantage of the go-kit logger is that it is easy to maintain
    some structure. Structure logic is essential to have reliable parsers for automatic
    log analysis with logging backends like [OpenSearch](https://oreil.ly/RohpZ) or
    [Loki](https://oreil.ly/Fw9I3). To measure latency, let’s go through an example
    of logger usage in [Example 6-4](#code-latency-log). Its output is shown in [Example 6-5](#code-latency-log-result).
    We use the [`go-kit` module](https://oreil.ly/vOafG), but other libraries follow
    similar patterns.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免意外陷入这些陷阱，在我参与的项目中，我们决定使用第三方流行的[go-kit](https://oreil.ly/ziBdb)^([7](ch06.html#idm45606832242864))日志记录器。go-kit日志记录器的额外优势是很容易保持一定的结构。结构逻辑对于拥有可靠解析器的自动日志分析与像[OpenSearch](https://oreil.ly/RohpZ)或[Loki](https://oreil.ly/Fw9I3)这样的日志后端非常重要。为了衡量延迟，让我们通过一个日志记录器使用示例来演示，在[示例 6-4](#code-latency-log)中展示了它的输出。我们使用了[`go-kit`模块](https://oreil.ly/vOafG)，但其他库遵循类似的模式。
- en: Example 6-4\. Capturing latency though logging using the [`go-kit` logger](https://oreil.ly/9uCWi)
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-4\. 使用[`go-kit`日志记录器](https://oreil.ly/9uCWi)捕获延迟
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_efficiency_observability_CO4-1)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_efficiency_observability_CO4-1)'
- en: We initialize the logger. Libraries usually allow you to output the log lines
    to a file (e.g., standard output or error) or directly push it to some collections
    tool, e.g., to [fluentbit](https://oreil.ly/pUcmX) or [vector](https://oreil.ly/S0aqR).
    Here we choose to output all logs to standard error^([8](ch06.html#idm45606832090240))
    with a timestamp attached to each log line. We also choose to format logs in the
    human-accessible way with `NewLogfmtLogger` (still structured so that it can be
    parsed by software, with space as the delimiter).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初始化了日志记录器。通常，库允许你将日志行输出到文件（例如标准输出或错误），或直接推送到某些集合工具，例如[fluentbit](https://oreil.ly/pUcmX)或[vector](https://oreil.ly/S0aqR)。在这里，我们选择将所有日志输出到标准错误中，并为每行日志附加时间戳。我们还选择以人类可读的方式格式化日志，使用`NewLogfmtLogger`（仍然是结构化的，可以被软件解析，以空格作为分隔符）。
- en: '[![2](assets/2.png)](#co_efficiency_observability_CO4-2)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_efficiency_observability_CO4-2)'
- en: In [Example 6-1](#code-latency-simplest), we simply printed the latency number.
    Here we add certain metadata to it to use that information more easily across
    processes and different operations happening in the system. Notice that we maintain
    a certain structure. We pass an even number of arguments representing key values.
    This allows our log line to be structured for easier use by automation. Additionally,
    we choose `level.Info`, meaning this log line will be not printed if we choose
    levels like errors only.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 6-1](#code-latency-simplest)中，我们只是简单地打印了延迟数字。在这里，我们为其添加了某些元数据，以便更轻松地在系统中的不同进程和操作中使用该信息。请注意，我们保持了一定的结构。我们传递了偶数个参数，代表键值对。这使得我们的日志行结构化，更易于自动化使用。此外，我们选择了`level.Info`，这意味着如果我们选择了仅错误级别，这行日志将不会被打印出来。
- en: Example 6-5\. Example output logs generated by [Example 6-4](#code-latency-log)
    (wrapped for readability)
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-5\. 通过[示例 6-4](#code-latency-log)生成的示例输出日志（为了可读性进行了换行）
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_efficiency_observability_CO5-1)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_efficiency_observability_CO5-1)'
- en: Thanks to the log structure, it’s both readable to us and automation can clearly
    distinguish among different fields like `msg`, `elapsed`, `info`, etc. without
    expensive and error-prone fuzzy parsing.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了日志结构，这对我们来说很容易阅读，而且自动化可以清楚地区分诸如`msg`、`elapsed`、`info`等不同字段，无需昂贵且易出错的模糊解析。
- en: Logging with a logger might still be the simplest way to deliver our latency
    information manually to us. We can tail the file (or use `docker log` if our Go
    process was running in Docker, or `kubectl logs` if we deployed it on Kubernetes)
    to read those log lines for further analysis. It is also possible to set up an
    automation that tails those from files or pushes them directly to the collector,
    adding further information. Collectors can be then configured to push those log
    lines into free and open source logging backends like [OpenSearch](https://oreil.ly/RohpZ),
    [Loki](https://oreil.ly/Fw9I3), [Elasticsearch](https://oreil.ly/EUlts), or many
    of the paid vendors. As a result, you can keep log lines from many processes in
    a single place, search, visualize, analyze them, or build further automation to
    handle them as you want.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Is logging a good fit for our efficiency observability? Yes and no. For microbenchmarks
    explained in [“Microbenchmarks”](ch08.html#ch-obs-micro), logging is our primary
    tool of measurements because of its simplicity. On the other hand, on a macro
    level, like [“Macrobenchmarks”](ch08.html#ch-obs-macro), we tend to use logging
    for a raw event type of observability, which on such a scale gets very complex
    and expensive to analyze and keep reliable. Still, because logging is so common,
    we can find efficiency bottlenecks in a bigger system with logging.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Logging tools are also constantly evolving. For example, many tools allow us
    to derive metrics from log lines, like Grafana Loki’s [Metric queries inside LogQL](https://oreil.ly/fdoNm).
    In practice, however, simplicity has its cost. One of the problems stems from
    the fact that sometimes logs are used directly by humans, and sometimes by automation
    (e.g., deriving metrics or reacting to situations found in logs). As a result,
    logs are often unstructured. Even with amazing loggers like go-kit in [Example 6-4](#code-latency-log),
    logs are inconsistently structured, making it very hard and expensive to parse
    for automation. For example, things like inconsistent units (as in [Example 6-5](#code-latency-log-result)
    for latency measurements), which are great for humans, become almost impossible
    to derive the value as a metric. Solutions like [Google mtail](https://oreil.ly/Q4wAC)
    try to approach this with custom parsing language. Still, the complexity and ever-changing
    logging structure make it hard to use this signal to measure our code’s efficiency.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the next observability signal—tracing—to learn in which areas
    it can help us with our efficiency goals.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the lack of consistent structure in logging, tracing signals emerged to
    tackle some of the logging problems. In contrast to logging, tracing is a piece
    of structured information about your system. The structure is built around the
    transaction, for example, requests-response architecture. This means that things
    like status codes, the result of the operation, and the latency of operations
    are natively encoded, thus easier to use by automation and tools. As a trade-off,
    you need an additional mechanism (e.g., a user interface) to expose this information
    to humans in a readable way.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏一致结构的日志记录导致了追踪信号的出现，以解决部分日志问题。与日志记录相比，追踪是关于系统的结构化信息片段。该结构围绕事务建立，例如请求-响应架构。这意味着诸如状态码、操作结果和操作延迟等内容本地编码，因此更易于被自动化工具使用。作为一种权衡，你需要额外的机制（例如用户界面）以可读的方式向人类公开这些信息。
- en: On top of that, operations, suboperations, and even cross-process calls (e.g.,
    RPCs) can be linked together, thanks to context propagation mechanisms working
    well with standard network protocols like HTTP. This feels like a perfect choice
    for measuring latency for our efficiency needs, right? Let’s find out.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，操作、子操作甚至跨进程调用（例如RPC）都可以通过上下文传播机制链接在一起，这些机制与标准的网络协议（如HTTP）良好配合。这感觉就像是我们效率需求中测量延迟的完美选择，对吧？让我们找出答案。
- en: As with logging, there are many different manual instrumentation libraries you
    can choose from. Popular, open source choices for Go are the [OpenTracing](https://oreil.ly/gJeAV)
    library (currently deprecated but still viable), [OpenTelemetry](https://oreil.ly/uxKoW),
    or clients from the dedicated tracing vendor. Unfortunately, at the moment of
    writing, the OpenTelemetry library has a too-complex API to explain in this book,
    plus it’s still changing, so I started a [small project called tracing-go](https://oreil.ly/rs6fQ)
    that encapsulates the OpenTelemetry client SDK into minimal tracing instrumentation.
    While tracing-go is my interpretation of the minimal set of tracing functionalities
    to use, it should teach you the basics of context propagation and span logic.
    Let’s explore an example manual instrumentation using tracing-go to measure dummy
    `doOperation` function latency (and more!) using tracing in [Example 6-6](#code-latency-trace).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '与日志记录一样，你可以选择许多不同的手动仪表化库。对于Go语言，流行的开源选择包括[OpenTracing](https://oreil.ly/gJeAV)库（目前已弃用但仍然可用）、[OpenTelemetry](https://oreil.ly/uxKoW)，或来自专用追踪供应商的客户端。不幸的是，在撰写本书时，OpenTelemetry库具有过于复杂的API，难以在本书中进行解释，并且它仍在变化，因此我开始了一个名为[tracing-go](https://oreil.ly/rs6fQ)的小项目，将OpenTelemetry客户端SDK封装成最小的追踪仪器。虽然tracing-go是我对最小追踪功能集的解释，但它应该教会你上下文传播和span逻辑的基础知识。让我们探索一个使用tracing-go进行手动仪表化的示例，以测量在[示例6-6](#code-latency-trace)中使用追踪测量虚拟`doOperation`函数延迟（及更多！）。  '
- en: Example 6-6\. Capturing latencies of the operation and potential suboperations
    using [tracing-go](https://oreil.ly/1027d)
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-6。使用[tracing-go](https://oreil.ly/1027d)捕获操作和潜在子操作的延迟
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_efficiency_observability_CO6-1)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_efficiency_observability_CO6-1)'
- en: As with everything, we have to initialize our library. In our example, usually,
    it means creating an instance of `Tracer` that is capable of sending the spans
    that will form traces. We push spans to some collector and eventually to the tracing
    backend. This is why we have to specify some address to send to. In this example,
    you could specify a gRPC `host:port` address of the collector (e.g., [OpenTelemetry
    Collector](https://oreil.ly/z0Pjt)) endpoint that supports the [gRPC OTLP trace
    protocol](https://oreil.ly/4IaBd).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 和其他一切一样，我们必须初始化我们的库。在我们的例子中，通常意味着创建一个能够发送形成跟踪的span的`Tracer`实例。我们将span推送到某个收集器，最终传输到追踪后端。这就是为什么我们必须指定某个地址以发送到的原因。在这个例子中，你可以指定收集器的gRPC
    `host:port`地址（例如，支持[gRPC OTLP追踪协议](https://oreil.ly/4IaBd)的[OpenTelemetry收集器](https://oreil.ly/z0Pjt)端点）。
- en: '[![2](assets/2.png)](#co_efficiency_observability_CO6-2)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_efficiency_observability_CO6-2)'
- en: With the tracer, we can create an initial root `span`. The root means the span
    that spans the whole transaction. A `traceID` is created during creation, identifying
    all spans in the trace. Span represents individual work done. For example, we
    can add a different name or even baggage items like logs or events. We also get
    a `context.Context` instance as part of creation. This Go native context interface
    can be used to create subspans if our `doOperation` function will do any subwork
    pieces worth instrumenting.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用跟踪器，我们可以创建一个初始根 `span`。根表示跨越整个事务的 span。在创建过程中会创建一个`traceID`，用于标识跟踪中的所有 span。每个
    span 代表一个个体工作单元。例如，我们可以添加不同的名称，甚至像日志或事件这样的 baggage 项。创建过程还会得到一个 `context.Context`
    实例。作为创建的一部分，此 Go 原生上下文接口可用于创建子 span，如果我们的 `doOperation` 函数将执行任何值得检测的子工作单元。
- en: '[![3](assets/3.png)](#co_efficiency_observability_CO6-3)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_efficiency_observability_CO6-3)'
- en: In the manual instrumentation, we have to tell the tracing provider when the
    work was done and with what result. In the `tracing-go` library, we can use `end.Stop(<error
    or nil>)` for that. Once you stop the span, it will record the span’s latency
    from its start, the potential error, and mark itself as ready to be sent asynchronously
    by `Tracer`. Tracer exporter implementations usually won’t send spans straightaway
    but buffer them for batch pushes. `Tracer` will also check if a trace containing
    some spans can be sent to the endpoint based on the chosen sampling strategy (more
    on that later).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在手动工具中，我们必须告诉跟踪提供程序工作何时完成以及结果如何。在 `tracing-go` 库中，我们可以使用 `end.Stop(<error 或
    nil>)`。一旦停止 span，它将记录从开始到结束的延迟，潜在的错误，并标记自身为可以异步发送给 `Tracer`。跟踪导出器实现通常不会立即发送 span，而是将它们缓冲以进行批量推送。`Tracer`
    还会检查基于所选采样策略的端点是否可以发送包含某些 span 的跟踪（稍后会详细介绍）。
- en: '[![4](assets/4.png)](#co_efficiency_observability_CO6-4)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_efficiency_observability_CO6-4)'
- en: Once you have context with the injected span creator, we can add subspans to
    it. It’s useful when you want to debug different parts and sequences involved
    in doing one piece of work.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 有了注入的 span 创建器上下文之后，我们可以向其添加子 span。当您希望调试涉及执行一项工作的不同部分和顺序时，这将非常有用。
- en: One of the most valuable parts of tracing is context propagation. This is what
    separates distributed tracing from nondistributed signals. I did not reflect this
    in our examples, but imagine if our operation makes a network call to other microservices.
    Distributed tracing allows passing various tracing information like `traceID`,
    or sampling via a propagation API (e.g., certain encoding using HTTP headers).
    See a [related blog post](https://oreil.ly/Qz6lF) about context propagation. For
    that to work in Go, you have to add a special middleware or HTTP client with propagation
    support, e.g., [OpenTelemetry HTTP transport](https://oreil.ly/Rvq6i).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪中最有价值的部分之一是上下文传播。这就是分布式跟踪与非分布式信号的区别所在。我们在示例中没有反映这一点，但想象一下，如果我们的操作向其他微服务发出网络调用。分布式跟踪允许通过传播
    API（例如，使用 HTTP 标头的某种编码）传递各种跟踪信息，如`traceID`或采样。参见关于上下文传播的[相关博客文章](https://oreil.ly/Qz6lF)。为了在
    Go 中实现这一点，您必须添加一个具有传播支持的特殊中间件或 HTTP 客户端，例如[OpenTelemetry HTTP 传输](https://oreil.ly/Rvq6i)。
- en: Because of the complex structure, raw traces and spans are not readable by humans.
    This is why many projects and vendors help users by providing solutions to use
    tracing effectively. Open source solutions like [Grafana Tempo with Grafana UI](https://oreil.ly/CQ1Aq)
    and [Jaeger](https://oreil.ly/enkG9) exist, which offer nice user interfaces and
    trace collection so you can observe your traces. Let’s look at how our spans from
    [Example 6-6](#code-latency-trace) look in the latter project. [Figure 6-2](#img-obs-tracing)
    shows a multitrace search view, and [Figure 6-3](#img-obs-spans) shows what our
    individual `doOperation` trace looks like.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其复杂结构，原始跟踪和 span 对人类来说不可读。这就是为什么许多项目和供应商通过提供有效使用跟踪的解决方案来帮助用户。存在像 [Grafana
    Tempo with Grafana UI](https://oreil.ly/CQ1Aq) 和 [Jaeger](https://oreil.ly/enkG9)
    这样的开源解决方案，它们提供良好的用户界面和跟踪收集，以便您观察自己的跟踪。让我们看看我们的跟踪从 [Example 6-6](#code-latency-trace)
    在后者项目中的展示。图 6-2 显示了多跟踪搜索视图，图 6-3 显示了我们的单个 `doOperation` 跟踪的外观。
- en: '![efgo 0602](assets/efgo_0602.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![efgo 0602](assets/efgo_0602.png)'
- en: Figure 6-2\. View of one hundred operations presented as one hundred traces
    with their latency results
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2。将一百个操作显示为一百个跟踪，并显示它们的延迟结果
- en: '![efgo 0603](assets/efgo_0603.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![efgo 0603](assets/efgo_0603.png)'
- en: Figure 6-3\. Click one trace to inspect all of its spans and associated data
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3。单击一个跟踪以查看其所有 span 和关联数据
- en: Tools and user interfaces can vary, but generally they follow the same semantics
    I explain in this section. The view in [Figure 6-2](#img-obs-tracing) allows us
    to search through traces based on their timestamp, durations, service involved,
    etc. The current search matches our one hundred operations, which are then listed
    on the screen. A convenient, interactive graph of its latencies is placed, so
    we can navigate to the operation we want. Once clicked, the view in [Figure 6-3](#img-obs-spans)
    is presented. In this view, we can see a distribution of spans for this operation.
    If the operation spans multiple processes and we used network context propagation,
    all linked spans will be listed here. For example, from [Figure 6-3](#img-obs-spans)
    we can immediately tell that the first operation was responsible for most of the
    latency, and the last operation introduced the error.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'All the benefits of tracing make it an excellent tool for learning the system
    interactions, debugging, or finding fundamental efficiency bottlenecks. It can
    also be used for ad hoc verification of system latency measurements (e.g., in
    our TFBO flow to assess latency). But unfortunately, there are a few downsides
    of tracing that you have to be aware of when planning to use it in practice for
    efficiency or other needs:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Readability and maintainability
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of tracing is that you can put a huge amount of useful context
    into your code. In extreme cases, you could potentially be able to rewrite the
    whole program or even system just by looking at all traces and their emitted spans.
    But there is a catch. All this manual instrumentation requires code lines. More
    code lines connected to our existing code increases the complexity of our code,
    which in turn decreases readability. We also need to ensure that our instrumentation
    stays updated with ever-changing code.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the tracing industry tends to prefer autoinstrumentation, which
    in theory can add, maintain, and hide such instrumentation automatically. Proxies
    like Envoy (especially with service mesh technologies) are great examples of successful
    (yet simpler) autoinstrumentation tools for tracing that record the inter-process
    HTTP calls. But unfortunately, more involved auto-instrumentation is not so easy.
    The main problem is that the automation has to hook on to some generic path like
    common database or library operations, HTTP requests, or syscalls (e.g., through
    eBPF probes in Linux). Moreover, it is often hard for those tools to understand
    what more you would like to capture in your application (e.g., the ID of the client
    in a specific code variable). On top of that, tools like eBPF are pretty unstable
    and dependent on the kernel version.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Hiding Instrumentation Under Abstractions
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a middle ground between manual and fully autonomous instrumentation.
    We can manually instrument only a few common Go functions and libraries, so all
    code that uses them will be traced consistently implicitly (automatically!).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: For example, we could add a trace for every HTTP or gRPC request to our process.
    There are already [HTTP middlewares](https://oreil.ly/wZ559) and [gRPC interceptors](https://oreil.ly/7gXVF)
    for that purpose.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以为每个HTTP或gRPC请求向我们的进程添加一个追踪。对于这一目的，已经存在[HTTP 中间件](https://oreil.ly/wZ559)和[gRPC
    拦截器](https://oreil.ly/7gXVF)。
- en: Cost and reliability
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 成本和可靠性
- en: Traces by design fall into the raw event category of observability. This means
    that tracing is typically more expensive than pre-aggregated equivalents. The
    reason is the sheer amount of data we send using tracing. Even if we are very
    moderate with this instrumentation for a single operation, we ideally have dozens
    of tracing spans. These days, systems have to sustain many QPS (queries per second).
    In our example, even for 100 QPS, we would generate over 1,000 spans. Each span
    must be delivered to some backend to be used effectively, with replication on
    both the ingestion and storage sides. Then you need a lot of computation power
    to analyze this data to find, for example, average latency across traces or spans.
    This can easily surpass your price for running the systems without observability!
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 由设计追踪事件落入可观察性的原始事件类别。这意味着追踪通常比预先聚合的等效方式更昂贵。原因是我们使用追踪发送的大量数据。即使我们对单个操作非常适度地进行仪表化，我们理想情况下也应该有数十个追踪跨度。如今，系统必须支持许多查询每秒（QPS）。在我们的示例中，即使是100
    QPS，我们也会生成超过1,000个跨度。每个跨度必须被传送到某个后端以有效使用，并在摄取和存储的两端进行复制。然后，您需要大量的计算能力来分析这些数据，以查找例如跨度或追踪之间的平均延迟。这很容易超过您在没有可观察性的情况下运行系统的价格！
- en: The industry is aware of this, and this is why we have tracing sampling, so
    some decision-making configuration or code decides what data to pass forward and
    what to ignore. For example, you might want to only collect traces for failed
    operations or operations that took more than 120 seconds.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 该行业对此有所了解，这就是为什么我们有追踪采样，因此某些决策配置或代码决定传递哪些数据以及忽略哪些数据。例如，您可能希望仅收集失败的操作或操作所花费超过120秒的追踪数据。
- en: Unfortunately, sampling comes with its downsides. For example, it’s challenging
    to perform tail sampling.^([9](ch06.html#idm45606831687072)) Last but not least,
    sampling makes us miss some data (similar to profiling). In our latency example,
    this might mean that the latency we measure represents only part of all operations
    that happened. Sometimes it might be enough, but it’s easy to [get wrong conclusions
    with sampling](https://oreil.ly/R4gtX), which might lead to wrong optimization
    decisions.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，采样也有其缺点。例如，执行尾部采样是具有挑战性的。^([9](ch06.html#idm45606831687072)) 最后但并非最不重要的是，采样使我们错过了一些数据（类似于分析剖析）。在我们的延迟示例中，这可能意味着我们测量的延迟仅代表发生的所有操作的一部分。有时可能足够，但很容易通过采样得出错误的结论，这可能导致错误的优化决策。
- en: Short duration
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 短暂的持续时间
- en: We will discuss this in detail in [“Latency”](#ch-obs-latency), but tracing
    won’t tell us much when we try to improve very fast functions that last only a
    few milliseconds or less. Similar to the `time` package, the span itself introduces
    some latency. On top of that, adding span for many small operations can add a
    huge cost to the overall ingestion, storage, and querying of traces.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[“延迟”](#ch-obs-latency)中详细讨论这一点，但在尝试优化持续时间仅为几毫秒或更短的非常快速函数时，追踪将不会提供太多信息。与`time`包类似，跨度本身会引入一些延迟。此外，为许多小操作添加跨度可能会大大增加整体摄取、存储和追踪查询的成本。
- en: This is especially visible in streamed algorithms like chunked encodings, compressions,
    or iterators. If we perform partial operations, we are still often interested
    in the latency of the sum of all iterations for certain logic. We can’t use tracing
    for that, as we would need to create tiny spans for every iteration. For those
    algorithms, [“Profiling in Go”](ch09.html#ch-obs-profiling) yields the best observability.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这在流式算法中尤为明显，例如分块编码、压缩或迭代器。如果我们执行部分操作，通常我们仍然对某些逻辑所有迭代的延迟感兴趣。我们无法使用追踪来做到这一点，因为我们需要为每次迭代创建微小的跨度。对于这些算法，《Go
    语言中的分析剖析》（“Profiling in Go”）提供了最佳的可观察性。
- en: Despite some downsides, tracing becomes very powerful and even replaces the
    logging signal in many cases. Vendors and projects add more features, for example,
    [Tempo project’s metric generator](https://oreil.ly/SSLye) that allows recording
    metrics from traces (e.g., average or tail latency for our efficiency needs).
    Undoubtedly, tracing would not grow so quickly without the push from the [OpenTelemetry](https://oreil.ly/sPiw9)
    community. Amazing things will come from this community if you are into tracing.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在一些缺点，跟踪在许多情况下变得非常强大，甚至取代日志记录信号。供应商和项目增加了更多功能，例如，[Tempo项目的指标生成器](https://oreil.ly/SSLye)，允许从跟踪记录指标（例如，适合我们效率需求的平均或尾部延迟）。毫无疑问，如果您对跟踪感兴趣，[OpenTelemetry](https://oreil.ly/sPiw9)社区将带来令人惊奇的成果。
- en: The downsides of one framework are often strengths of other frameworks that
    choose different trade-offs. For example, many tracing problems come from the
    fact that it naturally represents raw events happening in the system (that might
    trigger other events). Let’s now discuss a signal on the opposite spectrum—designed
    to capture aggregations changing over time.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一个框架的缺点往往是选择不同折衷方案的其他框架的优势。例如，许多跟踪问题源于其自然地表示系统中发生的原始事件（可能触发其他事件）。现在让我们讨论一种处于相反方向的信号——设计用于捕获随时间变化的聚合。
- en: Metrics
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指标
- en: Metrics is the observability signal that was designed to observe aggregated
    information. Such aggregation-oriented metric instrumentations might be the most
    pragmatic way of solving our efficiency goals. Metrics are also what I used the
    most in my day-to-day job as a developer and SRE to observe and debug production
    workloads. In addition, metrics are [the main signal used for monitoring at Google](https://oreil.ly/x6rNZ).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 指标是旨在观察聚合信息的可观察信号。这种面向聚合的指标工具可能是解决我们效率目标的最实用方式。作为开发人员和SRE，在我的日常工作中，观察和调试生产工作负载时，指标是我使用最多的工具。此外，指标是[谷歌用于监控的主要信号](https://oreil.ly/x6rNZ)。
- en: '[Example 6-7](#code-latency-metric) shows pre-aggregated instrumentation that
    can be used to measure latency. This example uses [Prometheus `client_golang`](https://oreil.ly/1r2zw).^([10](ch06.html#idm45606831665920))'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6-7](#code-latency-metric)显示了可用于测量延迟的预聚合仪器。此示例使用[Prometheus `client_golang`](https://oreil.ly/1r2zw)。^([10](ch06.html#idm45606831665920))'
- en: Example 6-7\. Measuring `doOperation` latency using the histogram metric with
    Prometheus `client_golang`
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-7。使用Prometheus `client_golang`和直方图指标测量`doOperation`的延迟
- en: '[PRE6]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_efficiency_observability_CO7-1)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_efficiency_observability_CO7-1)'
- en: Using the Prometheus library always starts with creating a new metric registry.^([11](ch06.html#idm45606831400064))
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Prometheus库始终从创建新的指标注册表开始。^([11](ch06.html#idm45606831400064))
- en: '[![2](assets/2.png)](#co_efficiency_observability_CO7-2)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_efficiency_observability_CO7-2)'
- en: The next step is to populate the registry with the metric definitions you want.
    Prometheus allows a few types of metrics, yet the typical latency measurements
    for efficiency are best done as histograms. So on top of type, help and histogram
    buckets are required. We will talk more about buckets and the choice of histograms
    later.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用您想要的指标定义填充注册表。Prometheus允许几种类型的指标，但是用于效率最佳的典型延迟测量最好是直方图。因此，除了类型之外，还需要帮助和直方图桶。稍后我们将更多地讨论桶和直方图的选择。
- en: '[![3](assets/3.png)](#co_efficiency_observability_CO7-3)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_efficiency_observability_CO7-3)'
- en: As the last parameter, we define the dynamic dimension of this metric. Here
    I propose to measure latency for different types of errors (or no error). This
    is useful as, very often, failures have other timing characteristics.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一个参数，我们定义此指标的动态维度。在这里，我建议测量不同类型的错误（或无错误）的延迟。这非常有用，因为很多时候，故障具有其他时间特性。
- en: '[![4](assets/4.png)](#co_efficiency_observability_CO7-4)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_efficiency_observability_CO7-4)'
- en: We observe the exact latency with a floating number of seconds. We run all operations
    in a simplified goroutine, so we can expose metrics while the functionality is
    performing. The `Observe` method will add such latency into the histogram of buckets.
    Notice that we observe this latency for certain errors. We also don’t take an
    arbitrary error string—we sanitize it to a type using some custom `errorType`
    function. This is important because the controlled number of values in the dimension
    keeps our metric valuable and cheap.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_efficiency_observability_CO7-5)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: The default way to consume those metrics is by allowing other processes (e.g.,
    [Prometheus server](https://oreil.ly/2Sa3P)) to pull the current state of the
    metrics. For example, in this simplified^([12](ch06.html#idm45606831336912)) code
    we serve those metrics from our registry through an HTTP endpoint on the `8080`
    port.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'The Prometheus data model supports four metric types, which are well described
    in the [Prometheus documentation](https://oreil.ly/mamdO): counters, gauges, histograms,
    and summaries. There is a reason why I chose a more complex histogram for observing
    latency instead of a counter or a gauge metric. I explain why in [“Latency”](#ch-obs-latency).
    For now, it’s enough to say that histograms allow us to capture distributions
    of the latencies, which is typically what we need when observing production systems
    for efficiency and reliability. Such metrics, defined and instrumented in [Example 6-7](#code-latency-metric),
    will be represented on an HTTP endpoint, as shown in [Example 6-8](#code-latency-metric-om).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-8\. Sample of the metric output from [Example 6-7](#code-latency-metric)
    when consumed from the [OpenMetrics compatible HTTP endpoint](https://oreil.ly/aZ6GT)
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](assets/1.png)](#co_efficiency_observability_CO8-1)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Each bucket represents a number (counters) of operations that had latency less
    than or equal to the value specified in `le`. For example, we can immediately
    see that we saw two successful operations from the process start. The first was
    faster than 0.1 seconds; and the second was faster than 1 second, but slower than
    0.1 seconds.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_efficiency_observability_CO8-2)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Every histogram also captures a number of observed operations and summarized
    value (sum of observed latencies, in this case).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in [“Observability”](#ch-obs-observability), every signal can be
    pulled or pushed. However, the Prometheus ecosystem defaults to the pull method
    for metrics. Not the naive pull, though. In the Prometheus ecosystem, we don’t
    pull a backlog of events or samples like we would when pulling (tailing) traces
    of logs from, for example, a file. Instead, applications serve HTTP payload in
    the OpenMetrics format (like in [Example 6-8](#code-latency-metric-om)), which
    is then periodically collected (scraped) by Prometheus servers or Prometheus compatible
    systems (e.g., Grafana Agent or OpenTelemetry collector). With the Prometheus
    data model, we scrape the latest information about the process.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: To use Prometheus with our Go program instrumented in [Example 6-7](#code-latency-metric),
    we have to start the Prometheus server and configure the scrape job that targets
    the Go process server. For example, assuming we have the code in [Example 6-7](#code-latency-metric)
    running, we could use the set of commands shown in [Example 6-9](#code-latency-metric-scrape)
    to start metric collection.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-9\. The simplest set of commands to run Prometheus from the terminal
    to start collecting metrics from [Example 6-7](#code-latency-metric)
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_efficiency_observability_CO9-1)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: For my demo purposes, I can limit the [Prometheus configuration](https://oreil.ly/4cPSa)
    to a single scrape job. One of the first decisions is to specify the scrape interval.
    Typically, it’s around 15–30 seconds for continuous, efficient metric collection.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_efficiency_observability_CO9-2)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: I also provide a target that points to our tiny instrumented Go program in [Example 6-7](#code-latency-metric).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_efficiency_observability_CO9-3)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus is just a single binary written in Go. We install it in [many ways](https://oreil.ly/9CxxD).
    In the simplest configuration, we can point it to a created configuration. When
    started, the UI will be available on the `localhost:9090`.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: With the preceding setup, we can start analyzing the data using Prometheus APIs.
    The simplest way is to use the Prometheus query language (PromQL) documented [here](https://oreil.ly/nY6Yi)
    and [here](https://oreil.ly/jH3nd). With Prometheus server started as in [Example 6-9](#code-latency-metric-scrape),
    we can use the Prometheus UI and query the data we collected.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: For example, [Figure 6-4](#img-obs-metric-buckets) shows the result of the simple
    query fetching the latest latency histogram numbers over time (from the moment
    of the process start) for our `operation_duration_seconds` metric name that represents
    successful operations. This generally matches the format we see in [Example 6-8](#code-latency-metric-om).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![efgo 0604](assets/efgo_0604.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. PromQL query results for simple query for all `operation_duration_​sec⁠onds_bucket`
    metrics graphed in the Prometheus UI
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To obtain the average latency of a single operation, we can use certain mathematical
    operations to divide the rates of `operation_duration_seconds_sum` by `oper⁠ation_duration_seconds_count`.
    We use the `rate` function to ensure accurate results across many processes and
    their restart. `rate` transforms Prometheus counters into a rate per second.^([13](ch06.html#idm45606831225328))
    Then we can use `/` to divide the rates of those metrics. The result of such an
    average query is presented in [Figure 6-5](#img-obs-metric-avg).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![efgo 0605](assets/efgo_0605.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. PromQL query results representing average latency captured by the
    [Example 6-7](#code-latency-metric) instrumentation graphed in the Prometheus
    UI
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With another query, we can check total operations or, even better, check the
    rate per minute of those using the `increase` function on our `operation_duration_​sec⁠onds_count`
    counter, as presented in [Figure 6-6](#img-obs-metric-incr).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![efgo 0606](assets/efgo_0606.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. PromQL query results representing a rate of operations per minute
    in our system graphed in the Prometheus UI
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are many other functions, aggregations, and ways of using metric data
    in the Prometheus ecosystem. We will unpack some of it in later sections.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'The amazing part about Prometheus with such a specific scrape technique is
    that pulling metrics allows our Go client to be ultrathin and efficient. As a
    result, the Go process does not need to:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Buffer data samples, spans, or logs in memory or on disk
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain information (and automatically update it!) on where to send potential
    data
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement complex buffering and persisting logic if the metric backend is down
    temporarily
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure a consistent sample push interval
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Know about any authentication, authorization, or TLS for metric payload
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On top of that, the observability experience is better when you pull the data
    in such a way that:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Metric users can easily control the scrape interval, targets, metadata, and
    recordings from a central place. This makes the metric usage simpler, more pragmatic,
    and generally cheaper.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is easier to predict the load of such a system, which makes it easier to
    scale it and react to the situations that require scaling the collection pipeline.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last but not least, pulling metrics allows you to reliably tell your application’s
    health (if we can’t scrape metrics from it, it is most likely unhealthy or down).
    We also typically know what sample is the last one for a metric (staleness).^([14](ch06.html#idm45606831205328))
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As with everything, there are some trade-offs. Each pulled, tailed, or scraped
    signal has its downsides. Typical problems of an observability pull-based system
    include:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: It is generally harder to pull data from short-lived processes (e.g., CLI and
    batch jobs).^([15](ch06.html#idm45606831202864))
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not every system architecture allows ingress traffic.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is generally harder to ensure that all the pieces of information will land
    safely in a remote place (e.g., this pulling is not suitable for auditing).
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Prometheus metrics are designed to mitigate downsides and leverage the strength
    of the pull model. Most of the metrics we use are counters, which means they only
    increase. This allows Prometheus to skip a few scrapes from the process but still,
    in the end, have a perfectly accurate number for each metric within larger time
    windows, like minutes.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, in the end, metrics (as numeric values) are what we need
    when it comes to assessing efficiency. It’s all about comparing and analyzing
    numbers. This is why a metric observability signal is a great way to gather required
    information pragmatically. We will use this signal extensively for [“Macrobenchmarks”](ch08.html#ch-obs-macro)
    and [“Root Cause Analysis, but for Efficiency”](ch09.html#ch-obs-cause). It’s
    simple, pragmatic, the ecosystem is huge (you can find metric exporters for almost
    all kinds of software and hardware), it’s generally cheap, and it works great
    with both human users and automation (e.g., alerting).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Metric observability signals, especially with the Prometheus data model, fit
    into aggregated information instrumentation. We discussed the benefits, but some
    limits and downsides are important to understand. All downsides come from the
    fact that we generally cannot narrow pre-aggregated data down to a state before
    aggregation, for example, a single event. We might know with metrics how many
    requests failed, but we don’t know the exact stack trace, error message, and so
    on for a singular error that happened. The most granular information we typically
    have is a type of error (e.g., status code). This makes the surface of possible
    questions we can ask a metric system smaller than if we would capture all raw
    events. Another essential characteristic that might be considered a downside is
    the cardinality of the metrics and the fact that it has to be kept low.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: High Metric Cardinality
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cardinality means the uniqueness of our metric. For example, imagine in [Example 6-7](#code-latency-metric)
    we would inject a unique error string instead of the `error_type` label. Every
    new label value creates a new, possibly short-lived unique metric. A metric with
    just a single or a few samples represents more of a raw event, not aggregation
    over time. Unfortunately, if users try to push event-like information to a system
    designed for metrics (like Prometheus), it tends to be expensive and slow.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: It is very tempting to push more cardinal data to a system designed for metrics.
    This is because it’s only natural to want to learn more from such cheap and reliable
    signal-like metrics. Avoid that and keep your cardinality low with metric budgets,
    recording rules, and allow-list relabeling. Switch to event-based systems like
    logging and tracing if you wish to capture unique information like exact error
    messages or the latency for a single, specific operation in the system!
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Whether gathered from logs, traces, profiles, or metric signals, we already
    touched on some metrics in previous chapters—for example, CPU core used per second,
    memory bytes allocated on the heap, or residential memory bytes used per operation.
    So let’s go through some of those in detail and talk about their semantics, how
    we should interpret them, potential granularity, and example code that illustrates
    them using signals you have just learned.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: There Is No Observability Silver Bullet!
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metrics are powerful. Yet as you learned in this chapter, logging and traces
    also give enormous opportunities to improve the efficiency observability experience
    with dedicated tools that allow us to derive metrics from them. In this book,
    you will see me using all of those tools (together with profiling, which we haven’t
    covered yet) to improve the efficiency of Go programs.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The pragmatic system captures enough of each of those observability signals
    that fit your use cases. It’s unlikely to build metric-only, trace-only, or profiling-only
    systems!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency Metrics Semantics
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Observability feels like a vast and deep topic that takes years to grasp and
    set up. The industry constantly evolves, and creating new solutions does not help.
    However, it will be easier to understand once we start using observability for
    a specific goal like the efficiency effort. Let’s talk about exactly which observability
    bits are essential to start measuring latency and consumption of the resources
    we care about, e.g., CPU and memory.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Metrics As Numeric Value Versus Metric Observability Signal
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [“Metrics”](#ch-obs-metrics), we discussed the metric observability signal.
    Here we discuss specific metric semantics that are useful to capture for efficiency
    efforts. To clarify, we can capture those specific metrics in various ways. We
    can use metric observability signals, but we can also derive them from other signals,
    like logs, traces, and profiling!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Two things can define every metric:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Semantics
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: What’s the meaning of that number? What do we measure? With what unit? How do
    we call it?
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Granularity
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: How detailed is this information? For example, is it per a unique operation?
    Is it per a result type of this operation (success versus error)? Per goroutine?
    Per process?
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Metric semantics and granularity both heavily depend on the instrumentation.
    This section will focus on defining the semantics, granularity, and example instrumentation
    for the typical metrics we can use to track resource consumption and latency of
    our software. It is essential to understand the specific measurements we will
    operate with to work effectively with the benchmark and profiling tools we will
    learn in [“Benchmarking Levels”](ch07.html#ch-obs-benchmarking) and [“Profiling
    in Go”](ch09.html#ch-obs-profiling). While iterating over those semantics, we
    will uncover common best practices and pitfalls we have to be aware of. Let’s
    go!
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Latency
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we want to improve how fast our program performs certain operations, we need
    to measure the latency. Latency means the duration of the operation from the start
    to either success or failure. Thus, the semantics we need feel pretty simple at
    first glance—we generally want the “amount of time” required to complete our software
    operation. Our metric will usually have a name containing the words *latency*,
    *duration*, or *elapsed* with the desired unit. But the devil is in the details,
    and as you will learn in this section, measuring latency is prone to mistakes.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: The preferable unit of the typical latency measurement depends on what kind
    of operations we measure. If we measure very short operations like compression
    latency or OS context switch latencies, we must focus on granular nanoseconds.
    Nanoseconds are also the most granular timing we can count on in typical modern
    computers. This is why the Go standard library [`time.Time`](https://oreil.ly/QGCme)
    and [`time.Duration`](https://oreil.ly/9agLb) structures measure time in nanoseconds.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally speaking, the typical measurements of software operations are almost
    always in milliseconds, seconds, minutes, or hours. This is why it’s often enough
    to measure latency in seconds, as a floating value, for up to nanoseconds granularity.
    Using seconds has another advantage: it is a base unit. Using the base unit is
    often what’s natural and consistent across many solutions.^([16](ch06.html#idm45606831155568))
    Consistency is critical here. You don’t want to measure one part of the system
    in nanoseconds, another in seconds, and another in hours if you can avoid it.
    It’s easy enough to get confused by our data and have a wrong conclusion without
    trying to guess a correct unit or writing transformations between those.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code examples in [“Example: Instrumenting for Latency”](#ch-obs-signals),
    we already mentioned many ways we can instrument latency using various observability
    signals. Let’s extend [Example 6-1](#code-latency-simplest) in [Example 6-10](#code-latency-simplest-ext)
    to show important details that ensure latency is measured as reliably as possible.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-10\. Manual and simplest latency measurement of a single operation
    that can error out and has to prepare and tear down phases
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#co_efficiency_observability_CO10-1)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: We capture the `start` time as close as possible to the start of our `doOperation`
    invocation. This ensures nothing unexpected will get between `start` and operation
    start that might introduce unrelated latency, which can mislead the conclusion
    we might take from this metric further on. This, by design, should exclude any
    potential preparation or setup we have to do for an operation we measure. Let’s
    measure those explicitly as another operation. This is also why you should avoid
    putting any newline (empty line) between `start` and the invocation of the operation.
    As a result, the next programmer (or yourself, after some time) won’t add anything
    in between, forgetting about the instrumentation you added.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_efficiency_observability_CO10-2)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, it’s important to capture the `finish` time using the `time.Since`
    helper as soon as we finish, so no unrelated duration is captured. For example,
    similar to excluding `prepare()` time, we want to exclude any potential close
    or `tearDown()` duration. Moreover, if you are an advanced Go programmer, your
    intuition is always to check errors when some functions finish. This is critical,
    but we should do that for instrumentation purposes after we capture the latency.
    Otherwise, we might increase the risk that someone will not notice our instrumentation
    and will add unrelated statements between what we measure and `time.Since`. On
    top of that, in most cases, you want to make sure you measure the latency of both
    successful and failed operations to understand the complete picture of what your
    program is doing.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Shorter Latencies Are Harder to Measure Reliably
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The method for measuring operation latency shown in [Example 6-10](#code-latency-simplest-ext)
    won’t work well for operations that finish under, let’s say, 0.1 microseconds
    (100 nanoseconds). This is because the effort of taking the system clock number,
    allocating variables, and further computing `time.Now()` and `time.Since` functions
    can take its time too, which is significant for such short measurements.^([17](ch06.html#idm45606831016880))
    Furthermore, as we will learn in [“Reliability of Experiments”](ch07.html#ch-obs-rel),
    every measurement has some variance. The shorter latency, the more impactful this
    noise can be.^([18](ch06.html#idm45606831013664)) This also applies to tracing
    spans measuring latency.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: One solution for measuring very fast functions is used by the Go benchmark as
    presented by [Example 6-3](#code-latency-go-bench), where we estimate average
    latency per operation by doing many of them. More on that in [“Microbenchmarks”](ch08.html#ch-obs-micro).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Time Is Infinite; the Software Structures Measuring that Time Are Not!
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When measuring latency, we have to be aware of the limitations of time or duration
    measurements in software. Different types can contain different ranges of numeric
    values, and not all of them can contain negative numbers. For example:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '`time.Time` can only measure time from January 1, 1885^([19](ch06.html#idm45606831005584))
    up until 2157.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `time.Duration` type can measure time (in nanoseconds) approximately between
    -290 years before your “starting” point and up to 290 years after your “starting”
    point.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to measure things outside of those typical values, you need to extend
    those types or use your own. Last but not least, Go is prone [to the leap second
    problem](https://oreil.ly/MeZ4b) and time skews of the operating systems. On some
    systems, the `time.Duration` (monotonic clock) will also stop if the computer
    goes to sleep (e.g., laptop or virtual machine suspend), which will lead to wrong
    measurements, so keep that in mind.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: We discussed some typical latency metric semantics. Now let’s move to the granularity
    question. We can decide to measure the latency of operation A or B in our process.
    We can measure a group of operations (e.g., transaction) or a single suboperation
    of it. We can gather this data across many processes or look only at one, depending
    on what we want to achieve.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: To make it even more complex, even if we choose a single operation as our granularity
    to measure latency, that single operation has many stages. In a single process
    this can be represented by stack trace, but for multiprocess systems with some
    network communication, we might need to establish additional boundaries.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take some programs as an example, as the Caddy HTTP web server explained
    in the previous chapter, with a simple [REST](https://oreil.ly/SHEor) HTTP call
    to retrieve an HTML as our example operation. What latencies should we measure
    if we install such a Go program in a cloud on production to serve our REST HTTP
    call to the client (e.g., someone’s browser)? The example granularities we could
    measure latency for are presented in [Figure 6-7](#img-obs-latency-stages).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![efgo 0607](assets/efgo_0607.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. Example latency stages we can measure for in our Go web server
    program communicating with the user’s web browser
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can outline five example stages:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Absolute (total) client-side latency
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The latency measured exactly from the moment the user hits Enter in the URL
    input in the browser, up until the whole response is retrieved, content is loaded,
    and the browser renders all.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: HTTP client-side latency (response time)
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: The latency captured from the moment the first bytes of the HTTP request on
    the client side are being written to a new or reused TCP connection, up until
    the client receives all bytes of the response. This excludes everything that happens
    before (e.g., DNS lookup) or after (rendering HTML and JavaScript in the browser)
    on the client side.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: HTTP server-side latency
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: The latency is measured from the moment the server receives the first bytes
    of the HTTP request from the client, up until the server finishes writing all
    bytes of the HTTP response. This is typically what we are measuring if we use
    [the HTTP middlewares pattern](https://oreil.ly/Js0NO) in Go.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Server-side latency (service time)
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: The latency of server-side computation required to answer the HTTP request,
    measured without HTTP request parsing and response encoding. Latency is from the
    moment of having the HTTP request parsed to the moment when we start encoding
    and sending the HTTP response.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Server-side function latency
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: The latency of a single server-side function computation from the moment of
    invocation, up until the function work is finished and return arguments are in
    the context of the caller function.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: These are just some of the many permutations we can use to measure latency in
    our Go programs or systems. Which one should we pick for our optimizations? Which
    matters the most? It turns out that all of them have their use case. The priority
    of what latency metric granularity we should use and when depends solely on our
    goals, the accuracy of measurements as explained in [“Reliability of Experiments”](ch07.html#ch-obs-rel),
    and the element we want to focus on as discussed in [“Benchmarking Levels”](ch07.html#ch-obs-benchmarking).
    To understand the big picture and find the bottleneck, we have to measure a few
    of those different granularities at once. As discussed in [“Root Cause Analysis,
    but for Efficiency”](ch09.html#ch-obs-cause), tools like tracing and profiling
    can help with that.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Whatever Metric Granularity You Choose, Understand and Document What You Measure!
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We waste a lot of time if we take the wrong conclusions from measurements. It
    is easy to forget or misunderstand what parts of granularity we are measuring.
    For example, you thought you were measuring server-side latency, but slow client
    software is introducing latency you felt you didn’t include in your metric. As
    a result, you might be trying to find a bottleneck on the server side, whereas
    a potential problem might be in a different process.^([20](ch06.html#idm45606830976592))
    Understand, document, and be explicit with your instrumentation to avoid those
    mistakes
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'In [“Example: Instrumenting for Latency”](#ch-obs-signals), we discussed how
    we could gather latencies. We mentioned that generally, we use two main measuring
    methods for efficiency needs in the Go ecosystem. Those two ways are typically
    the most reliable and cheapest (useful when performing load tests and benchmarks):'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Basic logging using [“Microbenchmarks”](ch08.html#ch-obs-micro) for isolated
    functionality, single process measurements
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics such as [Example 6-7](#code-latency-metric) for macro measurements that
    involve larger systems with multiple processes
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Especially in the second case, as mentioned previously, we have to measure
    latency many times for a single operation to get reliable efficiency conclusions.
    We don’t have access to raw latency numbers for each operation with metrics—we
    have to choose some aggregation. In [Example 6-2](#code-latency-simplest-aggr),
    we proposed a simple average aggregation mechanism inside instrumentation. With
    metric instrumentation, this would be trivial to achieve. It’s as easy as creating
    two counters: one for the `sum` of latencies and one for the `count` of operations.
    We can evaluate collected data with those two metrics into a mean (arithmetic
    average).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the average is too naive an aggregation. We can miss lots of
    important information about the characteristics of our latency. In [“Microbenchmarks”](ch08.html#ch-obs-micro),
    we can do a lot with the mean for basic statistics (this is what the Go benchmarking
    tool is using), but in measuring the efficiency of our software in the bigger
    system with more unknowns, we have to be mindful. For example, imagine we want
    to improve the latency of one operation that used to take around 10 seconds. We
    made a potential optimization using our TFBO flow. We want to assess the efficiency
    on the macro level. During our tests, the system performed 500 operations within
    5 seconds (faster!), but 50 operations were extremely slow, with a 40-second latency.
    Suppose we would stick to the average (8.1 seconds). In that case, we could make
    the wrong conclusion that our optimization was successful, missing the potential
    big problem that our optimization caused, leading to 9% of operations being extremely
    slow.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: This is why it’s helpful to measure specific metrics (like latency) in percentiles.
    This is what [Example 6-7](#code-latency-metric) instrumentation is for with the
    metric histogram type for our latency measurements.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Most metrics are better thought of as distributions rather than averages. For
    example, for a latency SLI [service level indicator], some requests will be serviced
    quickly, while others will invariably take longer—sometimes much longer. A simple
    average can obscure these tail latencies, as well as changes in them. (...) Using
    percentiles for indicators allows you to consider the shape of the distribution
    and its differing attributes: a high-order percentile, such as the 99th or 99.9th,
    shows you a plausible worst-case value, while using the 50th percentile (also
    known as the median) emphasizes the typical case.'
  id: totrans-263
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-264
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: C. Jones et al., [*Site Reliability Engineering*, “Service Level Objectives”](https://oreil.ly/rMBW3)
    (O’Reilly, 2016)
  id: totrans-265
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The histogram metric I mentioned in [Example 6-8](#code-latency-metric-om) is
    great for latency measurements, as it counts how many operations fit into a certain
    latency range. In [Example 6-7](#code-latency-metric), I have chosen^([21](ch06.html#idm45606830952080))
    exponential buckets `0.001, 0.01, 0.1, 1, 10, 100`. The largest bucket should
    represent the longest operation duration you expect in your system (e.g., a timeout).^([22](ch06.html#idm45606830949872))
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: In [“Metrics”](#ch-obs-metrics), we discussed how we can use metrics using `PromQL`.
    For the histogram type of metrics and our latency semantics, the best way to understand
    this is to use the `histogram_quantile` function. See the example output in [Figure 6-8](#img-obs-metric-perc-5)
    for the median, and [Figure 6-9](#img-obs-metric-perc-9) for the 90th percentile.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![efgo 0608](assets/efgo_0608.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. Fiftieth percentile (median) of latency across an operation per
    error type from our [Example 6-7](#code-latency-metric) instrumentation
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![efgo 0609](assets/efgo_0609.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: Figure 6-9\. Ninetieth percentile of latency across the operation per error
    type from our [Example 6-7](#code-latency-metric) instrumentation
  id: totrans-271
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Both results can lead to interesting conclusions for the program I measured.
    We can observe a few things:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Half of the operations were generally faster than 590 milliseconds, while 90%
    were faster than 1 second. So if our RAER ([“Resource-Aware Efficiency Requirements”](ch03.html#ch-conq-req))
    states that 90% of operations should be less than 1 second, it could mean we don’t
    need to optimize further.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operations that failed with `error_type=error1` were considerably slower (most
    likely some bottleneck exists in that code path).
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Around 17:50 UTC, we can see a slight increase in latencies for all operations.
    This might mean some side effect or change in the environment that caused my laptop’s
    operating system to give less CPU to my test.^([23](ch06.html#idm45606830933632))
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Such measured and defined latency can help us determine if our latency is good
    enough for our requirements and if any optimization we do helps or not. It can
    also help us to find parts that cause slowness using different benchmarking and
    bottleneck-finding strategies. We will explore those in [Chapter 7](ch07.html#ch-observability2).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'With the typical latency metric definition and example instrumentation, let’s
    move to the next resource we might want to measure in our efficiency journey:
    CPU usage.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: CPU Usage
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html#ch-hardware), you learned how CPU is used when we execute
    our Go programs. I also explained that we look at CPU usage to reduce CPU-driven
    latency^([24](ch06.html#idm45606830921280)) and cost, and to enable running more
    processes on the same machine.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'A variety of metrics allow us to measure different parts of our program’s CPU
    usage. For example, with Linux tools like the [`proc` filesystem](https://oreil.ly/MJVHl)
    and [`perf`](https://oreil.ly/QPMD9), we can measure our [Go program’s miss and
    hit rates, CPU branch prediction hit rates](https://oreil.ly/VdENl), and other
    low-level statistics. However, for basic CPU efficiency, we usually focus on the
    CPU cycles, instructions, or time used:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: CPU cycles
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: The total number of CPU clock cycles used to execute the program thread instructions
    on each CPU core.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: CPU instructions
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: The total number of CPU instructions of our program’s threads executed in each
    CPU core. On some CPUs from the [RISC architecture](https://oreil.ly/ofvB7) (e.g.,
    ARM processors), this might be equal to the number of cycles, as one instruction
    always takes one cycle (amortized cost). However, on the CISC architecture (e.g.,
    AMD and Intel x64 processors), different instructions might use additional cycles.
    Thus, counting how many instructions our CPU had to do to complete some program’s
    functionality might be more stable.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Both cycles and instructions are great for comparing different algorithms with
    each other. It is because they are less noisy as:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: They don’t depend on the frequency the CPU core had during the program run
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latency of memory fetches, including different caches, misses, and RAM latency
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU time
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: The time (in seconds or nanoseconds) our program thread spends executing on
    each CPU core. As you will learn in [“Off-CPU Time”](ch09.html#ch-obs-pprof-latency),
    this time is different (longer or shorter) from the latency of our program, as
    CPU time does not include I/O waiting time and OS scheduling time. Furthermore,
    our program’s OS threads might execute simultaneously on multiple CPU cores. Sometimes
    we also use CPU time divided by the CPU capacity, often referred to as CPU usage.
    For example, 1.5 CPU usage in seconds means our program requires (on average)
    one CPU core for 1 second and a second core for 0.5 seconds.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'On Linux, the CPU time is often split into User and System time:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: User time represents the time the program spends executing on the CPU in the
    user space.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System time is the CPU time spent executing certain functions in the kernel
    space on behalf of the user, e.g., syscalls like [`read`](https://oreil.ly/xEQuM).
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually, on higher levels such as containers, we don’t have the luxury of having
    all three metrics. We mostly have to rely on CPU time. Fortunately, the CPU time
    is typically a good enough metric to track down the work needed from our CPUs
    to execute our workload. On Linux, the simplest way to retrieve the current CPU
    time counted from the start of the process is to go to */proc/`<PID>`/stat* (where
    `PID` means the process ID). We also have similar statistics on the thread level
    in */proc/`<PID>`/tasks/`<TID>`/stat* (where `TID` means the thread ID). This
    is exactly what utilities like `ps` or `htop` use.^([25](ch06.html#idm45606830897680))
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ps` and `htop` tools might be indeed the simplest tools to measure the
    CPU time in the current moment. However, we usually need to assess the CPU time
    required for the full functionality we are optimizing. Unfortunately, [“Go Benchmarks”](ch08.html#ch-obs-micro-go)
    is not providing CPU time (only latency and allocations) per operation. You could
    perhaps obtain that number from the `stat` file, e.g., programmatically using
    the [`procfs` Go library](https://oreil.ly/ZcCDn), but there are two main ways
    I would suggest instead:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: CPU profiling, explained in [“CPU”](ch09.html#ch-obs-pprof-cpu).
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prometheus metric instrumentation. Let’s quickly look at that method next.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [Example 6-7](#code-latency-metric), I showed a Prometheus instrumentation
    that registers custom latency metrics. It’s also very easy to add the CPU time
    metric, but the Prometheus [client library](https://oreil.ly/1r2zw) has already
    built helpers for that. The recommended way is presented in [Example 6-11](#code-cpu-metric).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-11\. Registering `proc` `stat` instrumentation about your process
    for Prometheus use
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_efficiency_observability_CO11-1)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: The only thing you have to do to have the CPU time metric with Prometheus is
    to register the `collectors.NewProcessCollector` that uses the `/proc` `stat`
    file mentioned previously.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: The `collectors.ProcessCollector` provides multiple metrics, like `pro⁠cess_​open_fds`,
    `process_max_fds`, `process_start_time_seconds`, and so on. But the one we are
    interested in is `process_cpu_seconds_total`, which is a counter of CPU time used
    from the beginning of our program. What’s special about using Prometheus for this
    task is that it collects the values of this metric periodically from our Go program.
    This means we can query Prometheus for the process CPU time for a certain time
    window and map that to real time. We can do that with the [`rate`](https://oreil.ly/8BaUw)
    function duration that gives us the per second rate of that CPU time in a given
    time window. For example, `rate(process_cpu_sec⁠onds_​total{}[5m])` will give
    us the average CPU per second time that our program had during the last five minutes.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: You will find an example CPU time analysis based on this kind of metric in [“Understanding
    Results and Observations”](ch08.html#ch-obs-macro-results). However, for now,
    I would love to show you one interesting and common case, where `process_cpu_seconds_total`
    helps narrow down a major efficiency problem. Imagine your machine has only two
    CPU cores (or we limit our program to use two CPU cores), you run the functionality
    you want to assess, and you see the CPU time rate of your Go program looking like
    [Figure 6-10](#img-obs-metric-cpu).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to this view, we can tell that the `labeler` process is experiencing
    a state of CPU saturation. This means that our Go process requires more CPU time
    than was available. Two signals tell us about the CPU saturation:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: The typical “healthy” CPU usage is spikier (e.g., as presented in [Figure 8-4](ch08.html#img-macrobench-cpu)
    later in the book). This is because it’s unlikely that typical applications use
    the same amount of CPU all the time. However, in [Figure 6-10](#img-obs-metric-cpu),
    we see the same CPU usage for five minutes.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of this, we never want our CPU time to be so close to the CPU limit
    (two in our case). In [Figure 6-10](#img-obs-metric-cpu), we can clearly see a
    small choppiness around the CPU limit, which indicates full CPU saturation.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![efgo 0610](assets/efgo_0610.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
- en: Figure 6-10\. The Prometheus graph view of the CPU time for the `labeler` Go
    program (we will use it in an example in [“Macrobenchmarks”](ch08.html#ch-obs-macro))
    after a test
  id: totrans-308
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Knowing when we are at saturation of our CPU is critical. First of all, it might
    give the wrong impression that the current CPU time is the maximum that the process
    needs. Moreover, this situation also significantly slows down our program’s execution
    time (increases latency) or even stalls it completely. This is why the Prometheus-based
    CPU time metric, as you learned here, has proven to be critical for me in learning
    about such saturation cases. It is also one of the first things you must find
    out when analyzing your program’s efficiency. When saturation happens, we have
    to give more CPU cores to the process, optimize the CPU usage, or decrease the
    concurrency (e.g., limit the number of HTTP requests it can do concurrently).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, CPU time allows us to find out about opposite cases where
    the process might be blocked. For example, if you expect CPU-bound functionality
    to run with 5 goroutines, and you see the CPU time of 0.5 (50% of one CPU core),
    it might mean the goroutines are blocked (more on that in [“Off-CPU Time”](ch09.html#ch-obs-pprof-latency))
    or whole machine and OS are busy.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now look at memory usage metrics.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Memory Usage
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we learned in [Chapter 5](ch05.html#ch-hardware2), there are complex layers
    of different mechanics on how our Go program uses memory. This is why the actual
    physical memory (RAM) usage is one of the most tricky to measure and attribute
    to our program. On most systems with an OS memory management mechanism like virtual
    memory, paging, and shared pages, every memory usage metric will be only an estimation.
    While imperfect, this is what we have to work with, so let’s take a short look
    at what works best for the Go program.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main sources of memory usage information for our Go process:
    the Go runtime heap memory statistics and the information that OS holds about
    memory pages. Let’s start with the in-process runtime stats.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: runtime heap statistics
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we learned in [“Go Memory Management”](ch05.html#ch-hw-go-mem), the heap
    segment of the Go program virtual memory can be an adequate proxy for memory usage.
    This is because most bytes are allocated on the heap for typical Go applications.
    Moreover, such memory is also never evicted from the RAM (unless the swap is enabled).
    As a result, we can effectively assess our functionality’s memory usage by looking
    at the heap size.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'We are often most interested in assessing the memory space or the number of
    memory blocks needed to perform a certain operation. To try to estimate this,
    we usually use two semantics:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: The total allocations of bytes or objects on the heap allow us to look at memory
    allocations without often nondeterministic GC impact.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of currently in-use bytes or objects on the heap.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding statistics are very accurate and quick to access because Go runtime
    is responsible for heap management, so it tracks all the information we need.
    Before Go 1.16, the recommended way to access those statistics programmatically
    was using the [`runtime.ReadMemStats` function](https://oreil.ly/AwX75). It still
    works for compatibility reasons, but unfortunately, it requires STW (stop the
    world) events to gather all memory statistics. As a result of Go 1.16, we should
    all use the [`runtime/metrics`](https://oreil.ly/WYiOd) package that provides
    many cheap-to-collect insights about GC, memory allocations, and so on. The example
    usage of this package to get memory usage metrics is presented in [Example 6-12](#code-rtm).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-12\. The simplest code prints total heap allocated bytes and currently
    used ones
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_efficiency_observability_CO12-1)'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: To read samples from `runtime/metrics`, we must first define them by referencing
    the desired metric name. The full list of metrics might be different (mostly added
    ones) across different Go versions, and you can see the list with descriptions
    at [*pkg.go.dev*](https://oreil.ly/HWGUJ). For example, we can obtain the number
    of objects in a heap.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_efficiency_observability_CO12-2)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Memory statistics are recorded right after a GC run, so we can trigger GC to
    have the latest information about the heap.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_efficiency_observability_CO12-3)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '`metrics.Read` populates the value of our samples. You can reuse the same sample
    slice if you only care about the latest values.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_efficiency_observability_CO12-4)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Both metrics are of `uint64` type, so we use the `Uint64()` method to retrieve
    the value.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'Programmatically accessing this information is useful for local debugging purposes,
    but it’s not sustainable on every optimization attempt. That’s why in the community,
    we typically see other ways to access that data:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Go benchmarking, explained in [“Go Benchmarks”](ch08.html#ch-obs-micro-go)
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heap profiling, explained in [“Heap”](ch09.html#ch-obs-pprof-heap)
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prometheus metric instrumentation
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To register `runtime/metric` as Prometheus metrics, we can add a single line
    to [Example 6-11](#code-cpu-metric): `reg.MustRegister(collectors.NewGoCollector())`.
    The Go collector is a structure that, by default, exposes [various memory statistics](https://oreil.ly/Ib8D2).
    For historical reasons, those map to the `MemStats` Go structure, so the equivalents
    to the metrics defined in [Example 6-12](#code-rtm) would be `go_mem⁠stats_​heap_alloc_bytes_total`
    for a counter, and `go_memstats_heap_alloc_bytes` for a current usage gauge. We
    will show an analysis of Go heap metrics in [“Go e2e Framework”](ch08.html#ch-obs-macro-example).'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, heap statistics are only an estimation. It is likely that the
    smaller the heap on our Go program, the better the memory efficiency. However,
    suppose you add some deliberate mechanisms like large off-heap memory allocations
    using explicit `mmap` syscall or thousands of goroutines with large stacks. In
    that case, that can cause an OOM on your machine, yet it’s not reflected in the
    heap statistics. Similarly, in [“Go Allocator”](ch05.html#ch-hw-allocator), I
    explained rare cases where only part of the heap space is allocated on physical
    memory.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Still, despite the downsides, heap allocations remain the most effective way
    to measure memory usage in modern Go programs.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: OS memory pages statistics
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can check the numbers the Linux OS tracks per thread to learn more realistic
    yet more complex memory usage statistics. Similar to [“CPU Usage”](#ch-obs-cpu-usage),
    `/proc/*<PID>*/statm` provides the memory usage statistics, measured in pages.
    Even more accurate numbers can be retrieved from per memory mapping statistics
    that we can see in `/proc/*<PID>*/smaps` ([“OS Memory Mapping”](ch05.html#ch-hw-memory-mmap-os)).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Each page in this mapping can have a different state. A page might or might
    not be allocated on physical memory. Some pages might be shared across processes.
    Some pages might be allocated in physical memory and accounted for as memory used,
    yet marked by the program as “free” (see the `MADV_FREE` release method mentioned
    in [“Garbage Collection”](ch05.html#ch-hw-garbage)). Some pages might not even
    be accounted for in the `smaps` file, because for example, [it’s part of filesystem
    Linux cache buffers](https://oreil.ly/uchws). For these reasons, we should be
    very skeptical about the absolute values observed in the following metrics. In
    many cases, OS is lazy in releasing memory; e.g., part of the memory used by the
    program is cached in the best way that will be released immediately as long as
    somebody else is needing that.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few typical memory usage metrics we can obtain from the OS about
    our process:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: VSS
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Virtual set size represents the number of pages (or bytes, depending on instrumentation)
    allocated for the program. Not very useful metrics, as most virtual pages are
    never allocated on RAM.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: RSS
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Residential set size represents the number of pages (or bytes) resident in RAM.
    Note that different metrics might account for that differently; e.g., the [cgroups
    RSS metric](https://oreil.ly/NL5Ab) does not include file-mapped memory, which
    is tracked separately.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: PSS
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: Proportional set size represents memory with shared memory pages divided equally
    among all users.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: WSS
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Working set size estimates the number of pages (or bytes) currently used to
    perform work by our program. It was initially [introduced by Brendan Gregg](https://oreil.ly/rWy8D)
    as the hot, frequently used memory—the minimum memory requirement by the program.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that a program might have allocated 500 GB of memory, but within
    a couple of minutes, it might use only 50 MB for some localized computation. The
    rest of the memory could be, in theory, safely offloaded to disk.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: There are many implementations of WSS, but the most common I see is the [cadvisor
    interpretation](https://oreil.ly/mXjA3) using the [cgroup memory controller](https://oreil.ly/ovSlH).
    It calculates the WSS as the RSS (including file mapping), plus some part of the
    cache pages (cache used for disk reads or writes), minus the `inactive_file` entry—so
    file mapping that were not touched for some time. It does not include inactive
    anonymous pages because the typical OS configuration can’t offload anonymous pages
    to disk (swap is disabled).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: In practice, RSS or WSS is used to determine the memory usage of our Go program.
    Which one highly depends on the other workloads on the same machine and follows
    the flow of the RAM usage expanding to all available space, as mentioned in [“Do
    We Have a Memory Problem?”](ch05.html#ch-hw-memory). The usefulness of each depends
    on the current Go version and instrumentation that gives you those metrics. In
    my experience, with the latest Go version and cgroup metrics, the RSS metric tends
    to give more reliable results.^([26](ch06.html#idm45606830443712)) Unfortunately,
    accurate or not, WSS is used in systems like [Kubernetes to trigger evictions
    (e.g., OOM)](https://oreil.ly/lnDkI), thus we should use it to assess memory efficiency
    that might lead to OOMs.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: Given my focus on infrastructure Go programs, I heavily lean on a metric exporter
    called [cadvisor](https://oreil.ly/RJzKd) that converts cgroup metrics to Prometheus
    metrics. I will explain using it in detail in [“Go e2e Framework”](ch08.html#ch-obs-macro-example).
    It allows analyzing metrics like `container_memory_rss + container_memory_mapped_file`
    and `container_memory_working_set_bytes`, which are commonly used in the community.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern observability offers a set of techniques essential for our efficiency
    assessments and improvements. However, some argue that this kind of observability
    designed primarily for DevOps, SREs, and cloud-native solutions can’t work for
    developer use cases (in the past known as Application Performance Monitoring [APM]).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: I would argue that the same tools can be used for both developers (for those
    efficiency and debugging journeys) and system admins, operators, DevOps, and SREs
    to ensure the programs delivered by others are running effectively.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed the three first observability signals: metrics,
    logs, and tracing. Then, we went through example instrumentations for those in
    Go. Finally, I explained common semantics for the latency, CPU time, and memory
    usage measurements we will use in later chapters.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Now it’s time to learn how to use that efficiency observability to make data-driven
    decisions in practice. First, we will focus on how to simulate our program to
    assess the efficiency on different levels.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch06.html#idm45606832729952-marker)) Some of you might ask why I am sticking
    to the word *observability* and don’t mention monitoring. In my eyes, I have to
    agree with my friend [Björn Rabenstein](https://oreil.ly/9ado0) that the difference
    between monitoring and observability tends to be driven by marketing needs too
    much. One might say that observability has become meaningless these days. In theory,
    monitoring means answering known unknown problems (known questions), whereas observability
    allows learning about unknown unknowns (any question you might have in the future).
    In my eyes, monitoring is a subset of observability. In this book, we will stay
    pragmatic. Let’s focus on how we can leverage observability practically, not using
    theoretical concepts.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch06.html#idm45606832725840-marker)) The fourth signal, profiling, just
    started to be considered by some as an observability signal. This is because only
    recently did the industry see a value and need for gathering profiling continuously.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch06.html#idm45606832699376-marker)) As a recent example, we can give
    [this repository](https://oreil.ly/sPlPe) that gathers information through eBPF
    probes and tries to search popular functions or libraries.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch06.html#idm45606832688544-marker)) In some way, I am trying in this
    book to establish helpful processes around optimizations and efficiency, which
    by design yield standard questions we know up front. This aggregated information
    is usually enough for us here.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch06.html#idm45606832254464-marker)) Given Go compatibility guarantees,
    even if the community agrees to improve it, we cannot change it until Go 2.0.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch06.html#idm45606832248016-marker)) A nonexecutable module or package
    intended to be imported by others.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch06.html#idm45606832242864-marker)) There are many Go libraries for logging.
    `go-kit` has a good enough API that allows us to do all kinds of logging we need
    in all the Go projects I have helped with so far. This does not mean `go-kit`
    is without flaws (e.g., it’s easy to forget you have to put an even number of
    arguments for the key-value–like logic). There is also a pending proposal from
    the Go community on [structure logging in standard libraries (`slog` package)](https://oreil.ly/qnJ6y).
    Feel free to use any other libraries, but make sure their API is simple, readable,
    and useful. Also make sure that the library of your choice is not introducing
    efficiency problems.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch06.html#idm45606832090240-marker)) It’s a typical pattern allowing processes
    to print something useful to standard output and keep logs separate in the `stderr`
    Linux file.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch06.html#idm45606831687072-marker)) Tail sampling is a logic that defers
    the decision if the trace should be excluded or sampled at the end of the transaction,
    for example, only after we know its status code. The problem with tail sampling
    is that your instrumentation might have already assumed that all spans will be
    sampled.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch06.html#idm45606831665920-marker)) I maintain this library together
    with the Prometheus team. The `client_golang` is also the most used metric client
    SDK for Go when writing this book, [with over 53,000 open source projects](https://oreil.ly/UW0fG)
    using it. It is free and open source.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch06.html#idm45606831400064-marker)) It’s tempting to use global `prometheus.DefaultRegistry`.
    Don’t do this. We try to get away from this pattern that can cause many problems
    and side effects.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch06.html#idm45606831336912-marker)) Always check errors and perform
    graceful termination on process teardown. See production-grade usage in the [Thanos
    project](https://oreil.ly/yvvTM) that leverages the [run goroutine helper](https://oreil.ly/sDIwW).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch06.html#idm45606831225328-marker)) Note that doing `rate` on the gauges
    type of metric will yield incorrect results.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch06.html#idm45606831205328-marker)) On the contrary, for the push-based
    system, if you don’t see expected data, it’s hard to tell if it’s because the
    sender is down or the pipeline to send is down.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch06.html#idm45606831202864-marker)) See our talk from [KubeCon EU 2022](https://oreil.ly/TtKwH)
    about such cases.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch06.html#idm45606831155568-marker)) This is why the [Prometheus ecosystem
    suggests base units](https://oreil.ly/oJozb).
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch06.html#idm45606831016880-marker)) For example, on my machine `time.Now`
    and `time.Since` take around 50–55 nanoseconds.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch06.html#idm45606831013664-marker)) This is why it’s better to make
    thousands or even more of the same operation, measure the total latency, and get
    the average by dividing it by a number of operations. As a result, this is what
    Go benchmark is doing, as we will learn in [“Go Benchmarks”](ch08.html#ch-obs-micro-go).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: ^([19](ch06.html#idm45606831005584-marker)) Did you know this date was picked
    simply because of [*Back to the Future Part II*](https://oreil.ly/Oct6X)?
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: ^([20](ch06.html#idm45606830976592-marker)) The noteworthy example from my experience
    is measuring server-side latency of REST with a large response or HTTP/gRPC with
    a streamed response. The server-side latency does not depend only on the server
    but also on how fast the network and client side can consume those bytes (and
    write back acknowledge packets within [TCP control flow](https://oreil.ly/jcrSF)).
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: ^([21](ch06.html#idm45606830952080-marker)) Right now, the choice of buckets
    in a histogram if you want to use Prometheus is manual. However, the Prometheus
    community is working on [sparse histograms](https://oreil.ly/qFdC1) with a dynamic
    number of buckets that adjust automatically.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: ^([22](ch06.html#idm45606830949872-marker)) More on using histograms can be
    read [here](https://oreil.ly/VrWGe).
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: ^([23](ch06.html#idm45606830933632-marker)) It makes sense. I was utilizing
    my web browser heavily during the test, which confirms the knowledge we will discuss
    in [“Reliability of Experiments”](ch07.html#ch-obs-rel).
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: ^([24](ch06.html#idm45606830921280-marker)) As a reminder, we can improve the
    latency of our program’s functionality in many ways other than just by optimizing
    its CPU usage. We can improve that latency using concurrent execution that often
    increases total CPU time.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: ^([25](ch06.html#idm45606830897680-marker)) Also a useful [`procfs` Go library](https://oreil.ly/ZcCDn)
    that allows retrieving `stats` file data number programmatically.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: ^([26](ch06.html#idm45606830443712-marker)) One reason is the [issue](https://oreil.ly/LKmSA)
    in cadvisor that includes some still-reclaimable memory in the WSS.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
