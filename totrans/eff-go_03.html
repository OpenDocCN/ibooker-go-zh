<html><head></head><body><section data-pdf-bookmark="Chapter 3. Conquering Efficiency" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch-efficiency">&#13;
<h1><span class="label">Chapter 3. </span>Conquering Efficiency</h1>&#13;
&#13;
&#13;
<p><a data-primary="efficiency (generally)" data-type="indexterm" id="ix_ch03-asciidoc0"/><a data-primary="efficiency (generally)" data-secondary="conquering" data-type="indexterm" id="ix_ch03-asciidoc1"/>It’s action time! In <a data-type="xref" href="ch01.html#ch-efficiency-matters">Chapter 1</a>, we learned that software efficiency matters. In <a data-type="xref" href="ch02.html#ch-go">Chapter 2</a>, we studied the Go programming language—its basics and advanced features. Next, we discussed Go’s capabilities of being easy to read and write. Finally, we mentioned that it could also be an effective language for writing efficient code.</p>&#13;
&#13;
<p>Undoubtedly, achieving better efficiency in your program does not come without work. In some cases, the functionality you try to improve is already well optimized, so further optimization without system redesign might take a lot of time and only make a marginal difference. However, there might be other cases where the current implementation is heavily inefficient. Removing instances of wasted work can improve the program’s efficiency in only a few hours of developer time. The true skill here as an engineer is to know, ideally after a short amount of research, which situation you are currently in:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Do you need to improve anything on the performance side?</p>&#13;
</li>&#13;
<li>&#13;
<p>If yes, is there a potential for the removal of wasted cycles?</p>&#13;
</li>&#13;
<li>&#13;
<p>How much work is needed to reduce the latency of function X?</p>&#13;
</li>&#13;
<li>&#13;
<p>Are there any suspicious overallocations?</p>&#13;
</li>&#13;
<li>&#13;
<p>Should you stop overusing network bandwidth and sacrifice memory space instead?</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This chapter will teach you the tools and methodologies to help you answer these questions effectively.</p>&#13;
&#13;
<p>If you are struggling with these skills, don’t worry! It’s normal. The efficiency topic is not trivial. Despite the demand, this space is still not mastered by many, and even major software players sometimes make poor decisions. It’s surprising how often what looks like high-quality software is shipped with fairly apparent inefficiencies. For instance, at the beginning of 2021, one user <a href="https://oreil.ly/ast0m">optimized the loading time of the popular game <em>Grand Theft Auto Online</em> from six minutes to two minutes</a> without access to the source code! As mentioned in <a data-type="xref" href="ch01.html#ch-efficiency-matters">Chapter 1</a>, this game cost a staggering ~$140 million and a few years to make. Yet, it had an obvious efficiency bottleneck with a naive JSON parsing algorithm and deduplication logic that took most of the game loading time and worsened the game experience. This person’s work is outstanding, but they used the same techniques you are about to learn. The only difference is that our job might be a bit easier—hopefully, you don’t need to reverse engineer the binary written in C++ code on the way!</p>&#13;
&#13;
<p>In the preceding example, the company behind the game missed the apparent waste of computation impacting the game’s loading performance. It’s unlikely that the company didn’t have the resources to get an expert to optimize this part. Instead, it’s a decision based on specific trade-offs, where the optimization wasn’t worth the investment since there might have been higher-priority development tasks. In the end, one would say that an inefficiency like this didn’t stop the success of the game. It did the job, yes, but for example, my friends and I were never fans of the game because of the loading time. I would argue that without this silly “waste,” success might have been even bigger.</p>&#13;
<div data-type="note" epub:type="note"><h1>Laziness or Deliberate Efficiency Descoping?</h1>&#13;
<p>There are other amusing examples of situations where a certain aspect of software efficiency could be descoped given certain circumstances. For instance, there is <a href="https://oreil.ly/mJ8Mi">the amusing story about missile software developers</a> who decided to accept certain memory leaks since the missile would be destroyed at the end of the application run. Similarly, we hear <a href="https://oreil.ly/PgzHQ">the story about “deliberate” memory leaks in low-latency trading software</a> that is expected to run only for very short &#13;
<span class="keep-together">durations.</span></p>&#13;
</div>&#13;
&#13;
<p>You could say that the examples where the efficiency work was avoided and nothing tragically bad happened were pragmatic approaches. In the end, extra knowledge and work needed to fix leaks or slowdowns were avoided. Potentially yes, but what if these decisions were not data driven? We don’t know, but these decisions might have been made out of laziness and ignorance without any valid data points that the fix would indeed take too much effort. What if developers in each example didn’t fully understand the small effort needed? What if they didn’t know how to optimize the problematic parts of the software? Would they make better decisions otherwise? Take less risk? I would argue yes.</p>&#13;
&#13;
<p>In this chapter, I will introduce the topic of optimizations, starting with explaining the definition and initial approach in <a data-type="xref" data-xrefstyle="select:nopage" href="#ch-conq-opt">“Beyond Waste, Optimization Is a Zero-Sum Game”</a>. In the next section, <a data-type="xref" href="#ch-conq-challenges">“Optimization Challenges”</a>, we will summarize the challenges we have to overcome while attempting to improve the efficiency of our software.</p>&#13;
&#13;
<p>In <a data-type="xref" href="#ch-conq-perf-goal">“Understand Your Goals”</a>, we will try to tame our software’s tendency and temptation to maximize optimization effort by setting clear efficiency goals. We need only to be fast or efficient “enough.” This is why setting the correct performance requirements from the start is so important. Next, in <a data-type="xref" href="#ch-conq-req">“Resource-Aware Efficiency Requirements”</a>, I will propose a template and pragmatic process anyone can follow. Finally, those efficiency requirements will be useful in <a data-type="xref" href="#ch-conq-issue-handling">“Got an Efficiency Problem? Keep Calm!”</a>, where I will teach you a professional flow for handling performance issues you or someone else has reported. You will learn that the optimization process could be your last resort.</p>&#13;
&#13;
<p>In <a data-type="xref" href="#ch-conq-opt-levels">“Optimization Design Levels”</a>, I will explain how to divide and isolate your optimization effort for easier conquering. Finally, in <a data-type="xref" href="#ch-conq-eff-flow">“Efficiency-Aware Development Flow”</a>, we will combine all the pieces into a unified optimization process I always use and want to recommend to you: reliable flow, which applies to any software or design level.</p>&#13;
&#13;
<p>There is a lot of learning ahead of us, so let’s start understanding what optimization means.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Beyond Waste, Optimization Is a Zero-Sum Game" data-type="sect1"><div class="sect1" id="ch-conq-opt">&#13;
<h1>Beyond Waste, Optimization Is a Zero-Sum Game</h1>&#13;
&#13;
<p><a data-primary="optimization" data-secondary="as zero-sum game" data-secondary-sortas="zero" data-type="indexterm" id="ix_ch03-asciidoc2"/>It is not a secret that one of many weapons in our arsenal to overcome efficiency issues is an effort called “optimization.” But what does optimization mean, exactly? What’s the best way to think about it and master it?</p>&#13;
&#13;
<p><a data-primary="optimization" data-secondary="general definition" data-type="indexterm" id="idm45606836668864"/>Optimization is not exclusively reserved for software efficiency topics. We also tend to optimize many things in our life, sometimes unconsciously. For example, if we cook a lot, we probably have salt in a well-accessible place. If our goal is to gain weight, we eat more calories. If we travel in the early morning, we pack and prepare the day before. If we commute, we tend to use that time by listening to audiobooks. If our commute to the office is painful, we consider moving closer to a better transportation system. All of these are optimization techniques that are meant to improve our life toward a specific goal. Sometimes we need a significant change. On the other hand, minor incremental improvements are often enough as they are magnified through repetition for a more substantial impact.</p>&#13;
&#13;
<p><a data-primary="optimization" data-secondary="engineering definition" data-type="indexterm" id="idm45606836667504"/>In engineering, the word “optimization” has its roots in <a href="https://oreil.ly/a11ou">mathematics</a>, which means finding the best solution from all possible solutions for a problem constrained by a set of rules. <a data-primary="optimization" data-secondary="computer science definition" data-type="indexterm" id="idm45606836665616"/>Typically in computer science, however, we use the word “optimization” to describe an act of improving the system or program execution for a specific aspect. For instance, we can optimize our program to load a file faster or decrease peak memory utilization while serving a request on a web server.</p>&#13;
<div class="note2" data-type="note" epub:type="note"><h1>We Can Optimize for Anything</h1>&#13;
<p>Generally, optimization does not necessarily need to improve our program’s efficiency characteristics if that is not our goal. For example, if we aim to improve security, maintainability, or code size, we can optimize for that too. Yet, in this book, when we talk about optimizations, they will be on an efficiency background (improving resource consumption or speed).</p>&#13;
</div>&#13;
&#13;
<p>The goal of efficiency optimization should be to modify code (generally without changing its functionality<sup><a data-type="noteref" href="ch03.html#idm45606836661792" id="idm45606836661792-marker">1</a></sup>) so that its execution is either overall more efficient or at least more efficient in the categories we care about (and worse in others).</p>&#13;
&#13;
<p>The important part is that, from a high-level view, we can perform the optimization by doing either of two things (or both):</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>We can eliminate “wasted” resource consumption.</p>&#13;
</li>&#13;
<li>&#13;
<p>We can trade one resource consumption for another or deliberately sacrifice other software qualities (so-called trade-off).</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Let me explain the difference between these two by describing the first type of &#13;
<span class="keep-together">change—reducing</span> so-called waste.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Reasonable Optimizations" data-type="sect2"><div class="sect2" id="ch-conq-opt-reasonable">&#13;
<h2>Reasonable Optimizations</h2>&#13;
&#13;
<p><a data-primary="optimization" data-secondary="reasonable" data-type="indexterm" id="ix_ch03-asciidoc3"/><a data-primary="reasonable optimizations" data-type="indexterm" id="ix_ch03-asciidoc4"/>Our program consists of a code—a set of instructions that operates on some data and uses various resources on our machines (CPU, memory, disk, power, etc.). We write this code so our program can perform the requested functionality. But everything involved in the process is rarely perfect (or integrated perfectly): our programmed code, compiler, operating systems, and even hardware. <a data-primary="waste" data-type="indexterm" id="ix_ch03-asciidoc5"/>As a result, we sometimes introduce “waste.” Wasted resource consumption represents a relatively unnecessary operation in our programs that takes precious time, memory, or CPU time, etc. Such waste might have been introduced as a deliberate simplification, by accident, tech debt, oversight, or just unawareness of better approaches. For example:</p>&#13;
&#13;
<ul class="less_space pagebreak-before">&#13;
<li>&#13;
<p>We might have accidentally left some debugging code that introduces massive latency in the heavily used function (e.g., <code>fmt.Println</code> statements).</p>&#13;
</li>&#13;
<li>&#13;
<p>We performed an unnecessary, expensive check because the caller has already verified the input.</p>&#13;
</li>&#13;
<li>&#13;
<p>We forgot to stop certain goroutines (a concurrency paradigm we will explain in detail in <a data-type="xref" href="ch04.html#ch-hw-concurrency">“Go Runtime Scheduler”</a>), which are no longer required, yet still running, which wastes our memory and CPU time.<sup><a data-type="noteref" href="ch03.html#idm45606836647264" id="idm45606836647264-marker">2</a></sup></p>&#13;
</li>&#13;
<li>&#13;
<p>We used a nonoptimized function from a third-party library, when an optimized one exists in a different, well-maintained library that does the same thing faster.</p>&#13;
</li>&#13;
<li>&#13;
<p>We saved the same piece of data a couple of times on disk, while it could be just reused and stored once.</p>&#13;
</li>&#13;
<li>&#13;
<p>Our algorithm might have performed checks too many times when it could have done less for free (e.g., naive search versus binary search on sorted data).</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The operation performed by our program or consumption of specific resources is a “waste” if, by eliminating it, we don’t sacrifice anything else. And “anything” here means anything we particularly care for, such as extra CPU time, other resource consumption, or nonefficiency-related qualities like readability, flexibility, or portability. Such elimination makes our software, overall, more efficient. Looking closer, you might be surprised at how much waste every program has. It just waits for us to notice it and take it back!</p>&#13;
&#13;
<p>Our program’s optimization by reducing “waste” is a simple yet effective technique. In this book, we will call it a reasonable optimization, and I suggest doing it every time you notice such waste, even if you don’t have time to benchmark it afterward. Yes. You heard me right. It should be part of coding hygiene. Note that to treat it as “reasonable” optimization, it has to be obvious. As the developer, you need to be sure that:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Such optimization eliminates some additional work of the program.</p>&#13;
</li>&#13;
<li>&#13;
<p>It does not sacrifice any other meaningful software quality or functionality, especially readability.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Look for the things that might be “obviously” unnecessary. Eliminating such unnecessary work is easily obtainable and does no harm (otherwise, it’s not waste).</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Be Mindful of Readability</h1>&#13;
<p><a data-primary="readability/unreadability of optimized code" data-secondary="waste reduction versus readability reduction" data-type="indexterm" id="idm45606836638960"/>The first thing that usually gets impacted by any code modification is readability. If reducing some obvious waste meaningfully reduces readability, or you need to spend a few hours experimenting on readable abstractions for it, it is not a reasonable &#13;
<span class="keep-together">optimization.</span></p>&#13;
&#13;
<p>That’s fine. We can deal with that later, and we will talk about it in <a data-type="xref" href="#ch-conq-opt-deliberate">“Deliberate Optimizations”</a>. If it impacts readability, we need data to prove it’s worth it.</p>&#13;
</div>&#13;
&#13;
<p>Cutting “waste” is also an effective mental model.  Like humans who are rewarded for being <a href="https://oreil.ly/u8IDm">intelligently lazy</a>, we also want to maximize the value our program brings with minimum runtime work.</p>&#13;
&#13;
<p><a data-primary="premature optimization" data-type="indexterm" id="idm45606836634080"/>One would say that reasonable optimization is an example of the anti-pattern often called “premature optimization” that <a href="https://oreil.ly/drziD">many have been warned against</a>. And I cannot agree more that reducing obvious waste like this is a premature optimization since we don’t assess and measure its impact. But I would argue that if we are sure that such premature optimization deals no harm, other than a little extra work, let’s acknowledge that it is premature optimization but is reasonable, still do it, and move on.</p>&#13;
&#13;
<p>If we go back to our commute to work example, if we notice we have a few stones in our shoes, of course we pick them out so we can walk without pain. We don’t need to assess, measure, or compare if removing the stones improved our commute time or not. Getting rid of stones will help us somehow, and it’s not harmful to do so (we don’t need to take stones with us every time we go)! :)<a data-primary="Meyers, Scott, on optimization" data-type="indexterm" id="idm45606836631600"/></p>&#13;
<blockquote>&#13;
<p>If you are dealing with something which is the noise, you don’t deal with that right away because the payoff of investing time and energy is very small. But if you are walking through your codebase and you notice an opportunity for notable improvement (say 10% or 12%), of course, you reach down and pick it up.</p>&#13;
<p data-type="attribution">Scott Meyers, <a href="https://oreil.ly/T9VFz">“Things That Matter”</a></p>&#13;
</blockquote>&#13;
&#13;
<p>Initially, when you are new to programming or a particular language, you might not know which operations are unnecessary waste or if eliminating the potential waste will harm your program. That’s fine. The “obviousness” comes from practice, so don’t guess here. If you are guessing, it means the optimization is not obvious. You will learn what’s reasonable with experience, and we will practice this together in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch10.html#ch-opt">10</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch11.html#ch-opt2">11</a>.</p>&#13;
&#13;
<p>Reasonable optimizations yield consistent performance improvements and often simplify or make our code more readable<a data-startref="ix_ch03-asciidoc5" data-type="indexterm" id="idm45606836625264"/>.<a data-startref="ix_ch03-asciidoc4" data-type="indexterm" id="idm45606836624432"/><a data-startref="ix_ch03-asciidoc3" data-type="indexterm" id="idm45606836623728"/> However, we might want to take a more deliberate approach for bigger efficiency impacts, where the result might be less obvious, as explained in the next section.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deliberate Optimizations" data-type="sect2"><div class="sect2" id="ch-conq-opt-deliberate">&#13;
<h2>Deliberate Optimizations</h2>&#13;
&#13;
<p><a data-primary="deliberate optimizations" data-type="indexterm" id="idm45606836620928"/><a data-primary="optimization" data-secondary="deliberate" data-type="indexterm" id="idm45606836620256"/>Beyond waste, we have operations that are critically important for our functionality. In this case, we can say we have a zero-sum game.<sup><a data-type="noteref" href="ch03.html#idm45606836619040" id="idm45606836619040-marker">3</a></sup> This means we have a situation where we cannot eliminate a certain operation that uses resource A (e.g., memory) without using more resource B (e.g., CPU time) or other quality (e.g., readability, portability, or correctness).</p>&#13;
&#13;
<p>The optimizations that are not obvious or require us to make a certain trade-off can be called <em>deliberate</em><sup><a data-type="noteref" href="ch03.html#idm45606836617440" id="idm45606836617440-marker">4</a></sup> since we have to spend a little bit more time on them. We can understand the trade-off, measure or assess it, and decide to keep it or throw it away.</p>&#13;
&#13;
<p>Deliberate optimizations are not worse in any way. On the contrary, they often significantly impact the latency or resource consumption you want to cut. For example, if our request is too slow on a web server, we can consider optimizing latency by introducing a cache. Caching will allow us to save the result from expensive computation for requests asking for the same data. In addition, it saves CPU time and the need to introduce complex parallelization logic. Yet we will sacrifice memory or disk usage during the server’s lifetime and potentially introduce some code complexity. As a result, deliberate optimization might not improve the program’s overall efficiency, but it can improve the efficiency of a particular resource usage that we care about at the moment. Depending on the situation, the sacrifice might be worth it.</p>&#13;
&#13;
<p>However, the implication of having certain sacrifices means we have to perform such optimization in a separate development phase isolated from the functionality one, as explained in <a data-type="xref" href="#ch-conq-eff-flow">“Efficiency-Aware Development Flow”</a>. The reason for this is simple. First, we have to be sure that we understand what we sacrifice and whether the impact is not too big. Unfortunately, humans are quite bad at estimating such impacts.</p>&#13;
&#13;
<p>For example, a common way to reduce network bandwidth and disk usage is to compress the data before sending it or storing it. However, simultaneously it requires us to decompress (decode) when receiving or reading the data. The potential balance of the resources used by our software before and after introducing compression can be seen in <a data-type="xref" href="#img-opt-sum">Figure 3-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-opt-sum">&#13;
<img alt="efgo 0301" src="assets/efgo_0301.png"/>&#13;
<h6><span class="label">Figure 3-1. </span>Potential impact on latency and resource usage if we compress the data before sending it over the network and saving it on disk</h6>&#13;
</div></figure>&#13;
&#13;
<p>The exact numbers will vary, but the CPU resource will potentially be used more after compression addition. Instead of a simple data write operation, we must go through all bytes and compress them. It takes some time, even for the best lossless compression algorithms (e.g., <code>snappy</code> or <code>gzip</code>). Still, a smaller amount of messages to send over the network and disk writes might improve the total latency of such an operation. All of the compression algorithms require some extra buffers, so additional memory usage is also expected.</p>&#13;
&#13;
<p>To sum up, there are strong implications for categorizing optimization reasonably and deliberately. If we see a potential efficiency improvement, we must be aware of its unintended consequences. There might be cases where it’s reasonable and easy to obtain optimization. For example, we might have peeled some unnecessary operations from our program for free. But more often than not, making our software efficient in every aspect is impossible, or we impact other software qualities. This is when we get into a zero-sum game, and we must take a deliberate look at these problems. In this book and practice, you will learn what situations you are in and how to predict these consequences.<a data-startref="ix_ch03-asciidoc2" data-type="indexterm" id="idm45606836607952"/></p>&#13;
&#13;
<p>Before we bring the two types of optimizations into our development flow, let’s discuss the efficiency optimization challenges we must be aware of. We will go through the most important ones in the next section.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Optimization Challenges" data-type="sect1"><div class="sect1" id="ch-conq-challenges">&#13;
<h1>Optimization Challenges</h1>&#13;
&#13;
<p><a data-primary="optimization" data-secondary="fundamental problems/challenges" data-type="indexterm" id="ix_ch03-asciidoc6"/>I wouldn’t need to write this book if optimizing our software was easy. It’s not. The process can be time-consuming and prone to mistakes. This is why many developers tend to ignore this topic or learn it later in their careers. But don’t feel demotivated! Everyone can be an effective and pragmatic efficiency-aware developer after some practice. Knowing about the optimization obstacles should give us a good indication of what we should focus on to improve. Let’s go through some fundamental &#13;
<span class="keep-together">problems:</span></p>&#13;
<dl>&#13;
<dt>Programmers are bad at estimating what part is responsible for the performance &#13;
<span class="keep-together">problem.</span></dt>&#13;
<dd>&#13;
<p>We are really bad at guessing which part of the program consumes the most resources and how much. However, it’s essential to find these problems because, generally, <a href="https://oreil.ly/eZIl5">the Pareto Principle</a> applies. It states that 80% of the time or resources consumed by our program come only from 20% of the operations it performs. Since any optimization is time-consuming, we want to focus on that critical 20% of operations, not some noise. Fortunately, there are tools and methods for estimating this, which we will touch on in <a data-type="xref" href="ch09.html#ch-observability3">Chapter 9</a>.</p>&#13;
</dd>&#13;
<dt>Programmers are notoriously bad at estimating exact resource consumption.</dt>&#13;
<dd>&#13;
<p>Similarly, we often make wrong assumptions on whether certain optimizations should help. Our guesses get better with experience (and hopefully after reading this book). Yet, it’s best to <em>never trust your judgment</em>, and always measure and verify all numbers after deliberate optimizations (discussed in depth in <a data-type="xref" href="ch07.html#ch-observability2">Chapter 7</a>). There are just too many layers in software executions with many unknowns and variables.</p>&#13;
</dd>&#13;
<dt>Maintaining efficiency over time is hard.</dt>&#13;
<dd>&#13;
<p>The complex software execution layers mentioned previously are constantly changing (new versions of operating systems, hardware, firmware, etc.), not to mention the program’s evolution and future developers who might touch your code. We might have spent weeks optimizing one part, but it could be irrelevant if we don’t guard against regressions. There are ways to automate or at least structure the benchmarking and verification process for the efficiency of our program, because things change every day, as discussed in <a data-type="xref" href="ch06.html#ch-observability">Chapter 6</a>.</p>&#13;
</dd>&#13;
<dt>Reliable verification of current performance is very difficult.</dt>&#13;
<dd>&#13;
<p>As we will learn in <a data-type="xref" href="#ch-conq-eff-flow">“Efficiency-Aware Development Flow”</a>, the &#13;
<span class="keep-together">solution to</span> the aforementioned challenges is to benchmark, measure, and validate the efficiency. Unfortunately, these are difficult to perform and prone to errors. There are many reasons: inability to simulate the production environment closely enough, external factors like noisy neighbors, lack of warm-up phase, wrong data sets, or microbenchmark accidental compiler optimizations. This is why we will spend some time on this topic in <a data-type="xref" href="ch07.html#ch-obs-rel">“Reliability of Experiments”</a>.</p>&#13;
</dd>&#13;
<dt>Optimizing can easily impact other software qualities.</dt>&#13;
<dd>&#13;
<p>Solid software is great at many qualities: functionality, compatibility, usability, reliability, security, maintainability, portability, and efficiency. Each of these characteristics is nontrivial to get right, so they cause some cost to the development process. The importance of each can differ depending on your use cases. However, there are safe minimums of each software quality to be maintained for your program to be useful. This might be challenging when you add more features and optimization.</p>&#13;
</dd>&#13;
<dt>Specifically, in Go we don’t have strict control over memory management.</dt>&#13;
<dd>&#13;
<p>As we learned in <a data-type="xref" href="ch02.html#ch-go-runtime">“Go Runtime”</a>, Go is garbage-collected language. While it’s lifesaving for the simplicity of our code, memory safety, and developer velocity, it has downsides that can be seen when we want to be memory efficient. There are ways to improve our Go code to use less memory, but things can get tricky since the memory release model is eventual. Usually, the solution is simply to allocate less. We will go through memory management in <a data-type="xref" href="ch05.html#ch-hw-memory">“Do We Have a Memory Problem?”</a>.</p>&#13;
</dd>&#13;
<dt>When is our program efficient “enough”?</dt>&#13;
<dd>&#13;
<p>In the end, all optimizations are never fully free. They require a bigger or smaller effort from the developer. Both reasonable and deliberate optimizations require prior knowledge and time spent on implementation, experimentations, testing, and benchmarking. Given that, we need to find justification for this effort. Otherwise, we can spend this time somewhere else. Should we optimize away this waste? Should we trade the consumption of resource X for resource Y? Is such conversion useful for us? The answer might be “no.” And if “yes,” how much efficiency improvement is enough?</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Regarding the last point, this is why it’s extremely important to know your goals. What things, resources, and qualities do you (or your boss) care about during the development? It can vary depending on what you build. In the next section, I &#13;
<span class="keep-together">will propose</span> a pragmatic way of stating performance requirements for a piece of<a data-startref="ix_ch03-asciidoc6" data-type="indexterm" id="idm45606836583024"/> &#13;
<span class="keep-together">software.</span></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Understand Your Goals" data-type="sect1"><div class="sect1" id="ch-conq-perf-goal">&#13;
<h1>Understand Your Goals</h1>&#13;
<blockquote>&#13;
<p>Before you proceed toward such lofty goals [program efficiency optimization], you should examine your reasons for doing so. Optimization is one of many desirable goals in software engineering and is often antagonistic to other important goals such as stability, maintainability, and portability. At its most cursory level (efficient implementation, clean non-redundant interfaces), optimization is beneficial and should always be applied. But at its most intrusive (inline assembly, pre-compiled/self-modified code, loop unrolling, bit-fielding, superscalar and vectorizing) it can be an unending source of time-consuming implementation and bug hunting. Be cautious and wary of the cost of optimizing your code.</p>&#13;
<p data-type="attribution">Paul Hsieh, <a href="https://oreil.ly/PQ4pk">“Programming Optimization”</a></p>&#13;
</blockquote>&#13;
&#13;
<p><a data-primary="efficiency (generally)" data-secondary="understanding goals" data-type="indexterm" id="ix_ch03-asciidoc7"/><a data-primary="goals, understanding of" data-type="indexterm" id="ix_ch03-asciidoc8"/><a data-primary="optimization" data-secondary="understanding goals" data-type="indexterm" id="ix_ch03-asciidoc9"/>By <a data-primary="Hsieh, Paul, on cost of optimization" data-type="indexterm" id="idm45606836573744"/>our definition, efficiency optimization improves our program resource consumption or latency. It’s highly addictive to challenge ourselves and explore how fast our program can be.<sup><a data-type="noteref" href="ch03.html#idm45606836572944" id="idm45606836572944-marker">5</a></sup> First, however, we need to understand that optimization aims to not make our program perfectly efficient or “optimal” (as that might be simply impossible or feasible) but rather suboptimal enough. But what does “enough” mean for us? When do you stop? What if there isn’t a need to even start optimizing?</p>&#13;
&#13;
<p><a data-primary="optimization" data-secondary="stakeholders and" data-type="indexterm" id="idm45606836571056"/><a data-primary="stakeholders, optimization requests from" data-type="indexterm" id="idm45606836570080"/>One answer is to optimize when stakeholders (or users) ask for better efficiency in the software we develop until they are happy. But unfortunately, this is usually very difficult for a few reasons:</p>&#13;
<dl>&#13;
<dt><a href="https://oreil.ly/AolRQ">XY problem</a>.</dt>&#13;
<dd>&#13;
<p>Stakeholders often ask for better efficiency, whereas a better solution is elsewhere. For example, many people complain about the heavy memory usage of the metric system if they try to monitor unique events. Instead, the potential solution might be to use logging or tracing systems for such data instead of making the metric system faster.<sup><a data-type="noteref" href="ch03.html#idm45606836566448" id="idm45606836566448-marker">6</a></sup> As a result, we can’t always trust the initial user requests, especially around efficiency.</p>&#13;
</dd>&#13;
<dt>Efficiency is not a zero-sum game.</dt>&#13;
<dd>&#13;
<p>Ideally, we need to see the big picture of all efficiency goals. As we learned in <a data-type="xref" href="#ch-conq-opt-deliberate">“Deliberate Optimizations”</a>, one optimization for latency might cause more memory usage or impact other resources, so we can’t react to every user complaint about efficiency without thinking. Of course, it helps when software is generally lean and efficient, but most likely we can’t produce a single software that satisfies both the user who needs a latency-sensitive real-time event-capturing solution and the user who needs ultra-low memory used during such an operation.</p>&#13;
</dd>&#13;
<dt>Stakeholders might not understand the optimization cost.</dt>&#13;
<dd>&#13;
<p>Everything costs, especially optimization effort and maintaining highly optimized code. Technically speaking, only physics laws limit us on how optimized software can be.<sup><a data-type="noteref" href="ch03.html#idm45606836561456" id="idm45606836561456-marker">7</a></sup> At some point, however, the benefit we gain from optimization versus the cost of finding and developing such optimization is impractical. Let’s expand on the last point.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p><a data-type="xref" href="#img-opt-cost">Figure 3-2</a> shows a typical correlation between the efficiency of the software and different costs.</p>&#13;
&#13;
<figure><div class="figure" id="img-opt-cost">&#13;
<img alt="efgo 0302" src="assets/efgo_0302.png"/>&#13;
<h6><span class="label">Figure 3-2. </span>Beyond the “sweet spot,” the cost of gaining higher efficiency might be extremely high</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#img-opt-cost">Figure 3-2</a> explains why at some “sweet spot” point, it might not be feasible to invest more time and resources in our software efficiency. Beyond some point, the cost of optimizing and developing optimized code can quickly surpass the benefits we get from leaner software, like computational cost and opportunities. We might need to spend exponentially more of the expensive developer time, and need to introduce clever, nonportable tricks, dedicated machine code, dedicated operating systems, or even specialized hardware.</p>&#13;
&#13;
<p>In many cases, optimizations beyond the sweet spot aren’t worth it, and it might be better to design a different system or use other flows to avoid such work. Unfortunately, there is also no single answer to where the sweet spot is. Typically, the longer the lifetime planned for the software, the larger its deployment is, and the more investment is worth putting into it. On the other hand, if you plan to use your program only a few short times, your sweet spot might be at the beginning of this diagram, with very poor efficiency.</p>&#13;
&#13;
<p>The problem is that users and stakeholders will not be aware of this. While ideally, product owners help us find that out, it’s often the developer’s role to advise the level of those different costs, using tools we will learn in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.html#ch-observability">6</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch07.html#ch-observability2">7</a>.</p>&#13;
&#13;
<p>However, whatever numbers we agree on, the best idea to solve the “when is enough” problem and have clear efficiency requirements is to write them down. In the next section, I will explain why. In <a data-type="xref" href="#ch-conq-req">“Resource-Aware Efficiency Requirements”</a>, I will introduce the lightweight formula for them. Then in <a data-type="xref" href="#ch-conq-acquiring-raer">“Acquiring and Assessing Efficiency Goals”</a>, we will discuss how to acquire and assess those &#13;
<span class="keep-together">efficiency requirements.</span></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Efficiency Requirements Should Be Formalized" data-type="sect2"><div class="sect2" id="ch-conq-req-formal">&#13;
<h2>Efficiency Requirements Should Be Formalized</h2>&#13;
&#13;
<p><a data-primary="efficiency (generally)" data-secondary="formalizing of requirements" data-type="indexterm" id="ix_ch03-asciidoc10"/><a data-primary="FR (functional requirements) stage" data-type="indexterm" id="ix_ch03-asciidoc11"/><a data-primary="functional requirements (FR) stage" data-type="indexterm" id="ix_ch03-asciidoc12"/><a data-primary="goals, understanding of" data-secondary="formalizing of efficiency requirements" data-type="indexterm" id="ix_ch03-asciidoc13"/><a data-primary="optimization" data-secondary="formalizing of efficiency requirements" data-type="indexterm" id="ix_ch03-asciidoc14"/>As you probably already know, every software development starts with the functional requirements gathering stage (FR stage). An architect, product manager, or yourself has to go through potential stakeholders, interview them, gather use cases and, ideally, write them down in some functional requirements document. The development team and stakeholders then review and negotiate functionality details in this document. The FR document describes what input your program should accept, and what behavior and output a user expects. It also mentions prerequisites, like what operating systems the application is meant to be running on. Ideally, you get formal approval on the FR document, and it becomes your “contract” between both parties. Having this is extremely important, especially when you are compensated for building the software:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>FR tells developers what they should focus on. It tells you what inputs should be valid and what things a user can configure. It dictates what you should focus on. Are you spending your time on something stakeholders paid for?</p>&#13;
</li>&#13;
<li>&#13;
<p>It’s easier to integrate with software with a clear FR. For example, stakeholders might want to design or order further system pieces that will be compatible with your software. They can start doing this before your software is even finished!</p>&#13;
</li>&#13;
<li>&#13;
<p>FR enforces clear communication. Ideally, the FR is written and formal. This is helpful, as people tend to forget things, and it’s easy to miscommunicate. That’s why you write it all down and ask stakeholders for review. Maybe you misheard something?</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>You do formal functional requirements for bigger systems and features. For a smaller piece of software, you tend to write them up for some issue in your backlog, e.g., &#13;
<span class="keep-together">GitHub</span> or GitLab issues, and then document them. Even for tiny scripts or little programs, set some goals and prerequisites—maybe a specific environment (e.g., Python version) and some dependencies (GPU on the machine). When you want others to use it effectively, you have to mention your software’s functional requirements and goals.</p>&#13;
&#13;
<p>Defining and agreeing on functional requirements is well adopted in the software industry. Even if a bit bureaucratic, developers tend to like those specifications because it makes their life easier—requirements are then more stable and specific.</p>&#13;
&#13;
<p>Probably you know where I am going with this. Surprisingly, we often neglect to define similar requirements focused on the more nonfunctional aspects of the software we are expected to build, for example, describing a required efficiency and speed of the desired functionality.<sup><a data-type="noteref" href="ch03.html#idm45606836534640" id="idm45606836534640-marker">8</a></sup></p>&#13;
&#13;
<p><a data-primary="non-functional requirement (NFR) documentation" data-type="indexterm" id="idm45606836532992"/>Such efficiency requirements are typically part of the <a href="https://oreil.ly/AQWLm">nonfunctional requirement (NFR)</a> documentation or specification. Its gathering process ideally should be similar to the FR process, but for all other qualities requested, software should have: portability, maintainability, extensibility, accessibility, operability, fault tolerance and reliability, compliance, documentation, execution efficiency, and so on. The list is long.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<p>The NFR name can be in some way misleading since many qualities, including efficiency, massively impact our software functionality. As we learned in <a data-type="xref" href="ch01.html#ch-efficiency-matters">Chapter 1</a>, efficiency and speed are critical for user experience.</p>&#13;
</div>&#13;
&#13;
<p>In reality, NFRs are not very popular to use during software development, based on my experience and research. I found multiple reasons:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Conventional NFR specification is considered bureaucratic and full of boilerplate. Especially if the mentioned qualities are not quantifiable and not specific, NFR for every software will look obvious and more or less similar. Of course, all software should be readable, maintainable, as fast as possible using minimum resources, and usable. This is not helpful.</p>&#13;
</li>&#13;
<li>&#13;
<p>There are no easy-to-use, open, and accessible standards for this process. The most popular <a href="https://oreil.ly/IzqJo">ISO/IEC 25010:2011 standard</a> costs around $200 to read. It has a staggering 34 pages, and hasn’t been changed since the last revision in 2017.</p>&#13;
</li>&#13;
<li>&#13;
<p>NFRs are usually too complex to be applicable in practice. For example, the ISO/IEC 25010 standard previously mentioned specifies <a href="https://oreil.ly/0MMcb">13 product characteristics with 42 subcharacteristics in total</a>. It is hard to understand and takes too much time to gather and walk through.</p>&#13;
</li>&#13;
<li>&#13;
<p>As we will learn in <a data-type="xref" href="#ch-conq-opt-levels">“Optimization Design Levels”</a>, our software’s speed and execution efficiency depend on more factors than our code. The typical developer usually can impact the efficiency by optimizing algorithms, code, and compiler. It’s then up to the operator or admin to install that software, fit it into a bigger system, configure it, and provide the operating system and hardware for that workload. When developers are not in the domain of running their software on “production,” it’s hard for them to talk about runtime efficiency.</p>&#13;
<div data-type="tip"><h1>The SRE Domain</h1>&#13;
<p><a data-primary="Site Reliability Engineering (SRE)" data-type="indexterm" id="idm45606836520016"/><a href="https://sre.google">Site Reliability Engineering (SRE)</a> introduced by Google is a role focused on marrying these two domains: software development and operators/administrators. Such engineers have experience running and building their software on a large scale. With more hands-on experience, it’s easier to talk about efficiency requirements.</p>&#13;
</div>&#13;
</li>&#13;
<li>&#13;
<p>Last but not least, we are humans and full of emotions. Because it’s hard to estimate the efficiency of our software, especially in advance, it’s not uncommon to feel humiliated when setting efficiency or speed goals. This is why we sometimes unconsciously refrain from agreeing to quantifiable performance goals. It can be uncomfortable, and that’s normal.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>OK, scratch that, we aren’t going there. We need something more pragmatic and easier to work with. Something that will state our rough goals for efficiency and speed of the requested software and will be a starting point for some contracts between consumers and the development team. Having such efficiency requirements on top of functional ones up front is enormously helpful because:</p>&#13;
<dl>&#13;
<dt>We know exactly how fast or resource efficient our software has to be.</dt>&#13;
<dd>&#13;
<p>For instance, let’s say we agree that a certain operation should use 1 GB of memory, 2 CPU seconds, and take 2 minutes at maximum. If our tests show that it takes 2 GB of memory and 1 CPU second for 1 minute, then there is no point in optimizing latency.</p>&#13;
</dd>&#13;
<dt>We know if we have room for a trade-off or not.</dt>&#13;
<dd>&#13;
<p>In the preceding example, we can precalculate or compress things to improve memory efficiency. We still have 1&#13;
CPU second to spare, and we can be slower for 1 minute.</p>&#13;
</dd>&#13;
<dt>Without official requirements, users will implicitly assume some efficiency expectations.</dt>&#13;
<dd>&#13;
<p>For example, maybe our program was accidentally very fast for a certain &#13;
<span class="keep-together">input. Users</span> can assume this is by design, and they will depend on the fact in the &#13;
<span class="keep-together">future, or</span> for other parts of the systems. This can lead to poor user experience and surprises.<sup><a data-type="noteref" href="ch03.html#idm45606836510192" id="idm45606836510192-marker">9</a></sup></p>&#13;
</dd>&#13;
<dt>It’s easier to use your software in a bigger system.</dt>&#13;
<dd>&#13;
<p>More often than not, your software will be a dependency on another piece of software and form a bigger system. Even a basic efficiency requirements document can tell system architects what to expect from the component. It can help enormously with further system performance assessments and capacity planning tasks.</p>&#13;
</dd>&#13;
<dt>It’s easier to provide operational support.</dt>&#13;
<dd>&#13;
<p>When users do not know what performance to expect from your software, you will have difficulty supporting it over time. There will be many back-and-forths with the user on what is acceptable efficiency and what’s not. Instead, with clear efficiency requirements, it is easier to tell if your software was underutilized or not, and as a result, the issue might be on the user side.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Let’s summarize our situation. We know efficiency requirements can be enormously useful. On the other hand, we also know they can be tedious and full of boilerplate. So let’s explore some options and see if we can find some balance between the requirement gathering effort and the value it brings.<a data-startref="ix_ch03-asciidoc14" data-type="indexterm" id="idm45606836504480"/><a data-startref="ix_ch03-asciidoc13" data-type="indexterm" id="idm45606836503776"/><a data-startref="ix_ch03-asciidoc12" data-type="indexterm" id="idm45606836503104"/><a data-startref="ix_ch03-asciidoc11" data-type="indexterm" id="idm45606836502432"/><a data-startref="ix_ch03-asciidoc10" data-type="indexterm" id="idm45606836501760"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resource-Aware Efficiency Requirements" data-type="sect2"><div class="sect2" id="ch-conq-req">&#13;
<h2>Resource-Aware Efficiency Requirements</h2>&#13;
&#13;
<p><a data-primary="goals, understanding of" data-secondary="Resource-Aware Efficiency Requirements" data-type="indexterm" id="ix_ch03-asciidoc15"/><a data-primary="optimization" data-secondary="Resource-Aware Efficiency Requirements" data-type="indexterm" id="ix_ch03-asciidoc16"/><a data-primary="Resource-Aware Efficiency Requirements (RAER)" data-secondary="about" data-type="indexterm" id="ix_ch03-asciidoc17"/>No one has defined a good standard process for creating efficiency requirements, so let’s try to <a href="https://oreil.ly/DCzpu">define one</a>! Of course, we want it to be as lightweight a process as possible, but let’s start with the ideal situation. What is the perfect set of information someone could put into some Resource-Aware Efficiency Requirements (RAER) document? Something that will be more specific and actionable than “I want this program to run adequately snappy.”</p>&#13;
&#13;
<p>In <a data-type="xref" href="#code-opt-raer">Example 3-1</a>, you can see an example of a data-driven, minimal RAER for a single operation in some software.</p>&#13;
<div class="less_space pagebreak-before" data-type="example" id="code-opt-raer">&#13;
<h5><span class="label">Example 3-1. </span>The example RAER entry</h5>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">Program: "The Ruler"&#13;
Operation: "Fetching alerting rules for one tenant from the storage using HTTP."&#13;
Dataset: "100 tenants having 1000 alerting rules each."&#13;
&#13;
Maximum Latency: "2s  for 90th percentile"&#13;
CPU Cores Limit: "2"&#13;
Memory Limit: "500 MB"&#13;
Disk Space Limit: "1 GB"&#13;
...</pre></div>&#13;
&#13;
<p>Ideally, this RAER is a set of records with efficiency requirements for certain operations. In principle, a single record should have information like:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The operation, API, method, or function it relates to.</p>&#13;
</li>&#13;
<li>&#13;
<p>The size and shape dataset we operate on, e.g., input or data stored (if any).</p>&#13;
</li>&#13;
<li>&#13;
<p>Maximum latency of the operation.</p>&#13;
</li>&#13;
<li>&#13;
<p>The resource consumption budget for this operation on that dataset, e.g., memory, disk, network bandwidth, etc.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Now, there is bad news and good news. The bad news is that, strictly speaking, such records are unrealistic to gather for all small operations. This is because:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>There are potentially hundreds of different operations that run during the software execution.</p>&#13;
</li>&#13;
<li>&#13;
<p>There is an almost infinite number of dataset shapes and sizes (e.g., imagine an SQL query being an input, and stored SQL data being a dataset: we have a near-infinite amount of option permutations).</p>&#13;
</li>&#13;
<li>&#13;
<p>Modern hardware with an operating system has thousands of elements that can be “consumed” when we execute our software. Overall, CPU seconds and memory are common, but what about the space and bandwidth of individual CPU caches, memory bus bandwidth, number of TCP sockets taken, file descriptors used, and thousands of other elements? Do we have to specify all that can be used?</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The good news is that we don’t need to provide all the small details. This is similar to how we deal with functional requirements. Do we focus on all possible user stories and details? No, just the most important ones. Do we define all possible permutations of valid inputs and expected outputs? No, we only define a couple of basic characteristics around boundaries (e.g., information has to be a positive integer). Let’s look at how we can simplify the level of details of the RAER entry:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Focus on the most utilized and expensive operations our software does first. These will impact the software resource usage the most. We will discuss benchmarking and profiling that will help you with this later in this book.</p>&#13;
</li>&#13;
<li>&#13;
<p>We don’t need to outline requirements for all tiny resources that might be consumed. Start with those that have the highest impact and matter the most. Usually, it means specific requirements toward CPU time, memory space, and storage (e.g., disk space). From there, we can iterate and add other resources that will matter in the future. Maybe our software needs some unique, expensive, and hard-to-find resources that are worth mentioning (e.g., GPU). Maybe a certain consumption poses a limit to overall scalability, e.g., we could fit more processes on a single machine if our operation would use fewer TCP sockets or disk IOPS. Add them only if they matter.</p>&#13;
</li>&#13;
<li>&#13;
<p>Similar to what we do in unit tests when validating functionality, we can focus only on important categories of inputs and datasets. If we pick edge cases, we have a high chance of providing resource requirements for the worst- and best-case datasets. That is an enormous win already.</p>&#13;
</li>&#13;
<li>&#13;
<p>Alternatively, there is a way to define the relation of input (or dataset) to the allowed resource consumption. <a data-primary="complexity, in RAER context" data-type="indexterm" id="idm45606836470144"/>We can then describe this relation in the form of mathematical functions, which we usually call <em>complexity</em> (discussed in <a data-type="xref" href="ch07.html#ch-hw-algo-bigo">“Asymptotic Complexity with Big O Notation”</a>). Even with some approximation, it’s quite an effective method. Our RAER for the operation <code>/rules</code> in <a data-type="xref" href="#code-opt-raer">Example 3-1</a> could then be described, as seen in <a data-type="xref" href="#code-opt-raer-func">Example 3-2</a>.</p>&#13;
</li>&#13;
</ul>&#13;
<div data-type="example" id="code-opt-raer-func">&#13;
<h5><span class="label">Example 3-2. </span>The example RAER entry with complexities or throughput instead of absolute numbers</h5>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">Program: "The Ruler"&#13;
Operation: "Fetching alerting rules for one tenant from the storage using HTTP."&#13;
Dataset: "X tenants having Y alerting rules each."&#13;
&#13;
Maximum Latency: "2*Y ms for 90th percentile"&#13;
CPU Cores Limit: "2"&#13;
Memory Limit: "X + 0.4 * Y MB"&#13;
Disk Space Limit: "0.1 * X GB"&#13;
...</pre></div>&#13;
&#13;
<p>Overall, I would even propose to include the RAER in the functional requirement (FR) document mentioned previously. Put it in another section called “Efficiency Requirements.” After all, without rational speed and efficiency, our software can’t be called fully functional, can it?</p>&#13;
&#13;
<p class="less_space pagebreak-before">To sum up, in this section we defined the Resource-Aware Efficiency Requirements specification that gives us approximations of the needs and expected performance toward our software efficiency. It will be extremely helpful for the further development and optimization techniques we learn in this book. Therefore, I want to encourage you to understand the performance you aim for, ideally before you start developing your software and optimizing or adding more features to it.</p>&#13;
&#13;
<p>Let’s explain how we can possess or create such RAERs ourselves for the system, application, or function we aim to provide.<a data-startref="ix_ch03-asciidoc17" data-type="indexterm" id="idm45606836439856"/><a data-startref="ix_ch03-asciidoc16" data-type="indexterm" id="idm45606836439472"/><a data-startref="ix_ch03-asciidoc15" data-type="indexterm" id="idm45606836438976"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Acquiring and Assessing Efficiency Goals" data-type="sect2"><div class="sect2" id="ch-conq-acquiring-raer">&#13;
<h2>Acquiring and Assessing Efficiency Goals</h2>&#13;
&#13;
<p><a data-primary="efficiency (generally)" data-secondary="acquiring/assessing goals" data-type="indexterm" id="idm45606836436928"/><a data-primary="goals, understanding of" data-secondary="acquiring/assessing efficiency goals" data-type="indexterm" id="idm45606836433584"/><a data-primary="optimization" data-secondary="acquiring/assessing efficiency goals" data-type="indexterm" id="idm45606836432672"/>Ideally, when you come to work on any software project, you have something like a RAER already specified. In bigger organizations, you might have dedicated people like project or product managers who will gather such efficiency requirements on top of functional requirements. They should also make sure the requirements are possible to fulfill. If they don’t gather the RAER, don’t hesitate to ask them to provide such information. It’s often their job to give it.</p>&#13;
&#13;
<p>Unfortunately, in most cases, there are no specific efficiency requirements, especially in smaller companies, community-driven projects, or, obviously, your personal projects. In those cases, we need to acquire the efficiency goals ourselves. How do we start?</p>&#13;
&#13;
<p>This task is, again, similar to functional goals. We need to bring value to users, so ideally, we need to ask them what they need in terms of speed and running costs. So we go to the stakeholders or customers and ask what they need in terms of efficiency and speed, what they are willing to pay for, and what the constraints are on their side (e.g., the cluster has only four servers or the GPU has only 512 MB of internal memory). Similarly, with features, good product managers and developers will try to translate user performance needs into efficiency goals, which is not trivial if the stakeholders are not from the engineering space. For example, the “I want this application to run fast” statement has to be translated into specifics.</p>&#13;
<div data-type="tip">&#13;
<p>If the stakeholder can’t give the latency numbers they might expect from your software, just pick a number. It can be high for a start, which is great for you, but it will make your life easier later. Perhaps this will trigger discussions on the stakeholder side on the implications of that number.</p>&#13;
</div>&#13;
&#13;
<p class="less_space pagebreak-before">Very often, there are multiple personas of the system users too. For example, let’s imagine our company will run our software as a service for the customer, and the service has already defined a price. In this case, the user cares about the speed and correctness, and our company will care about the efficiency of the software, as this translates to how much net profit the running service will have (or loss if the &#13;
<span class="keep-together">computation</span> cost of running our software is too large). In this typical software as a service (SaaS) example, we have not one but two sources of input for our RAER.</p>&#13;
<div data-type="note" epub:type="note"><h1>Dogfooding</h1>&#13;
<p><a data-primary="dogfooding" data-type="indexterm" id="idm45606836426480"/>Very often, for smaller coding libraries, tools, and our infrastructure software, we are both developers and users. In this case, setting RAERs from the user’s perspective is much easier. That is only one of the reasons why using the software you create is a <a href="https://oreil.ly/xBgef">good practice</a>. This approach is often called “eating your own dog food” (dogfooding).</p>&#13;
</div>&#13;
&#13;
<p>Unfortunately, even if a user is willing to define the RAER, the reality is not so perfect. Here comes the difficult part. Are we sure that what was proposed from the user perspective is doable within the expected amount of time? We know the demand, but we must validate it with the supply we can provide regarding our team skill set, technological possibilities, and time needed. Usually, even if some RAER is given, we need to perform our own diligence and define or assess the RAER from an achievability perspective. This book will teach you all that is required to accomplish this task.</p>&#13;
&#13;
<p>In the meantime, let’s go through one example of the RAER definition process.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Example of Defining RAER" data-type="sect2"><div class="sect2" id="example-defining-raer">&#13;
<h2>Example of Defining RAER</h2>&#13;
&#13;
<p><a data-primary="efficiency (generally)" data-secondary="defining/assessing requirements" data-type="indexterm" id="ix_ch03-asciidoc18"/><a data-primary="Resource-Aware Efficiency Requirements (RAER)" data-secondary="defining/assessment example" data-type="indexterm" id="ix_ch03-asciidoc19"/>Defining and assessing complex RAERs can get complicated. However, starting with potentially trivial yet clear requirements is reasonable if you have to do it from scratch.</p>&#13;
&#13;
<p>Setting these requirements boils down to the user perspective. We need to find the minimum requirements that make your software valuable in its context. For example, let’s say we need to create software that applies image enhancements on top of a set of images in JPEG format. In RAER, we can now treat such image transforming as an <em>operation</em>, and the set of image files and chosen enhancement as our <em>input</em>.</p>&#13;
&#13;
<p><a data-primary="latency" data-secondary="in RAER" data-secondary-sortas="RAER" data-type="indexterm" id="idm45606836397920"/>The second item in our RAER is the latency of our operation. It is better to have it as fast as possible from a user perspective. Yet our experience should tell us that there are limits on how quickly we can apply the enhancement to images (especially if large and many). But how can we find a reasonable latency number requirement that would work for potential users and make it possible for our software?</p>&#13;
&#13;
<p>It’s not easy to agree on a single number, especially when we are new to the efficient world. For example, we could potentially guess that 2 hours for a single image process might be too long, and 20 nanoseconds is not achievable, but it’s hard to find the middle ground here. Yet as mentioned in <a data-type="xref" href="#ch-conq-req-formal">“Efficiency Requirements Should Be Formalized”</a>, I would encourage you to try defining one number, as it would make your software much easier to assess!</p>&#13;
<div data-type="note" epub:type="note"><h1>Defining Efficiency Requirements Is Like Negotiating Salary</h1>&#13;
<p>Agreeing to someone’s compensation for their work is similar to finding the requirement sweet spot for our program’s latency or resource usage. The candidate wants the salary to be the highest possible. As an employer, you don’t want to overpay. It’s also hard to assess the value the person will be providing and how to set meaningful goals for such work. What works in salary negotiating works when defining RAER: don’t set too high expectations, look at other competitors, negotiate, and have trial periods!</p>&#13;
</div>&#13;
&#13;
<p>One way to define RAER details like latency or resource consumption is to check the competition. Competitors are already stuck in some kind of limits and framework for stating their efficiency guarantees. You don’t need to set those as your numbers, but they can give you some clue of what’s possible or what customers want.</p>&#13;
&#13;
<p>While useful, checking competition is often not enough. Eventually, we have to estimate what’s roughly possible with the system and algorithm we have in mind and the modern hardware. We can start by defining the initial naive algorithm. We can assume our first algorithm won’t be the most efficient, but it will give us a good start on what’s achievable with little effort. For example, let’s assume for our problem that we want to read an image in JPEG format from disk (SSD), decode it to memory, apply enhancement, encode it back, and write it to disk.</p>&#13;
&#13;
<p>With the algorithm, we can start discussing its potential efficiency. However, as &#13;
<span class="keep-together">you will</span> learn in <a data-type="xref" href="#ch-conq-opt-levels">“Optimization Design Levels”</a> and&#13;
<a data-type="xref" href="ch07.html#ch-obs-rel">“Reliability of Experiments”</a>, efficiency depends on many factors! It’s tough to measure it on &#13;
<span class="keep-together">an existing system,</span> not to mention forecasting it just from the unimplemented &#13;
<span class="keep-together">algorithm.</span></p>&#13;
&#13;
<p>This is where the complexity analysis with napkin math comes into play!</p>&#13;
<div data-type="note" epub:type="note"><h1>Napkin Math</h1>&#13;
<p><a data-primary="napkin math" data-type="indexterm" id="idm45606836386640"/>Sometimes referred to as back-of-the-envelope calculation, <em>napkin math</em> is a technique of making rough calculations and estimations based on simple, theoretical assumptions. For example, we could assume latency for certain operations in computers, e.g., a sequential read of 8 KB from SSD is taking approximately 10 μs while writing 1 ms.<sup><a data-type="noteref" href="ch03.html#idm45606836385424" id="idm45606836385424-marker">10</a></sup> With that, we could calculate how long it takes to read and write 4 MB of sequential data. Then we can go from there and calculate overall latency if we make a few reads in our system, etc.</p>&#13;
&#13;
<p>Napkin math is only an estimate, so we need to treat it with a grain of salt. Sometimes it can be intimidating to do since it all feels abstract. Yet such quick calculation is always a fantastic test on whether our guesses and initial system ideas are correct. It gives early feedback worth our time, especially around common efficiency requirements like latency, memory, or CPU usage.</p>&#13;
</div>&#13;
&#13;
<p>We will discuss both complexity analysis and napkin math in detail in <a data-type="xref" href="ch07.html#ch-hw-complexity">“Complexity Analysis”</a>, but let’s quickly define the initial RAER for our example JPEG enhancement problem space.</p>&#13;
&#13;
<p><a data-primary="complexity, in RAER context" data-type="indexterm" id="idm45606836381296"/>Complexity allows us to represent efficiency as the function of the latency (or resource usage) to the input. What’s our input for the RAER discussion? Assume the worst case first. Find the slowest part of your system and what input can trigger that. In our example, we can imagine that the largest image we allow in our input (e.g., 8K resolution) is the slowest to process. The requirement of processing a set of images makes things a bit tricky. For now, we can assume the worst case and start negotiating with that. The worst case is that images are different, and we don’t use concurrency. This means our latency will potentially be a function of <em>x</em> * <em>N</em>, where <em>x</em> is the latency of the biggest image, and <em>N</em> is the number of images in the set.</p>&#13;
&#13;
<p>Given the worst-case input of an 8K image in JPEG format, we can try to estimate the complexities. The size of the input depends on the number of unique colors, but most of the images I found were around 4 MB, so let’s have this number represent our average input size. Using data from <a data-type="xref" href="app01.html#appendix-napkin-math">Appendix A</a>, we can calculate that such input will take at least 5 ms to read and 0.5 s to save on a disk. Similarly, encoding and decoding from JPEG format likely means at least looping through and allocating up to 7680 × 4320 pixels (around 33 million) in memory. Looking at the <a href="https://oreil.ly/3Fnbz"><code>image/jpeg</code> standard Go library</a>, each pixel is represented by three <a href="https://oreil.ly/JmgZf"><code>uint8</code> numbers</a> to represent color in <a href="https://oreil.ly/lWiTf">YCbCr format</a>. That means approx 100 million unsigned 8-byte integers. We can then find out both the potential runtime and space complexities:</p>&#13;
<dl>&#13;
<dt>Runtime</dt>&#13;
<dd>&#13;
<p>We need to fetch each element from memory (~5 ns for a sequential read from RAM) twice (one for decode, one for encode), which means 2 * 100 million * 5 ns, so 1 second. As a result of this quick math, we now know that without applying any enhancements or more tricky algorithms, such an operation for the single image will be no faster than 1s + 0.5s, so 1.5 seconds.</p>&#13;
&#13;
<p>Since napkin math is only an estimate, plus we did not account for the actual enhancing operation, it would be safe to assume we are wrong up to three times. This means we could use 5 seconds as the initial latency requirement for a single image to be safe, so 5 * <em>N</em> seconds for <em>N</em> images.</p>&#13;
</dd>&#13;
<dt>Space</dt>&#13;
<dd>&#13;
<p>For the naive algorithm that reads the whole image to memory, storing that image will probably be the operation that allocates the most memory. With the mentioned three <code>uint8</code> numbers per pixel, we have 33 million * 3 * 8 bytes, so a maximum of 755 MB of memory usage.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>We assumed typical cases and unoptimized algorithms, so we expect to be able to improve those initial numbers. But it might as well be fine for the user to wait 50 seconds for 10 images and use 1 GB of memory on each image. Knowing those numbers allows descoping efficiency work when possible!</p>&#13;
&#13;
<p>To be more confident of the calculations we did, or if you are stuck in napkin math calculations, we could perform a quick benchmark<sup><a data-type="noteref" href="ch03.html#idm45606836367744" id="idm45606836367744-marker">11</a></sup> for the critical, slowest operation in our system. So I wrote a single benchmark for reading, decoding, encoding, and saving 8K images using the standard Go <code>jpeg</code> library. <a data-type="xref" href="#bench-enhance">Example 3-3</a> shows the summarization of the benchmark results.</p>&#13;
<div data-type="example" id="bench-enhance">&#13;
<h5><span class="label">Example 3-3. </span>Go microbenchmark results of reading, decoding, encoding, and saving an 8K JPEG file</h5>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">name       time/op&#13;
DecEnc-12  1.56s ±2%&#13;
name       alloc/op&#13;
DecEnc-12  226MB ± 0%&#13;
name       allocs/op&#13;
DecEnc-12   18.8 ±3%</pre></div>&#13;
&#13;
<p>It turns out that our runtime calculations were quite accurate. It takes 1.56 seconds on average to perform a basic operation on an 8K image! However, the allocated memory is over three times better than we thought. Closer inspection of the <a href="https://oreil.ly/lm3T4"><code>YCbCr struct's comment</code></a> reveals that this type stores on <code>Y</code> sample per pixel, but each <code>Cb</code> and <code>Cr</code> sample can span over one or more pixels, which might explain the difference.</p>&#13;
&#13;
<p>Acquiring and assessing RAERs seems complex, but I recommend doing the exercise and getting those numbers before any serious development. Then, with benchmarking and napkin math, we can quickly understand if the RAERs are achievable with the rough algorithm we have in mind. The same process can also be used to tell if there is room for more easy-to-achieve optimization, as described in <a data-type="xref" href="#ch-conq-opt-levels">“Optimization Design Levels”</a>.</p>&#13;
&#13;
<p>With the ability to obtain, define, and assess your RAER, we can finally attempt to conquer some efficiency issues! In the next section, we will discuss steps I would recommend to handle such sometimes stressful situations professionally<a data-startref="ix_ch03-asciidoc19" data-type="indexterm" id="idm45606836357072"/><a data-startref="ix_ch03-asciidoc18" data-type="indexterm" id="idm45606836356656"/>.<a data-startref="ix_ch03-asciidoc9" data-type="indexterm" id="idm45606836355968"/><a data-startref="ix_ch03-asciidoc8" data-type="indexterm" id="idm45606836352560"/><a data-startref="ix_ch03-asciidoc7" data-type="indexterm" id="idm45606836351888"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Got an Efficiency Problem? Keep Calm!" data-type="sect1"><div class="sect1" id="ch-conq-issue-handling">&#13;
<h1>Got an Efficiency Problem? Keep Calm!</h1>&#13;
&#13;
<p><a data-primary="efficiency (generally)" data-secondary="reacting to efficiency problems" data-type="indexterm" id="ix_ch03-asciidoc20"/>First of all, don’t panic! We all have been there. We wrote a piece of code and tested it on our machine, which worked great. Then, proud of it, we released it to others, and immediately someone reported performance issues. Maybe it can’t run fast enough on other people’s machines. Perhaps it uses an unexpected amount of RAM with other users’ datasets.</p>&#13;
&#13;
<p>When facing efficiency issues in the program we build, manage, or are responsible for, we have several choices. But before you make any decisions, there is one critical thing you have to do. When issues happen, clear your mind from negative emotions about yourself or the team you worked with. It’s very common to blame yourself or others for mistakes. It is only natural to feel an uncomfortable sense of guilt when someone complains about your work. However, everyone (including us) must understand that the topic of efficiency is challenging. On top of that, inefficient or buggy code happens every day, even for the most experienced developers. Therefore, there should be no shame in making mistakes.</p>&#13;
&#13;
<p>Why do I write about emotions in a programming book? <a data-primary="emotions, in reaction to efficiency problems" data-type="indexterm" id="idm45606836347600"/><a data-primary="psychological safety, efficiency problems and" data-type="indexterm" id="idm45606836346928"/>Because psychological safety is an important reason why developers take the wrong approach toward code efficiency. Procrastinating, feeling stuck, and being afraid to try new things or scratch bad ideas are only some of the negative consequences. From my own experience, if we start blaming ourselves or others, we won’t solve any problems. Instead, we kill innovation and productivity, and introduce anxiety, toxicity, and stress. Those feelings can further prevent you from making a professional, reasonable decision on how to proceed with the reported efficiency issues or any other problems.</p>&#13;
<div data-type="tip"><h1>Blameless Culture Matters</h1>&#13;
<p><a data-primary="blameless culture, efficiency problems and" data-type="indexterm" id="idm45606836345056"/>Highlighting a blameless attitude is especially important during the “postmortem” process, which the Site Reliability Engineers perform after incidents. For example, sometimes costly mistakes are triggered by a single person. While we don’t want to discourage this person or punish them, it is crucial to understand the cause of the incident to prevent it. Furthermore, the blameless approach enables us to be honest about facts while respecting others, so everyone feels safe to escalate issues without fear.</p>&#13;
</div>&#13;
&#13;
<p>We should stop worrying too much, and with a clear mind, we should follow a systematic, almost robotic process (yes, ideally all of this is automated someday!). Let’s face it, practically speaking, not every performance issue has to be followed by optimization. The potential flow for the developer I propose is presented in <a data-type="xref" href="#img-issue-handling">Figure 3-3</a>. Note that the optimization step is not on the list yet!</p>&#13;
&#13;
<figure><div class="figure" id="img-issue-handling">&#13;
<img alt="efgo 0303" src="assets/efgo_0303.png"/>&#13;
<h6><span class="label">Figure 3-3. </span>Recommended flow for efficiency issue triaging</h6>&#13;
</div></figure>&#13;
&#13;
<p class="less_space pagebreak-before">Here, we outline six steps to do when an efficiency issue is reported:</p>&#13;
<dl>&#13;
<dt>Step 1: An efficiency issue was reported on our bug tracker.</dt>&#13;
<dd>&#13;
<p>The whole process starts when someone reports an efficiency issue for the software we are responsible for. If more than one issue was reported, always begin the process shown in <a data-type="xref" href="#img-issue-handling">Figure 3-3</a> for every single issue (divide and conquer).</p>&#13;
&#13;
<p>Note that going through this process and putting things through a bug tracker should be your habit, even for small personal projects. How else would you remember in detail all the things you want to improve?</p>&#13;
</dd>&#13;
<dt>Step 2: Check for duplicates.</dt>&#13;
<dd>&#13;
<p>This might be trivial, but try to be organized. Combine multiple issues for a single, focused conversation. Save time. Unfortunately, we are not yet at the stage where automation (e.g., artificial intelligence) can reliably find duplicates for us.</p>&#13;
</dd>&#13;
<dt>Step 3: Validate the circumstances against functional requirements.</dt>&#13;
<dd>&#13;
<p>In this step, we have to ensure that the efficiency issue reporter used supported functionality. We design software for specific use cases defined in functional requirements. Due to the high demand for solving various unique yet sometimes similar use cases, users often try to “abuse” our software to do something it was never meant to do. Sometimes they are lucky, and things work. Sometimes it ends with crashes, unexpected resource usage, or slowdowns.<sup><a data-type="noteref" href="ch03.html#idm45606836314240" id="idm45606836314240-marker">12</a></sup></p>&#13;
&#13;
<p>Similarly, we should do the same if the agreed prerequisites are not matched. For example, the unsupported, malformed request was sent, or the software was deployed on a machine without the required GPU resource.</p>&#13;
</dd>&#13;
<dt>Step 4: Validate the situation against RAERs.</dt>&#13;
<dd>&#13;
<p>Some expectations toward speed and efficiency cannot or do not need to be satisfied. This is where the formal efficiency requirements specification discussed in <a data-type="xref" href="#ch-conq-req">“Resource-Aware Efficiency Requirements”</a> is invaluable. If the reported observation (e.g., response latency for the valid request) is still within the agreed-on software performance numbers, we should communicate that fact and move on.<sup><a data-type="noteref" href="ch03.html#idm45606836310064" id="idm45606836310064-marker">13</a></sup></p>&#13;
&#13;
<p>Similarly, when the issue author deployed our software with an HDD disk where SSD was required, or the program was running on a machine with lower CPU cores than stated in the formal agreement, we should politely close such a bug report.</p>&#13;
<div data-type="note" epub:type="note"><h1>Functional or Efficiency Requirements Can Change!</h1>&#13;
<p>There might also be cases where the functional or efficiency specification did not predict certain corner cases. As a result, the specification might need to be revised to match reality. Requirements and demands evolve, and so should performance specifications and expectations.</p>&#13;
</div>&#13;
</dd>&#13;
<dt>Step 5: Acknowledge the issue, note it for prioritization, and move on.</dt>&#13;
<dd>&#13;
<p>Yes, you read it right. After you check the impact and all the previous steps, it’s often acceptable (and even recommended!) to do almost nothing about the reported problem at the current moment. There might be more important things that need our attention—maybe an important, overdue feature or another efficiency issue in a different part of the code.</p>&#13;
&#13;
<p>The world is not perfect. We can’t solve everything. Exercise your assertiveness. Notice that this is not the same as ignoring the problem. We still have to acknowledge that there is an issue and ask follow-up questions that will help find the bottleneck and optimize it at a later date. Make sure to ask for the exact software version they are running. Try to provide a workaround or hints on what’s happening so the user can help you find the root cause. Discuss ideas of what could be wrong. Write it all down in the issue. This will help you or another developer have a great starting point later. Communicate clearly that you will prioritize this issue with the team in the next prioritization session for the potential optimization effort.</p>&#13;
</dd>&#13;
<dt>Step 6: Done, issue was triaged.</dt>&#13;
<dd>&#13;
<p>Congratulations, the issue is handled. It’s either closed or open. If it’s open after all those steps, we can now consider its urgency and discuss the next steps with the team. Once we plan to tackle a specific issue, the efficiency flow in <a data-type="xref" href="#ch-conq-eff-flow">“Efficiency-Aware Development Flow”</a> will tell you how to do it effectively. Fear not. It might be easier than you think!</p>&#13;
</dd>&#13;
</dl>&#13;
<div data-type="note" epub:type="note"><h1>This Flow Is Applicable for Both SaaS and Externally Installed Software</h1>&#13;
<p>The same flow is applicable for the software that is installed and executed by the user on their laptop, smartphone, or servers (sometimes called “on-premise” installation), as well as when it’s managed by our company “as a service” (software as a service—SaaS). We developers should still try to triage all issues &#13;
<span class="keep-together">systematically.</span></p>&#13;
</div>&#13;
&#13;
<p>We divided optimizations into reasonable and deliberate. Let’s not hesitate and &#13;
<span class="keep-together">make the</span> next division. To simplify and isolate the problem of software efficiency &#13;
<span class="keep-together">optimizations,</span> we can divide it into levels, which we can then design and optimize in isolation. We will discuss those in the next section.<a data-startref="ix_ch03-asciidoc20" data-type="indexterm" id="idm45606836298784"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Optimization Design Levels" data-type="sect1"><div class="sect1" id="ch-conq-opt-levels">&#13;
<h1>Optimization Design Levels</h1>&#13;
&#13;
<p><a data-primary="optimization" data-secondary="design levels" data-type="indexterm" id="ix_ch03-asciidoc21"/>Let’s take our previous real-life example of the long commute to work every day (we will use this example a couple of times in this chapter!). If such a commute makes you unhappy because it takes a considerable effort and is too long, it might make sense to optimize it. There are, however, so many levels we can do this on:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>We can start small, by buying more comfortable shoes for walking distances.</p>&#13;
</li>&#13;
<li>&#13;
<p>We could buy an electric scooter or a car if that helps.</p>&#13;
</li>&#13;
<li>&#13;
<p>We could plan the journey so it takes less time or distance to travel.</p>&#13;
</li>&#13;
<li>&#13;
<p>We could buy an ebook reader and invest in a book-reading hobby to not waste time.</p>&#13;
</li>&#13;
<li>&#13;
<p>Finally, we could move closer to the workplace or even change jobs.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>We could do one such optimization in those separate “levels” or all, but each optimization takes some investment, trade-off (buying a car costs money), and effort. Ideally, we want to minimize the effort while maximizing value and making a difference.</p>&#13;
&#13;
<p>There is another crucial aspect of those levels: optimizations from one level can be impacted or devalued if we do optimization on a higher level. For instance, let’s say we did many optimizations to our commute on one level. We bought a better car, organized car sharing to save money on fuel, changed our work time to avoid traffic, etc. Imagine we would now decide to optimize on a higher level: move to an apartment within walking distance of our workplace. In such a case, any effort and investment in previous optimizations are now less valuable (if not fully wasted). This is the same in the engineering field. We should be aware of where we spend our optimization effort and when.</p>&#13;
&#13;
<p>When studying computer science, one of the students’ first encounters with optimization is learning theory about algorithms and data structures. They explore how to optimize programs using different algorithms with better time or space complexities (explained in <a data-type="xref" href="ch07.html#ch-hw-algo-bigo">“Asymptotic Complexity with Big O Notation”</a>). While changing the algorithm we use in our code is an important optimization technique, we have many more areas and variables we can optimize to improve our software efficiency. To appropriately talk about the performance, there are more levels that software depends on.</p>&#13;
&#13;
<p><a data-type="xref" href="#img-opt-levels">Figure 3-4</a> presents the levels that take a significant part in software execution. <a data-primary="Bentley, Jon Louis" data-secondary="list of levels in software execution" data-type="indexterm" id="idm45606836285552"/>This list of levels is inspired by Jon Louis Bentley’s list made in 1982,<sup><a data-type="noteref" href="ch03.html#idm45606836284512" id="idm45606836284512-marker">14</a></sup> and it’s still very accurate.</p>&#13;
&#13;
<figure><div class="figure" id="img-opt-levels">&#13;
<img alt="efgo 0304" src="assets/efgo_0304.png"/>&#13;
<h6><span class="label">Figure 3-4. </span>Levels that take part in software execution. We can provide optimization in each of these in isolation.</h6>&#13;
</div></figure>&#13;
&#13;
<p>This book outlines five optimization design levels, each with its optimization approaches and verification strategies. So let’s dig into them, from the highest to the lowest:</p>&#13;
<dl>&#13;
<dt>System level</dt>&#13;
<dd>&#13;
<p><a data-primary="system level of optimization design" data-type="indexterm" id="idm45606836278368"/>In most cases, our software is part of some bigger system. Maybe it’s one of many distributed processes or a thread in the bigger monolith application. In all cases, the system is structured around multiple modules. A module is a small software component that encapsulates certain functionality behind the method, interface, or other APIs (e.g., network API or file format) to be interchanged and modified more easily.</p>&#13;
&#13;
<p>Each Go application, even the smallest, is an executable module that imports the code from other modules. As a result, your software depends on other components. Optimizing at the system level means changing what modules are used, how they are linked together, who calls which component, and how often. We could say we are designing algorithms that work across modules and APIs, which are our data structures.</p>&#13;
&#13;
<p>It is nontrivial work that requires multiple-team efforts and good architecture design up front. But, on the other hand, it often brings enormous efficiency improvements.</p>&#13;
</dd>&#13;
<dt>Intramodule algorithm and data structure level</dt>&#13;
<dd>&#13;
<p><a data-primary="data-driven optimization level" data-type="indexterm" id="idm45606836274400"/><a data-primary="algorithm and data structure optimization level" data-type="indexterm" id="idm45606836273680"/>Given a problem to solve, its input data, and expected output, the module developer usually starts by designing two main elements of the procedure. First is the <em>algorithm</em>, a finite number of computer instructions that operate on data and can solve our problem (e.g., produce correct output). You have probably heard about many popular ones: binary search, quicksort, merge sort, map-reduce, and others, but any custom set of steps your program does can be called an algorithm.</p>&#13;
&#13;
<p>The second element is <em>data structures</em>, often implied by a chosen algorithm. They allow us to store data on our computer, e.g., input, output, or intermittent data. There are unlimited options here, too: arrays, hash maps, linked lists, stacks, queues, others, mixes, or custom ones. A solid choice of the algorithms within your module is extremely important. They have to be revised for your specific goals (e.g., request latency) and the input characteristics.</p>&#13;
</dd>&#13;
<dt>Implementation (code) level</dt>&#13;
<dd>&#13;
<p><a data-primary="code level optimization" data-type="indexterm" id="idm45606836270064"/><a data-primary="implementation (code) level of optimization design" data-type="indexterm" id="idm45606836269360"/>Algorithms in the module do not exist until they are written in code,  compilable to machine code. Developers have huge control here. We can have an inefficient algorithm implemented efficiently, which fulfils our RAERs. On the other hand, we can have an amazing, efficient algorithm implemented poorly that causes unintended system slowdowns. Optimizing at the code level means taking a program written in a higher-level language (e.g., Go) that implements a specific algorithm, and producing a more efficient program in any aspect we want (e.g., latency) that uses the same algorithm and yields the same, correct output.</p>&#13;
&#13;
<p>Typically, we optimize on both algorithm and code levels together. In other cases, settling on one algorithm and focusing only on code optimizations is easier. You will see both approaches in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch10.html#ch-opt">10</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch11.html#ch-opt2">11</a>.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<p>Some previous materials consider the compilation step as an individual level. I would argue that code-level optimization techniques have to embody compiler-level ones. There is a deep synergy between your implementation and how the compiler will translate it to machine code. As developers, we &#13;
<span class="keep-together">have to</span> understand this relationship. We will explore Go compiler implications more in <a data-type="xref" href="ch04.html#ch-hw-compilation">“Understanding Go Compiler”</a>.</p>&#13;
</div>&#13;
</dd>&#13;
<dt>Operating system level</dt>&#13;
<dd>&#13;
<p><a data-primary="operating system (OS) optimization level" data-type="indexterm" id="idm45606836261152"/>These days, our software is never executed directly on the machine hardware and never runs alone. Instead, we run operating systems that split each software &#13;
<span class="keep-together">execution</span> into processes (then threads), schedule them on CPU cores, and provide other essential services, like memory and IO management, device access, and more. On top of that, we have additional virtualization layers (virtual machines, containers) that we can put in the operating system bucket, especially in cloud-native environments.</p>&#13;
&#13;
<p>All those layers pose some overhead that can be optimized by those who control the operating system development and configuration. In this book, I assume that Go developers can rarely impact this level. Yet, we can gain a lot by understanding the challenges and usage patterns that will help us achieve efficiency on other, higher levels. We will go through them in <a data-type="xref" href="ch04.html#ch-hardware">Chapter 4</a>, mainly focusing on Unix operating systems and popular virtualization techniques. I assume in this book that device drivers and firmware also fit into this category.</p>&#13;
</dd>&#13;
<dt>Hardware level</dt>&#13;
<dd>&#13;
<p><a data-primary="hardware level of optimization design" data-type="indexterm" id="idm45606836256496"/>Finally, at some point, a set of instructions translated from our code is executed by the computer CPU units, with internal caches that are connected to other essential parts in the motherboard: RAM, local disks, network interfaces, input and output devices, and more. Usually, as developers or operators, we can abstract away from this complexity (which also varies across hardware products) thanks to the operating system level mentioned before. Yet the performance of our applications is limited by hardware constraints. Some of them might be surprising. For example, were you aware of <a href="https://oreil.ly/r1slU">the existence of NUMA nodes for multicore machines and how they can affect our performance</a>? Did you know that memory buses between CPU and memory nodes have limited bandwidth? It’s an extensive topic that may impact our software efficiency optimization processes. We will explore this topic briefly in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.html#ch-hardware">4</a> and&#13;
<a data-type="xref" data-xrefstyle="select:labelnumber" href="ch05.html#ch-hardware2">5</a>, together with the mechanisms Go employs to tackle these issues.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>What are the practical benefits of dividing our problem space into levels? First of all, studies<sup><a data-type="noteref" href="ch03.html#idm45606836251792" id="idm45606836251792-marker">15</a></sup> show that when it comes to application speed, it is often possible to achieve speedups with factors of 10 to 20 at any of the mentioned levels, if not more. This is also similar to my experience.</p>&#13;
&#13;
<p>The good news is that this implies the possibility of focusing our optimizations on just one level to gain the desired system efficiency.<sup><a data-type="noteref" href="ch03.html#idm45606836249888" id="idm45606836249888-marker">16</a></sup> However, suppose you optimized your implementation 10 to 20 times on one level. In that case, it might be hard to optimize this level further without significant sacrifices in development time, readability, and maintainability (our sweet spot from <a data-type="xref" href="#img-opt-cost">Figure 3-2</a>). So you might have to look at another level to gain more.</p>&#13;
&#13;
<p>The bad news is that you might be unable to change certain levels. For example, as programmers, we generally don’t have the power to easily change the compiler, operating system, or hardware. Similarly, system administrators won’t be able to change the algorithm the software is using. Instead, they can replace systems and configure or tune them.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Beware of the Optimization Biases!</h1>&#13;
<p><a data-primary="biases, in optimization" data-type="indexterm" id="idm45606836246352"/><a data-primary="optimization" data-secondary="guarding against biases" data-type="indexterm" id="idm45606836245648"/>It is sometimes funny (and scary!) how different engineering groups within a single company come up with highly distinct solutions to the same efficiency problems.</p>&#13;
&#13;
<p>If the group has more system administrators or DevOps engineers, the solution is often to switch to another system, software, or operating system or try to “tune” them. In contrast, the software engineering group will mostly iterate on the same codebase, optimizing system, algorithm, or code levels.</p>&#13;
&#13;
<p>This bias comes from the experience of changing each level, but it can have negative impacts. For example, switching the whole system, e.g., from <a href="https://oreil.ly/ZVYo1">RabbitMQ</a> to <a href="https://oreil.ly/wPpUD">Kafka</a>, is a considerable effort. If you are doing this only because RabbitMQ “feels slow” without trying to contribute, perhaps a simple code-level optimization might be excessive. Or another way around, trying to optimize the efficiency of the system designed for different purposes on the code level might not be<a data-startref="ix_ch03-asciidoc21" data-type="indexterm" id="idm45606836242048"/> &#13;
<span class="keep-together">sufficient.</span></p>&#13;
</div>&#13;
&#13;
<p>We discussed what optimization is, and we mentioned how to set performance goals, handle efficiency issues, and the design levels we operate in. Now it’s time to hook everything together and combine this knowledge into the complete development cycle.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Efficiency-Aware Development Flow" data-type="sect1"><div class="sect1" id="ch-conq-eff-flow">&#13;
<h1>Efficiency-Aware Development Flow</h1>&#13;
<blockquote>&#13;
<p>The primary concerns of the programmer during the early part of a program’s life should be the overall organization of the programming project and producing correct and maintainable code. Furthermore, in many contexts, the cleanly designed program is often efficient enough for the application at hand.&#13;
</p>&#13;
<p data-type="attribution">Jon Louis Bentley, <i>Writing Efficient Programs</i></p>&#13;
</blockquote>&#13;
&#13;
<p><a data-primary="development, efficiency-aware flow" data-type="indexterm" id="ix_ch03-asciidoc22"/><a data-primary="efficiency (generally)" data-secondary="efficiency-aware development flow" data-type="indexterm" id="ix_ch03-asciidoc23"/><a data-primary="efficiency-aware development flow" data-type="indexterm" id="ix_ch03-asciidoc24"/>Hopefully<a data-primary="Bentley, Jon Louis" data-secondary="on primary concerns of programmer" data-type="indexterm" id="idm45606836232176"/>, at this point, you are aware that we have to think about performance, ideally from the early development stages. But there are risks—we don’t develop code for it to be just efficient. We write programs for specific functionality that match the functional requirements we set or get from stakeholders. Our job is to get this work done effectively, so a pragmatic approach is necessary. How might developing a working but efficient code look from a high-level point of view?</p>&#13;
&#13;
<p><a data-primary="TFBO (test, fix, benchmark, optimize) development flow" data-type="indexterm" id="ix_ch03-asciidoc25"/>We can simplify the development process into nine steps, as presented in <a data-type="xref" href="#img-opt-flow">Figure 3-5</a>. For lack of a better term, let’s call it the <em>TFBO</em> flow—test, fix, benchmark, and &#13;
<span class="keep-together">optimize.</span></p>&#13;
&#13;
<figure><div class="figure" id="img-opt-flow">&#13;
<img alt="efgo 0305" src="assets/efgo_0305.png"/>&#13;
<h6><span class="label">Figure 3-5. </span>Efficiency-aware development flow</h6>&#13;
</div></figure>&#13;
&#13;
<p>The process is systematic and highly iterative. Requirements, dependencies, and environments are changing, so we have to work in smaller chunks too. The TFBO process can feel a little strict, but trust me, mindful and effective software development requires some discipline. It applies to cases when you create new software from scratch, add a feature, or change the code. TFBO should work for software written in any language, not only Go. It is also applicable for all levels mentioned in <a data-type="xref" href="#ch-conq-opt-levels">“Optimization Design Levels”</a>. Let’s go through the nine TFBO steps.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Functionality Phase" data-type="sect2"><div class="sect2" id="idm45606836223456">&#13;
<h2>Functionality Phase</h2>&#13;
<blockquote>&#13;
<p>It is far, far easier to make a correct program fast than it is to make a fast program <span class="keep-together">correct.</span></p>&#13;
<p data-type="attribution">H. Sutter and A. Alexandrescu, <a href="https://oreil.ly/hq0zw"><i>C++ Coding Standards: 101 Rules, Guidelines, and Best Practices</i></a> (Addison-Wesley, 2004)</p>&#13;
</blockquote>&#13;
&#13;
<p><a data-primary="efficiency-aware development flow" data-secondary="functionality phase" data-type="indexterm" id="ix_ch03-asciidoc26"/><a data-primary="functionality phase of TFBO" data-type="indexterm" id="ix_ch03-asciidoc27"/>Always <a data-primary="Alexandrescu, Andrei" data-secondary="on speed versus correctness" data-type="indexterm" id="idm45606836216192"/><a data-primary="Sutter, H." data-secondary="on speed versus correctness" data-type="indexterm" id="idm45606836215200"/>start with functionality first. Whether we aim to start a new program, add new functionality, or just optimize an existing program, we should always begin with the design or implementation of the functionality. Make it work, make it simple, readable, maintainable, secure, etc., according to goals we have set, ideally in written form. Especially when you are starting your journey as a software engineer, focus on one thing at a time. With practice, we can add more reasonable optimizations &#13;
<span class="keep-together">early on.</span></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="1. Test functionality first" data-type="sect3"><div class="sect3" id="idm45606836212944">&#13;
<h3>1. Test functionality first</h3>&#13;
&#13;
<p>It might feel counterintuitive for some, but you should almost always start with a verification framework for the expected functionality. The more automated it is, the better. This also applies when you have a blank page and start developing a new program. <a data-primary="test-driven development (TDD)" data-type="indexterm" id="idm45606836211024"/>This development paradigm is called test-driven development (TDD). It is mainly focused on code reliability and feature delivery velocity efficiency. In a strict form, on the code level, it mandates a specific flow:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Write a test (or extend an existing one) that expects the feature to be &#13;
<span class="keep-together">implemented.</span></p>&#13;
</li>&#13;
<li>&#13;
<p>Make sure to run all tests and see the new tests failing for expected reasons. If you don’t see the failure or other failures, fix those tests first.</p>&#13;
</li>&#13;
<li>&#13;
<p>Iterate with the smallest possible changes until all tests pass and the code is clean.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>TDD eliminates many unknowns. Imagine if we would not follow TDD. For example, we add a feature, and we write a test. It’s easy to make a mistake that always passes the test even without our feature. Similarly, let’s say we add the test after implementation, which passes, but other previously added tests fail. Most likely, we did not run a test before the implementation, so we don’t know if everything worked before. TDD ensures you don’t run into those questions at the end of your work, enormously improving reliability. It also reduces implementation time, allowing safe code modifications and giving you feedback early.</p>&#13;
&#13;
<p>Furthermore, what if the functionality we wanted to implement is already done and we didn’t notice? Writing a test first would reveal that quickly, saving us time. Spoiler alert: we will use the same principles for benchmark-driven optimization in step 4 later!</p>&#13;
&#13;
<p>The TDD can be easily understood as a code-level practice, but what if you design or optimize algorithms and systems? The answer is that the flow remains the same, but our testing strategy must be applied on a different level, e.g., validating system design.</p>&#13;
&#13;
<p>Let’s say we implemented a test or performed an assessment on what is currently designed or implemented. What’s next?</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="2. Do we pass the functional tests?" data-type="sect3"><div class="sect3" id="idm45606836203616">&#13;
<h3>2. Do we pass the functional tests?</h3>&#13;
&#13;
<p>With the results from step 1, our work is much easier—we can perform data-driven decisions on what to do next! First, we should compare tests or assessment results with our agreed functional requirements. Is the current implementation or design fulfilling the specification? Great, we can jump to step 4. However, if tests fail or the functionality assessment shows some functionality gap, it’s time to go to step 3 and fix this situation.</p>&#13;
&#13;
<p>The problem is when you don’t have those functional requirements stated anywhere. As discussed in <a data-type="xref" href="#ch-conq-req-formal">“Efficiency Requirements Should Be Formalized”</a>, this is why asking for functional requirements or defining them on your own is so important. Even the simplest bullet-point list of goals, written in the project README, is better than nothing.</p>&#13;
&#13;
<p>Now, let’s explore what to do if the current state of our software doesn’t pass functional verification.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="3. If the tests fail, we have to fix, implement, or design the missing parts" data-type="sect3"><div class="sect3" id="idm45606836199984">&#13;
<h3>3. If the tests fail, we have to fix, implement, or design the missing parts</h3>&#13;
&#13;
<p>Depending on the design level we are at, in this step, we should design, implement, or fix the functional parts to close the gap between the current state and the functional expectation. As we discussed in <a data-type="xref" href="#ch-conq-opt-reasonable">“Reasonable Optimizations”</a>, no optimizations other than the obvious, reasonable optimizations are allowed here. Focus on readability, design of modules, and simplicity. For example, don’t bother thinking if it’s more optimal to pass an argument by pointer or value or if parsing integers here will be too slow unless it’s obvious. Just do whatever makes sense from a functional and readability standpoint. We don’t validate efficiency yet, so let’s forget about deliberate optimizations for now.</p>&#13;
&#13;
<p>As you might have noticed in <a data-type="xref" href="#img-opt-flow">Figure 3-5</a>, steps 1, 2, and 3 compose a small loop. This gives us an early feedback loop whenever we change things in our code or design. Step 3 is like us steering the direction of our boat called “software” when sailing over the ocean. We know where we want to go and understand how to look at the sun or stars in the right direction. Yet without precise feedback tools like GPS, we can end up sailing to the wrong place and only realizing it after weeks have gone by. This is why it’s beneficial to validate our sailing position in short intervals for early feedback!</p>&#13;
&#13;
<p>This is the same for our code. We don’t want to work for months only to learn that we didn’t get closer to what we expected from the software. Leverage the functionality phase loop by making a small iteration of code or design change, going to step 1 (run tests), step 2, and going back to step 3 to do another little correction.<sup><a data-type="noteref" href="ch03.html#idm45606836194960" id="idm45606836194960-marker">17</a></sup> This is the most effective development cycle engineers have found over the years. All modern methodologies like <a href="https://oreil.ly/rhx8W">extreme programming</a>, Scrum, Kanban, and other <a href="https://oreil.ly/sKZUA">Agile</a> techniques are built on a small iterations premise.</p>&#13;
&#13;
<p>After potentially hundreds of iterations, we might have software or design that fulfills, in step 2, the functional requirements we have set for ourselves for this development session. Finally, it’s time to ensure our software is fast and efficient enough! Let’s look at that in the next section.<a data-startref="ix_ch03-asciidoc27" data-type="indexterm" id="idm45606836191296"/><a data-startref="ix_ch03-asciidoc26" data-type="indexterm" id="idm45606836190592"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Efficiency Phase" data-type="sect2"><div class="sect2" id="idm45606836189792">&#13;
<h2>Efficiency Phase</h2>&#13;
&#13;
<p><a data-primary="efficiency phase of TFBO" data-type="indexterm" id="ix_ch03-asciidoc28"/><a data-primary="efficiency-aware development flow" data-secondary="efficiency phase" data-type="indexterm" id="ix_ch03-asciidoc29"/>Once we are happy with the functional aspects of our software, it’s time to ensure it matches the expected resource consumption and speed.</p>&#13;
&#13;
<p>Splitting phases and isolating them from each other seems like a burden at first glance, but it will organize your developer workflow better. It gives us deep focus, ruling our early unknowns and mistakes, and helps us avoid expensive focus context switches.</p>&#13;
&#13;
<p>Let’s start our efficiency phase by performing the initial (baseline) efficiency validation in step 4. Then, who knows, maybe our software is efficient enough without any changes!</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="4. Efficiency assessment" data-type="sect3"><div class="sect3" id="idm45606836184432">&#13;
<h3>4. Efficiency assessment</h3>&#13;
&#13;
<p>Here we employ a similar strategy to step 1 of the functionality phase, but toward efficiency space. We can define an equivalent of the TDD method explained in step 1. Let’s call it benchmark-driven optimization (BDO). In practice, step 4 looks like this process at the code level:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Write benchmarks (or extend existing ones) for all the operations from the efficiency requirements we want to compare against. Do it even if you know that the current implementation is not efficient yet. We will need that work later. It is not trivial, and we will discuss this aspect in detail in <a data-type="xref" href="ch08.html#ch-benchmarking">Chapter 8</a>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Ideally, run all the benchmarks to ensure your changes did not impact unrelated operations. In practice, this takes too much time, so focus on one part of the &#13;
<span class="keep-together">program</span> (e.g., one operation) you want to check and run benchmarks only for that. Save the results for later. This will be our baseline.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>Similar to step 1, the higher-level assessment might require different tools. Equipped with results from benchmarks or assessments, let’s go to step 5.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="5. Are we within RAERs?" data-type="sect3"><div class="sect3" id="idm45606836177808">&#13;
<h3>5. Are we within RAERs?</h3>&#13;
&#13;
<p><a data-primary="Resource-Aware Efficiency Requirements (RAER)" data-secondary="EADF and" data-type="indexterm" id="idm45606836176608"/>In this step, we must compare the results from step 4 with the RAERs we gathered. For example, is our latency within the acceptable norm for the current implementation? Is the amount of resources our operation consumes within what we agreed? If yes, then no optimization is needed!</p>&#13;
&#13;
<p>Again, similar to step 2, we have to establish requirements or rough goals for efficiency. Otherwise, we have zero ideas if the numbers we see are acceptable or not. Again, refer to <a data-type="xref" href="#ch-conq-acquiring-raer">“Acquiring and Assessing Efficiency Goals”</a> on how to define RAERs.</p>&#13;
&#13;
<p>With this comparison, we should have a clear answer. Are we within acceptable thresholds? If yes, we can jump straight to the release process in step 9. If not, there is exciting optimization logic ahead of us in steps 6, 7, and 8. Let’s walk through those now.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="6. Find the main bottleneck" data-type="sect3"><div class="sect3" id="ch-conq-eff-flow-6">&#13;
<h3>6. Find the main bottleneck</h3>&#13;
&#13;
<p>Here we must address the first challenge mentioned in <a data-type="xref" href="#ch-conq-challenges">“Optimization Challenges”</a>. We are typically bad at guessing which part of the operation causes the biggest bottleneck; unfortunately, that’s where our optimization should focus first.</p>&#13;
&#13;
<p>The word <em>bottleneck</em> describes a place where most consumption of specific resources or software comes from. It might be a significant number of disk reads, deadlock, memory leak, or a function executed millions of times during a single operation. A single program usually has only a few of these bottlenecks. To perform effective optimization, we must first understand the bottleneck’s consequences.</p>&#13;
&#13;
<p>As part of this process, we need first to understand the underlying root cause of the problem we found in step 5. We will discuss the best tools for this job in <a data-type="xref" href="ch09.html#ch-observability3">Chapter 9</a>.</p>&#13;
&#13;
<p>Let’s say we found the set of functions executed the most or another part of a program that consumes the most resources. What’s next?</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="7. Choice of level" data-type="sect3"><div class="sect3" id="idm45606836166624">&#13;
<h3>7. Choice of level</h3>&#13;
&#13;
<p>In step 7, we must choose how we want to tackle the optimization. Should we make the code more efficient? Perhaps we could improve the algorithm? Or maybe optimize on the system level? In extreme cases, we might also want to optimize the operating system or hardware!</p>&#13;
&#13;
<p>The choice depends on what’s more pragmatic at the moment and where we are in our efficiency spectrum in <a data-type="xref" href="#img-opt-sum">Figure 3-1</a>. The important part is to stick to single-level optimization at one optimization iteration. Similar to the functionality phase, make short iterations and small corrections.</p>&#13;
&#13;
<p>Once we know the level we want to make more efficient or faster, we are ready to perform optimization!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="8. Optimize!" data-type="sect3"><div class="sect3" id="idm45606836162176">&#13;
<h3>8. Optimize!</h3>&#13;
&#13;
<p>This is what everyone was waiting for. Finally, after all that effort, we know:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>What place in the code or design to optimize for the most impact.</p>&#13;
</li>&#13;
<li>&#13;
<p>What to optimize for—what resource consumption is too large.</p>&#13;
</li>&#13;
<li>&#13;
<p>How much sacrifice we can make on other resources because we have RAER. There will be trade-offs.</p>&#13;
</li>&#13;
<li>&#13;
<p>On what level we are optimizing.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>These elements make the optimization process much easier and often even make it possible to begin with. Now we focus on the mental model we introduced in <a data-type="xref" href="#ch-conq-opt">“Beyond Waste, Optimization Is a Zero-Sum Game”</a>. We are looking for <em>waste</em>. We are looking for places where we can do <em>less work</em>. There are always things that can be eliminated, either for free or by doing other work using another resource. I will introduce some patterns in <a data-type="xref" href="ch11.html#ch-opt2">Chapter 11</a> and show examples in <a data-type="xref" href="ch10.html#ch-opt">Chapter 10</a>.</p>&#13;
&#13;
<p>Let’s say we found some ideas for improvement. This is when you should implement it or design it (depending on the level). But what’s next? We cannot just release our optimization like this simply because:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>We don’t know that we did not introduce functional issues (bugs).</p>&#13;
</li>&#13;
<li>&#13;
<p>We don’t know if we improved any performance.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This is why we have to perform the full cycle now (no exceptions!). It’s critical to go to step 1 and test the optimized code or design. If there are problems, we must fix them or revert optimization (steps 2 and 3).</p>&#13;
<div data-type="warning" epub:type="warning">&#13;
<p>It is tempting to ignore the functional testing phase when iterating on optimizations. For example, what can go wrong if you only reduce one allocation by reusing some memory?</p>&#13;
&#13;
<p>I often caught myself doing this, and it was a painful mistake. Unfortunately, when you find that your code cannot pass tests after a few iterations of optimizations, it is hard to find what caused it. Usually, you have to revert all and start from scratch. Therefore, I encourage you to run a scoped unit test every time after the optimization attempt.</p>&#13;
</div>&#13;
&#13;
<p>Once we gain confidence that our optimization did not break any basic functionality, it’s crucial to check if our optimization improved the situation we want to improve. It’s important to run <em>the same</em> benchmark, ensuring that nothing changes except the optimization you did (step 4). This allows us to reduce unknowns and iterate on our optimization in small parts.</p>&#13;
&#13;
<p>With the results from this recent step 4, compare it with the baseline made in the initial visit to step 4. This crucial step will tell us if we optimized anything or introduced performance regression. Again, don’t assume anything. Let the data speak for itself! Go has amazing tools for that, which we will discuss in <a data-type="xref" href="ch08.html#ch-benchmarking">Chapter 8</a>.</p>&#13;
&#13;
<p>If the new optimization doesn’t have a better efficiency result, we simply try different ideas again until it works out. If the optimization has better results, we save our work and go to step 5 to check if it’s enough. If not, we have to make another iteration. It’s often useful to build another optimization on what we already did. Maybe there is something more to improve!</p>&#13;
&#13;
<p>We repeat this cycle, and after a few (or hundreds), we hopefully have acceptable results in step 5. In this case, we can move to step 9 and enjoy our work!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="9. Release and enjoy!" data-type="sect3"><div class="sect3" id="idm45606836143120">&#13;
<h3>9. Release and enjoy!</h3>&#13;
&#13;
<p>Great job! You went through the full iteration of the efficiency-aware development flow. Your software is now fairly safe to be released and deployed in the wild.<a data-startref="ix_ch03-asciidoc29" data-type="indexterm" id="idm45606836141792"/><a data-startref="ix_ch03-asciidoc28" data-type="indexterm" id="idm45606836141088"/> The process might feel bureaucratic, but it’s easy to build an instinct for it and follow it naturally.<a data-startref="ix_ch03-asciidoc25" data-type="indexterm" id="idm45606836140160"/> Of course, you might already be using this flow without noticing!<a data-startref="ix_ch03-asciidoc24" data-type="indexterm" id="idm45606836139360"/><a data-startref="ix_ch03-asciidoc23" data-type="indexterm" id="idm45606836138608"/><a data-startref="ix_ch03-asciidoc22" data-type="indexterm" id="idm45606836137936"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45606836137008">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>As we learned in this chapter, conquering efficiency is not trivial. However, certain patterns exist that help to navigate this process systematically and effectively. For example, the TFBO flow was immensely helpful for me to keep my efficiency-aware development pragmatic and effective.</p>&#13;
&#13;
<p>Some of the frameworks incorporated in the TFBO, like test-driven development and benchmark-driven optimizations, might seem tedious initially. However, similar to the saying, <a href="https://oreil.ly/qNPId">“Give me six hours to chop a tree, I will spend four hours sharpening an axe”</a>, you will notice that spending time on a proper test and benchmark will save you tons of effort in the long term!</p>&#13;
&#13;
<p>The main takeaways are that we can divide optimizations into reasonable and deliberate ones. Then, to be mindful of the trade-offs and our effort, we discussed defining RAER so we can assess our software toward a formal goal everyone understands. Next, we mentioned what to do when an efficiency problem occurs and what optimizations levels there are. Finally, we discussed TFBO flow, which guides us through the practical development process.</p>&#13;
&#13;
<p>To sum up, finding optimization can be considered a problem-solving skill. Noticing waste is not easy, and it comes with a lot of practice. This is somewhat similar to being good at programming interviews. In the end, what helps is the experience of seeing past patterns that were not efficient enough and how they were improved. Through this book, we will exercise those skills and uncover many tools that can help us in this journey.</p>&#13;
&#13;
<p>Yet before that, there are important things to learn about modern computer architecture. We can learn typical optimization patterns by examples, but <a href="https://oreil.ly/eNkOY">the optimizations do not generalize very well</a>. We won’t be able to find them effectively and apply them in unique contexts without understanding the mechanisms that make those optimizations effective. In the next chapter, we will discuss how Go interacts with the key resources in typical computer architecture.<a data-startref="ix_ch03-asciidoc1" data-type="indexterm" id="idm45606836130800"/><a data-startref="ix_ch03-asciidoc0" data-type="indexterm" id="idm45606836130128"/></p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45606836661792"><sup><a href="ch03.html#idm45606836661792-marker">1</a></sup> There might be exceptions. There might be domains where it’s acceptable to approximate results. Sometimes we can (and should) also drop nice-to-have features if they block the critical efficiency characteristics we want.</p><p data-type="footnote" id="idm45606836647264"><sup><a href="ch03.html#idm45606836647264-marker">2</a></sup> Situations where resources are not cleaned after each periodic functionality due to leftover concurrent routine are often referred to as memory leaks.</p><p data-type="footnote" id="idm45606836619040"><sup><a href="ch03.html#idm45606836619040-marker">3</a></sup> Zero-sum game comes from game and economic theory. It describes a situation where one player can only win X if other players in total lost exactly X.</p><p data-type="footnote" id="idm45606836617440"><sup><a href="ch03.html#idm45606836617440-marker">4</a></sup> I got inspired for dividing optimizations on reasonable and deliberate by the community-driven <a href="https://oreil.ly/RuxfU">go-perfbook</a> led by Damian Gryski. In his book, he also mentioned the “dangerous” optimization category. I don’t see a value in splitting classes further since there is a fuzzy borderline between deliberate and dangerous that depends on the situation and personal taste.</p><p data-type="footnote" id="idm45606836572944"><sup><a href="ch03.html#idm45606836572944-marker">5</a></sup> No one said challenging ourselves is bad in certain situations. If you have time, playing with initiatives like <a href="https://oreil.ly/zT0Bl">Advent of Code</a> is a great way to learn or even compete! This is, however, different than the situation where we are paid to develop functional software effectively.</p><p data-type="footnote" id="idm45606836566448"><sup><a href="ch03.html#idm45606836566448-marker">6</a></sup> I experienced this a lot while maintaining the <a href="https://prometheus.io">Prometheus project</a>, where we were constantly facing situations where users tried to ingest unique events into Prometheus. The problem is that we designed Prometheus as an efficient metric monitoring solution with a bespoke time-series database that assumed storing aggregated samples over time. If the ingested series were labeled with unique values, Prometheus slowly but surely began to use many resources (we call it a high-cardinality situation).</p><p data-type="footnote" id="idm45606836561456"><sup><a href="ch03.html#idm45606836561456-marker">7</a></sup> Just imagine, with all the resources in the world, we could try optimizing the software execution to the limits of physics. And once we are there, we could spend decades on research that pushes boundaries with things beyond the current physics we know. But, practically speaking, we might never find the “true” limit in our lifetime.</p><p data-type="footnote" id="idm45606836534640"><sup><a href="ch03.html#idm45606836534640-marker">8</a></sup> I was never explicitly asked to create a nonfunctional specification, and the same with <a href="https://oreil.ly/Ui2tu">people around me</a>.</p><p data-type="footnote" id="idm45606836510192"><sup><a href="ch03.html#idm45606836510192-marker">9</a></sup> Funnily enough, with enough program users, even with a formal performance and reliability contract, all your system’s observable behaviors will depend on somebody. This is known as <a href="https://oreil.ly/UcrQo">Hyrum’s Law</a>.</p><p data-type="footnote" id="idm45606836385424"><sup><a href="ch03.html#idm45606836385424-marker">10</a></sup> We use napkin math more often in this book and during optimizations, so I prepared a small cheat sheet for latency assumptions in <a data-type="xref" href="app01.html#appendix-napkin-math">Appendix A</a>.</p><p data-type="footnote" id="idm45606836367744"><sup><a href="ch03.html#idm45606836367744-marker">11</a></sup> We will discuss benchmarks in detail in <a data-type="xref" href="ch07.html#ch-observability2">Chapter 7</a>.</p><p data-type="footnote" id="idm45606836314240"><sup><a href="ch03.html#idm45606836314240-marker">12</a></sup> For example, see the instance of the XY problem mentioned in <a data-type="xref" href="#ch-conq-perf-goal">“Understand Your Goals”</a>.</p><p data-type="footnote" id="idm45606836310064"><sup><a href="ch03.html#idm45606836310064-marker">13</a></sup> The reporter of the issue can obviously negotiate a change in the specification with the product owner if they think it’s important enough or they want to pay additionally, etc.</p><p data-type="footnote" id="idm45606836284512"><sup><a href="ch03.html#idm45606836284512-marker">14</a></sup> Jon Louis Bentley, <em>Writing Efficient Programs</em> (Prentice Hall, 1982).</p><p data-type="footnote" id="idm45606836251792"><sup><a href="ch03.html#idm45606836251792-marker">15</a></sup> Raj Reddy and Allen Newell’s “Multiplicative Speedup of Systems” (in <em>Perspectives on Computer Science</em>, A.K. Jones, ed., Academic Press) elaborates on potential speedups of a factor of about 10 for each software design level. What’s even more exciting is the fact that for hierarchical systems, the speedups from different levels multiplies, which offers massive potential for performance boost when optimizing.</p><p data-type="footnote" id="idm45606836249888"><sup><a href="ch03.html#idm45606836249888-marker">16</a></sup> This is a quite powerful thought. For example, imagine you have your application returning a result in 10 m. Reducing it to 1 m by optimizing on one level (e.g., an algorithm) is a game changer.</p><p data-type="footnote" id="idm45606836194960"><sup><a href="ch03.html#idm45606836194960-marker">17</a></sup> Ideally, we would have functionality checks for every code stroke or event of the saved code file. The earlier the feedback loop, the better. The main blocker for this is the time required to perform all tests and their  <span class="keep-together">reliability.</span></p></div></div></section></body></html>