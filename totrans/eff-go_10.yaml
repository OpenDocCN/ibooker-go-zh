- en: Chapter 10\. Optimization Examples
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 优化示例
- en: It’s finally time to collect all the tools, skills, and knowledge you gathered
    from the previous chapters and apply some optimizations! In this chapter, we will
    try to reinforce the pragmatic optimization flow by going through some examples.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在终于是时候将你从前几章中收集的所有工具、技能和知识应用到一些优化上了！在本章中，我们将尝试通过一些示例来加强务实的优化流程。
- en: We will attempt to optimize the naive implementation of the `Sum` from [Example 4-1](ch04.html#code-sum).
    I will show you how the TFBO (from [“Efficiency-Aware Development Flow”](ch03.html#ch-conq-eff-flow))
    can be applied to three different sets of efficiency requirements.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试优化 [示例 4-1](ch04.html#code-sum) 的天真实现。我将向您展示如何应用 TFBO（来自 [“效率感知开发流程”](ch03.html#ch-conq-eff-flow)）到三组不同的效率要求中。
- en: Optimizations/pessimizations don’t generalize very well. It all depends on the
    code, so measure each time and don’t cast absolute judgments.
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 优化/悲观化并不能很好地概括。一切都取决于代码，因此每次都要测量，不要做绝对的评判。
- en: ''
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bartosz Adamczewski, [Tweet](https://oreil.ly/oW3ND) (2022)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Bartosz Adamczewski，[Tweet](https://oreil.ly/oW3ND)（2022）
- en: We will use our optimization stories as a foundation for some optimization patterns
    summarized in the next chapter. Learning about thousands of optimization cases
    that happened in the past is not very useful. Every case is different. The compiler
    and language change, so any “brute-force” attempt to try those thousands of optimizations
    one by one is not pragmatic.^([1](ch10.html#idm45606823780656)) Instead, I have
    focused on equipping you with the knowledge, tools, and practices that will let
    you find a more efficient solution to your problem!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把我们的优化故事作为下一章中总结的一些优化模式的基础。了解过去数千次优化案例并不是非常有用的。每个案例都是不同的。编译器和语言会改变，所以任何“蛮力”尝试逐个尝试这些数千种优化并不现实。^([1](ch10.html#idm45606823780656))
    相反，我专注于为您提供知识、工具和实践，让您能够找到更高效的问题解决方案！
- en: Please don’t focus on particular optimizations, e.g., the specific algorithmic
    or code changes I applied. Instead, try to follow how I came up with those changes,
    how I found what piece of code to optimize first, and how I assessed the change.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 请不要专注于特定的优化，例如我应用的具体算法或代码更改。相反，请试着跟随我是如何得出这些改变的，我是如何首先找到需要优化的代码片段，以及我如何评估这些改变。
- en: 'We will start in [“Sum Examples”](#ch-opt-sum) by introducing the three problems.
    Then we will take the `Sum` and perform the optimizations in [“Optimizing Latency”](#ch-opt-latency-example),
    [“Optimizing Memory Usage”](#ch-opt-mem-example), and [“Optimizing Latency Using
    Concurrency”](#ch-opt-latency-concurrency-example). Finally, we will mention some
    other ways we could solve our goals in [“Bonus: Thinking Out of the Box”](#ch-opt-bonus).
    Let’s go!'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 [“求和示例”](#ch-opt-sum) 开始介绍三个问题。然后我们将进行`Sum`并在 [“优化延迟”](#ch-opt-latency-example)、[“优化内存使用”](#ch-opt-mem-example)
    和 [“利用并发优化延迟”](#ch-opt-latency-concurrency-example) 中进行优化。最后，我们将提及一些其他方法，可以在 [“奖励：打破思维定式”](#ch-opt-bonus)
    中实现我们的目标。让我们开始吧！
- en: Sum Examples
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 求和示例
- en: 'In [Chapter 4](ch04.html#ch-hardware), we introduced a simple `Sum` implementation
    in [Example 4-1](ch04.html#code-sum) that sums large numbers of integers provided
    in a file.^([2](ch10.html#idm45606823764528)) Let’s leverage all the learning
    you have gained and use it to optimize [Example 4-1](ch04.html#code-sum). As we
    learned in [“Resource-Aware Efficiency Requirements”](ch03.html#ch-conq-req),
    we can’t “just” optimize—we have to have some goal in mind. In this section, we
    will repeat the efficiency optimization flow three times, each time with different
    requirements:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第四章](ch04.html#ch-hardware) 中，我们介绍了一个简单的 `Sum` 实现，在 [示例 4-1](ch04.html#code-sum)
    中对文件中提供的大量整数求和。^([2](ch10.html#idm45606823764528)) 让我们利用你所学的所有知识，并用它来优化 [示例 4-1](ch04.html#code-sum)。正如我们在
    [“资源有效性要求”](ch03.html#ch-conq-req) 中学到的那样，我们不能“只是”优化——我们必须有一些目标。在本节中，我们将三次重复效率优化流程，每次都有不同的要求：
- en: Lower latency with a maximum of one CPU used
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最多只使用一个 CPU 的低延迟。
- en: Minimal amount of memory
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小内存使用量
- en: Even lower latency with four CPU cores available for the workload
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使有四个 CPU 核心可用于工作负载，延迟也更低。
- en: The terms *lower* or *minimal* are not very professional. Ideally, we have some
    more specific numbers to aim for, in a written form like a RAER. A quick Big O
    analysis can tell us that the `Sum` runtime complexity is at least O(*N*)—we have
    to revisit all lines at least once to compute the sum. Thus, the absolute latency
    goal, like “`Sum` has to be faster than 100 milliseconds,” won’t work as its problem
    space depends on the input. We can always find big enough input that violates
    any latency goals.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: “较低”或“最小”这样的术语并不太专业。理想情况下，我们有一些更具体的数字来作为目标，像RAER这样的书面形式。快速的大O分析可以告诉我们，`Sum`的运行时复杂度至少是O(*N*)
    ——我们必须至少重新访问所有行一次来计算总和。因此，像“`Sum`必须快于100毫秒”这样的绝对延迟目标是行不通的，因为它的问题空间取决于输入。我们总是可以找到足够大的输入来违反任何延迟目标。
- en: 'One way to address this is to specify the maximum possible input with some
    assumptions and latency goals. The second is to define the required runtime complexity
    as a function that depends on input—so throughput. Let’s do the latter and specify
    the amortized latency function for the `Sum`. We can do the same with memory.
    So let’s be more specific. Imagine that, for my hardware, a system design stakeholder
    came up with the following required goals for the `Sum` in [Example 4-1](ch04.html#code-sum):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这个问题的一种方法是指定一些假设和延迟目标下的最大可能输入。第二种方法是定义一个取决于输入的运行时复杂度的函数 ——所以吞吐量。让我们选择后者，并为[示例 4-1](ch04.html#code-sum)中的`Sum`指定摊销的延迟函数。我们可以在内存上做同样的事情。所以让我们更具体些。想象一下，对于我的硬件，一个系统设计利益相关者为[示例 4-1](ch04.html#code-sum)中的`Sum`制定了以下所需的目标：
- en: Maximum latency of 10 nanoseconds per line (10 * *N* nanoseconds) with maximum
    one CPU used
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每行最多10纳秒的延迟（10 * *N* 纳秒），最多使用一个CPU
- en: Latency as above and a maximum of 10 KB of memory allocated on the heap for
    any input
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像上面那样的延迟和任何输入的堆内存最多分配10 KB
- en: Maximum latency of 2.5 nanoseconds per line (2.5 * *N* nanoseconds) with maximum
    four CPU used
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每行最多2.5纳秒的延迟（2.5 * *N* 纳秒），最多使用四个CPU
- en: What If We Can’t Match This Goal?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果我们无法达到这个目标怎么办？
- en: It might be the case that the goals we initially aimed for will be hard to achieve
    due to underestimation of the problem, new requirements, or new knowledge. This
    is fine. In many cases, we can try to renegotiate the goals. For example, as we
    dissected in [“Optimization Design Levels”](ch03.html#ch-conq-opt-levels), every
    optimization beyond a certain point costs more and more in time, effort, risk,
    and readability, so it might be cheaper to add more machines, CPUs, or RAM to
    the problem. The key is to estimate those costs roughly and help stakeholders
    decide what’s best for them.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于问题的低估、新的需求或新的知识，我们最初设定的目标可能很难实现。这没问题。在许多情况下，我们可以尝试重新协商目标。例如，正如我们在[“优化设计级别”](ch03.html#ch-conq-opt-levels)中详细讨论的那样，每一次超越某一点的优化在时间、精力、风险和可读性方面的成本都会越来越高，所以增加更多的机器、CPU或RAM可能更便宜。关键是大致估计这些成本，并帮助利益相关者决定什么对他们最好。
- en: Following the TFBO flow, before we optimize, we first have to benchmark. Fortunately,
    we already discussed designs of benchmarks for the `Sum` code in [“Go Benchmarks”](ch08.html#ch-obs-micro-go),
    so we can go ahead and use [Example 8-13](ch08.html#code-sum-go-bench3) for our
    benchmarks. I used the command presented in [Example 10-1](#code-sum-bench-cmd)
    to perform 5 10-second benchmarks with a 2 million integer input file and limited
    to 1 CPU.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 按照TFBO流程，在优化之前，我们首先要进行基准测试。幸运的是，我们已经讨论过在[“Go基准测试”](ch08.html#ch-obs-micro-go)中为`Sum`代码设计基准测试，所以我们可以继续使用[示例 8-13](ch08.html#code-sum-go-bench3)进行我们的基准测试。我使用了[示例 10-1](#code-sum-bench-cmd)中呈现的命令，对一个包含200万个整数的文件执行了5个10秒钟的基准测试，并限制为1个CPU。
- en: Example 10-1\. The command to invoke the benchmark
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-1\. 调用基准测试的命令
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With [Example 4-1](ch04.html#code-sum), the preceding benchmark yielded the
    following results: 101 ms, 60.8 MB space allocated, and 1.60 million allocations
    per operation. Therefore, we will use that as our baseline.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[示例 4-1](ch04.html#code-sum)，前述基准测试产生了以下结果：101 毫秒，分配了60.8 MB空间，并且每次操作分配了1.60百万次内存。因此，我们将以此为基准。
- en: Optimizing Latency
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化延迟
- en: Our requirements are clear. We need to make the `Sum` function in [Example 4-1](ch04.html#code-sum)
    faster to achieve a throughput of at least 10 * *N* nanoseconds. The baseline
    results give us 50 * *N* nanoseconds. Time to see if there are any quick optimizations!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的要求很明确。我们需要使[示例 4-1](ch04.html#code-sum)中的`Sum`函数更快，以达到至少10 * *N* 纳秒的吞吐量。基准结果显示我们的基准值是50
    * *N* 纳秒。是时候看看是否有快速的优化方法了！
- en: In [“Complexity Analysis”](ch07.html#ch-hw-complexity), I shared a detailed
    complexity of the `Sum` function that clearly outlines the problems and bottlenecks.
    However, I used information from this section to define that. For now, let’s forget
    that we discussed such complexity and try to find all the information from scratch.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [“复杂度分析”](ch07.html#ch-hw-complexity) 中，我分享了 `Sum` 函数的详细复杂性，清楚地概述了问题和瓶颈。然而，我使用了这一部分的信息来定义。现在，让我们忘记我们讨论过这样的复杂性，并试图从头开始找到所有信息。
- en: The best way is to perform a bottleneck analysis using the profiles explained
    in [Chapter 9](ch09.html#ch-observability3). I captured the CPU profile on every
    benchmark with [Example 8-4](ch08.html#code-sum-go-bench-all), so I could quickly
    bring the Flame Graph of the CPU time, as presented in [Figure 10-1](#img-opt-lat-v1).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的方法是使用 [第 9 章](ch09.html#ch-observability3) 中解释的配置文件执行瓶颈分析。我在每个基准测试中捕获了 CPU
    配置文件，所以我可以快速带来 CPU 时间的 Flame Graph，正如 [图 10-1](#img-opt-lat-v1) 所示。
- en: '![efgo 1001](assets/efgo_1001.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![efgo 1001](assets/efgo_1001.png)'
- en: Figure 10-1\. Flame Graph view of [Example 4-1](ch04.html#code-sum) CPU time
    with function granularity
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. [Example 4-1](ch04.html#code-sum) CPU 时间的 Flame Graph 视图，函数粒度
- en: 'Profiling gives us a great overview of the situation. We see four clear major
    contributors to the CPU time usage:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 分析配置文件为我们提供了情况的概述。我们看到四个明确的主要 CPU 时间使用贡献者：
- en: '`bytes.Split`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bytes.Split`'
- en: '`strconv.ParseInt`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strconv.ParseInt`'
- en: Runtime function `runtime.slicebytetostr...`, which ends with `runtime.malloc`,
    meaning we spent a lot of CPU time allocating memory
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行时函数 `runtime.slicebytetostr...`，以 `runtime.malloc` 结尾，这意味着我们花费了大量 CPU 时间来分配内存
- en: Runtime function `runtime.gcBgMarkWorker`, which indicates GC runs
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行时函数 `runtime.gcBgMarkWorker`，表示 GC 运行
- en: The CPU profile gives us a list of functions we can go through and potentially
    cut out some CPU usage. However, as we learned in [“Off-CPU Time”](ch09.html#ch-obs-pprof-latency),
    the CPU time might not be a bottleneck here. Therefore, we must first confirm
    if our function here is CPU bound, I/O bound, or mixed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 配置文件提供了一个函数列表，我们可以查看并可能削减一些 CPU 使用率。然而，正如我们在 [“Off-CPU Time”](ch09.html#ch-obs-pprof-latency)
    中学到的，这里的 CPU 时间可能并不是一个瓶颈。因此，我们必须首先确认我们的函数是 CPU 绑定、I/O 绑定还是混合型。
- en: One way of doing this is by manually reading the source code. We can see that
    the only external medium used in [Example 4-1](ch04.html#code-sum) is a file,
    which we use to read bytes from. The rest of the code should only perform computations
    using the memory and CPU.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是通过手动阅读源代码来完成这项工作。我们可以看到，在 [Example 4-1](ch04.html#code-sum) 中唯一使用的外部介质是文件，我们用它来读取字节。代码的其余部分应只使用内存和
    CPU 进行计算。
- en: This makes this code a mixed-bound job, but how mixed? Should we start with
    file reads optimization or CPU time?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得这段代码成为混合型任务，但是多混合？我们应该从文件读取优化还是 CPU 时间开始？
- en: The best way to find this out is the data-driven way. Let’s check both CPU and
    off-CPU latency thanks to the full goroutine profile (`fgprof`) discussed in [“Off-CPU
    Time”](ch09.html#ch-obs-pprof-latency). To collect it in the Go benchmark, I quickly
    wrapped our benchmark from [Example 8-13](ch08.html#code-sum-go-bench3) with the
    `fgprof` profile in [Example 10-2](#code-sum-go-bench-fgprof).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 发现这一点最好的方法是数据驱动的方式。我们来检查 CPU 和 off-CPU 的延迟情况，这得益于完整的 goroutine 配置文件（`fgprof`），在
    [Example 8-13](ch08.html#code-sum-go-bench3) 的基准测试中，我快速地用 `fgprof` 配置文件包装了我们的基准测试。
- en: Example 10-2\. Go benchmark with `fgprof` profiling
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-2\. 使用 `fgprof` 进行 Go 基准测试
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_optimization_examples_CO1-1)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimization_examples_CO1-1)'
- en: To get more reliable results, we have to measure for longer than five seconds.
    Let’s measure for 60 seconds to be sure.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更可靠的结果，我们必须进行长达 60 秒的测量。让我们测量 60 秒以确保。
- en: '[![2](assets/2.png)](#co_optimization_examples_CO1-2)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_optimization_examples_CO1-2)'
- en: To reuse code and have better reliability, we can execute the same [Example 8-13](ch08.html#code-sum-go-bench3)
    benchmark, just wrapped with the `fgprof` profile.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了重复使用代码并提高可靠性，我们可以执行相同的 [Example 8-13](ch08.html#code-sum-go-bench3) 基准测试，只是用
    `fgprof` 配置文件包装起来。
- en: The resulting `fgprof.pprof` profile after 60 seconds is presented in [Figure 10-2](#img-opt-lat-v1-fgprof).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过 60 秒后，生成的 `fgprof.pprof` 配置文件显示在 [图 10-2](#img-opt-lat-v1-fgprof) 中。
- en: '![efgo 1002](assets/efgo_1002.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![efgo 1002](assets/efgo_1002.png)'
- en: Figure 10-2\. Flame Graph view of [Example 4-1](ch04.html#code-sum) CPU and
    off-CPU time with function granularity
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. [Example 4-1](ch04.html#code-sum) CPU 和 off-CPU 时间的 Flame Graph 视图，函数粒度
- en: 'The full goroutine profile confirms that our workload is a mix of I/O (5%^([3](ch10.html#idm45606823492960)))
    and CPU time (majority). So while we have to worry about latency introduced by
    file I/O at some point, we can optimize CPU time first. So let’s go ahead and
    focus on the biggest bottleneck first: the `bytes.Split` function that takes almost
    36% of the `Sum` CPU time, as seen in [Figure 10-1](#img-opt-lat-v1).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的goroutine配置文件确认我们的工作负载是I/O（5%^([3](ch10.html#idm45606823492960)))和CPU时间（大部分）。因此，虽然我们必须担心某些时候由文件I/O引入的延迟，但我们可以先优化CPU时间。所以让我们继续专注于最大的瓶颈：几乎占用`Sum`
    CPU时间36%的`bytes.Split`函数，如[图 10-1](#img-opt-lat-v1)所示。
- en: Optimize One Thing at a Time
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一次优化一件事情
- en: Thanks to [Figure 10-1](#img-opt-lat-v1), we found four main bottlenecks. However,
    I have chosen to focus on the biggest one in our first optimization in [Example 10-3](#code-sum2).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了[图 10-1](#img-opt-lat-v1)，我们找到了四个主要瓶颈。然而，在我们的第一次优化中，我选择集中在[示例 10-3](#code-sum2)中的最大瓶颈。
- en: It is important to iterate one optimization at a time. It feels slower than
    if we would try to optimize all we know about now, but in practice, it is more
    effective. Each optimization might affect the other and introduce more unknowns.
    We can draw more reliable conclusions, e.g., compare the contributions percentage
    between profiles. Furthermore, why eliminate four bottlenecks if optimizing first
    might be enough to match our requirements?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是一次进行一种优化。感觉比起现在尝试优化我们所知的一切要慢，但实际上更有效。每种优化可能会影响其他优化，并引入更多未知因素。我们可以得出更可靠的结论，例如比较配置文件之间的贡献百分比。此外，为什么消除四个瓶颈，如果优化首先可能足以满足我们的需求？
- en: Optimizing bytes.Split
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化`bytes.Split`
- en: 'To figure out where the CPU time is spent in `bytes.Split`, we have to try
    to understand what this function does and how. By [definition](https://oreil.ly/UqAg8),
    it splits a large byte slice into smaller slices based on the potentially multicharacter
    separator `sep`. Let’s quickly look at the [Figure 10-1](#img-opt-lat-v1) profile
    and focus on that function using the `Refine` options. This would show [`bytes.Index`](https://oreil.ly/DQrCS),
    and impact allocations and garbage collections with functions like `makeslice`
    and `runtime.gcWriteBarrierDX`. Furthermore, we could quickly look into the Go
    source code for the [`genSplit`](https://oreil.ly/pCMH1) used by `bytes.Split`
    to check how it’s implemented. This should give us a few warning signals. There
    might be things that `bytes.Split` does but might not be necessary for our case:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出CPU时间花在`bytes.Split`中的地方，我们必须试着理解这个函数做什么以及如何做到的。根据[定义](https://oreil.ly/UqAg8)，它根据可能的多字符分隔符`sep`将一个大字节切片分割成更小的切片。让我们快速查看[图 10-1](#img-opt-lat-v1)的配置文件，并专注于使用`Refine`选项的该函数。这将显示[`bytes.Index`](https://oreil.ly/DQrCS)，并使用`makeslice`和`runtime.gcWriteBarrierDX`等函数影响分配和垃圾收集。此外，我们还可以快速查看用于`bytes.Split`的[`genSplit`](https://oreil.ly/pCMH1)的Go源代码，以检查它是如何实现的。这应该给我们一些警告信号。也许`bytes.Split`做的事情可能对我们的情况不必要：
- en: '`genSplit` goes through the slices first [to count how many slices we expect](https://oreil.ly/Wq6F4)
    to have.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`genSplit`首先通过切片[计算我们预期的切片数](https://oreil.ly/Wq6F4)。'
- en: '`genSplit` allocates [a two-dimensional byte slice](https://oreil.ly/YzXdr)
    to put the results in. This is scary because for a large 7.2 MB byte slice with
    2 million lines, it will allocate a slice with 2 million elements. A memory profile
    confirms that a lot of memory is allocated by this line.^([4](ch10.html#idm45606823466592))'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`genSplit`分配[一个二维字节切片](https://oreil.ly/YzXdr)来放置结果。这很可怕，因为对于一个大的7.2 MB字节切片，有200万行，它将分配一个有200万元素的切片。内存配置文件确认这一行分配了大量的内存。^([4](ch10.html#idm45606823466592))'
- en: Then it will iterate two million times using the [`bytes.Index`](https://oreil.ly/8diMw)
    function we saw in the profile. That is two million times we will go and gather
    bytes until the next separator.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后它将使用我们在配置文件中看到的[`bytes.Index`](https://oreil.ly/8diMw)函数迭代两百万次。这是我们将收集字节直到下一个分隔符的两百万次。
- en: The separator in `bytes.Split` is a multicharacter, which requires a more complicated
    algorithm. Yet we need a simple, single-line newline separator.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bytes.Split`中的分隔符是多字符的，需要更复杂的算法。然而，我们需要一个简单的，单行的换行符分隔符。'
- en: Unfortunately, such an analysis of the mature standard library functions might
    be difficult for more beginner Go developers. What parts of this CPU time or memory
    usage are excessive, and what aren’t?
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这种对成熟标准库函数的分析可能对于初学者Go开发者来说有些困难。这些CPU时间或内存使用中哪些是过度的，哪些不是？
- en: What always helps me to answer this question is to go back to the algorithm
    design phase and try to design my own simplest splitting-lines algorithm tailored
    for the `Sum` problem. When we understand what a simple, efficient algorithm could
    look like and we are happy with it, we can then start challenging existing implementations.
    It turns out there is a very simple flow that might work for [Example 4-1](ch04.html#code-sum).
    Let’s go through it in [Example 10-3](#code-sum2).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 始终帮助我回答这个问题的是回到算法设计阶段，尝试设计适合 `Sum` 问题的最简单分割线算法。当我们了解一个简单而高效的算法可能是什么样子，并对其满意时，我们可以开始挑战现有的实现。结果表明，有一个非常简单的流程可能适用于
    [例子 4-1](ch04.html#code-sum)。让我们在 [例子 10-3](#code-sum2) 中详细了解一下。
- en: Example 10-3\. `Sum2` is [Example 4-1](ch04.html#code-sum) with optimized CPU
    bottleneck of `bytes.Split`
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-3\. `Sum2` 是优化了 `bytes.Split` 的 CPU 瓶颈的 [例子 4-1](ch04.html#code-sum)。
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_optimization_examples_CO2-1)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimization_examples_CO2-1)'
- en: We record the index of the last seen newline, plus one, to tell where the next
    line starts.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们记录了最后一个看到的换行符的索引，再加一，以告知下一行从哪里开始。
- en: '[![2](assets/2.png)](#co_optimization_examples_CO2-2)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_optimization_examples_CO2-2)'
- en: Compared to `bytes.Split`, we can hardcode a new line as our separator. In one
    loop iteration, while reusing the `b` byte slice, we can find the full line, parse
    the integer, and perform the sum. This algorithm is also often called “in place.”
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `bytes.Split` 相比，我们可以硬编码一个新行作为我们的分隔符。在一个循环迭代中，同时重用 `b` 字节切片，我们可以找到完整的行，解析整数并执行求和。这种算法通常也被称为“原地”。
- en: Before we come to any conclusion, we have to first check if our new algorithm
    works functionally. After successfully verifying it using the unit test, I ran
    [Example 8-13](ch08.html#code-sum-go-bench3) with the `Sum2` function instead
    of `Sum` to assess its efficiency. The results are optimistic, with 50 ms and
    12.8 MB worth of allocations. Compared to `bytes.Split`, we could perform 50%
    less work while using 78% less memory. Knowing that `bytes.Split` was responsible
    for ~36% of CPU time and 78.6% of memory allocations, such an improvement tells
    us we completely removed this bottleneck from our code!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在得出任何结论之前，我们必须首先检查我们的新算法是否在功能上工作正常。在成功通过单元测试验证后，我使用 `Sum2` 函数而不是 `Sum` 运行了 [例子 8-13](ch08.html#code-sum-go-bench3)
    来评估其效率。结果乐观，耗时 50 毫秒，分配了 12.8 MB。与 `bytes.Split` 相比，我们可以完成的工作少 50%，同时内存减少了 78%。知道
    `bytes.Split` 负责约 36% 的 CPU 时间和 78.6% 的内存分配，这样的改进告诉我们我们已完全消除了代码中的这一瓶颈！
- en: Standard Functions Might Not Be Perfect for All Cases
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准函数可能并非所有情况都完美适用
- en: The preceding example of working optimization asks why the `bytes.Split` function
    wasn’t optimal for us. Can’t the Go community optimize it?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的工作优化示例问为什么 `bytes.Split` 函数对我们不够优化。难道 Go 社区不能优化它吗？
- en: The answer is that `bytes.Split` and other standard or custom functions you
    might import on the internet could be not as efficient as the tailored algorithm
    for your requirements. Such a popular function has to be, first of all, reliable
    for many edge cases that you might not have (e.g., multicharacter separator).
    Those are often optimized for cases that might be more involved and complex than
    our own.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是 `bytes.Split` 和其他标准或自定义的从互联网上可能导入的函数可能不如专门为您的需求量身定制的算法高效。这样一个流行的函数首先必须对许多您可能没有的边缘情况（例如多字符分隔符）可靠。这些通常针对可能比我们自己更复杂的情况进行了优化。
- en: It doesn’t mean we have to rewrite all imported functions now. No, we should
    just be aware of the possibility of easy efficiency gains by providing a tailored
    implementation for critical paths. Still, we should use known and battle-tested
    code like a standard library. In most cases, it’s good enough!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着我们现在必须重新编写所有导入的函数。不，我们只需要意识到通过为关键路径提供量身定制的实现，可能会轻松获得效率提升的可能性。尽管如此，我们仍应使用已知并经过战斗测试的标准库代码。在大多数情况下，它已经足够好了！
- en: Is our [Example 10-3](#code-sum2) optimization our final one? Not quite—while
    we improved the throughput, we are at the 25 * *N* nanoseconds mark, still far
    from our goal.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 [例子 10-3](#code-sum2) 优化是最终版本吗？并不完全是 —— 虽然我们提高了吞吐量，但我们仍处于 25 * *N* 纳秒的标记之外。
- en: Optimizing runtime.slicebytetostring
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化 `runtime.slicebytetostring`
- en: The CPU profile from the [Example 10-3](#code-sum2) benchmark should give us
    a clue about the next bottleneck, shown in [Figure 10-3](#img-opt-lat-v2).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[例子 10-3](#code-sum2) 基准测试的 CPU 分析应该为我们提供下一个瓶颈的线索，显示在 [图 10-3](#img-opt-lat-v2)
    中。'
- en: '![efgo 1003](assets/efgo_1003.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![efgo 1003](assets/efgo_1003.png)'
- en: Figure 10-3\. Flame Graph view of [Example 10-3](#code-sum2) CPU time with function
    granularity
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. [示例 10-3](#code-sum2) CPU 时间的 Flame Graph 视图，具有函数粒度
- en: As the next bottleneck, let’s take this odd `runtime.slicebytetostring` function
    that spends most of its CPU time allocating memory. If we look for it in the Source
    or Peek view, it points us to the `num, err := strconv.ParseInt(string(b[last:i]),
    10, 64)` line in [Example 10-3](#code-sum2). Since this CPU time contribution
    is not accounted for to `strconv.ParseInt` (a separate segment), it tells us that
    it has to be executed before we invoke `strconv ParseInt`, yet in the same code
    line. The only dynamically executed things are the `b` byte slice subslicing and
    conversion to string. On further inspection, we can tell that the string conversion
    is expensive here.^([5](ch10.html#idm45606823189696))
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一个瓶颈，让我们看看这个奇怪的 `runtime.slicebytetostring` 函数，它在大部分 CPU 时间中用于分配内存。如果我们在
    Source 或 Peek 视图中查找它，它会指向[示例 10-3](#code-sum2)中的 `num, err := strconv.ParseInt(string(b[last:i]),
    10, 64)` 行。由于这段 CPU 时间贡献没有计入 `strconv ParseInt`（一个单独的部分），它告诉我们，在调用 `strconv ParseInt`
    之前必须执行它，然而在同一行代码中。唯一动态执行的事情是 `b` 字节切片的子切片和转换为字符串。进一步检查时，我们可以看到这里的字符串转换是昂贵的。^([5](ch10.html#idm45606823189696))
- en: What’s interesting is that [`string`](https://oreil.ly/7dv5w) is essentially
    a special [`byte` slice](https://oreil.ly/fYwwq) with no `Cap` field (capacity
    in `string` is always equal to length). As a result, at first it might be surprising
    that the Go compiler spends so much time and memory on this. The reason is that
    `string(<byte slice>)` is equivalent to creating a new byte slice with the same
    number of elements, copying all bytes to a new byte, and then returning the string
    from it. The main reason for copying is that, by design, [`string` type is immutable](https://oreil.ly/I4fER),
    so every function can use it without worrying about potential races. There is,
    however, a relatively safe way to convert `[]byte` to `string`. Let’s do that
    in [Example 10-4](#code-sum3).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，[`string`](https://oreil.ly/7dv5w) 本质上是一个没有 `Cap` 字段（字符串的容量始终等于长度）的特殊[`字节切片`](https://oreil.ly/fYwwq)。因此，起初可能会感到惊讶的是，Go
    编译器在这方面花费了这么多时间和内存。原因在于，`string(<byte slice>)` 相当于创建一个具有相同元素数量的新字节切片，将所有字节复制到一个新的字节中，然后从中返回字符串。复制的主要原因是，按设计，[`string`
    类型是不可变的](https://oreil.ly/I4fER)，因此每个函数都可以在不用担心潜在竞争的情况下使用它。然而，有一种相对安全的方法可以将 `[]byte`
    转换为 `string`。我们在[示例 10-4](#code-sum3)中来做这个。
- en: Example 10-4\. `Sum3` is [Example 10-3](#code-sum2) with optimized CPU bottleneck
    of string conversion
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-4\. `Sum3` 是优化了 CPU 瓶颈的字符串转换的[示例 10-3](#code-sum2)
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_optimization_examples_CO3-1)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimization_examples_CO3-1)'
- en: We can use the `unsafe` package to remove the type information from `b` and
    form an `unsafe.Pointer`. Then we can dynamically cast this to different types,
    e.g., `string`. It is unsafe because if the structures do not share the same layout,
    we might have memory safety problems or nondeterministic values. Yet the layout
    is shared between `[]byte` and `string`, so it’s safe for us. It is used in production
    in many projects, including Prometheus, known as [`yoloString`](https://oreil.ly/QmqCn).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `unsafe` 包从 `b` 中删除类型信息并形成 `unsafe.Pointer`。然后我们可以动态地将其转换为不同的类型，例如 `string`。这是不安全的，因为如果结构体的布局不相同，可能会出现内存安全问题或者非确定性的值。然而，`[]byte`
    和 `string` 之间的布局是共享的，所以对我们来说是安全的。它在许多项目中的生产环境中使用，包括被称为[`yoloString`](https://oreil.ly/QmqCn)的
    Prometheus。
- en: The `zeroCopyToString` allows us to convert file bytes to string required by
    `ParseInt` with almost no overhead. After functional tests, we can confirm this
    by using the same benchmark with the `Sum3` function again. The benefit is clear—`Sum3`
    takes 25.5 ms for 2 million integers and 7.2 MB of allocated space. This means
    it is 49.2% faster than [Example 10-3](#code-sum2) when it comes to CPU time.
    The memory usage is also better, with our program allocating almost precisely
    the size of the input file—no more, no less.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`zeroCopyToString` 允许我们将文件字节转换为 `ParseInt` 所需的字符串，几乎没有额外开销。在功能测试后，通过与 `Sum3`
    函数相同的基准测试，我们可以确认这一点。好处显而易见——对于 200 万个整数，`Sum3` 只需 25.5 毫秒，并且分配了 7.2 MB 的空间。这意味着在
    CPU 时间上，它比[示例 10-3](#code-sum2)快了 49.2%。内存使用也更好，我们的程序几乎精确分配了输入文件的大小——既不多也不少。'
- en: Deliberate Trade-offs
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 明智的权衡
- en: With unsafe, no-copy bytes to string conversion, we enter a deliberate optimization
    area. We introduced potentially unsafe code and added more nontrivial complexity
    to our code. While we clearly named our function `zeroCopyToString`, we have to
    justify and use such optimization only if necessary. In our case, it helps us
    reach our efficiency goals, so we can accept these drawbacks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不安全的、零拷贝的字节到字符串转换，我们进入了一个有意的优化领域。我们引入了潜在的不安全代码，并为我们的代码增加了更多非平凡的复杂性。虽然我们明确地将我们的函数命名为
    `zeroCopyToString`，但我们必须仅在必要时才能证明并使用这样的优化。在我们的情况下，它帮助我们达到了效率目标，因此我们可以接受这些缺点。
- en: Are we fast enough? Not yet. We are almost there with 12.7 * *N* nanoseconds
    throughput. Let’s see if we can optimize something more.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们足够快了吗？还不够。我们的吞吐量接近于 12.7 * *N* 纳秒。让我们看看是否还能进一步优化。
- en: Optimizing strconv.Parse
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化 strconv.Parse
- en: Again, let’s look at the newest CPU profile from the [Example 10-4](#code-sum3)
    benchmark to see the latest bottleneck we could try to check, as shown in [Figure 10-4](#img-opt-lat-v3).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们从 [示例 10-4](#code-sum3) 的最新 CPU 分析中看到我们可以尝试检查的最新瓶颈，如 [图 10-4](#img-opt-lat-v3)
    所示。
- en: '![efgo 1004](assets/efgo_1004.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![efgo 1004](assets/efgo_1004.png)'
- en: Figure 10-4\. Flame Graph view of [Example 10-4](#code-sum3) CPU time with function
    granularity
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. [示例 10-4](#code-sum3) 的 Flame Graph 视图，显示函数粒度的 CPU 时间
- en: 'With `strconv.Parse` using 72.6%, we can gain a lot if we can improve its CPU
    time. Similar to `bytes.Split`, we should check its profile and [implementation](https://oreil.ly/owR53).
    Following both paths, we can immediately outline a couple of elements that feel
    like excessive work:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `strconv.Parse` 进行 72.6% 的时间，如果能改进其 CPU 时间，我们可以获得很多好处。与 `bytes.Split` 类似，我们应该检查其性能和[实现](https://oreil.ly/owR53)。遵循这两条路径，我们可以立即勾画出一些感觉像是过度工作的元素：
- en: We check for an empty string twice, in [`ParseInt`](https://oreil.ly/gqJpb)
    and [`ParseUint`](https://oreil.ly/BB9Ie). Both are visible as nontrivial CPU
    time used in our profile.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在 [`ParseInt`](https://oreil.ly/gqJpb) 和 [`ParseUint`](https://oreil.ly/BB9Ie)
    中都检查了空字符串两次。在我们的性能分析中，这两者都显示出非常显著的 CPU 时间。
- en: '`ParseInt` allows us to parse to integers with different bases and bit sizes.
    We don’t need this generic functionality or extra input to check our `Sum3` code.
    We only care about 64-bit integers of base 10.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ParseInt` 允许我们以不同的基数和位大小解析整数。但我们不需要这种通用功能或额外的输入来检查我们的 `Sum3` 代码。我们只关心十进制的
    64 位整数。'
- en: 'One solution here is similar to `bytes.Split`: finding or implementing our
    own `ParseInt` function that focuses on efficiency—does what we need and nothing
    more. The standard library offers the [`strconv.Atoi` function](https://oreil.ly/CpZeF),
    which looks promising. However, it still requires strings as input, which forces
    us to use unsafe package code. Instead, let’s try to come up with our own quick
    implementation. After a few iterations of testing and microbenchmarking my new
    `ParseInt` function,^([6](ch10.html#idm45606822879264)) we can come up with the
    fourth iteration of our sum functionality, presented in [Example 10-5](#code-sum4).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个解决方案类似于 `bytes.Split`：找到或实现我们自己的 `ParseInt` 函数，专注于效率——只做我们需要的事情，不多做。标准库提供了
    [`strconv.Atoi` 函数](https://oreil.ly/CpZeF)，看起来很有前途。然而，它仍然需要字符串作为输入，这迫使我们使用不安全包中的代码。相反，让我们尝试自己快速实现一个版本。经过几轮测试和微基准测试我的新
    `ParseInt` 函数^([6](ch10.html#idm45606822879264))，我们得到了我们的求和功能的第四个迭代，展示在 [示例 10-5](#code-sum4)
    中。
- en: Example 10-5\. `Sum4` is [Example 10-4](#code-sum3) with optimized CPU bottleneck
    of string conversion
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-5\. `Sum4` 是带有优化的 CPU 瓶颈的字符串转换的 [示例 10-4](#code-sum3)
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The side effect of our integer parsing optimization is that we can tailor our
    `ParseInt` to parse from a byte slice, not a string. As a result, we can simplify
    our code and avoid unsafe `zeroCopyToString` conversion. After tests and benchmarks,
    we see that `Sum4` achieves 13.6 ms, 46.66% less than [Example 10-4](#code-sum3),
    with the same memory allocations. The full comparison of our sum functions is
    presented in [Example 10-6](#code-sum-go-bench-benchstat-v4) using our beloved
    `benchstat` tool.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们整数解析优化的副作用是，我们可以将我们的 `ParseInt` 适配为从字节片段解析，而不是从字符串解析。因此，我们可以简化我们的代码，避免使用不安全的
    `zeroCopyToString` 转换。经过测试和基准测试，我们看到 `Sum4` 达到了 13.6 毫秒，比 [示例 10-4](#code-sum3)
    减少了 46.66%，同时具有相同的内存分配。我们的求和函数的全面比较呈现在 [示例 10-6](#code-sum-go-bench-benchstat-v4)
    中，使用我们喜爱的 `benchstat` 工具。
- en: Example 10-6\. Running `benchstat` on the results from all four iterations with
    a two million line file
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-6\. 在一个两百万行文件上运行 `benchstat`，查看所有四个迭代结果
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_optimization_examples_CO4-1)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimization_examples_CO4-1)'
- en: Notice that `benchstat` can round some numbers for easier comparison with the
    large number from *v1.txt*. The *v4.txt* result is 13.6 ms, not 14 ms, which can
    make a difference in throughput calculations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`benchstat` 可以四舍五入一些数字，以便与*v1.txt*中的大数字进行比较。*v4.txt* 的结果是13.6 毫秒，而不是14 毫秒，这在吞吐量计算中可能有所不同。
- en: It seems like our hard work paid off. With the current results, we achieved
    6.9 * *N* nanoseconds throughput, which is more than enough to fulfill our first
    goal. However, we only checked it with two million integers. Are we sure the same
    throughput can be maintained with larger or smaller input sizes? Our Big O runtime
    complexity O(*N*) would suggest so, but I ran the same benchmark with 10 million
    integers just in case. The 67.8 ms result gives the 6.78 * *N* nanoseconds throughput.
    This more or less confirms our throughput number.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们的辛勤工作有了回报。通过当前的结果，我们实现了6.9 * *N* 纳秒的吞吐量，这已经足够实现我们的第一个目标。但是，我们只检查了200万个整数。我们能确定相同的吞吐量能够在更大或更小的输入大小下保持吗？我们的大
    O 运行时复杂度 O(*N*) 表明可以，但为了确保，我也用1000万个整数运行了相同的基准测试。67.8 毫秒的结果给出了6.78 * *N* 纳秒的吞吐量。这多少证实了我们的吞吐量数字。
- en: The code in [Example 10-5](#code-sum4) is not the fastest or most memory-efficient
    solution possible. There might be more optimizations to the algorithm or code
    to improve things further. For example, if we profile [Example 10-5](#code-sum4),
    we would see a relatively new segment, indicating 14% of total CPU time used.
    It’s `os.ReadFile` code that wasn’t so visible on past profiles, given other bottlenecks
    and something we didn’t touch with our optimizations. We will mention its potential
    optimization in [“Pre-Allocate If You Can”](ch11.html#ch-basic-prealloc). We could
    also try concurrency (which we will do in [“Optimizing Latency Using Concurrency”](#ch-opt-latency-concurrency-example)).
    However, with one CPU, we cannot expect a lot of gains here.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 10-5](#code-sum4)中的代码并不是可能的最快或最节省内存的解决方案。可能有更多的优化算法或代码以进一步改进。例如，如果我们分析[示例 10-5](#code-sum4)，我们将看到一个相对较新的段，表明总
    CPU 时间的14% 被使用。这是`os.ReadFile`的代码，在过去的配置文件中不太显眼，因为有其他瓶颈和我们未进行优化的一些内容。我们将在[“如果可以的话进行预分配”](ch11.html#ch-basic-prealloc)中提到其潜在优化。我们还可以尝试并发（我们将在[“使用并发优化延迟”](#ch-opt-latency-concurrency-example)中进行）。但是，由于只有一个
    CPU，我们不能在这里期望太多的收益。'
- en: What’s important is that there is no need to improve anything else in this iteration,
    as we achieved our goal. We can stop the work and claim success! Fortunately,
    we did not need to add magic or dangerous nonportable tricks to our optimization
    flow. Only readable and easier deliberate optimizations were required.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，此迭代中无需改进其他任何内容，因为我们已经实现了我们的目标。我们可以停止工作并宣布成功！幸运的是，我们不需要在优化流程中添加魔法或危险的非可移植技巧。只需要可读性和更容易进行的刻意优化即可。
- en: Optimizing Memory Usage
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化内存使用
- en: 'In the second scenario, our goal is focused on memory consumption while maintaining
    the same throughput. Imagine we have a new business customer for our software
    with `Sum` functionality that needs to run on an IoT device with little RAM available
    for this program. As a result, the requirement is to have a streaming algorithm:
    no matter the input size, it can only use 10 KB of heap memory in a single moment.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种情况下，我们的目标是在保持相同吞吐量的同时集中在内存消耗上。假设我们的软件有一个新的商业客户需要在一台只有少量 RAM 的 IoT 设备上运行具有`Sum`功能的程序。因此，要求具有流式算法：无论输入大小如何，它在任何时刻只能使用10
    KB 堆内存。
- en: Such a requirement might look extreme at first glance, given the naive code
    in [Example 4-1](ch04.html#code-sum) has a quite large space complexity. If a
    10 million line, 36 MB file requires 304 MB of heap memory for [Example 4-1](ch04.html#code-sum),
    how can we ensure the same file (or bigger!) can take a maximum of 10 KB of memory?
    Before we start to worry, let’s analyze what we can do on this subject.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这种要求乍一看似乎有些极端，因为[示例 4-1](ch04.html#code-sum)中的天真代码具有相当大的空间复杂度。如果一个有1000万行、36
    MB 大小的文件需要[示例 4-1](ch04.html#code-sum)的304 MB 堆内存，那么我们如何确保同一个文件（或更大！）能最多使用10 KB
    的内存？在我们开始担心之前，让我们先分析一下我们在这个主题上可以做些什么。
- en: Fortunately, we already did some optimization work that improved memory allocations
    as a side effect. Since the latency goal still applies, let’s start with `Sum4`
    in [Example 10-5](#code-sum4), which fulfills that. The space complexity of `Sum4`
    seems to be around O(*N*). It still depends on the input size and is far from
    our 10 KB goal.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们已经做了一些优化工作，作为副作用改进了内存分配。由于延迟目标仍然适用，让我们从[示例 10-5](#code-sum4)中的 `Sum4`
    开始，这符合目标。`Sum4` 的空间复杂度似乎在 O(*N*) 左右。它仍然取决于输入大小，并且远未达到我们的 10 KB 目标。
- en: Moving to Streaming Algorithm
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转向流算法
- en: Let’s pull up the heap profile from the `Sum4` benchmark in [Figure 10-5](#img-opt-mem-v4)
    to figure out what we can improve.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从[图 10-5](#img-opt-mem-v4)中拉取`Sum4`基准的堆配置文件，以找出我们可以改进的地方。
- en: '![efgo 1005](assets/efgo_1005.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![efgo 1005](assets/efgo_1005.png)'
- en: Figure 10-5\. Flame Graph view of [Example 10-5](#code-sum4) heap allocations
    with function granularity (`alloc_space`)
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. [示例 10-5](#code-sum4)的堆分配的火焰图视图，具有函数粒度 (`alloc_space`)
- en: The memory profile is very boring. The first line allocates 99.6% of memory
    in [Example 10-5](#code-sum4). We essentially read the whole file into memory
    so we can iterate over the bytes in memory. Even if we waste some allocation elsewhere,
    we can’t see it because of excessive allocation from `os.ReadFile`. Is there anything
    we can do about that?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 内存配置文件非常无聊。第一行在[示例 10-5](#code-sum4)中分配了99.6% 的内存。我们基本上将整个文件读入内存，以便可以在内存中迭代字节。即使我们在其他地方浪费了一些分配，也无法看到，因为由于从
    `os.ReadFile` 中过度分配而无法看到它。有什么办法可以解决这个问题吗？
- en: During our algorithm, we must go through all the bytes in the file; thus, we
    have to read all bytes eventually. However, we don’t need to read all of them
    to memory at the same time. Technically, we only need a byte slice big enough
    to hold all digits for an integer to be parsed. This means we can try to design
    [the external memory algorithm](https://oreil.ly/Dr3MB) to stream bytes in chunks.
    We can try using the existing bytes scanner from the standard library—the [`bufio.Scanner`](https://oreil.ly/CqiG7).
    For example, `Sum5` in the [Example 10-7](#code-sum5) implementation uses it to
    scan enough memory to read and parse a line.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的算法中，我们必须遍历文件中的所有字节；因此，我们最终必须读取所有字节。但是，我们不需要一次将所有字节都读入内存。从技术上讲，我们只需要一个足够大的字节切片来容纳整数的所有数字以进行解析。这意味着我们可以尝试设计[外部内存算法](https://oreil.ly/Dr3MB)以按块流式传输字节。我们可以尝试使用标准库中的现有字节扫描器——[`bufio.Scanner`](https://oreil.ly/CqiG7)。例如，[示例 10-7](#code-sum5)中的
    `Sum5` 实现就使用它来扫描足够的内存以读取和解析一行。
- en: Example 10-7\. `Sum5` is [Example 10-5](#code-sum4) with `bufio.Scanner`
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-7\. `Sum5` 是使用 `bufio.Scanner` 的[示例 10-5](#code-sum4)
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_optimization_examples_CO5-1)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimization_examples_CO5-1)'
- en: Instead of reading the whole file into memory, we open the file descriptor here.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 不是将整个文件读入内存，我们在此打开文件描述符。
- en: '[![2](assets/2.png)](#co_optimization_examples_CO5-2)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_optimization_examples_CO5-2)'
- en: We have to make sure the file is closed after the computation so as not to leak
    resources. We use `errcapture` to get notified about potential errors in the deferred
    file `Close`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须确保在计算完成后关闭文件，以防资源泄漏。我们使用 `errcapture` 在延迟关闭的文件中通知可能的错误。
- en: '[![3](assets/3.png)](#co_optimization_examples_CO5-3)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_optimization_examples_CO5-3)'
- en: The scanner `.Scan()` method tells us if we hit the end of the file. It returns
    true if we still have bytes to result in splitting. The split is based on the
    provided function in the `.Split` method. By default, [`ScanLines`](https://oreil.ly/YUpLU)
    is what we want.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描器的 `.Scan()` 方法告诉我们是否达到文件末尾。如果还有字节要导致分割，则返回 true。分割基于 `.Split` 方法中提供的函数。默认情况下，我们想要的是
    [`ScanLines`](https://oreil.ly/YUpLU)。
- en: '[![4](assets/4.png)](#co_optimization_examples_CO5-4)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_optimization_examples_CO5-4)'
- en: Don’t forget to check the scanner error! With such iterator interfaces, it’s
    very easy to forget to check its error.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记检查扫描器的错误！使用这样的迭代器接口，很容易忘记检查其错误。
- en: To assess efficiency, now focusing more on memory, we can use the same [Example 8-13](ch08.html#code-sum-go-bench3)
    with `Sum5`. However, given our past optimizations, we’ve moved dangerously close
    to what can be reasonably measured within the accuracy and overhead of our tools
    for input files on the order of a million lines. If we got into microsecond latencies,
    our measurements might be skewed, given limits in the instrumentation accuracy
    and benchmarking tool overheads. So let’s increase the file to 10 million lines.
    The benchmarked `Sum4` in [Example 10-5](#code-sum4) for that input results in
    67.8 ms and 36 MB of memory allocated per operation. The `Sum5` with the scanner
    outputs 157.1 ms and 4.33 KB per operation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估效率，现在更专注于内存，我们可以使用相同的[示例 8-13](ch08.html#code-sum-go-bench3)来进行`Sum5`。然而，考虑到我们过去的优化，我们已经接近可以合理测量的限度，考虑到工具在处理百万行输入文件时的准确性和开销。如果我们陷入微秒级的延迟中，由于仪器的准确性和基准测试工具的开销限制，我们的测量结果可能会有所偏差。因此，让我们将文件增加到1000万行。对于这个输入，在[示例 10-5](#code-sum4)中的基准化`Sum4`每次操作结果为67.8毫秒，内存分配36
    MB。带有扫描器输出的`Sum5`为157.1毫秒和4.33 KB每次操作。
- en: In terms of memory usage, this is great. If we look at the implementation, the
    scanner [allocates an initial 4 KB](https://oreil.ly/jbpJc) and uses it for reading
    the line. It increases this if needed when the line is longer, but our file doesn’t
    have numbers longer than 10 digits, so it stays at 4 KB. Unfortunately, the scanner
    isn’t fast enough for our latency requirement. With a 131% slowdown to `Sum4`,
    we hit 15.6 * *N* nanoseconds latency, which is too slow. We have to optimize
    latency again, knowing we still have around 6 KB to allocate to stay within the
    10 KB memory goal.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 就内存使用而言，这是很好的。如果我们看看实现，扫描器[分配了初始的4 KB](https://oreil.ly/jbpJc)，并将其用于读取行。如果行更长，它会根据需要增加这个值，但是我们的文件没有超过10位数字的情况，所以保持在4
    KB。不幸的是，扫描器对于我们的延迟要求来说并不快速。与`Sum4`相比，减慢了131%，我们达到了15.6 **N** 纳秒的延迟，这太慢了。我们必须再次优化延迟，知道我们还有大约6
    KB可以分配，以便在10 KB的内存目标内。
- en: Optimizing bufio.Scanner
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化bufio.Scanner
- en: What can we improve? As usual, it’s time to check the source code and profile
    of [Example 10-7](#code-sum5) in [Figure 10-6](#img-opt-lat-v5).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能改进些什么？像往常一样，现在是时候检查[示例 10-7](#code-sum5)的源代码和性能分析了，参见[图 10-6](#img-opt-lat-v5)。
- en: '![efgo 1006](assets/efgo_1006.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![efgo 1006](assets/efgo_1006.png)'
- en: Figure 10-6\. Graph view of [Example 10-7](#code-sum5) CPU time with function
    granularity
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-6\. [示例 10-7](#code-sum5)的CPU时间图表，具有函数粒度
- en: The commentary on the `Scanner` structure in the standard library gives us a
    hint. It tells us that “[`Scanner` is for safe, simple jobs”](https://oreil.ly/6eXZE).
    The `ScanLines` is the main bottleneck here, and we can swap the implementation
    with a more efficient one. For example, the original function removes [carriage
    return (CR) control characters](https://oreil.ly/wwUbC), which wastes cycles for
    us as our input does not have them. I managed to provide optimized `ScanLines`,
    which improves the latency by 20.5% to 125 ms, which is still too slow.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 标准库中`Scanner`结构的评论给了我们一个提示。它告诉我们“[`Scanner`适用于安全、简单的工作”](https://oreil.ly/6eXZE)。`ScanLines`在这里是主要的瓶颈，我们可以用一个更高效的实现来替换它。例如，原始函数去除了[回车（CR）控制字符](https://oreil.ly/wwUbC)，这对我们来说浪费了循环，因为我们的输入中没有它们。我设法提供了优化的`ScanLines`，它将延迟提高了20.5%，达到了125毫秒，这仍然太慢了。
- en: Similar to previous optimizations, it might be worth writing a custom streamed
    scanning implementation instead of `bufio.Scanner`. The `Sum6` in [Example 10-8](#code-sum6)
    presents a potential solution.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于之前的优化，可能值得编写一个自定义的流式扫描实现，而不是使用`bufio.Scanner`。[示例 10-8](#code-sum6)中的`Sum6`提供了一个潜在的解决方案。
- en: Example 10-8\. `Sum6` is [Example 10-5](#code-sum4) with buffered read
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-8\. `Sum6`是带有缓冲读取的[示例 10-5](#code-sum4)。
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](assets/1.png)](#co_optimization_examples_CO6-1)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimization_examples_CO6-1)'
- en: We create a single 8 KB buffer of bytes we will use for reading. I chose 8 KB
    and not 10 KB to leave some headroom within our 10 KB limit. The 8 KB also feels
    like a great number given the OS page is 4 KB, so we know it will need only 2
    pages.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个单一的8 KB字节缓冲区用于读取。我选择了8 KB而不是10 KB，以留出在我们10 KB限制内的一些余地。8 KB对于操作系统页面为4
    KB来说也是一个不错的数字，所以我们知道它只需要2页。
- en: This buffer assumes that no integer is larger than ~8,000 digits. We can make
    it much smaller, even down to 10, as we know our input file does not have numbers
    with more than 9 digits (plus the newline). However, this would make the algorithm
    much slower due to the certain waste explained in the next steps. Additionally,
    even without waste reading, 8 KB is faster than reading 8 bytes 1,024 times due
    to overhead.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此缓冲区假设没有大于~8,000位的整数。我们可以将其缩小得多，甚至降至10，因为我们知道我们的输入文件没有超过9位数字（加上换行符）。然而，由于下一步解释的某些浪费，这将使算法变得更慢。此外，即使没有浪费读取，由于开销，8
    KB也比1,024次读取8字节快得多。
- en: '[![2](assets/2.png)](#co_optimization_examples_CO6-2)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_optimization_examples_CO6-2)'
- en: This time, let’s separate functionality behind the convenient `io.Reader` interface.
    This will allow us to reuse `Sum6Reader` in the future.^([7](ch10.html#idm45606821709296))
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，让我们将便捷的`io.Reader`接口背后的功能分离出来。这将允许我们在未来重用`Sum6Reader`。^([7](ch10.html#idm45606821709296))
- en: '[![3](assets/3.png)](#co_optimization_examples_CO6-3)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_optimization_examples_CO6-3)'
- en: In each iteration, we read the next 8 KB, minus `offset` bytes from a file.
    We start reading more file bytes after `offset` bytes to leave potential room
    for digits we didn’t parse yet. This can happen if we read bytes that split some
    numbers into parts, e.g., we read `...\n12` and `34/n...` in two different chunks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代中，我们从文件中读取接下来的8 KB，减去`offset`字节。我们从`offset`字节后开始读取更多的文件字节，以留出尚未解析的数字的空间。如果我们读取的字节将一些数字分成多个部分，比如我们分两个不同的块读取`...\n12`和`34/n...`。
- en: '[![4](assets/4.png)](#co_optimization_examples_CO6-4)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_optimization_examples_CO6-4)'
- en: In the error handling, we excluded the `io.EOF` sentinel error, which indicated
    we hit the end of the file. That’s not an error for us—we still want to process
    the remaining bytes.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在错误处理中，我们排除了`io.EOF`哨兵错误，这表示我们到达文件末尾。对于我们来说，这不是错误——我们仍然希望处理剩余的字节。
- en: '[![5](assets/5.png)](#co_optimization_examples_CO6-5)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_optimization_examples_CO6-5)'
- en: The number of bytes we have to process from the buffer is exactly `n + offset`,
    where `n` is the number of bytes read from a file. The end of file `n` can be
    smaller than what we asked for (length of the `buf`).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须从缓冲区处理的字节数恰好是`n + offset`，其中`n`是从文件中读取的字节数。文件结束的`n`可能小于我们请求的长度（`buf`的长度）。
- en: '[![6](assets/6.png)](#co_optimization_examples_CO6-6)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_optimization_examples_CO6-6)'
- en: We iterate over `n` bytes in the `buf` buffer.^([8](ch10.html#idm45606821736720))
    Notice that we don’t iterate over the whole slice because in an `err == io.EOF`
    situation, we might read less than 10 KB of bytes, so we need to process only
    `n` of them. We process all lines found in our 10 KB buffer in each loop iteration.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`buf`缓冲区中迭代`n`字节。^([8](ch10.html#idm45606821736720))请注意，我们不会在整个切片上进行迭代，因为在`err
    == io.EOF`的情况下，我们可能读取的字节数少于10 KB，因此我们只需要处理其中的`n`字节。在每次循环迭代中，我们处理在我们的10 KB缓冲区中找到的所有行。
- en: '[![7](assets/7.png)](#co_optimization_examples_CO6-7)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_optimization_examples_CO6-7)'
- en: We calculate `offset`, and if there is a need for one, we shift the remaining
    bytes to the front. This creates a small waste in CPU, but we don’t allocate anything
    additional. Benchmarks will tell us if this is fine or not.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算`offset`，如果需要，我们将剩余的字节移到前面。这会在CPU中创建少量的浪费，但我们不会额外分配任何东西。基准测试将告诉我们这是否可以接受。
- en: Our `Sum6` code got a bit bigger and more complex, so hopefully, it gives good
    efficiency results to justify the complexity. Indeed, after the benchmark, we
    see it takes 69 ms and 8.34 KB. Just in case, let’s put [Example 10-8](#code-sum6)
    to the extra test by computing an even larger file—100 million lines. With bigger
    input, `Sum6` yields 693 ms and around 8 KB. This gives us a 6.9 * *N* nanoseconds
    latency (runtime complexity) and space (heap) complexity of ~8 KB, which satisfies
    our goal.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`Sum6`代码变得有些庞大和复杂，因此希望它能够通过效率测试来证明复杂性的合理性。确实，在基准测试之后，我们看到它需要69毫秒和8.34 KB。以防万一，让我们通过计算更大的文件——1亿行，来对[Example 10-8](#code-sum6)进行额外测试。对于更大的输入，`Sum6`产生693毫秒和约8
    KB。这给出了6.9 * *N*纳秒的延迟（运行时复杂度）和约8 KB的空间（堆）复杂度，这符合我们的目标。
- en: Careful readers might still be wondering if I didn’t miss anything. Why is space
    complexity 8 KB, not 8 + *x* KB? There are some additional bytes allocated for
    10 million line files and even more bytes for larger ones. How do we know that
    at some point for a hundred-times larger file, the memory allocation would not
    exceed 10 KB?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细的读者可能仍然在想我是否漏掉了什么。为什么空间复杂度是 8 KB，而不是 8 + *x* KB？对于 1 千万行文件，会分配一些额外的字节，对于更大的文件则会分配更多的字节。我们如何知道在某个时刻对于百倍大小的文件，内存分配不会超过
    10 KB？
- en: If we are very strict and tight on that 10 KB allocation goal, we can try to
    figure out what happens. The most important thing is to validate that there is
    nothing that grows allocation with the file size. This time the memory profile
    is also invaluable, but to understand things fully, let’s ensure we record all
    allocations by adding `runtime.MemProfileRate = 1` in our `BenchmarkSum` benchmark.
    The resulting profile is presented in [Figure 10-7](#img-opt-mem-v6).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对 10 KB 分配目标非常严格和严密，我们可以尝试弄清楚发生了什么。最重要的是验证，除了文件大小之外没有任何增长分配的东西。这次，内存分析也是无价的，但为了全面理解事物，让我们确保通过在我们的
    `BenchmarkSum` 基准测试中添加 `runtime.MemProfileRate = 1` 来记录所有分配。生成的分析显示在 [图 10-7](#img-opt-mem-v6)
    中。
- en: '![efgo 1007](assets/efgo_1007.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![efgo 1007](assets/efgo_1007.png)'
- en: Figure 10-7\. Flame Graph view of [Example 10-8](#code-sum6) memory with function
    granularity and profile rate 1
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-7\. [示例 10-8](#code-sum6) 内存的火焰图视图，具有函数粒度和 1 的配置文件速率
- en: We can see more allocations from the `pprof` package than our function. This
    indicates a relatively large allocation overhead by the profiling itself! Still,
    it does not prove that `Sum` does not allocate anything else on the heap than
    our 8 KB buffer. The Source view turns out to be helpful, presented in [Figure 10-8](#img-opt-mem-source-v6).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 `pprof` 包中的分配比我们的函数更多。这表明了通过分析本身的较大分配开销！但是，这并不能证明 `Sum` 除了我们的 8 KB 缓冲区之外没有在堆上分配其他任何东西。源视图证明是有帮助的，显示在
    [图 10-8](#img-opt-mem-source-v6) 中。
- en: '![efgo 1008](assets/efgo_1008.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![efgo 1008](assets/efgo_1008.png)'
- en: Figure 10-8\. Source view of [Example 10-8](#code-sum6) memory with profile
    rate 1 after benchmark with 1,000 iterations and 10 MB input file
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-8\. [示例 10-8](#code-sum6) 内存的源视图，使用 1 的配置文件速率进行 1,000 次迭代和 10 MB 输入文件的基准测试后
- en: It shows that `Sum6` has only one heap allocation point. We can also benchmark
    without CPU profiling, which now gives stable 8,328 heap allocated bytes for any
    input size.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 它显示 `Sum6` 只有一个堆分配点。我们还可以在不使用 CPU 分析的情况下进行基准测试，这现在为任何输入大小稳定分配了 8,328 个字节的堆。
- en: Success! Our goal is met, and we can move to the last task. The overview of
    each iteration’s achieved result is shown in [Example 10-9](#code-sum-go-bench-benchstat-v6).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们的目标已达成，我们可以转向最后的任务。各次迭代的概述结果显示在 [示例 10-9](#code-sum-go-bench-benchstat-v6)
    中。
- en: Example 10-9\. Running `benchstat` on the results from all 3 iterations with
    a 10 million line file
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-9\. 对来自所有 3 次迭代的结果运行 `benchstat`，使用 1 千万行文件
- en: '[PRE8]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Optimizing Latency Using Concurrency
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用并发优化延迟
- en: 'Hopefully, you are ready for the last challenge: getting our latency down even
    more to the 2.5 nanoseconds per line level. This time we have four CPU cores available,
    so we can try introducing some concurrency patterns to achieve it.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您已经准备好迎接最后的挑战：将我们的延迟进一步降低到每行 2.5 纳秒的水平。这次我们有四个 CPU 内核可用，因此我们可以尝试引入一些并发模式来实现它。
- en: In [“When to Use Concurrency”](ch04.html#ch-hw-concurrency-when), we mentioned
    the clear need for concurrency to employ asynchronous programming or event handling
    in our code. We talked about relatively easy gains where our Go program does a
    lot of I/O operations. However, in this section, I would love to show you how
    to improve the speed of our `Sum` in the [Example 4-1](ch04.html#code-sum) code
    using concurrency with two typical pitfalls. Because of the tight latency requirement,
    let’s take an already optimized version of `Sum`. Given we don’t have any memory
    requirements, and `Sum4` in [Example 10-5](#code-sum4) is only a little slower
    than `Sum6`, yet has a smaller amount of lines, let’s take that as a start.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [“何时使用并发”](ch04.html#ch-hw-concurrency-when) 中，我们提到了在代码中使用异步编程或事件处理需要并发的明确需求。我们谈到了在我们的
    Go 程序中进行大量 I/O 操作时可以轻松获得的性能提升。然而，在本节中，我愿意向您展示如何通过使用并发来改进我们在 [示例 4-1](ch04.html#code-sum)
    中 `Sum` 的速度，其中包含两个典型的陷阱。由于严格的延迟要求，让我们采用已经优化过的 `Sum` 的版本。鉴于我们没有任何内存需求，并且 [示例 10-5](#code-sum4)
    中的 `Sum4` 比 `Sum6` 稍慢一些，但代码行数较少，让我们从那里开始。
- en: A Naive Concurrency
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个天真的并发
- en: As usual, let’s pull out the [Example 10-5](#code-sum4) CPU profile, shown in
    [Figure 10-9](#img-opt-lat-v4).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，让我们提取 [Example 10-5](#code-sum4) 的 CPU 分析，显示在 [图 10-9](#img-opt-lat-v4)
    中。
- en: '![efgo 1009](assets/efgo_1009.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![efgo 1009](assets/efgo_1009.png)'
- en: Figure 10-9\. Graph view of [Example 10-5](#code-sum4) CPU time with function
    granularity
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '[Example 10-5](#code-sum4) 的 Graph 视图，显示了函数细粒度的 CPU 时间'
- en: As you might have noticed, most of [Example 10-5](#code-sum4) CPU time comes
    from `ParseInt` (47.7%). Since we’re back to reading the whole file at the beginning
    of the program, the rest of the program is strictly CPU bound. As a result, with
    only one CPU we couldn’t expect better latency with [the concurrency](https://oreil.ly/rsLff).
    However, given that within this task we have four CPU cores available, our task
    now is to find a way to evenly split the work of parsing the file’s contents with
    as little coordination^([9](ch10.html#idm45606821549792)) between goroutines as
    possible. Let’s explore three example approaches to optimize [Example 10-5](#code-sum4)
    with concurrency.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能注意到的那样，[Example 10-5](#code-sum4) 的大部分 CPU 时间来自于 `ParseInt`（47.7%）。由于我们又回到了程序开头对整个文件的读取，所以程序的其余部分严格受限于
    CPU。因此，即使只有一个 CPU，我们也不能指望使用并发技术获得更好的延迟。然而，考虑到我们有四个 CPU 核心可用，我们现在的任务是找到一种方法，尽可能少地在
    goroutine 之间协调，均匀地分割解析文件内容的工作。让我们探索三种优化 [Example 10-5](#code-sum4) 的并发方法。
- en: The first thing we have to do is find computations we can do independently at
    the same time—computations that do not affect each other. Because the sum is commutative,
    it does not matter in what order numbers are added. The naive, concurrent implementation
    could parse the integer from the string and add the result atomically to the shared
    variable. Let’s explore this rather simple solution in [Example 10-10](#code-sum-concurrent1).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要做的事情是找到可以同时独立进行的计算——不会相互影响的计算。因为求和是可交换的，数字加法的顺序无关紧要。朴素的并发实现可以将整数从字符串中解析出来，并原子地将结果添加到共享变量中。让我们在
    [Example 10-10](#code-sum-concurrent1) 中探索这个相当简单的解决方案。
- en: Example 10-10\. Naive concurrent optimization to [Example 10-5](#code-sum4)
    that spins a new goroutine for each line to compute
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例子 10-10\. 对 [Example 10-5](#code-sum4) 的朴素并发优化，每行为计算创建一个新的 goroutine
- en: '[PRE9]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: After the successful functional test, it’s time for benchmarking. Similar to
    previous steps, we can reuse the same [Example 8-13](ch08.html#code-sum-go-bench3)
    by simply replacing `Sum` with `ConcurrentSum1`. I also changed the `-cpu` flag
    to 4 to unlock the four CPU cores. Unfortunately, the results are not very promising—for
    a 2 million line input, it takes about 540 ms and 151 MB of allocated space per
    operation! Almost 40 times more time than the simpler, noncurrent [Example 10-5](#code-sum4).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功完成功能测试后，现在是进行基准测试的时候了。与之前的步骤类似，我们可以简单地将 `Sum` 替换为 `ConcurrentSum1`，并将 `-cpu`
    标志更改为 4，以解锁四个 CPU 核心。不幸的是，结果并不太理想——对于 200 万行的输入，每个操作大约需要 540 毫秒和 151 MB 的分配空间！比更简单、非并发的
    [Example 10-5](#code-sum4) 多花了大约 40 倍的时间。
- en: A Worker Approach with Distribution
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用分发的工作方法
- en: Let’s check the CPU profile in [Figure 10-10](#img-opt-lat-vc1) to learn why.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [图 10-10](#img-opt-lat-vc1) 中的 CPU 分析结果，以了解原因。
- en: '![efgo 1010](assets/efgo_1010.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![efgo 1010](assets/efgo_1010.png)'
- en: Figure 10-10\. Flame Graph view of [Example 10-10](#code-sum-concurrent1) CPU
    time with function granularity
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '[Example 10-10](#code-sum-concurrent1) 的 Flame Graph 视图，显示了函数细粒度的 CPU 时间'
- en: 'The Flame Graph clearly shows the goroutine creation and scheduling overhead
    indicated by blocks called `runtime.schedule` and `runtime.newproc`. There are
    three main reasons why [Example 10-10](#code-sum-concurrent1) is too naive and
    not recommended for our case:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Flame Graph 明确显示了由 `runtime.schedule` 和 `runtime.newproc` 所指示的 goroutine 创建和调度开销。以下是
    [Example 10-10](#code-sum-concurrent1) 太过朴素且在我们的情况下不推荐的三个主要原因：
- en: The concurrent work (parsing and adding) is too fast to justify the goroutine
    overhead (both in memory and CPU usage).
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并发工作（解析和添加）速度太快，无法证明 goroutine 的开销（无论在内存还是 CPU 使用上）是值得的。
- en: For larger datasets, we create potentially millions of goroutines. While goroutines
    are relatively cheap and we can have hundreds of them, there is always a limit,
    given only four CPU cores to execute. So you can imagine the delay of the scheduler
    that tries to fairly schedule millions of goroutines on four CPU cores.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于较大的数据集，我们可能会创建数百万个 goroutine。虽然 goroutine 相对较便宜且我们可以有数百个，但鉴于只有四个 CPU 核心可执行，调度器可能会出现延迟问题。
- en: Our program will have a nondeterministic performance depending on the number
    of lines in the file. We can potentially hit a problem of unbounded concurrency
    since we will spam as many goroutines as the external file has lines (something
    outside our program control).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的程序的性能将是不确定的，这取决于文件中的行数。由于我们会像外部文件的行数一样生成多个 goroutine（这超出了我们程序的控制），因此可能会遇到无界并发的问题。
- en: That is not what we want, so let’s improve our concurrent implementation. There
    are many ways we could go from here, but let’s try to address all three problems
    we notice. We can solve problem number one by assigning more work to each goroutine.
    We can do that thanks to the fact that addition is also associative and cumulative.
    We can essentially group work into multiple lines, parse and add numbers in each
    goroutine, and add partial results to the total sum. Doing that automatically
    helps with problem number two. Grouping work means we will schedule fewer goroutines.
    The question is, what is the best number of lines in a group? Two? Four? A hundred?
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是我们想要的，所以让我们改进我们的并发实现。我们可以从这里尝试多种方法，但让我们试图解决我们注意到的三个问题。我们可以通过给每个 goroutine
    分配更多的工作来解决问题一。由于加法也是可结合和累积的，我们可以将工作分组成多行，在每个 goroutine 中解析和添加数字，并将部分结果添加到总和中。这自动有助于解决问题二。分组工作意味着我们将调度更少的
    goroutine。问题是，每组行的最佳数量是多少？两个？四个？一百个？
- en: The answer most likely depends on the number of goroutines we want in our process
    and the number of CPUs available. There is also problem number three—unbounded
    concurrency. The typical solution here is to use a worker pattern (sometimes called
    goroutine pooling). In this pattern, we agree on a number of goroutines up front,
    and we schedule all of them at once. Then we can create another goroutine that
    will distribute the work evenly. Let’s see an example implementation of that algorithm
    in [Example 10-11](#code-sum-concurrent2). Can you predict if this implementation
    will be faster?
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分答案可能取决于我们希望在我们的进程中使用多少 goroutine 和可用的 CPU 数量。还有第三个问题——无界并发。在这里的典型解决方案是使用工作模式（有时称为
    goroutine 池）。在这种模式下，我们事先确定了一些 goroutine 的数量，并一次性调度它们。然后我们可以创建另一个 goroutine，它将工作均匀地分发。让我们看一个在
    [示例 10-11](#code-sum-concurrent2) 中实现该算法的例子。你能预测这个实现会更快吗？
- en: Example 10-11\. Concurrent optimization of [Example 10-5](#code-sum4) that maintains
    a finite set of goroutines that computes a group of lines. Lines are distributed
    using another goroutine.
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 10-11\. 并发优化 [示例 10-5](#code-sum4) 的方法，保持一组有限的 goroutine 来计算一组行。使用另一个
    goroutine 进行行的分发。
- en: '[PRE10]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_optimization_examples_CO7-1)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimization_examples_CO7-1)'
- en: Remember, the sender is usually responsible for the closing channel. Even if
    our flow does not depend on it, it’s a good practice to always close channels
    after use.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，发送方通常负责关闭通道。即使我们的流程不依赖它，始终在使用后关闭通道是一个好的实践。
- en: '[![2](assets/2.png)](#co_optimization_examples_CO7-2)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_optimization_examples_CO7-2)'
- en: Beware of common mistakes. The `for _, line := range <-workCh` would sometimes
    compile as well, and it looks logical, but it’s wrong. It will wait for the first
    message from the `workCh` channel and iterate over single bytes from the received
    byte slice. Instead, we want to iterate over messages.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要注意常见的错误。`for _, line := range <-workCh` 有时也会编译通过，看起来逻辑上没有问题，但是这是错误的。它将等待从 `workCh`
    通道接收的第一个消息，并迭代接收到的字节片。相反，我们希望迭代消息。
- en: Tests pass, so we can start benchmarking. Unfortunately, on average, this implementation
    with 4 goroutines takes 207 ms to complete a single operation (using 7 MB of space).
    Still, this is 15 times slower than simpler, sequential [Example 10-5](#code-sum4).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 测试通过，所以我们可以开始进行基准测试。不幸的是，平均而言，这个使用 4 个 goroutine 的实现每次完成一个操作需要 207 毫秒（使用 7 MB
    空间）。仍然比简单的顺序执行的 [示例 10-5](#code-sum4) 慢了 15 倍。
- en: A Worker Approach Without Coordination (Sharding)
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无协调的工作方法（分片）
- en: What’s wrong this time? Let’s investigate the CPU profile presented in [Figure 10-11](#img-opt-lat-vc2).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这次有什么问题？让我们来研究一下 [图 10-11](#img-opt-lat-vc2) 中展示的 CPU 分析报告。
- en: '![efgo 1011](assets/efgo_1011.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![efgo 1011](assets/efgo_1011.png)'
- en: Figure 10-11\. Flame Graph view of [Example 10-11](#code-sum-concurrent2) CPU
    time with function granularity
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-11\. [示例 10-11](#code-sum-concurrent2) 的火焰图视图，显示了 CPU 时间与函数粒度
- en: 'If you see a profile like this, it should immediately tell you that the concurrency
    overhead is again too large. We still don’t see the actual work, like parsing
    integers, since this work has outnumbered the overhead. This time the overhead
    is caused by three elements:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您看到这样的性能分析，它应立即告诉您，并发开销再次过大。我们仍然看不到实际工作，比如解析整数，因为这项工作比开销多得多。这次开销是由三个元素引起的：
- en: '`runtime.schedule`'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`runtime.schedule`'
- en: The runtime code responsible for scheduling goroutines.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 负责调度goroutines的运行时代码。
- en: '`runtime.chansend`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`runtime.chansend`'
- en: In our case, waiting on the lock to send to our single channel.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，等待锁以发送到我们的单个通道。
- en: '`runtime.chanrecv`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`runtime.chanrecv`'
- en: The same as `chansend` but waiting on a read from the receive channel.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 与`chansend`相同，但等待从接收通道读取。
- en: As a result, parsing and additions are faster than the communication overhead.
    Essentially, coordination and distribution of the work take more CPU resources
    than the work itself.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，解析和添加比通信开销更快。基本上，协调和工作的分发比工作本身需要更多的CPU资源。
- en: We have multiple options for improvement here. In our case, we can try to remove
    the effort of distributing the work. We can accomplish this via a coordination-free
    algorithm that will shard (split) the workload evenly across all goroutines. It’s
    coordination free because there is no communication to agree on which part of
    the work is assigned to each goroutine. We can do that thanks to the fact that
    the file size is known up front, so we can use some sort of heuristic to assign
    each part of the file with multiple lines to each goroutine worker. Let’s see
    how this could be implemented in [Example 10-12](#code-sum-concurrent3).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里有多种改进选项。在我们的情况下，我们可以尝试消除分发工作的努力。我们可以通过无需协调的算法来实现这一点，该算法将工作负载均匀分布到所有goroutines中。这是无需协调的，因为没有通信来协商分配给每个goroutine的工作的部分。由于文件大小是预先知道的，因此我们可以使用某种启发式方法将每个文件部分分配给每个goroutine工作者。让我们看看如何在[示例 10-12](#code-sum-concurrent3)中实现这一点。
- en: Example 10-12\. Concurrent optimization of [Example 10-5](#code-sum4) that maintains
    a finite set of goroutines that computes groups of lines. Lines are sharded without
    coordination.
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-12\. 对[示例 10-5](#code-sum4)的并发优化，维护了一组有限的goroutines，计算线组。线条被分片而无需协调。
- en: '[PRE11]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_optimization_examples_CO8-1)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimization_examples_CO8-1)'
- en: '`shardedRange` is not supplied for clarity. This function takes the size of
    the input file and splits into `bytesPerWorker` shards (four in our case). Then
    it gives each worker the `i`-th shard. You can see the full code [here](https://oreil.ly/By9wO).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，未提供`shardedRange`。此函数将输入文件的大小分割成`bytesPerWorker`（我们的情况下是四个）个片段，然后将第`i`个片段分配给每个工作器。您可以在[这里](https://oreil.ly/By9wO)查看完整的代码。
- en: Tests pass too, so we confirmed that [Example 10-12](#code-sum-concurrent3)
    is functionally correct. But is it faster? Yes! The benchmark shows 7 ms and 7
    MB per operation, which is almost twice as fast as sequential [Example 10-5](#code-sum4).
    Unfortunately, this puts us in 3.4 * *N* nanoseconds throughput, which is failing
    our goal of 2.5 * *N*.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 测试也通过了，所以我们确认了[示例 10-12](#code-sum-concurrent3)在功能上是正确的。但是速度呢？是的！基准测试显示每个操作需要
    7 毫秒和 7 MB，几乎比顺序执行的[示例 10-5](#code-sum4)快了一倍。不幸的是，这使我们的吞吐量达到了 3.4 * *N* 纳秒，远低于我们的目标
    2.5 * *N*。
- en: A Streamed, Sharded Worker Approach
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式分片工作器方法
- en: Let’s profile in [Figure 10-12](#img-opt-lat-vc3) one more time to check if
    we can improve anything easily.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次在[图 10-12](#img-opt-lat-vc3)中进行分析，看看是否可以轻松改进一些内容。
- en: The CPU profile shows that the work done by our goroutines takes the most CPU
    time. However, ~10% of CPU time is spent reading all bytes, which we can also
    try to do concurrently. This effort does not look promising at first glance. However,
    even if we would remove all 10% of the CPU time, 10% better throughput gives us
    only the 3.1 * *N* nanoseconds number, so not enough.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: CPU分析显示，我们的goroutines完成的工作占用了大部分CPU时间。然而，大约10%的CPU时间用于读取所有字节，我们也可以尝试并发执行。乍一看，这项工作似乎并不看好。然而，即使我们去除所有10%的CPU时间，10%的性能提升只能给我们带来3.1
    * *N* 纳秒的数字，还不够。
- en: '![efgo 1012](assets/efgo_1012.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![efgo 1012](assets/efgo_1012.png)'
- en: Figure 10-12\. Flame Graph view of [Example 10-12](#code-sum-concurrent3) CPU
    time with function granularity
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-12\. [示例 10-12](#code-sum-concurrent3)的火焰图视图，以函数粒度查看CPU时间
- en: This is where we have to be vigilant, though. As you can imagine, reading files
    is not a CPU-bound job, so perhaps the actual real time spend on that 10% of CPU
    time makes `os.ReadFile` a bigger bottleneck, thus a better option for us to optimize.
    As in [“Optimizing Latency”](#ch-opt-latency-example), let’s perform a benchmark
    wrapped with the `fgprof` profile! The resulting full goroutine profile is presented
    in [Figure 10-13](#img-opt-fgprof-vc3).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们在这里必须保持警惕。正如您所想象的，读取文件并不是一个CPU绑定的工作，因此也许实际的实时CPU时间花费在占总CPU时间的10%的`os.ReadFile`上，这使得它成为我们优化的更好选择。就像在[“优化延迟”](#ch-opt-latency-example)中所述，让我们进行一个带有`fgprof`分析的基准测试！结果的完整协程分析呈现在图 10-13 中。
- en: '![efgo 1013](assets/efgo_1013.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![efgo 1013](assets/efgo_1013.png)'
- en: Figure 10-13\. Flame Graph view of [Example 10-12](#code-sum-concurrent3) full
    goroutine profile with function granularity
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-13\. [示例 10-12](#code-sum-concurrent3) 的火焰图视图完整协程分析与函数粒度
- en: The `fgprof` profile shows that a lot can be gained in latency if we try to
    read files concurrently, as it currently takes around 50% of the real time! This
    is way more promising, so let’s try to move file reads to worker goroutines. The
    example implementation is shown in [Example 10-13](#code-sum-concurrent4).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`fgprof`分析显示，如果我们尝试并发读取文件，可以显著减少延迟，因为当前实际时间大约耗费了50%！这更具前景，所以让我们尝试将文件读取移动到工作协程中。示例实现在[示例 10-13](#code-sum-concurrent4)中展示。'
- en: Example 10-13\. Concurrent optimization of [Example 10-12](#code-sum-concurrent3)
    that also reads from a file concurrently using separate buffers
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-13\. 并发优化[示例 10-12](#code-sum-concurrent3)，同时使用单独的缓冲区从文件读取
- en: '[PRE12]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_optimization_examples_CO9-1)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimization_examples_CO9-1)'
- en: Instead of splitting the bytes from the input file in memory, we tell each goroutine
    what bytes from the file it can read. We can do this thanks to the [`SectionReader`](https://oreil.ly/j4cQd),
    which returns a reader that only allows reading from a particular section. There
    is a small complexity in [`shardedRangeFrom​Rea⁠derAt`](https://oreil.ly/PwNty)
    to make sure we read all lines (we don’t know where the newlines in a file are),
    but it can be done in the relatively easy algorithm presented here.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是在内存中拆分输入文件的字节，而是告诉每个协程可以从文件中读取哪些字节。我们可以做到这一点，多亏了[`SectionReader`](https://oreil.ly/j4cQd)，它返回一个只允许从特定部分读取的读取器。在[`shardedRangeFrom​Rea⁠derAt`](https://oreil.ly/PwNty)
    中有一些复杂性，以确保我们读取所有行（我们不知道文件中的换行符在哪里），但可以使用这里提出的相对简单的算法来完成。
- en: '[![2](assets/2.png)](#co_optimization_examples_CO9-2)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_optimization_examples_CO9-2)'
- en: We can reuse [Example 10-8](#code-sum6) for this job as it knows how to use
    any `io.Reader` implementation, so in our example, both `*os.File` and `*io.SectionReader`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重用[示例 10-8](#code-sum6) 来完成此任务，因为它知道如何使用任何`io.Reader`实现，所以在我们的示例中，既有`*os.File`也有`*io.SectionReader`。
- en: Let’s assess the efficiency of that code. Finally, after all this work, [Example 10-13](#code-sum-concurrent4)
    yields an astonishing 4.5 ms per operation for 2 million lines, and 23 ms for
    10 million lines. This takes us into ~2.3 * *N* nanosecond throughput, which satisfies
    our goal! A full comparison of latencies and memory allocations for successful
    iterations is presented in [Example 10-14](#code-sum-go-bench-benchstat-vc4).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们评估该代码的效率。最终，在所有这些工作之后，[示例 10-13](#code-sum-concurrent4) 对于200万行操作每次仅需惊人的4.5毫秒，对于1000万行则为23毫秒。这将我们带入了~2.3
    * *N* 纳秒吞吐量，这符合我们的目标！成功迭代的延迟和内存分配的全面比较呈现在[示例 10-14](#code-sum-go-bench-benchstat-vc4)中。
- en: Example 10-14\. Running `benchstat` on the results from all four iterations
    with a two million line file
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-14\. 对四次迭代结果运行`benchstat`，使用200万行文件
- en: '[PRE13]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To summarize, we went through three exercises showcasing the optimization flow
    focused on different goals. I also have some possible concurrency patterns that
    allow utilizing our multicore machines. Generally, I hope you saw how critical
    benchmarking and profiling were throughout this journey! Sometimes the results
    might surprise you, so always seek confirmation of your ideas.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们通过三个展示不同目标优化流程的练习。我还有一些可能的并发模式，可以利用我们的多核机器。总的来说，我希望您能看到基准测试和分析在整个旅程中有多么关键！有时候结果可能会让您惊讶，因此请始终确认您的想法。
- en: There is, however, another way to solve those exercises in an innovative way
    that might work for certain use cases. Sometimes it allows us to avoid the huge
    optimization effort we did in the past three sections. Let’s take a look!
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有另一种创新解决这些练习的方式，可能适用于某些用例。有时，它使我们能够避免过去三个部分中进行的巨大优化工作。让我们来看看！
- en: 'Bonus: Thinking Out of the Box'
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励：超越传统思维
- en: Given the challenging goals we set in this chapter, I spent a lot of time optimizing
    and explaining optimization for the naive `Sum` implementation in [Example 4-1](ch04.html#code-sum).
    This showed you some optimization ideas, practices, and generally a mental model
    I use during optimization efforts. But hard optimization work is not always an
    answer—there are numerous ways to reach our goals.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于本章设定的挑战性目标，我花了大量时间优化和解释在[示例 4-1](ch04.html#code-sum)中的原始 `Sum` 实现。这展示了一些优化思路、实践以及我在优化工作中使用的一般心理模型。但是艰苦的优化工作并不总是答案——达成目标的方式有很多。
- en: For example, what if I told you there is a way to get amortized runtime complexity
    of a few nanoseconds and zero allocations (and just four more code lines)? Let’s
    see [Example 10-15](#code-sum7).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我告诉你有一种方法可以达到摊销的运行时复杂度仅为几纳秒，并且零分配（只需再加四行代码）？我们来看看[示例 10-15](#code-sum7)。
- en: Example 10-15\. Adding simplest caching to [Example 4-1](ch04.html#code-sum)
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-15\. 向[示例 4-1](ch04.html#code-sum)添加最简单的缓存
- en: '[PRE14]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_optimization_examples_CO10-1)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimization_examples_CO10-1)'
- en: '`sumByFile` represents the simplest storage for cache. There are tons of more
    production read-caching implementations you can consider as well. We can write
    our own that will be goroutine safe. If we need more involved eviction policies,
    I would recommend [HashiCorp’s golang-lru](https://oreil.ly/nnYoM) and the even
    more optimized [Dgraph’s ristretto](https://oreil.ly/QNshi). For distributed systems,
    you should use distributed caching services like [Memcached](https://oreil.ly/fudbQ),
    [Redis](https://oreil.ly/1ovP1), or peer-to-peer caching solutions like [groupcache](https://oreil.ly/vJONo).'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`sumByFile` 表示缓存的最简存储方式。还有大量生产读取缓存实现可供考虑。我们可以编写自己的实现，保证协程安全。如果需要更复杂的驱逐策略，我建议使用[HashiCorp
    的 golang-lru](https://oreil.ly/nnYoM)甚至更优化的[Dgraph 的 ristretto](https://oreil.ly/QNshi)。对于分布式系统，应使用像[Memcached](https://oreil.ly/fudbQ)、[Redis](https://oreil.ly/1ovP1)或对等缓存解决方案如[groupcache](https://oreil.ly/vJONo)的分布式缓存服务。'
- en: The functional test passes, and the benchmarks show amazing results—for 100
    million line files, we see 228 ns and 0 bytes allocated! This example is, of course,
    a very trivial one. It’s unlikely our optimization journey is always as easy as
    that. Simple caching is limited and can’t be used if the file input constantly
    changes. But what if we can?
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 功能测试通过了，基准显示出惊人的结果——对于一亿行文件，我们看到了 228 纳秒和 0 字节的分配！当然，这个例子非常简单。我们的优化之旅不可能总是这么轻松。简单缓存是有限的，如果文件输入经常变化，则无法使用。但如果我们可以呢？
- en: Think smart, not hard. It might be the case that we don’t need to optimize [Example 4-1](ch04.html#code-sum)
    because the same input files are constantly used. Caching a single sum value for
    each file is cheap—even if we would have a million of those files, we can cache
    all using a few megabytes. If that’s not the case, perhaps the file content often
    repeats, but the filename is unique. In that case, we could calculate the checksum
    of the file and cache based on that. It would be faster than parsing all lines
    into integers.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 要聪明，不要愚蠢。也许我们不需要对[示例 4-1](ch04.html#code-sum)进行优化，因为相同的输入文件经常被使用。为每个文件缓存单个求和值是廉价的——即使我们有百万个这样的文件，我们也可以使用几兆字节的缓存全部缓存。如果不是这种情况，也许文件内容经常重复，但文件名是唯一的。在这种情况下，我们可以计算文件的校验和，并基于此进行缓存。这比将所有行解析为整数要快。
- en: Focus on the goal and be smart and innovative. For example, a hard, week-long,
    deep optimization effort might not be worth it if there is some smart solution
    that avoids that work!
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 着眼于目标，要聪明和创新。例如，如果有一种聪明的解决方案可以避免艰苦、持续一周的深度优化工作，那么这种优化可能就不值得了！
- en: Summary
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'We did it! We optimized the initial naive implementation of [Example 4-1](ch04.html#code-sum)
    using the TFBO flow from [“Efficiency-Aware Development Flow”](ch03.html#ch-conq-eff-flow).
    Guided by the requirements, we managed to improve the `Sum` code significantly:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功了！我们通过从[“效率感知开发流程”](ch03.html#ch-conq-eff-flow)中引入的 TFBO 流程优化了[示例 4-1](ch04.html#code-sum)最初的原始实现。在需求的指导下，我们成功地改进了
    `Sum` 代码：
- en: We improved the runtime complexity from around 50.5 * *N* nanoseconds (where
    N is a number of lines) to 2.25 * *N*. This means around 22 times faster latency,
    even though both naive and most optimized algorithms are linear (we optimized
    O(*N*) constants).
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将运行时复杂度从大约 50.5 * *N* 纳秒（其中 N 是行数）优化到了 2.25 * *N*。这意味着大约比之前快了 22 倍，尽管原始和大多数优化算法都是线性的（我们优化了
    O(*N*) 的常数）。
- en: We improved the space complexity from around 30.4 * *N* bytes to 8 KB, which
    means our code had O(*N*) asymptotic complexity but now has constant space complexity.
    This means the new `Sum` code will be much more predictable for the users and
    more friendly for the garbage collector.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将空间复杂度从约30.4 * *N*字节降低到8 KB，这意味着我们的代码原本具有O(*N*)的渐进复杂度，但现在具有常量空间复杂度。这意味着新的`Sum`代码对用户更加可预测，对垃圾收集器更加友好。
- en: To sum up, sometimes efficiency problems require a long and careful optimization
    process, as we did for `Sum`. On the other hand, sometimes, you can find quick
    and pragmatic optimization ideas that fulfill your goals quickly. Nevertheless,
    we all learned a lot from the exercises in this chapter (including me!).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，有时效率问题需要一个漫长而仔细的优化过程，就像我们为`Sum`所做的一样。另一方面，有时候，你可以找到快速而实用的优化思路，迅速实现你的目标。无论如何，我们都从本章的练习中学到了很多（包括我自己！）。
- en: Let’s move to the last chapter of this book, where we will summarize some learning
    and patterns we saw during our exercises in this chapter, and what I have seen
    in the community from my experience.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们转向本书的最后一章，在这一章中，我们将总结我们在练习中看到的一些学习和模式，以及我从社区经验中看到的一些内容。
- en: ^([1](ch10.html#idm45606823780656-marker)) For example, I already know about
    a [`strconv.ParseInt` optimization](https://oreil.ly/IZxm7) coming to Go 1.20,
    which would change the memory efficiency of the naive [Example 4-1](ch04.html#code-sum)
    without any optimization from my side.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.html#idm45606823780656-marker)) 例如，我已经了解到一个[`strconv.ParseInt`优化](https://oreil.ly/IZxm7)将在Go
    1.20中推出，这将改变朴素的[示例 4-1](ch04.html#code-sum)的内存效率，而无需我进行任何优化。
- en: ^([2](ch10.html#idm45606823764528-marker)) If you are interested in what input
    files I used, see [the code I used](https://oreil.ly/0SMxA) for generating the
    input.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.html#idm45606823764528-marker)) 如果你对我使用的输入文件感兴趣，请参见我用于生成输入的[代码](https://oreil.ly/0SMxA)。
- en: ^([3](ch10.html#idm45606823492960-marker)) There is a small segment in [Figure 10-2](#img-opt-lat-v1-fgprof)
    that shows `ioutil.ReadFile` latency with 0.38% of all samples. When we unfold
    the `ReadFile`, the `syscall.Read` (which we could assume is an I/O latency) takes
    0.25%, given the `sum.BenchmarkSum_fgprof` contributes to 4.67% of overall wall
    time (the rest is taken by benchmarking and CPU profiling). The (0.25 * 100%)/4.67
    is equal to 5.4%.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch10.html#idm45606823492960-marker)) 在[图 10-2](#img-opt-lat-v1-fgprof)中有一个小段显示`ioutil.ReadFile`的延迟，占所有样本的0.38%。当我们展开`ReadFile`时，`syscall.Read`（我们可以假设是I/O延迟）占据0.25%，考虑到`sum.BenchmarkSum_fgprof`占据了整体壁钟时间的4.67%（其余由基准测试和CPU分析占据）。计算出来（0.25
    * 100%）/ 4.67等于5.4%。
- en: ^([4](ch10.html#idm45606823466592-marker)) We can further inspect that using
    [“Heap”](ch09.html#ch-obs-pprof-heap) profile, which would in my tests show us
    that 78.6% of the total 60.8 MB of allocation per operation is taken by `bytes.Split`!
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch10.html#idm45606823466592-marker)) 我们可以进一步检查使用[“Heap”](ch09.html#ch-obs-pprof-heap)性能分析，根据我的测试，每次操作的总60.8
    MB分配中，有78.6%被`bytes.Split`占用！
- en: ^([5](ch10.html#idm45606823189696-marker)) We can deduce that from the `runtime.slicebytetostring`
    function name in the profile. We can also split this line into three lines (string
    conversion in one, subslicing in the second, and invoking the parsing function
    in the third) and profile again to be sure.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch10.html#idm45606823189696-marker)) 我们可以从性能分析中的`runtime.slicebytetostring`函数名推断出这一点。我们还可以将这一行拆分为三行（第一行进行字符串转换，第二行进行子切片，第三行调用解析函数），并再次进行性能分析，以确保。
- en: ^([6](ch10.html#idm45606822879264-marker)) In benchmarks, I also found that
    my `ParseInt` is also faster by 10% to `strconv.Atoi` for the `Sum` test data.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch10.html#idm45606822879264-marker)) 在基准测试中，我还发现我的`ParseInt`对于`Sum`测试数据比`strconv.Atoi`快10%。
- en: ^([7](ch10.html#idm45606821709296-marker)) Interestingly enough, just adding
    a new function call and interface slows down the program by 7% per operation on
    my machine, proving that we are on a very high efficiency level already. However,
    given reusability, perhaps we can afford that slowdown.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch10.html#idm45606821709296-marker)) 有趣的是，仅仅添加一个新的函数调用和接口，在我的机器上每次操作都会减慢程序7%，证明我们已经处于非常高的效率水平。然而，考虑到可重用性，也许我们可以承受这种减速。
- en: ^([8](ch10.html#idm45606821736720-marker)) As an interesting fact, if we replace
    this line with a technically simpler loop like `for i := 0; i < n; i++ {`, the
    code is 5% slower! Don’t take it as a rule (always measure!), as it probably depends
    on your workload, but it’s interesting to see the `range` loop (without a second
    argument) be more efficient here.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch10.html#idm45606821736720-marker)) 有趣的是，如果我们将这行代码替换为技术上更简单的循环，比如 `for
    i := 0; i < n; i++ {`，代码会慢 5%！不要将其视为规则（始终测量！），因为这可能取决于你的工作负载，但看到没有第二个参数的 `range`
    循环在这里更有效是很有趣的。
- en: ^([9](ch10.html#idm45606821549792-marker)) We discussed synchronization primitives
    in [“Go Runtime Scheduler”](ch04.html#ch-hw-concurrency).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch10.html#idm45606821549792-marker)) 我们在 [“Go Runtime Scheduler”](ch04.html#ch-hw-concurrency)
    中讨论了同步原语。
