- en: Chapter 7\. Running gRPC in Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 章 运行 gRPC 在生产中
- en: In previous chapters, we focused on various aspects of designing and developing
    gRPC-based applications. Now, it’s time to dive into the details of running gRPC
    applications in production. In this chapter, we’ll discuss how you can develop
    unit testing or integration testing for your gRPC services and client as well
    as how you can integrate them with continuous integration tools. Then we’ll move
    into the continuous deployment of a gRPC application where we explore some deployment
    patterns on virtual machines (VMs), Docker, and Kubernetes. Finally, to operate
    your gRPC applications in production environments, you need to have a solid observability
    platform. This is where we will discuss different observability tools for gRPC
    applications and explore troubleshooting and debugging techniques for gRPC applications.
    Let’s begin our discussion with testing these applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们专注于设计和开发基于 gRPC 的应用程序的各个方面。现在，是时候深入探讨如何在生产环境中运行 gRPC 应用程序的细节了。在本章中，我们将讨论如何为您的
    gRPC 服务和客户端开发单元测试或集成测试，以及如何将它们与持续集成工具集成。然后，我们将进入 gRPC 应用程序的持续部署，探讨虚拟机（VM）、Docker
    和 Kubernetes 上的一些部署模式。最后，在生产环境中操作您的 gRPC 应用程序时，您需要一个稳固的可观察性平台。这就是我们将讨论不同的 gRPC
    应用程序可观察性工具，并探索 gRPC 应用程序的故障排除和调试技术的地方。让我们从测试这些应用程序开始讨论。
- en: Testing gRPC Applications
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试 gRPC 应用程序
- en: Any software application that you develop (including gRPC applications) needs
    to have associated unit testing along with the application. As gRPC applications
    always interact with the network, the testing should also cover the network RPC
    aspect of both the server and client gRPC applications. We’ll start by testing
    the gRPC server.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 开发的任何软件应用程序（包括 gRPC 应用程序）都需要与应用程序一起进行关联的单元测试。由于 gRPC 应用程序总是与网络交互，因此测试还应涵盖服务器和客户端
    gRPC 应用程序的网络 RPC 方面。我们将从测试 gRPC 服务器开始。
- en: Testing a gRPC Server
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试 gRPC 服务器
- en: gRPC service testing is often done using a gRPC client application as part of
    the test cases. The server-side testing consists of starting a gRPC server with
    the required gRPC service and then connecting to the server using the client application
    where you implement your test cases. Let’s take a look at a sample test case written
    for the Go implementation of our `ProductInfo` service. In Go, the implementation
    of the gRPC test case should be implemented as a generic test case of Go using
    the `testing` package (see [Example 7-1](#EX7-1)).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC 服务测试通常使用 gRPC 客户端应用程序作为测试用例的一部分进行。服务器端测试包括启动具有所需 gRPC 服务的 gRPC 服务器，然后使用客户端应用程序连接到服务器，在其中实现您的测试用例。让我们看一下为我们的
    `ProductInfo` 服务的 Go 实现编写的示例测试用例。在 Go 中，gRPC 测试用例的实现应作为使用 `testing` 包的通用测试用例来实现（参见
    [Example 7-1](#EX7-1)）。
- en: Example 7-1\. gRPC server-side test using Go
  id: totrans-6
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 7-1\. 使用 Go 进行 gRPC 服务器端测试
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO1-1)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO1-1)'
- en: Conventional test that starts a gRPC server and client to test the service with
    RPC.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 传统测试启动 gRPC 服务器和客户端，以测试通过 RPC 提供服务。
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO1-2)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO1-2)'
- en: Starting a conventional gRPC server running on HTTP/2.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 启动运行在 HTTP/2 上的传统 gRPC 服务器。
- en: '[![3](assets/3.png)](#co_running_grpc_in_production_CO1-3)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_running_grpc_in_production_CO1-3)'
- en: Connecting to the server application.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到服务器应用程序。
- en: '[![4](assets/4.png)](#co_running_grpc_in_production_CO1-4)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_running_grpc_in_production_CO1-4)'
- en: Sends RPC for `AddProduct` method.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 发送用于 `AddProduct` 方法的 RPC。
- en: '[![5](assets/5.png)](#co_running_grpc_in_production_CO1-5)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_running_grpc_in_production_CO1-5)'
- en: Verification of the response message.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 验证响应消息。
- en: As gRPC test cases are based on standard language test cases, the way in which
    you execute them will not be different from a standard test case. One special
    thing about the server-side gRPC tests is that they require the server application
    to open up a port the client application connects to. If you prefer not to do
    this, or your testing environment doesn’t allow it, you can use a library to help
    avoid starting up a service with a real port number. In Go, you can use the [`bufconn`
    package](https://oreil.ly/gOq46), which provides a `net.Conn` implemented by a
    buffer and related dialing and listening functionality. You can find the full
    code sample in the source code repository for this chapter. If you are using Java
    you can use a test framework such as *JUnit* and follow the exact same procedure
    to write a server-side gRPC test. However, if you prefer to write the test case
    without starting a gRPC server instance, then you can use the gRPC in-process
    server of the Java implementation. You can find a complete Java code example for
    this in the code repository of this book.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于gRPC测试用例基于标准语言测试用例，你执行它们的方式与标准测试用例没有区别。关于服务器端的gRPC测试的一个特别之处是，它们要求服务器应用程序打开一个客户端应用程序连接到的端口。如果你不希望这样做，或者你的测试环境不允许这样做，你可以使用一个库来帮助避免使用真实端口号启动服务。在Go中，你可以使用[`bufconn`包](https://oreil.ly/gOq46)，它提供了一个由缓冲区实现的`net.Conn`及相关的拨号和监听功能。你可以在本章的源代码存储库中找到完整的代码示例。如果你使用Java，你可以使用一个测试框架，如*JUnit*，并按照完全相同的过程编写服务器端的gRPC测试用例。但是，如果你希望在不启动gRPC服务器实例的情况下编写测试用例，那么你可以使用Java实现的gRPC内部服务器。你可以在本书的代码存储库中找到这种用Java编写的完整代码示例。
- en: It is also possible to unit test the business logic of the remote functions
    that you develop without going through the RPC network layer. You can instead
    directly test the functions by invoking them without using a gRPC client.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在不经过RPC网络层的情况下，直接测试你开发的远程函数的业务逻辑。你可以直接调用这些函数来进行测试，而不使用gRPC客户端。
- en: With this, we have learned how to write tests for gRPC services. Now let’s talk
    about how to test your gRPC client applications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们已经学会了如何为gRPC服务编写测试。现在让我们谈谈如何测试你的gRPC客户端应用程序。
- en: Testing a gRPC Client
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试gRPC客户端
- en: When we are developing tests for a gRPC client, one of the possible approaches
    to testing would be to start a gRPC server and implement a mock service. However,
    this won’t be a very straightforward task as it will have the overhead of opening
    a port and connecting to a server. Therefore, to test client-side logic without
    the overhead of connecting to a real server, you can use a mocking framework.
    Mocking of the gRPC server side enables developers to write lightweight unit tests
    to check functionalities on the client side without invoking RPC calls to a server.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们为gRPC客户端开发测试时，其中一种可能的测试方法是启动一个gRPC服务器并实现一个模拟服务。然而，这并不是一项非常直接的任务，因为它会产生打开端口和连接服务器的开销。因此，为了在不连接到真实服务器的情况下测试客户端逻辑，你可以使用一个模拟框架。通过对gRPC服务器端进行模拟，开发人员可以编写轻量级单元测试，以检查客户端侧的功能，而无需调用服务器的RPC调用。
- en: 'If you are developing a gRPC client application with Go, you can use [Gomock](https://oreil.ly/8GAWB)
    to mock the client interface (using the generated code) and programmatically set
    its methods to expect and return predetermined values. Using Gomock, you can generate
    mock interfaces for the gRPC client application using:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用Go开发gRPC客户端应用程序，你可以使用[Gomock](https://oreil.ly/8GAWB)来模拟客户端接口（使用生成的代码），并编程设置其方法来期望并返回预定值。使用Gomock，你可以为gRPC客户端应用程序生成模拟接口：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we’ve specified `ProductInfoClient` as the interface to be mocked. Then
    the test code you write can import the package generated by `mockgen` along with
    the `gomock` package to write unit tests around client-side logic. As shown in
    [Example 7-2](#EX7-2), you can create a mock object to expect a call to its method
    and return a response.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将`ProductInfoClient`指定为要模拟的接口。然后，你编写的测试代码可以导入`mockgen`生成的包以及`gomock`包，围绕客户端逻辑编写单元测试。如示例[7-2](#EX7-2)所示，你可以创建一个模拟对象来期待其方法调用并返回响应。
- en: Example 7-2\. gRPC client-side test with Gomock
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例7-2\. 使用Gomock进行gRPC客户端测试
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO2-1)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO2-1)'
- en: Creating a mock object to expect calls to remote methods.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个模拟对象来期待对远程方法的调用。
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO2-2)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO2-2)'
- en: Programming the mock object.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 编写模拟对象。
- en: '[![3](assets/3.png)](#co_running_grpc_in_production_CO2-3)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_running_grpc_in_production_CO2-3)'
- en: Expect a call to the `AddProduct` method.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 预期调用`AddProduct`方法。
- en: '[![4](assets/4.png)](#co_running_grpc_in_production_CO2-4)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_running_grpc_in_production_CO2-4)'
- en: Return a mock value for product ID.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 返回产品ID的模拟值。
- en: '[![5](assets/5.png)](#co_running_grpc_in_production_CO2-5)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_running_grpc_in_production_CO2-5)'
- en: Call the actual test method that invokes the remote method of the client stub.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 调用实际的测试方法，调用客户端存根的远程方法。
- en: If you are using Java, you can test the client application using [Mockito](https://site.mockito.org)
    and the in-process server implementation for the Java implementation of gRPC.
    You can refer to the source code repository for more details of these samples.
    Once you have the required server- and client-side testing in place you can integrate
    them with the continuous integration tools that you use.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用Java，可以使用[Mockito](https://site.mockito.org)测试客户端应用程序，并使用gRPC Java实现的进程内服务器实现。您可以参考源代码库以获取这些示例的更多详情。一旦您在服务器端和客户端完成所需的测试，就可以将它们与您使用的持续集成工具集成起来。
- en: It is important to keep in mind that mocking gRPC servers will not give you
    the exact same behavior as with a real gRPC server. So certain capabilities may
    not be able to be verified via test unless you re-implement all the error logic
    present in gRPC servers. In practice, you can verify a selected set of capabilities
    via mocking and the rest needs to be verified against the actual gRPC server implementation.
    Now let’s look at how you can do load testing and benchmarking of your gRPC applications.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，模拟gRPC服务器不会像真实的gRPC服务器那样提供完全相同的行为。因此，除非重新实现gRPC服务器中存在的所有错误逻辑，否则某些功能可能无法通过测试进行验证。实际上，您可以通过模拟验证一组选定的功能，其余功能需要与实际的gRPC服务器实现进行验证。现在让我们看看如何对您的gRPC应用程序进行负载测试和基准测试。
- en: Load Testing
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负载测试
- en: It is difficult to conduct load testing and benchmarking for gRPC applications
    using conventional tools, as these applications are more or less bound to specific
    protocols such as HTTP. Therefore, for gRPC we need tailor-made load-testing tools
    that can load test the gRPC server by generating a virtual load of RPCs to the
    server.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于gRPC应用程序，使用传统工具进行负载测试和基准测试比较困难，因为这些应用程序更多或多或少地受限于特定的协议，例如HTTP。因此，对于gRPC，我们需要定制的负载测试工具，可以通过向服务器生成RPC的虚拟负载来对gRPC服务器进行负载测试。
- en: '[ghz](https://ghz.sh) is such a load-testing tool; it is implemented as a command-line
    utility using Go. It can be used for testing and debugging services locally, and
    also in automated continuous integration environments for performance regression
    testing. For example, using ghz you can run a load test with the following command:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[ghz](https://ghz.sh)是这样一款负载测试工具；它是一个使用Go语言实现的命令行实用程序。它可用于本地测试和调试服务，并且在自动化持续集成环境中用于性能回归测试。例如，使用ghz可以通过以下命令运行负载测试：'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here we invoke a `SayHello` remote method of the `Greeter` service insecurely.
    We can specify the total number of requests (`-n 2000`) and concurrency (20 threads).
    The results can also be generated in various output formats.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，我们不安全地调用`Greeter`服务的`SayHello`远程方法。我们可以指定请求的总数（`-n 2000`）和并发数（20个线程）。结果也可以生成各种输出格式。
- en: Once you have the required server- and client-side testing in place, you can
    integrate them with the continuous integration tools that you use.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您在服务器端和客户端完成所需的测试，就可以将它们与您使用的持续集成工具集成起来。
- en: Continuous Integration
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持续集成
- en: If you are new to *continuous integration* (CI), it is a development practice
    that requires developers to frequently integrate code into a shared repository.
    During each check-in the code is then verified by an automated build, allowing
    teams to detect problems early. When it comes to gRPC applications, often the
    server- and client-side applications are independent and may be built with disparate
    technologies. So, as part of the CI process, you will have to verify the gRPC
    client- or server-side code using the unit and integration testing techniques
    that we learned in the previous section. Then based on the language that you use
    to build the gRPC application, you can integrate the testing (e.g., Go testing
    or Java JUnit) of those applications with the CI tool of your choice. For instance,
    if you have written tests using Go, then you can easily integrate your Go tests
    with tools such as [Jenkins](https://jenkins.io), [TravisCI](https://travis-ci.com),
    [Spinnaker](https://www.spinnaker.io), etc.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对*持续集成*（CI）还不熟悉，它是一种开发实践，要求开发人员频繁将代码集成到共享存储库中。在每次提交代码时，自动化构建会对代码进行验证，使团队能够早期发现问题。在涉及
    gRPC 应用程序时，通常服务器端和客户端应用程序是独立的，可能使用不同的技术构建。因此，在 CI 过程中，您将需要使用我们在前一节学习的单元测试和集成测试技术来验证
    gRPC 客户端或服务器端代码。然后，根据您用来构建 gRPC 应用程序的语言，您可以将这些应用程序的测试（例如，Go 测试或 Java JUnit）集成到您选择的
    CI 工具中。例如，如果您使用 Go 编写了测试，则可以轻松地将您的 Go 测试与[Jenkins](https://jenkins.io)，[TravisCI](https://travis-ci.com)，[Spinnaker](https://www.spinnaker.io)等工具集成。
- en: Once you establish a testing and CI procedure for your gRPC application, the
    next thing that you need to look into is the deployment of your gRPC applications.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您为您的 gRPC 应用程序建立了测试和持续集成（CI）流程，接下来需要关注的是部署您的 gRPC 应用程序。
- en: Deployment
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: Now, let’s look into the different deployment methods for the gRPC applications
    that we develop. If you intend to run a gRPC server or client application locally
    or on VMs, the deployment merely depends on the binaries that you generate for
    the corresponding programming language of your gRPC application. For local or
    VM-based deployment, the scaling and high availability of gRPC server applications
    is usually achieved using standard deployment practices such as using load balancers
    that support the gRPC protocol.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们开发的 gRPC 应用程序的不同部署方法。如果您打算在本地或虚拟机上运行 gRPC 服务器或客户端应用程序，则部署主要取决于您为 gRPC
    应用程序的相应编程语言生成的二进制文件。对于本地或基于虚拟机的部署，通常使用标准的部署实践来实现 gRPC 服务器应用程序的扩展性和高可用性，例如使用支持
    gRPC 协议的负载均衡器。
- en: Most modern applications are now deployed as containers. Therefore, it’s quite
    useful to take a look at how you can deploy your gRPC applications on containers.
    Docker is the standard platform for container-based application deployment.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代应用程序现在都以容器方式部署。因此，看看如何在容器上部署您的 gRPC 应用程序是非常有用的。Docker 是基于容器的应用程序部署的标准平台。
- en: Deploying on Docker
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Docker 上部署
- en: '[Docker](https://www.docker.com) is an open platform for developing, shipping,
    and running applications. Using Docker, you can separate your applications from
    your infrastructure. It offers the ability to package and run an application in
    an isolated environment called a *container* so that you can run multiple containers
    on the same host. Containers are much more lightweight than conventional VMs and
    run directly within the host machine’s kernel.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[Docker](https://www.docker.com) 是一个用于开发、发布和运行应用程序的开放平台。使用 Docker，您可以将应用程序与基础设施分离开来。它提供了在被称为*容器*的隔离环境中打包和运行应用程序的能力，以便您可以在同一主机上运行多个容器。容器比传统的虚拟机更轻量级，并直接在主机机器的内核中运行。'
- en: Let’s look at some examples of deploying a gRPC application as a Docker container.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些将 gRPC 应用程序部署为 Docker 容器的示例。
- en: Note
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The fundamentals of Docker are beyond the scope of this book. Hence, we recommend
    you refer to the [Docker documentation](https://docs.docker.com) and other resources
    if you are not familiar with Docker.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 的基础知识超出了本书的范围。因此，如果您对 Docker 不熟悉，我们建议您参考[Docker 文档](https://docs.docker.com)和其他资源。
- en: Once you develop a gRPC server application, you can create a Docker container
    for it. [Example 7-3](#EX7-3) shows a Dockerfile of a Go-based gRPC server. There
    are many gRPC-specific constructs in the Dockerfile. In this example, we have
    used a multistage Docker build where we build the application in stage 1, and
    then run the application in stage 2 as a much more lightweight runtime. The generated
    server-side code is also added into the container prior to building the application.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 开发完 gRPC 服务器应用程序后，您可以为其创建一个 Docker 容器。[示例 7-3](#EX7-3) 展示了一个基于 Go 的 gRPC 服务器的
    Dockerfile。在 Dockerfile 中有许多特定于 gRPC 的结构。在这个例子中，我们使用了多阶段的 Docker 构建，在第一阶段构建应用程序，然后在第二阶段作为更轻量级的运行时运行应用程序。生成的服务器端代码也被添加到构建应用程序之前的容器中。
- en: Example 7-3\. Dockerfile for Go gRPC server
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-3\. Go gRPC 服务器的 Dockerfile
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO3-1)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO3-1)'
- en: Only the Go language and Alpine Linux is needed to build the program.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 只需使用 Go 语言和 Alpine Linux 来构建程序。
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO3-2)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO3-2)'
- en: Download all the dependencies.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下载所有的依赖项。
- en: '[![3](assets/3.png)](#co_running_grpc_in_production_CO3-3)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_running_grpc_in_production_CO3-3)'
- en: Install all the packages.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 安装所有的包。
- en: '[![4](assets/4.png)](#co_running_grpc_in_production_CO3-4)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_running_grpc_in_production_CO3-4)'
- en: Building the server application.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 构建服务器应用程序。
- en: '[![5](assets/5.png)](#co_running_grpc_in_production_CO3-5)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_running_grpc_in_production_CO3-5)'
- en: Go binaries are self-contained executables.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Go 二进制文件是自包含的可执行文件。
- en: '[![6](assets/6.png)](#co_running_grpc_in_production_CO3-6)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_running_grpc_in_production_CO3-6)'
- en: Copy the binary that we built in the previous stage to the new location.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 将在上一个阶段构建的二进制文件复制到新位置。
- en: 'Once you create the Dockerfile you can build the Docker image using:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Dockerfile 后，您可以使用以下命令构建 Docker 镜像：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The gRPC client application can be created using the same approach. One exception
    here is that, since we are running our server application on Docker, the hostname
    and port that the client application uses to connect to gRPC is now different.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC 客户端应用程序可以使用相同的方法创建。这里唯一的例外是，由于我们在 Docker 上运行服务器应用程序，客户端应用程序用于连接 gRPC 的主机名和端口现在不同了。
- en: 'When we run both the server and client gRPC applications on Docker, they need
    to communicate with each other and the outside world via the host machine. So
    there has to be a layer of networking involved. Docker supports different types
    of networks, each fit for certain use cases. So, when we run the server and client
    Docker containers, we can specify a common network so that the client application
    can discover the location of the server application based on the hostname. This
    means that the client application code has to change so that it connects to the
    hostname of the server. For example, our Go gRPC application must be modified
    to call the service hostname instead of localhost:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在 Docker 上同时运行服务器和客户端的 gRPC 应用程序时，它们需要通过主机机器与彼此和外部世界进行通信。因此，必须涉及网络层。Docker
    支持不同类型的网络，每种适用于特定的用例。因此，当我们运行服务器和客户端 Docker 容器时，我们可以指定一个共同的网络，以便客户端应用程序可以根据主机名发现服务器应用程序的位置。这意味着客户端应用程序的代码必须更改，以便连接到服务器的主机名。例如，我们的
    Go gRPC 应用程序必须修改为调用服务的主机名，而不是 localhost：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You may read the hostname from the environment rather than hardcoding it in
    your client application. Once you are done with the changes to the client application,
    you need to rebuild the Docker image and then run both the server and client images
    as shown here:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从环境中读取主机名，而不是在客户端应用程序中硬编码它。完成对客户端应用程序的更改后，您需要重新构建 Docker 镜像，然后像这样运行服务器和客户端镜像：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO4-1)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO4-1)'
- en: Running the gRPC server with hostname `productinfo`, port 50051 on Docker network
    *my-net*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Docker 网络 *my-net* 上运行带有主机名为 `productinfo`，端口为 50051 的 gRPC 服务器。
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO4-2)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO4-2)'
- en: Running the gRPC client on Docker network *my-net*.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Docker 网络 *my-net* 上运行 gRPC 客户端。
- en: When starting Docker containers, you can specify a Docker network that a given
    container runs on. If the service shares the same network, then the client application
    can discover the actual address of the host service using the hostname provided
    along with the `docker run` command.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动 Docker 容器时，可以指定容器所在的 Docker 网络。如果服务共享同一个网络，那么客户端应用程序可以使用与 `docker run` 命令一起提供的主机名发现主机服务的实际地址。
- en: When the number of containers you run is small and their interactions are relatively
    simple, then you can possibly build your solution entirely on Docker. However,
    most real-world scenarios require the management of multiple containers and their
    interactions. Building such solutions solely based on Docker is quite tedious.
    That’s where a container orchestration platform comes into the picture.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行的容器数量较少且它们的交互相对简单时，您可能完全可以在 Docker 上构建解决方案。然而，大多数现实场景需要管理多个容器及其交互。仅基于 Docker
    构建这样的解决方案非常繁琐。这就是容器编排平台发挥作用的地方。
- en: Deploying on Kubernetes
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署在 Kubernetes 上
- en: Kubernetes is an open source platform for automating deployment, scaling, and
    management of containerized applications. When you run a containerized gRPC application
    using Docker, there’s no scalability or high-availability guarantee provided out
    of the box. You need to build those things outside the Docker containers. Kubernetes
    provides a wide range of such capabilities, so that you can offload most container-management
    and orchestration tasks to the underlying Kubernetes platform.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是一个开源平台，用于自动化部署、扩展和管理容器化应用程序。当您使用 Docker 运行容器化的 gRPC 应用程序时，默认情况下不提供可扩展性或高可用性保证。您需要在
    Docker 容器之外构建这些功能。Kubernetes 提供了广泛的功能，使您能够将大多数容器管理和编排任务卸载到底层 Kubernetes 平台上。
- en: Note
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[Kubernetes](https://kubernetes.io) provides a reliable and scalable platform
    for running containerized workloads. Kubernetes takes care of scaling requirements,
    failover, service, discovery, configuration management, security, deployment patterns,
    and much more.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kubernetes](https://kubernetes.io) 提供了一个可靠且可扩展的平台，用于运行容器化的工作负载。Kubernetes
    负责处理扩展需求、故障转移、服务发现、配置管理、安全性、部署模式等等。'
- en: The fundamentals of Kubernetes are beyond the scope of this book. Hence, we
    recommend that you refer to the [Kubernetes documentation](https://oreil.ly/csW_8)
    and other such resources to learn more.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的基础知识超出了本书的范围。因此，我们建议您参考 [Kubernetes 文档](https://oreil.ly/csW_8)
    和其他类似资源以获取更多信息。
- en: Let’s look at how your gRPC server application can be deployed into Kubernetes.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何将您的 gRPC 服务器应用程序部署到 Kubernetes 中。
- en: Kubernetes deployment resource for a gRPC server
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于 gRPC 服务器的 Kubernetes 部署资源
- en: To deploy in Kubernetes, the first thing you need to do is create a Docker container
    for your gRPC server application. We did exactly this in the previous section,
    and you can use the same container here. You can push the container image to a
    container registry such as Docker Hub.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Kubernetes 中部署，您需要做的第一件事是为您的 gRPC 服务器应用程序创建一个 Docker 容器。我们在前一节中已经做到了这一点，您可以在这里使用相同的容器。您可以将容器镜像推送到像
    Docker Hub 这样的容器注册表中。
- en: For this example, we have pushed the gRPC server Docker image to Docker Hub
    under the tag `kasunindrasiri/grpc-productinfo-server`. The Kubernetes platform
    doesn’t directly manage containers, rather, it uses an abstraction called *pods*.
    A pod is a logical unit that may contain one or more containers; it is the unit
    of replication in Kubernetes. For example, if you need multiple instances of the
    gRPC server application, then Kubernetes will create more pods. The containers
    running on a given pod share the same resources and local network. However, in
    our case, we only need to run a gRPC server container in our pod. So, it’s a pod
    with a single container. Kubernetes doesn’t manage pods directly. Rather, it uses
    another abstraction called a *deployment*. A deployment specifies the number of
    pods that should be running at a time. When a new deployment is created, Kubernetes
    spins up the number of pods specified in the deployment.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本示例，我们已将 gRPC 服务器 Docker 镜像推送到 Docker Hub，并使用标签 `kasunindrasiri/grpc-productinfo-server`。Kubernetes
    平台不直接管理容器，而是使用称为 *pod* 的抽象。Pod 是一个逻辑单元，可以包含一个或多个容器；它是 Kubernetes 中的复制单位。例如，如果您需要多个
    gRPC 服务器应用程序的实例，那么 Kubernetes 将创建更多的 pod。在给定 pod 上运行的容器共享相同的资源和本地网络。然而，在我们的情况下，我们只需要在
    pod 中运行一个 gRPC 服务器容器。因此，这是一个只有单个容器的 pod。Kubernetes 不直接管理 pod。而是使用另一个称为 *deployment*
    的抽象。部署指定了应该同时运行的 pod 数量。创建新部署时，Kubernetes 会启动部署中指定数量的 pod。
- en: To deploy our gRPC server application in Kubernetes, we need to create a Kubernetes
    deployment using the YAML descriptor shown in [Example 7-4](#EX7-4).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Kubernetes 中部署我们的 gRPC 服务器应用程序，我们需要使用 [示例 7-4](#EX7-4) 中显示的 YAML 描述符创建一个
    Kubernetes 部署。
- en: Example 7-4\. Kubernetes deployment descriptor of a Go gRPC server application
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-4\. Go gRPC 服务器应用程序的 Kubernetes 部署描述符
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO5-1)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO5-1)'
- en: Declaring a Kubernetes `Deployment` object.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 声明一个 Kubernetes `Deployment`对象。
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO5-2)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO5-2)'
- en: Name of the deployment.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的名称。
- en: '[![3](assets/3.png)](#co_running_grpc_in_production_CO5-3)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_running_grpc_in_production_CO5-3)'
- en: Number of gRPC server pods that should be running at a time.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 同时运行的 gRPC 服务器 pod 的数量。
- en: '[![4](assets/4.png)](#co_running_grpc_in_production_CO5-4)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_running_grpc_in_production_CO5-4)'
- en: Name of the associated gRPC server container.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 关联的 gRPC 服务器容器的名称。
- en: '[![5](assets/5.png)](#co_running_grpc_in_production_CO5-5)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_running_grpc_in_production_CO5-5)'
- en: Image name and tag of the gRPC server container.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC 服务器容器的镜像名称和标签。
- en: When you apply this descriptor in Kubernetes using `kubectl apply -f server/grpc-prodinfo-server.yaml`,
    you get a Kubernetes deployment of one gRPC server pod running in your Kubernetes
    cluster. However, if the gRPC client application has to access a gRPC server pod
    running in the same Kubernetes cluster, it has to find out the exact IP address
    and port of the pod and send the RPC. However, the IP address may change when
    the pod gets restarted, and if you are running multiple replicas you have to deal
    with multiple IP addresses of each replica. To overcome this limitation, Kubernetes
    provides an abstraction called a *service*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在 Kubernetes 中使用`kubectl apply -f server/grpc-prodinfo-server.yaml`应用此描述符时，您将在
    Kubernetes 集群中获得一个运行中的 gRPC 服务器 pod 的部署。然而，如果 gRPC 客户端应用程序必须访问运行在同一 Kubernetes
    集群中的 gRPC 服务器 pod，则必须找到 pod 的确切 IP 地址和端口并发送 RPC。但是，当 pod 重新启动时，IP 地址可能会更改，并且如果您运行多个副本，则必须处理每个副本的多个
    IP 地址。为了克服这个限制，Kubernetes 提供了一个称为*service*的抽象。
- en: Kubernetes service resource for a gRPC server
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于 gRPC 服务器的 Kubernetes 服务资源
- en: You can create a Kubernetes service and associate it with the matching pods
    (gRPC server pods in this case) and you will get a DNS name that will automatically
    route the traffic to any matching pod. So, you can think of a service as a web
    proxy or a load balancer that forwards the requests to the underlying pods. [Example 7-5](#EX7-5)
    shows the Kubernetes service descriptor for the gRPC server application.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以创建一个 Kubernetes 服务，并将其与匹配的 pod（在本例中为 gRPC 服务器 pod）关联起来，您将获得一个 DNS 名称，该名称将自动将流量路由到任何匹配的
    pod。因此，您可以将服务视为一个 Web 代理或负载均衡器，用于将请求转发到底层的 pod。[示例 7-5](#EX7-5) 显示了 gRPC 服务器应用程序的
    Kubernetes 服务描述符。
- en: Example 7-5\. Kubernetes service descriptor of a Go gRPC server application
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-5\. Go gRPC 服务器应用程序的 Kubernetes 服务描述符
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO6-1)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO6-1)'
- en: Specifying a `Service` descriptor.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 指定一个`Service`描述符。
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO6-2)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO6-2)'
- en: Name of the service. This will be used by the client application when connecting
    to the service.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的名称。客户端应用程序连接到服务时将使用此名称。
- en: '[![3](assets/3.png)](#co_running_grpc_in_production_CO6-3)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_running_grpc_in_production_CO6-3)'
- en: This tells the service to route requests to the pods for matching label `grpc-productinfo-server`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉服务将请求路由到具有匹配标签`grpc-productinfo-server`的 pod。
- en: '[![4](assets/4.png)](#co_running_grpc_in_production_CO6-4)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_running_grpc_in_production_CO6-4)'
- en: Service runs on port 50051 and forwards the requests to target port 50051.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 服务运行在端口 50051，并将请求转发到目标端口 50051。
- en: So, once you have created both the `Deployment` and `Service` descriptor, you
    can deploy this application into Kubernetes using `kubectl apply -f server/grpc-prodinfo-server.yaml`
    (you can have both descriptors in the same YAML file). A successful deployment
    of these objects should give you a running gRPC server pod, a Kubernetes service
    for a gRPC server, and a deployment.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，一旦您创建了`Deployment`和`Service`描述符，您可以使用`kubectl apply -f server/grpc-prodinfo-server.yaml`将此应用部署到
    Kubernetes 中（您可以将两个描述符放在同一个 YAML 文件中）。成功部署这些对象应该会给您一个运行中的 gRPC 服务器 pod，一个用于 gRPC
    服务器的 Kubernetes 服务以及一个部署。
- en: The next step is deploying the gRPC client into Kubernetes cluster.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将 gRPC 客户端部署到 Kubernetes 集群。
- en: Kubernetes Job for running a gRPC Client
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于运行 gRPC 客户端的 Kubernetes Job
- en: When you have the gRPC server up and running on the Kubernetes cluster, then
    you can also run the gRPC client application in the same cluster. The client can
    access the gRPC server via the gRPC service `productinfo` that we created in the
    previous step. So from the client’s code, you should refer to the Kubernetes service
    name as the hostname and use the service port as the port name of the gRPC server.
    Therefore, the client will be using `grpc.Dial("productinfo:50051", grpc.WithInsecure())`
    when connecting to the server in the Go implementation of the client. If we assume
    that our client application needs to run a specified number of times (i.e., just
    calls the gRPC service, logs the response, and exits), then rather than using
    a Kubernetes Deployment, we may use a Kubernetes *job*. A Kubernetes job is designed
    to run a Pod a specified number of times.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在 Kubernetes 集群上成功运行 gRPC 服务器后，还可以在同一集群中运行 gRPC 客户端应用程序。客户端可以通过我们在前一步创建的名为
    `productinfo` 的 gRPC 服务访问 gRPC 服务器。因此，在客户端的代码中，您应将 Kubernetes 服务名称用作主机名，并使用 gRPC
    服务器的服务端口作为端口名称。因此，客户端在连接 Go 实现的服务器时将使用 `grpc.Dial("productinfo:50051", grpc.WithInsecure())`。如果假设我们的客户端应用程序需要运行指定次数（即仅调用
    gRPC 服务、记录响应并退出），则我们可能不使用 Kubernetes 部署，而是使用 Kubernetes *job*。Kubernetes job 设计用于指定次数运行
    Pod。
- en: You can create the client application container the same way we did in the gRPC
    server. Once you have the container pushed into the Docker registry, then you
    can specify the Kubernetes `Job` descriptor as shown in [Example 7-6](#EX7-6).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以以与 gRPC 服务器相同的方式创建客户端应用程序容器。一旦容器推送到 Docker 注册表中，然后您可以按照 [示例 7-6](#EX7-6)
    中显示的 Kubernetes `Job` 描述符指定它。
- en: Example 7-6\. gRPC client application runs as a Kubernetes job
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-6\. gRPC 客户端应用程序作为 Kubernetes job 运行
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO7-1)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO7-1)'
- en: Specifying a Kubernetes `Job`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 指定一个 Kubernetes `Job`。
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO7-2)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO7-2)'
- en: Name of the job.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作业的名称.
- en: '[![3](assets/3.png)](#co_running_grpc_in_production_CO7-3)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_running_grpc_in_production_CO7-3)'
- en: Number of times that the pod needs to run successfully before the job is considered
    completed.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 成功运行的次数，作业才被视为完成。
- en: '[![4](assets/4.png)](#co_running_grpc_in_production_CO7-4)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_running_grpc_in_production_CO7-4)'
- en: How many pods should run in parallel.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 应并行运行的 Pod 数量。
- en: '[![5](assets/5.png)](#co_running_grpc_in_production_CO7-5)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_running_grpc_in_production_CO7-5)'
- en: Name of the associated gRPC client container.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 关联的 gRPC 客户端容器的名称。
- en: '[![6](assets/6.png)](#co_running_grpc_in_production_CO7-6)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_running_grpc_in_production_CO7-6)'
- en: Container image that this job is associated with.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 与此作业关联的容器镜像。
- en: Then you can deploy the Job for the gRPC client application using `kubectl apply
    -f client/grpc-prodinfo-client-job.yaml` and check the status of the pod.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您可以使用 `kubectl apply -f client/grpc-prodinfo-client-job.yaml` 部署 gRPC 客户端应用程序的作业，并检查
    Pod 的状态。
- en: Successful completion of the execution of this Job sends an RPC to add a product
    in our `ProductInfo` gRPC service. So you can observe the logs for both server
    and client pods to see whether we get the expected information.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 该作业执行成功后，会向我们的 `ProductInfo` gRPC 服务发送添加产品的 RPC。因此，您可以观察服务器和客户端 Pod 的日志，查看是否获得了预期的信息。
- en: Then we can proceed to exposing your gRPC services outside the Kubernetes cluster
    using ingress resources.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用入口资源将您的 gRPC 服务暴露到 Kubernetes 集群之外。
- en: Kubernetes Ingress for exposing a gRPC service externally
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于外部暴露 gRPC 服务的 Kubernetes Ingress
- en: So far what we have done is deploy a gRPC server on Kubernetes and make it accessible
    to another pod (which is running as a Job) running in the same cluster. What if
    we want to expose the gRPC service to the external applications outside the Kubernetes
    cluster? As you learned, the Kubernetes service construct is only meant to expose
    given Kubernetes pods to the other pods running in the cluster. So, the Kubernetes
    service is not accessible by the external applications that are outside the Kubernetes
    cluster. Kubernetes gives another abstraction called an *ingress* to serve this
    purpose.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在 Kubernetes 上部署了一个 gRPC 服务器，并使其对同一集群中作为作业运行的另一个 Pod 可访问。如果我们想将 gRPC
    服务暴露给 Kubernetes 集群之外的外部应用程序，会怎样？正如您所了解的那样，Kubernetes 服务构造仅用于将给定的 Kubernetes Pod
    暴露给集群中运行的其他 Pod。因此，Kubernetes 服务无法被位于 Kubernetes 集群之外的外部应用程序访问。Kubernetes 提供了另一个称为
    *ingress* 的抽象来实现此目的。
- en: We can think of an ingress as a load balancer that sits between the Kubernetes
    service and the external applications. `Ingress` routes the external traffic to
    the service; the service then routes the internal traffic between the matching
    pods. An ingress controller manages the ingress resource in a given Kubernetes
    cluster. The type and the behavior of the ingress controller may change based
    on the cluster you use. Also, when you expose a gRPC service to the external application,
    one of the mandatory requirements is to support gRPC routing at the ingress level.
    Therefore, we need to select an ingress controller that supports gRPC.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 Ingress 想象成位于 Kubernetes 服务与外部应用程序之间的负载均衡器。Ingress 将外部流量路由到服务；服务然后在匹配的
    Pod 之间路由内部流量。Ingress 控制器管理给定 Kubernetes 集群中的 Ingress 资源。Ingress 控制器的类型和行为可能会根据您使用的集群而变化。此外，当您向外部应用程序公开
    gRPC 服务时，支持在 Ingress 层面进行 gRPC 路由是必需的条件之一。因此，我们需要选择一个支持 gRPC 的 Ingress 控制器。
- en: For this example, we’ll use the [Nginx](https://oreil.ly/0UC0a) ingress controller,
    which is based on the [Nginx](https://www.nginx.com) load balancer. (Based on
    the Kubernetes cluster you use, you may select the most appropriate ingress controller
    that supports gRPC.) [Nginx Ingress](https://oreil.ly/wZo5w) supports gRPC for
    routing external traffic into internal services.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用基于 [Nginx](https://oreil.ly/0UC0a) 负载均衡器的 [Nginx](https://www.nginx.com)
    Ingress 控制器。根据您使用的 Kubernetes 集群，您可以选择支持 gRPC 的最合适的 Ingress 控制器。[Nginx Ingress](https://oreil.ly/wZo5w)
    支持将外部流量路由到内部服务的 gRPC。
- en: So, to expose our `ProductInfo` gRPC server application to the external world
    (i.e., outside the Kubernetes cluster), we can create an `Ingress` resource as
    shown in [Example 7-7](#EX7-7).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了将我们的 `ProductInfo` gRPC 服务器应用程序暴露给外部世界（即 Kubernetes 集群之外），我们可以创建如 [示例 7-7](#EX7-7)
    所示的 `Ingress` 资源。
- en: Example 7-7\. Kubernetes Ingress resource of a Go gRPC server application
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-7\. Go gRPC 服务器应用程序的 Kubernetes Ingress 资源
- en: '[PRE11]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO8-1)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO8-1)'
- en: Specifying an `Ingress` resource.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 指定 `Ingress` 资源。
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO8-2)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO8-2)'
- en: Annotations related to Nginx Ingress controller and specifying gRPC as the backend
    protocol.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Nginx Ingress 控制器相关的注解，并指定 gRPC 作为后端协议。
- en: '[![3](assets/3.png)](#co_running_grpc_in_production_CO8-3)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_running_grpc_in_production_CO8-3)'
- en: Name of the `Ingress` resource.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`Ingress` 资源的名称。'
- en: '[![4](assets/4.png)](#co_running_grpc_in_production_CO8-4)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_running_grpc_in_production_CO8-4)'
- en: This is the hostname exposed to the external world.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这是向外界公开的主机名。
- en: '[![5](assets/5.png)](#co_running_grpc_in_production_CO8-5)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_running_grpc_in_production_CO8-5)'
- en: Name of the associated Kubernetes service.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 关联的 Kubernetes 服务的名称。
- en: '[![6](assets/6.png)](#co_running_grpc_in_production_CO8-6)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_running_grpc_in_production_CO8-6)'
- en: Name of the service port specified in the Kubernetes service.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 服务中指定的服务端口的名称。
- en: You will need to install the Nginx Ingress controller prior to deploying the
    preceding ingress resource. You can find more details on installing and using
    the Nginx Ingress with gRPC in the [Ingress-Nginx](https://oreil.ly/l-vFp) repository
    of Kubernetes. Once you deploy this `Ingress` resource, any external application
    can invoke the gRPC server via the hostname (`productinfo`) and the default port
    (80).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署上述的入口资源之前，您需要安装 Nginx Ingress 控制器。您可以在 Kubernetes 的 [Ingress-Nginx](https://oreil.ly/l-vFp)
    仓库中找到有关安装和使用 Nginx Ingress 与 gRPC 的更多详细信息。一旦您部署了这个 `Ingress` 资源，任何外部应用程序都可以通过主机名
    (`productinfo`) 和默认端口 (80) 调用 gRPC 服务器。
- en: With that, you have learned all the fundamentals related to deploying a production-ready
    gRPC application on Kubernetes. As you have seen, owing to the capabilities that
    Kubernetes and Docker offer, we don’t really have to worry much about most nonfunctional
    requirements such as scalability, high availability, load balancing, failover,
    etc., because Kubernetes is providing them as part of the underlying platform.
    Hence, certain concepts that we learned in [Chapter 6](ch06.html#ch_06), such
    as load balancing, name resolving at the gRPC code level, etc., are not required
    if you are running your gRPC applications on Kubernetes.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以上内容，您已经学习了关于在 Kubernetes 上部署生产就绪的 gRPC 应用程序的所有基础知识。正如您所见，由于 Kubernetes 和
    Docker 提供的能力，我们不必过多担心大多数非功能性需求，如可伸缩性、高可用性、负载均衡、故障转移等，因为 Kubernetes 已作为底层平台的一部分提供了这些功能。因此，一些我们在
    [第 6 章](ch06.html#ch_06) 中学习的概念，如负载均衡、gRPC 代码层面的名称解析等，在运行 gRPC 应用程序时在 Kubernetes
    上是不必要的。
- en: Once you have a gRPC-based application up and running, you need to ensure the
    smooth operation of the application in production. To accomplish that goal, you
    need to consistently observe your gRPC application and take the necessary actions
    when required. Let’s look into the details of the observability aspects of gRPC
    applications.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的基于 gRPC 的应用程序运行起来，您需要确保生产环境中应用程序的平稳运行。为了实现这一目标，您需要持续观察您的 gRPC 应用程序，并在需要时采取必要的行动。让我们深入探讨
    gRPC 应用程序的可观测性方面的细节。
- en: Observability
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可观测性
- en: As we discussed in the previous section, gRPC applications are normally deployed
    and run in containerized environments where there are multiples of such containers
    running and talking to each other over the network. Then comes the problem of
    how to keep track of each container and make sure they are actually working. This
    is where *observability* comes into the picture.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中讨论的那样，gRPC 应用程序通常部署和运行在容器化环境中，其中有多个此类容器通过网络进行通信。然后出现了如何跟踪每个容器并确保它们实际工作的问题。这就是*可观测性*的作用所在。
- en: As the [Wikipedia definition](https://oreil.ly/FVPTN) states, “observability
    is a measure of how well internal states of a system can be inferred from knowledge
    of its external outputs.” Basically, the purpose of having observability into
    a system is to answer the question, “Is anything wrong in the system right now?”
    If the answer is yes, we should also be able to answer a bunch of other questions
    like “What is wrong?” and “Why is it happening?” If we can answer those questions
    at any given time and in any part of the system, we can say that our system is
    observable.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[维基百科的定义](https://oreil.ly/FVPTN)所述，“可观测性是指从系统外部输出的知识中推断出系统内部状态的程度。”基本上，有关系统可观测性的目的是回答问题：“系统当前有问题吗？”如果答案是肯定的，我们还应该能够回答一系列其他问题，如“出了什么问题？”和“为什么会发生这种情况？”如果我们能够在任何给定时间和系统的任何部分回答这些问题，我们可以说我们的系统是可观测的。
- en: It is also important to note that observability is an attribute of a system
    that is as important as efficiency, usability, and reliability. So it must be
    considered from the beginning when we are building gRPC applications.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，可观测性是系统的一个属性，与效率、可用性和可靠性一样重要。因此，在构建 gRPC 应用程序时，必须从一开始考虑它。
- en: 'When talking about observability, there are three main pillars that we normally
    talk about: metrics, logging, and tracing. These are the main techniques used
    to gain the observability of the system. Let’s discuss each of them separately
    in the following sections.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 谈论可观测性时，我们通常谈论三个主要支柱：指标、日志记录和跟踪。这些是用于获得系统可观测性的主要技术。让我们在接下来的部分分别讨论每一个。
- en: Metrics
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指标
- en: '*Metrics* are a numeric representation of data measured over intervals of time.
    When talking about metrics, there are two types of data we can collect. One is
    system-level metrics like CPU usage, memory usage, etc. The other one is application-level
    metrics like inbound request rate, request error rate, etc.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*指标*是对数据在时间间隔内的数值表示。谈到指标时，我们可以收集两种类型的数据。一种是系统级指标，如 CPU 使用率、内存使用率等。另一种是应用级指标，如入站请求速率、请求错误率等。'
- en: System-level metrics are normally captured when the application is running.
    These days, there are lots of tools to capture those metrics, and they’re usually
    captured by the DevOps team. But application-level metrics differ between applications.
    So when designing a new application, it is the task of an application developer
    to decide what kind of application-level metrics need to be captured to get an
    understanding of the behavior of a system. In this section, we are going to focus
    on how to enable application-level metrics in our applications.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序运行时通常会捕获系统级指标。如今，有许多工具用于捕获这些指标，通常由 DevOps 团队捕获。但是应用程序级指标在不同的应用程序之间有所不同。因此，在设计新应用程序时，应用程序开发人员需要决定需要捕获哪些类型的应用程序级指标，以了解系统行为。在本节中，我们将重点介绍如何在我们的应用程序中启用应用程序级指标。
- en: OpenCensus with gRPC
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 gRPC 的 OpenCensus
- en: For gRPC applications, there are standard metrics that are provided by the [OpenCensus](https://oreil.ly/EMfF-)
    library. We can easily enable them by adding handlers to both the client and server
    applications. We can also add our own metrics collector ([Example 7-8](#EX7-8)).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 gRPC 应用程序，[OpenCensus](https://oreil.ly/EMfF-) 库提供了标准指标。我们可以通过在客户端和服务器应用程序中添加处理程序来轻松启用它们。我们还可以添加自己的指标收集器（[示例
    7-8](#EX7-8)）。
- en: Note
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[OpenCensus](https://opencensus.io) is a set of open source libraries for collecting
    application metrics and distributed traces; it supports various languages. It
    collects metrics from the target application and transfers the data to the backend
    of your choice in real time. Supported backends currently available include Azure
    Monitor, Datadog, Instana, Jaeger, SignalFX, Stackdriver, and Zipkin. We can also
    write our own exporter for other backends.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenCensus](https://opencensus.io) 是一个用于收集应用程序指标和分布式跟踪的开源库集合；它支持多种语言。它从目标应用程序收集指标并实时传输数据到您选择的后端。目前支持的后端包括
    Azure Monitor、Datadog、Instana、Jaeger、SignalFX、Stackdriver 和 Zipkin。我们也可以为其他后端编写自定义导出器。'
- en: Example 7-8\. Enable OpenCensus monitoring for the gRPC Go server
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-8\. 为 gRPC Go 服务器启用 OpenCensus 监控
- en: '[PRE12]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO9-1)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO9-1)'
- en: Specify external libraries we need to add to enable monitoring. gRPC OpenCensus
    provides a predefined set of handlers to support OpenCensus monitoring. Here we
    are going to use those handlers.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 指定我们需要添加的外部库以启用监控。gRPC OpenCensus 提供了一组预定义的处理程序，以支持 OpenCensus 监控。在这里，我们将使用这些处理程序。
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO9-3)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO9-3)'
- en: Register stat exporters to export the collected data. Here we add `PrintExporter`
    and it logs exported data to the console. This is only for demonstration purposes;
    normally it’s not recommended that you log all production loads.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 注册统计导出器以导出收集的数据。这里我们添加 `PrintExporter`，它将导出的数据记录到控制台。这仅用于演示目的；通常不建议记录所有生产负载。
- en: '[![3](assets/3.png)](#co_running_grpc_in_production_CO9-4)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_running_grpc_in_production_CO9-4)'
- en: Register the views to collect the server request count. These are the predefined
    default service views that collect received bytes per RPC, sent bytes per RPC,
    latency per RPC, and completed RPC. We can write our own views to collect data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 注册视图以收集服务器请求计数。这些是预定义的默认服务视图，用于收集每个 RPC 的接收字节、发送字节、RPC 的延迟和已完成的 RPC。我们可以编写自己的视图来收集数据。
- en: '[![4](assets/4.png)](#co_running_grpc_in_production_CO9-5)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_running_grpc_in_production_CO9-5)'
- en: Create a gRPC server with a stats handler.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个带有统计处理程序的 gRPC 服务器。
- en: '[![5](assets/5.png)](#co_running_grpc_in_production_CO9-6)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_running_grpc_in_production_CO9-6)'
- en: Register our `ProductInfo` service to the gRPC server.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的 `ProductInfo` 服务注册到 gRPC 服务器。
- en: '[![6](assets/6.png)](#co_running_grpc_in_production_CO9-7)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_running_grpc_in_production_CO9-7)'
- en: Start listening to incoming messages on the port (50051).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 开始监听端口（50051）上传入的消息。
- en: '[![7](assets/7.png)](#co_running_grpc_in_production_CO9-2)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_running_grpc_in_production_CO9-2)'
- en: Starts a z-Pages server. An HTTP endpoint starts with the context of `/debug`
    in port 8081 for metrics visualization.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 z-Pages 服务器。一个 HTTP 端点从端口 8081 开始，用于可视化指标的上下文为 `/debug`。
- en: Similar to the gRPC server, we can enable OpenCensus monitoring in gRPC clients
    using client-side handlers. [Example 7-9](#EX7-9) provides the code snippet for
    adding a metrics handler to a gRPC client written in Go.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 gRPC 服务器，我们可以使用客户端端处理程序在 gRPC 客户端中启用 OpenCensus 监控。[示例 7-9](#EX7-9) 提供了向
    Go 编写的 gRPC 客户端添加度量处理程序的代码片段。
- en: Example 7-9\. Enable OpenCensus monitoring for the gRPC Go server
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-9\. 为 gRPC Go 服务器启用 OpenCensus 监控
- en: '[PRE13]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO10-1)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO10-1)'
- en: Specify external libraries we need to add to enable monitoring.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 指定我们需要添加的外部库以启用监控。
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO10-2)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO10-2)'
- en: Register stats and trace exporters to export the collected data. Here we will
    add `PrintExporter`, which logs exported data to the console. This is only for
    demonstration purposes. Normally it is not recommended to log all production loads.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 注册统计和跟踪导出器以导出收集的数据。这里我们将添加 `PrintExporter`，它将导出的数据记录到控制台。这仅用于演示目的。通常不建议记录所有生产负载。
- en: '[![3](assets/3.png)](#co_running_grpc_in_production_CO10-3)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_running_grpc_in_production_CO10-3)'
- en: Register the views to collect server request count. These are the predefined
    default service views that collect received bytes per RPC, sent bytes per RPC,
    latency per RPC, and completed RPC. We can write our own views to collect data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注册视图以收集服务器请求计数。这些是预定义的默认服务视图，用于收集每个 RPC 的接收字节、发送字节、RPC 的延迟和已完成的 RPC。我们可以编写自己的视图来收集数据。
- en: '[![4](assets/4.png)](#co_running_grpc_in_production_CO10-4)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_running_grpc_in_production_CO10-4)'
- en: Set up a connection to the server with client stats handlers.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 设置与服务器的连接，并注册客户端状态处理程序。
- en: '[![5](assets/5.png)](#co_running_grpc_in_production_CO10-6)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_running_grpc_in_production_CO10-6)'
- en: Create a client stub using the server connection.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 使用服务器连接创建客户端存根。
- en: '[![6](assets/6.png)](#co_running_grpc_in_production_CO10-5)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_running_grpc_in_production_CO10-5)'
- en: Close the connection when everything is done.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在一切都完成后关闭连接。
- en: Once we run the server and client, we can access the server and client metrics
    through the created HTTP endpoint (e.g., RPC metrics on [*http://localhost:8081/debug/rpcz*](http://localhost:8081/debug/rpcz)
    and traces on [*http://localhost:8081/debug/tracez)*](http://localhost:8081/debug/tracez)).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行服务器和客户端，我们就可以通过创建的HTTP端点访问服务器和客户端度量（例如，RPC度量位于[*http://localhost:8081/debug/rpcz*](http://localhost:8081/debug/rpcz)，跟踪位于[*http://localhost:8081/debug/tracez)*](http://localhost:8081/debug/tracez)）。
- en: As mentioned before, we can use predefined exporters to publish data to the
    supported backend or we can write our own exporter to send traces and metrics
    to any backend that is capable of consuming them.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们可以使用预定义的导出器将数据发布到支持的后端，也可以编写自己的导出器将跟踪和度量发送到能够消费它们的任何后端。
- en: In the next section we’ll discuss another popular technology, [Prometheus](https://prometheus.io),
    which is commonly used for enabling metrics for gRPC applications.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论另一个流行的技术，[Prometheus](https://prometheus.io)，通常用于为gRPC应用程序启用度量。
- en: Prometheus with gRPC
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Prometheus与gRPC
- en: Prometheus is an open source toolkit for system monitoring and alerting. You
    can use Prometheus for enabling metrics for your gRPC application using the [gRPC
    Prometheus library](https://oreil.ly/nm84_). We can easily enable this by adding
    an interceptor to both the client and server applications and we can also add
    our own metrics collector, too.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus是一个用于系统监控和警报的开源工具包。您可以使用[grpc Prometheus库](https://oreil.ly/nm84_)为您的gRPC应用程序启用度量。通过为客户端和服务器应用程序添加拦截器，我们可以轻松实现此功能，还可以添加自定义度量收集器。
- en: Note
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Prometheus collects metrics from the target application by calling an HTTP endpoint
    that starts with the context `/metrics`. It stores all collected data and runs
    rules over this data to either aggregate and record new time series from existing
    data or generate alerts. We can visualize those aggregated results using tools
    like [Grafana](https://grafana.com).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus通过调用以上下文`/metrics`开头的HTTP端点从目标应用程序收集度量。它存储所有收集的数据，并对这些数据运行规则，以聚合并记录新的时间序列或生成警报。我们可以使用诸如[Grafana](https://grafana.com)之类的工具可视化这些聚合结果。
- en: '[Example 7-10](#EX7-10) illustrates how to add a metrics interceptor and a
    custom metrics collector to our product management server written in Go.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 7-10](#EX7-10)演示了如何在我们使用Go编写的产品管理服务器中添加度量拦截器和自定义度量收集器。'
- en: Example 7-10\. Enable Prometheus monitoring for the gRPC Go server
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-10\. 为gRPC Go服务器启用Prometheus监控
- en: '[PRE14]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO11-1)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO11-1)'
- en: Specifies external libraries we need to add to enable monitoring. The gRPC ecosystem
    provides a predefined set of interceptors to support Prometheus monitoring. Here
    we are going to use those interceptors.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 指定我们需要添加的外部库以启用监控。gRPC生态系统提供了一组预定义的拦截器来支持Prometheus监控。在这里，我们将使用这些拦截器。
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO11-2)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO11-2)'
- en: Creates a metrics registry. This holds all data collectors registered in the
    system. If we need to add a new collector, we need to register it in this registry.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 创建度量注册表。该注册表保存系统中注册的所有数据收集器。如果需要添加新的收集器，我们需要在此注册表中注册它。
- en: '[![3](assets/3.png)](#co_running_grpc_in_production_CO11-3)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_running_grpc_in_production_CO11-3)'
- en: Creates standard client metrics. These are the predefined metrics defined in
    the library.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 创建标准客户端度量。这些是库中定义的预定义度量。
- en: '[![4](assets/4.png)](#co_running_grpc_in_production_CO11-4)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_running_grpc_in_production_CO11-4)'
- en: Creates a custom metrics counter with the name `product_mgt_server_handle_count`.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 创建名为`product_mgt_server_handle_count`的自定义度量计数器。
- en: '[![5](assets/5.png)](#co_running_grpc_in_production_CO11-5)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_running_grpc_in_production_CO11-5)'
- en: Registers standard server metrics and custom metrics collector to the registry
    created in step 2.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2步创建的注册表中注册标准服务器度量和自定义度量收集器。
- en: '[![6](assets/6.png)](#co_running_grpc_in_production_CO11-6)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_running_grpc_in_production_CO11-6)'
- en: Creates an HTTP server for Prometheus. An HTTP endpoint starts with the context
    `/metrics` on port 9092 for metrics collection.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 为Prometheus创建一个HTTP服务器。用于度量收集的HTTP端点在端口9092上以上下文`/metrics`开头。
- en: '[![7](assets/7.png)](#co_running_grpc_in_production_CO11-7)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_running_grpc_in_production_CO11-7)'
- en: Creates a gRPC server with a metrics interceptor. Here we use `grpcMetrics.UnaryServerInterceptor`,
    since we have unary service. There is another interceptor called `grpcMetrics.StreamServerInterceptor()`
    for streaming services.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个带有度量拦截器的 gRPC 服务器。这里我们使用 `grpcMetrics.UnaryServerInterceptor`，因为我们有一个一元服务。还有一个叫做
    `grpcMetrics.StreamServerInterceptor()` 的拦截器用于流式服务。
- en: '[![8](assets/8.png)](#co_running_grpc_in_production_CO11-8)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](assets/8.png)](#co_running_grpc_in_production_CO11-8)'
- en: Initializes all standard metrics.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化所有标准度量。
- en: Using the custom metrics counter created in step 4, we can add more metrics
    for monitoring. Let’s say we want to collect how many products with the same name
    are added to our product management system. As shown in [Example 7-11](#EX7-11),
    we can add a new metric to `customMetricCounter` in the `addProduct` method.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 使用第4步创建的自定义度量计数器，我们可以添加更多用于监控的度量指标。比如说，我们想要收集添加到产品管理系统中具有相同名称的产品的数量。如示例 [7-11](#EX7-11)
    所示，在 `addProduct` 方法中我们可以向 `customMetricCounter` 添加新的度量指标。
- en: Example 7-11\. Add new metrics to the custom metric counter
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-11\. 向自定义度量计数器添加新的度量指标
- en: '[PRE15]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Similar to the gRPC server, we can enable Prometheus monitoring in gRPC clients
    using client-side interceptors. [Example 7-12](#EX7-12) provides the code snippet
    for adding a metrics interceptor to the gRPC client written in Go.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 gRPC 服务器，我们可以使用客户端拦截器在 gRPC 客户端中启用 Prometheus 监控。示例 [7-12](#EX7-12) 提供了在
    Go 中编写的 gRPC 客户端添加度量拦截器的代码片段。
- en: Example 7-12\. Enable Prometheus monitoring for the gRPC Go client
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-12\. 为 gRPC Go 客户端启用 Prometheus 监控
- en: '[PRE16]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO12-1)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO12-1)'
- en: Specifies external libraries we need to add to enable monitoring.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 指定我们需要添加的外部库以启用监控。
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO12-2)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO12-2)'
- en: Creates a metrics registry. Similar to server code, this holds all data collectors
    registered in the system. If we need to add a new collector, we need to register
    it to this registry.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 创建度量注册表。与服务器代码类似，这个注册表包含系统中注册的所有数据收集器。如果我们需要添加一个新的收集器，我们需要将其注册到这个注册表中。
- en: '[![3](assets/3.png)](#co_running_grpc_in_production_CO12-3)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_running_grpc_in_production_CO12-3)'
- en: Creates standard server metrics. These are the predefined metrics defined in
    the library.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 创建标准服务器度量。这些是库中定义的预定义度量指标。
- en: '[![4](assets/4.png)](#co_running_grpc_in_production_CO12-4)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_running_grpc_in_production_CO12-4)'
- en: Registers standard client metrics to the registry created in step 2.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 将标准客户端度量注册到第2步创建的注册表中。
- en: '[![5](assets/5.png)](#co_running_grpc_in_production_CO12-5)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_running_grpc_in_production_CO12-5)'
- en: Sets up a connection to the server with the metrics interceptor. Here we use
    `grpcMetrics.UnaryClientInterceptor`, since we have a unary client. Another interceptor,
    called `grpcMetrics.StreamClientInterceptor()`, is used for streaming clients.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 使用度量拦截器建立与服务器的连接。这里我们使用 `grpcMetrics.UnaryClientInterceptor`，因为我们有一个一元客户端。另一个拦截器叫做
    `grpcMetrics.StreamClientInterceptor()`，用于流式客户端。
- en: '[![6](assets/6.png)](#co_running_grpc_in_production_CO12-6)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_running_grpc_in_production_CO12-6)'
- en: Creates an HTTP server for Prometheus. An HTTP endpoint starts with the context
    `/metrics` on port 9094 for metrics collection.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 为 Prometheus 创建一个 HTTP 服务器。一个 HTTP 端点从 `/metrics` 开始，监听9094端口用于度量收集。
- en: Once we run the server and client, we can access the server and client metrics
    through the created HTTP endpoint (e.g., server metrics on [*http://localhost:9092/metrics*](http://localhost:9092/metrics)
    and client metrics on [*http://localhost:9094/metrics)*](http://localhost:9094/metrics)).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦服务器和客户端运行，我们可以通过创建的 HTTP 端点访问服务器和客户端的度量（例如，服务器度量在 [*http://localhost:9092/metrics*](http://localhost:9092/metrics)，客户端度量在
    [*http://localhost:9094/metrics)*](http://localhost:9094/metrics)）。
- en: As we mentioned before, Prometheus can collect metrics by accessing the preceding
    URLs. Prometheus stores all metrics data locally and applies a set of rules to
    aggregate and create new records. And, using Prometheus as a data source, we can
    visualize metrics in a dashboard using tools like Grafana.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，Prometheus 可以通过访问上述 URL 收集度量。Prometheus 将所有度量数据存储在本地，并应用一组规则来聚合和创建新的记录。通过将
    Prometheus 作为数据源，我们可以使用像 Grafana 这样的工具在仪表板中可视化度量数据。
- en: Note
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Grafana is an open source metrics dashboard and graph editor for Graphite, Elasticsearch,
    and Prometheus. It allows you to query, visualize, and understand your metrics
    data.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 是一个开源的度量仪表板和图形编辑器，用于 Graphite、Elasticsearch 和 Prometheus。它允许您查询、可视化和理解您的度量数据。
- en: One advantage of metrics-based monitoring in the system is that the cost of
    handling metrics data doesn’t increase with the activities of the system. For
    example, an increase in the application’s traffic will not increase handling costs
    like disk utilization, processing complexity, speed of visualization, operational
    costs, etc. It has constant overhead. Also, once we collect metrics, we can do
    numerous mathematical and statistical transformations and create valuable conclusions
    about the system.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 系统中基于度量的监控的一个优势是处理度量数据的成本不会随系统活动的增加而增加。例如，应用程序流量的增加不会增加处理成本，如磁盘利用率、处理复杂性、可视化速度、运营成本等。它具有恒定的开销。此外，一旦收集到度量数据，我们可以进行多种数学和统计转换，并得出有关系统的有价值结论。
- en: Another pillar of observability is logs, which we’ll discuss in the next section.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 观察性的另一个支柱是日志，在下一节中我们将讨论它。
- en: Logs
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志
- en: Logs are immutable, time-stamped records of discrete events that happened over
    time. We, as application developers, normally dump data into logs to tell where
    and what the internal state of the system is at a given point. The benefit of
    logs is they are the easiest to generate and more granular than metrics. We can
    attach specific actions or a bunch of context to it like unique IDs, what we are
    going to do, stack traces, etc. The downside is that they are very expensive because
    we need to store and index them in a way that makes it easy to search and use
    them.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 日志是不可变的、带有时间戳的离散事件记录，记录了系统在时间上发生的具体事件。作为应用程序开发者，我们通常将数据转储到日志中，以便在特定时间点告知系统的内部状态在何处以及是什么。日志的好处在于它们最容易生成，并且比度量更加精细化。我们可以附加特定操作或一堆上下文信息，例如唯一标识符、我们将要执行的操作、堆栈跟踪等。不利之处在于它们非常昂贵，因为我们需要以易于搜索和使用的方式存储和索引它们。
- en: In gRPC applications, we can enable logging using interceptors. As we discussed
    in [Chapter 5](ch05.html#ch_05), we can attach a new logging interceptor on both
    the client side and server side and log request and response messages of each
    remote call.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在 gRPC 应用程序中，我们可以使用拦截器启用日志记录。正如我们在 [第五章](ch05.html#ch_05) 中讨论的那样，我们可以在客户端和服务器端都附加新的日志拦截器，并记录每次远程调用的请求和响应消息。
- en: Note
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The gRPC ecosystem provides a set of predefined logging interceptors for Go
    applications. This includes `grpc_ctxtags`, a library that adds a Tag map to context,
    with data populated from the request body; `grpc_zap`, integration of the [zap](https://oreil.ly/XMlIg)
    logging library into gRPC handlers; and `grpc_logrus`, integration of the [logrus](https://oreil.ly/oKJX5)
    logging library into gRPC handlers. For more information about these interceptors,
    check out the [gRPC Go Middleware repository](https://oreil.ly/8lNaH).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC 生态系统为 Go 应用程序提供了一组预定义的日志拦截器。其中包括 `grpc_ctxtags`，一个库，它将一个标签映射添加到上下文中，并从请求体中填充数据；`grpc_zap`，将
    [zap](https://oreil.ly/XMlIg) 日志库集成到 gRPC 处理程序中；以及 `grpc_logrus`，将 [logrus](https://oreil.ly/oKJX5)
    日志库集成到 gRPC 处理程序中。有关这些拦截器的更多信息，请查看 [gRPC Go Middleware 仓库](https://oreil.ly/8lNaH)。
- en: Once you add logs in your gRPC application, they’ll print in either the console
    or logfile, depending on how you configure logging. How to configure logging depends
    on the logging framework you used.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在您的 gRPC 应用程序中添加了日志，它们将根据您的日志配置打印到控制台或日志文件中。如何配置日志取决于您使用的日志框架。
- en: 'We’ve now discussed two pillars of observability: metrics and logs. These are
    sufficient for understanding the performance and behavior of individual systems,
    but they aren’t sufficient to understand the lifetime of a request that traverses
    multiple systems. Distributed tracing is a technique that brings visibility of
    the lifetime of a request across several systems.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经讨论了观察性的两个支柱：度量和日志。这些足以理解单个系统的性能和行为，但不足以理解跨多个系统遍历的请求的生命周期。分布式追踪是一种技术，它提供了对请求生命周期在多个系统中的可见性。
- en: Tracing
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 追踪
- en: A trace is a representation of a series of related events that constructs the
    end-to-end request flow through a distributed system. As we discussed in the section
    [“Using gRPC for Microservices Communication”](ch03.html#using_grpc_for_microservices_communication),
    in a real-world scenario we have multiple microservices serving different and
    specific business capabilities. Therefore, a request starting from the client
    is normally going through a number of services and different systems before the
    response going back to the client. All these intermediate events are part of the
    request flow. With tracing, we gain visibility into both the path traversed by
    a request as well as the structure of a request.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪是一系列相关事件的表示，构建了通过分布式系统的端到端请求流。正如我们在章节 [“使用 gRPC 进行微服务通信”](ch03.html#using_grpc_for_microservices_communication)
    中讨论的，在现实场景中，我们有多个微服务提供不同和特定的业务能力。因此，从客户端发起的请求通常会经过多个服务和不同的系统，然后响应返回给客户端。所有这些中间事件都是请求流的一部分。通过跟踪，我们可以看到请求所经过的路径以及请求的结构。
- en: In tracing, a trace is a tree of *spans*, which are the primary building blocks
    of distributed tracing. The span contains the metadata about the task, the latency
    (the time spent to complete the task), and other related attributes of the task.
    A trace has its own ID called TraceID and it is a unique byte sequence. This traceID
    groups and distinguishes spans from each other. Let’s try to enable tracing in
    our gRPC application.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在跟踪中，一个跟踪是 *跨度* 的树形结构，跨度是分布式跟踪的主要构建块。跨度包含任务的元数据、延迟（完成任务所花费的时间）和任务的其他相关属性。一个跟踪有自己的
    ID，称为 TraceID，它是一个唯一的字节序列。这个 TraceID 将不同的跨度分组和区分开来。让我们尝试在我们的 gRPC 应用程序中启用跟踪。
- en: Like metrics, the OpenCensus library provides support to enable tracing in gRPC
    applications. We will use OpenCensus to enable tracing in our Product Management
    application. As we said earlier, we can plug any supported exporters to export
    tracing data to different backends. We will use Jaeger for the distributed tracing
    sample.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于指标，OpenCensus 库支持在 gRPC 应用程序中启用跟踪。我们将使用 OpenCensus 在我们的产品管理应用程序中启用跟踪。正如前面所述，我们可以将任何支持的导出器插入到不同的后端以导出跟踪数据。我们将使用
    Jaeger 作为分布式跟踪样例的后端。
- en: By default, tracing is enabled in gRPC Go. So it only requires registering an
    exporter to start collecting traces with gRPC Go integration. Let’s initiate a
    Jaeger exporter in both client and server applications. [Example 7-13](#EX7-13)
    illustrates how we can initiate the OpenCensus Jaeger exporter using the library.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC Go 默认启用了跟踪功能。因此，只需注册一个导出器即可开始在 gRPC Go 集成中收集跟踪数据。让我们在客户端和服务器应用程序中启动一个 Jaeger
    导出器。示例 [7-13](#EX7-13) 演示了如何使用库初始化 OpenCensus Jaeger 导出器。
- en: Example 7-13\. Initialize OpenCensus Jaeger exporter
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-13\. 初始化 OpenCensus Jaeger 导出器
- en: '[PRE17]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO13-1)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO13-1)'
- en: Import the OpenTracing and Jaeger libraries.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 OpenTracing 和 Jaeger 库。
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO13-2)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO13-2)'
- en: Create the Jaeger exporter with the collector endpoint, service name, and agent
    endpoint.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Jaeger 导出器，并指定收集器端点、服务名称和代理端点。
- en: '[![3](assets/3.png)](#co_running_grpc_in_production_CO13-3)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_running_grpc_in_production_CO13-3)'
- en: Register the exporter with the OpenCensus tracer.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 将导出器注册到 OpenCensus 追踪器中。
- en: Once we register the exporter with the server, we can instrument the server
    by tracing. [Example 7-14](#EX7-14) illustrates how to instrument tracing in service
    method.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在服务器上注册了导出器，我们就可以通过跟踪来为服务器添加仪表。示例 [7-14](#EX7-14) 展示了如何在服务方法中添加仪表。
- en: Example 7-14\. Instrument gRPC service method
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-14\. 为 gRPC 服务方法添加仪表
- en: '[PRE18]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO14-1)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO14-1)'
- en: Start new span with span name and context.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 使用跨度名称和上下文启动新跨度。
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO14-2)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO14-2)'
- en: Stop the span when everything is done.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有操作完成时停止跨度。
- en: Similar to the gRPC server, we can instrument the client by tracing as shown
    in [Example 7-15](#EX7-15).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 gRPC 服务器，我们可以像示例 [7-15](#EX7-15) 中显示的那样通过跟踪来为客户端添加仪表。
- en: Example 7-15\. Instrument gRPC client
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-15\. 为 gRPC 客户端添加仪表
- en: '[PRE19]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO15-1)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO15-1)'
- en: Import the OpenTracing and Jaeger libraries.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 OpenTracing 和 Jaeger 库。
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO15-2)'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO15-2)'
- en: Call the `initTracing` function and initialize the Jaeger exporter instance
    and register with trace.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`initTracing`函数并初始化Jaeger导出器实例，并注册到追踪器。
- en: '[![3](assets/3.png)](#co_running_grpc_in_production_CO15-3)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_running_grpc_in_production_CO15-3)'
- en: Start new span with span name and context.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 使用span名称和上下文启动新的跨度。
- en: '[![4](assets/4.png)](#co_running_grpc_in_production_CO15-6)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_running_grpc_in_production_CO15-6)'
- en: Stop the span when everything is done.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 当一切都完成时停止跨度。
- en: '[![5](assets/5.png)](#co_running_grpc_in_production_CO15-4)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_running_grpc_in_production_CO15-4)'
- en: Invoke addProduct remote method by passing new product details.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递新产品详细信息调用addProduct远程方法。
- en: '[![6](assets/6.png)](#co_running_grpc_in_production_CO15-5)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_running_grpc_in_production_CO15-5)'
- en: Invoke getProduct remote method by passing productID.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递productID调用getProduct远程方法。
- en: Once we run the server and client, trace spans are published to the Jaeger agent
    for which a daemon process acts as a buffer to abstract out batch processing and
    routing from the clients. Once the Jaeger agent receives trace logs from the client,
    it forwards them to the collector. The collector processes the logs and stores
    them. From the Jaeger server, we can visualize tracing.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行了服务器和客户端，追踪跨度将被发布到Jaeger代理，其中一个守护进程用作缓冲，将批处理处理和路由从客户端抽象出来。一旦Jaeger代理接收到来自客户端的追踪日志，它将其转发到收集器。收集器处理日志并存储它们。通过Jaeger服务器，我们可以可视化追踪。
- en: From that, we are going to conclude the discussion of observability. Logs, metrics,
    and traces serve their own unique purpose, and it’s better to have all three pillars
    enabled in your system to gain maximum visibility of the internal state.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里我们将结束可观察性的讨论。日志、度量和跟踪各自具有其独特的目的，最好在系统中启用这三个支柱，以获取对内部状态的最大可见性。
- en: Once you have a gRPC-based observable application running in production, you
    can keep watching its state and easily find out whenever there is an issue or
    system outage. When you diagnose an issue in the system, it is important to find
    the solution, test it, and deploy it to production as soon as possible. To accomplish
    that goal, you need to have good debugging and troubleshooting mechanisms. Let’s
    look into the details of these mechanisms for gRPC applications.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您在生产环境中运行了基于gRPC的可观察应用程序，您可以随时观察其状态，并在出现问题或系统中断时轻松找到问题所在。在诊断系统问题时，找到解决方案、测试并尽快部署到生产环境非常重要。为了实现这个目标，您需要有良好的调试和故障排除机制。让我们深入了解gRPC应用程序的这些机制的细节。
- en: Debugging and Troubleshooting
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试和故障排除
- en: Debugging and troubleshooting is the process to find out the root cause of a
    problem and solve the issue that occurred in applications. In order to debug and
    troubleshoot the issue, we first need to reproduce the same issue in lower environments
    (referred to as dev or test environments). So we need a set of tools to generate
    similar kinds of request loads as the production environment.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 调试和故障排除是查找问题根本原因并解决应用程序中出现问题的过程。为了调试和排除问题，我们首先需要在较低的环境（称为开发或测试环境）中重现相同的问题。因此，我们需要一套工具来生成类似的请求负载，就像在生产环境中一样。
- en: This process is relatively harder in gRPC services than in the HTTP service,
    because tools need to support both encoding and decoding messages based on the
    service definition, and be able to support HTTP/2\. Common tools like curl or
    Postman, which are used to test HTTP services, cannot be used to test gRPC services.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在gRPC服务中相对较难，因为工具需要支持根据服务定义编码和解码消息，并且能够支持HTTP/2。常见的工具如curl或Postman，用于测试HTTP服务，不能用于测试gRPC服务。
- en: But there are a lot of interesting tools available for debugging and testing
    gRPC services. You can find a list of those tools in the [awesome gRPC repository](https://oreil.ly/Ki2aZ).
    It contains a great collection of resources available for gRPC. One of the most
    common ways of debugging gRPC applications is by using extra logging.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 但是有很多有趣的工具可用于调试和测试gRPC服务。您可以在[awesome gRPC repository](https://oreil.ly/Ki2aZ)中找到这些工具的列表。它包含了大量可用于gRPC的资源集合。调试gRPC应用程序的最常见方式之一是使用额外的日志记录。
- en: Enabling Extra Logging
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用额外的日志记录
- en: 'We can enable extra logs and traces to diagnose the problem of your gRPC application.
    In the gRPC Go application, we can enable extra logs by setting the following
    environment variables:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以启用额外的日志和跟踪来诊断您的gRPC应用程序问题。在gRPC Go应用程序中，我们可以通过设置以下环境变量来启用额外的日志：
- en: '[PRE20]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[![1](assets/1.png)](#co_running_grpc_in_production_CO16-1)'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_running_grpc_in_production_CO16-1)'
- en: '*Verbosity* means how many times any single info message should print every
    five minutes. The verbosity is set to 0 by default.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '*详细程度*意味着每五分钟任何单个信息消息应该打印多少次。默认情况下，详细程度设置为 0。'
- en: '[![2](assets/2.png)](#co_running_grpc_in_production_CO16-2)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_running_grpc_in_production_CO16-2)'
- en: Sets log severity level to `info`. All the informational messages will be printed.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 将日志严重级别设置为 `info`。所有信息性消息将被打印出来。
- en: 'In the gRPC Java application, there are no environment variables to control
    the log level. We can turn on extra logs by providing a *logging.properties* file
    with log-level changes. Let’s say we want to troubleshoot transport-level frames
    in our application. We can create a new *logging.properties* file in our application
    and set the lower log level to a specific Java package (netty transport package)
    as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在 gRPC Java 应用程序中，没有环境变量来控制日志级别。我们可以通过提供一个 *logging.properties* 文件来打开额外的日志，其中包含日志级别更改。假设我们想要在我们的应用程序中调试传输级别帧，我们可以在应用程序中创建一个新的
    *logging.properties* 文件，并将较低的日志级别设置为特定的 Java 包（netty transport package）如下：
- en: '[PRE21]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then start up the Java binary with the JVM flag:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用 JVM 标志启动 Java 二进制文件：
- en: '[PRE22]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Once we set the lower log level in our application, all the logs in which the
    level is equal or higher than the configured log level will print in the console/logfile.
    We can gain valuable insight into the state of the system by reading the logs.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在应用程序中设置了较低的日志级别，所有日志级别等于或高于配置的日志级别的日志将在控制台/日志文件中打印出来。通过阅读日志，我们可以深入了解系统的状态。
- en: With that, we have covered most of what you should know when running a gRPC
    application in production.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以上内容，我们已经覆盖了在生产环境中运行 gRPC 应用程序时应知道的大部分内容。
- en: Summary
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: Making production-ready gRPC applications requires us to focus on multiple aspects
    related to application development. We start by designing the service contract
    and generating code for the service or the client, then implementing our service’s
    business logic. Once we implement the service, we need to focus on the following
    to make the gRPC application production ready. *Testing* of gRPC server and client
    applications is essential.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 制作生产就绪的 gRPC 应用程序需要我们关注与应用程序开发相关的多个方面。我们首先通过设计服务契约并为服务或客户端生成代码来开始，然后实现我们服务的业务逻辑。一旦我们实现了服务，我们需要专注于以下几点，以使
    gRPC 应用程序达到生产就绪状态。*测试* gRPC 服务器和客户端应用程序是必不可少的。
- en: The *deployment* of gRPC applications follows the standard application development
    methodologies. For local and VM deployments, simply use the generated binaries
    of the server or client program. You can run gRPC applications as a Docker container,
    and find the sample standard Dockerfiles for deploying Go and Java applications
    on Docker. Running gRPC on Kubernetes is similar to standard Kubernetes deployment.
    When you run a gRPC application on Kubernetes, you use underlying features such
    as load balancing, high availability, ingress controllers,etc. Making gRPC applications
    observable is critical to using them in production, and gRPC application-level
    metrics are often used when gRPC applications operate in production.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC 应用程序的*部署*遵循标准的应用程序开发方法。对于本地和虚拟机部署，只需使用服务器或客户端程序的生成二进制文件。您可以将 gRPC 应用程序作为
    Docker 容器运行，并找到用于在 Docker 上部署 Go 和 Java 应用程序的标准 Dockerfile 示例。在 Kubernetes 上运行
    gRPC 类似于标准的 Kubernetes 部署。当您在 Kubernetes 上运行 gRPC 应用程序时，您会使用底层特性，如负载均衡、高可用性、入口控制器等。使
    gRPC 应用程序可观察对于在生产中使用它们至关重要，当 gRPC 应用程序在生产中运行时，通常使用 gRPC 应用程序级指标。
- en: In one of the most popular implementations for metrics support in gRPC, the
    gRPC-Prometheus library, we use an interceptor at the server and client side to
    collect metrics, while logging in gRPC is also enabled using an interceptor. For
    gRPC applications in production, you may need to troubleshoot or debug by enabling
    extra logging. In the next chapter, we’ll explore some of the gRPC ecosystem components
    that are useful in building gRPC applications.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在 gRPC 中最受欢迎的指标支持实现之一，即 gRPC-Prometheus 库中，我们在服务器和客户端端使用拦截器来收集指标，同时还使用拦截器启用
    gRPC 日志记录。对于生产中的 gRPC 应用程序，您可能需要通过启用额外的日志记录来进行故障排除或调试。在接下来的章节中，我们将探讨一些对于构建 gRPC
    应用程序很有用的 gRPC 生态系统组件。
